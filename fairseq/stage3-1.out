2023-05-20 11:58:29 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-05-20 11:58:31 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:50411
2023-05-20 11:58:31 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:50411
2023-05-20 11:58:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-05-20 11:58:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 0
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 2
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 7
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 1
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 3
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 5
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 6
2023-05-20 11:58:33 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 11:58:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 4
2023-05-20 11:58:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:50411', 'distributed_port': 50411, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'training/stage3', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage2/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=0, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage2/checkpoint_best.pt', save_dir='training/stage3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=0, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage2/checkpoint_best.pt', save_dir='training/stage3', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 100, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 2000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-20 11:58:39 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-05-20 11:58:49 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (1): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (2): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (3): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (4): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (5): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (6): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (7): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (8): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (9): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (10): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (11): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
    )
    (graph_embeddings): Linear(in_features=1024, out_features=64, bias=True)
    (graph_embeddings_inverse): Linear(in_features=1024, out_features=64, bias=True)
    (gamma_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-05-20 11:58:49 | INFO | fairseq_cli.train | task: GraphToSeq
2023-05-20 11:58:49 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-05-20 11:58:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-20 11:58:49 | INFO | fairseq_cli.train | num. shared model params: 559,173,888 (num. trained: 559,173,888)
2023-05-20 11:58:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-05-20 11:58:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-05-20 11:58:49 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-05-20 11:59:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-05-20 11:59:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-05-20 11:59:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-05-20 11:59:16 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-20 11:59:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 11:59:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-20 11:59:17 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-05-20 11:59:17 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-05-20 11:59:17 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage2/checkpoint_best.pt
2023-05-20 12:00:00 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage2/checkpoint_best.pt (epoch 53 @ 0 updates)
2023-05-20 12:00:00 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-05-20 12:00:00 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-05-20 12:00:00 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 12:00:00 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 12:00:00 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 12:00:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:00:04 | INFO | fairseq.trainer | begin training epoch 1
2023-05-20 12:00:04 | INFO | fairseq_cli.train | Start iterating over samples
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-05-20 12:00:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-05-20 12:00:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-05-20 12:00:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-05-20 12:00:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-05-20 12:01:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-05-20 12:01:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-20 12:01:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 12:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 12:01:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 12:03:10 | INFO | train_inner | epoch 001:     29 / 65 loss=4.965, nll_loss=2.917, ppl=7.55, wps=2342.6, ups=0.19, wpb=12117.9, bsz=537.3, num_updates=20, lr=6e-06, gnorm=35.428, clip=100, loss_scale=0.25, train_wall=183, gb_free=8.9, wall=233
2023-05-20 12:04:45 | INFO | train_inner | epoch 001:     49 / 65 loss=4.078, nll_loss=2.11, ppl=4.32, wps=2558.9, ups=0.21, wpb=12208.8, bsz=597.7, num_updates=40, lr=1.2e-05, gnorm=8.236, clip=100, loss_scale=0.25, train_wall=95, gb_free=9.5, wall=328
2023-05-20 12:05:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:05:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:06:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.124 | nll_loss 2.168 | ppl 4.49 | bleu 28.13 | wps 1483.7 | wpb 2785 | bsz 105.2 | num_updates 56
2023-05-20 12:06:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 56 updates
2023-05-20 12:06:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint1.pt
2023-05-20 12:06:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint1.pt
2023-05-20 12:06:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint1.pt (epoch 1 @ 56 updates, score 4.124) (writing took 33.95310062170029 seconds)
2023-05-20 12:06:55 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-20 12:06:55 | INFO | train | epoch 001 | loss 4.347 | nll_loss 2.354 | ppl 5.11 | wps 2042.1 | ups 0.17 | wpb 12018.9 | bsz 560.2 | num_updates 56 | lr 1.68e-05 | gnorm 17.075 | clip 100 | loss_scale 0.25 | train_wall 351 | gb_free 9.9 | wall 458
2023-05-20 12:06:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:06:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:06:55 | INFO | fairseq.trainer | begin training epoch 2
2023-05-20 12:06:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:07:13 | INFO | train_inner | epoch 002:      4 / 65 loss=3.887, nll_loss=1.937, ppl=3.83, wps=1589.5, ups=0.13, wpb=11782.9, bsz=551.6, num_updates=60, lr=1.8e-05, gnorm=5.223, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.7, wall=476
2023-05-20 12:08:47 | INFO | train_inner | epoch 002:     24 / 65 loss=3.769, nll_loss=1.831, ppl=3.56, wps=2614.1, ups=0.21, wpb=12299.8, bsz=575.6, num_updates=80, lr=2.4e-05, gnorm=3.328, clip=100, loss_scale=0.25, train_wall=94, gb_free=9.6, wall=570
2023-05-20 12:10:21 | INFO | train_inner | epoch 002:     44 / 65 loss=3.709, nll_loss=1.783, ppl=3.44, wps=2584.3, ups=0.21, wpb=12115.6, bsz=559.5, num_updates=100, lr=3e-05, gnorm=2.906, clip=100, loss_scale=0.25, train_wall=94, gb_free=10.1, wall=664
2023-05-20 12:11:51 | INFO | train_inner | epoch 002:     64 / 65 loss=3.638, nll_loss=1.718, ppl=3.29, wps=2710.7, ups=0.22, wpb=12170.2, bsz=562.4, num_updates=120, lr=2.96842e-05, gnorm=3.276, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.9, wall=754
2023-05-20 12:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:11:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:12:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.729 | nll_loss 1.753 | ppl 3.37 | bleu 34.26 | wps 1698.1 | wpb 2785 | bsz 105.2 | num_updates 121 | best_loss 3.729
2023-05-20 12:12:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 121 updates
2023-05-20 12:12:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint2.pt
2023-05-20 12:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint2.pt
2023-05-20 12:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint2.pt (epoch 2 @ 121 updates, score 3.729) (writing took 72.47687429562211 seconds)
2023-05-20 12:13:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-20 12:13:28 | INFO | train | epoch 002 | loss 3.714 | nll_loss 1.785 | ppl 3.45 | wps 1997.4 | ups 0.17 | wpb 12058.8 | bsz 559.8 | num_updates 121 | lr 2.96684e-05 | gnorm 3.339 | clip 100 | loss_scale 0.25 | train_wall 299 | gb_free 9.7 | wall 850
2023-05-20 12:13:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:13:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:13:28 | INFO | fairseq.trainer | begin training epoch 3
2023-05-20 12:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:15:00 | INFO | train_inner | epoch 003:     19 / 65 loss=3.551, nll_loss=1.623, ppl=3.08, wps=1249.5, ups=0.11, wpb=11805.3, bsz=546.1, num_updates=140, lr=2.93684e-05, gnorm=8.058, clip=100, loss_scale=0.25, train_wall=96, gb_free=9.7, wall=943
2023-05-20 12:15:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 12:16:41 | INFO | train_inner | epoch 003:     40 / 65 loss=3.773, nll_loss=1.866, ppl=3.65, wps=2383, ups=0.2, wpb=11983.6, bsz=536.7, num_updates=160, lr=2.90526e-05, gnorm=32.519, clip=100, loss_scale=0.125, train_wall=100, gb_free=7.6, wall=1044
2023-05-20 12:18:12 | INFO | train_inner | epoch 003:     60 / 65 loss=3.76, nll_loss=1.853, ppl=3.61, wps=2688.8, ups=0.22, wpb=12314.8, bsz=568, num_updates=180, lr=2.87368e-05, gnorm=18.893, clip=100, loss_scale=0.125, train_wall=91, gb_free=9.5, wall=1135
2023-05-20 12:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:18:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:19:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.176 | nll_loss 2.253 | ppl 4.77 | bleu 27.16 | wps 1114 | wpb 2785 | bsz 105.2 | num_updates 185 | best_loss 3.729
2023-05-20 12:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 185 updates
2023-05-20 12:19:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint3.pt
2023-05-20 12:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint3.pt
2023-05-20 12:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint3.pt (epoch 3 @ 185 updates, score 4.176) (writing took 21.649277795106173 seconds)
2023-05-20 12:19:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-20 12:19:25 | INFO | train | epoch 003 | loss 3.7 | nll_loss 1.787 | ppl 3.45 | wps 2157.8 | ups 0.18 | wpb 12058.5 | bsz 557.4 | num_updates 185 | lr 2.86579e-05 | gnorm 21.666 | clip 100 | loss_scale 0.125 | train_wall 305 | gb_free 9.9 | wall 1208
2023-05-20 12:19:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:19:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:19:25 | INFO | fairseq.trainer | begin training epoch 4
2023-05-20 12:19:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:20:34 | INFO | train_inner | epoch 004:     15 / 65 loss=3.667, nll_loss=1.751, ppl=3.37, wps=1642.9, ups=0.14, wpb=11679.7, bsz=538.1, num_updates=200, lr=2.84211e-05, gnorm=25.739, clip=100, loss_scale=0.125, train_wall=90, gb_free=10.1, wall=1277
2023-05-20 12:22:09 | INFO | train_inner | epoch 004:     35 / 65 loss=3.652, nll_loss=1.738, ppl=3.34, wps=2582.7, ups=0.21, wpb=12261.1, bsz=567.1, num_updates=220, lr=2.81053e-05, gnorm=16.858, clip=100, loss_scale=0.125, train_wall=95, gb_free=9.8, wall=1372
2023-05-20 12:23:42 | INFO | train_inner | epoch 004:     55 / 65 loss=3.578, nll_loss=1.664, ppl=3.17, wps=2630.7, ups=0.21, wpb=12252.2, bsz=580.6, num_updates=240, lr=2.77895e-05, gnorm=21.625, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.7, wall=1465
2023-05-20 12:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:24:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:24:43 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.742 | nll_loss 1.821 | ppl 3.53 | bleu 34.58 | wps 1873.4 | wpb 2785 | bsz 105.2 | num_updates 250 | best_loss 3.729
2023-05-20 12:24:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 250 updates
2023-05-20 12:24:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint4.pt
2023-05-20 12:24:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint4.pt
2023-05-20 12:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint4.pt (epoch 4 @ 250 updates, score 3.742) (writing took 29.31921461597085 seconds)
2023-05-20 12:25:12 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-20 12:25:12 | INFO | train | epoch 004 | loss 3.606 | nll_loss 1.69 | ppl 3.23 | wps 2259.1 | ups 0.19 | wpb 12048.9 | bsz 558.9 | num_updates 250 | lr 2.76316e-05 | gnorm 18.069 | clip 100 | loss_scale 0.125 | train_wall 298 | gb_free 9.9 | wall 1555
2023-05-20 12:25:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:25:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:25:12 | INFO | fairseq.trainer | begin training epoch 5
2023-05-20 12:25:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:25:58 | INFO | train_inner | epoch 005:     10 / 65 loss=3.465, nll_loss=1.542, ppl=2.91, wps=1719.7, ups=0.15, wpb=11686.6, bsz=538.6, num_updates=260, lr=2.74737e-05, gnorm=5.912, clip=100, loss_scale=0.125, train_wall=88, gb_free=9.3, wall=1601
2023-05-20 12:27:32 | INFO | train_inner | epoch 005:     30 / 65 loss=3.427, nll_loss=1.496, ppl=2.82, wps=2559.4, ups=0.21, wpb=12005, bsz=545, num_updates=280, lr=2.71579e-05, gnorm=6.077, clip=100, loss_scale=0.125, train_wall=94, gb_free=9.6, wall=1695
2023-05-20 12:29:03 | INFO | train_inner | epoch 005:     50 / 65 loss=3.443, nll_loss=1.542, ppl=2.91, wps=2713.8, ups=0.22, wpb=12378.1, bsz=600.2, num_updates=300, lr=2.68421e-05, gnorm=6.302, clip=100, loss_scale=0.125, train_wall=91, gb_free=9, wall=1786
2023-05-20 12:30:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:30:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:30:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.587 | nll_loss 1.663 | ppl 3.17 | bleu 37.01 | wps 1784.8 | wpb 2785 | bsz 105.2 | num_updates 315 | best_loss 3.587
2023-05-20 12:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 315 updates
2023-05-20 12:30:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint5.pt
2023-05-20 12:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint5.pt
2023-05-20 12:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint5.pt (epoch 5 @ 315 updates, score 3.587) (writing took 83.463398065418 seconds)
2023-05-20 12:31:57 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-20 12:31:57 | INFO | train | epoch 005 | loss 3.427 | nll_loss 1.51 | ppl 2.85 | wps 1931.8 | ups 0.16 | wpb 12050.9 | bsz 558.7 | num_updates 315 | lr 2.66053e-05 | gnorm 4.831 | clip 100 | loss_scale 0.125 | train_wall 302 | gb_free 10 | wall 1960
2023-05-20 12:31:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:31:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:31:57 | INFO | fairseq.trainer | begin training epoch 6
2023-05-20 12:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:32:22 | INFO | train_inner | epoch 006:      5 / 65 loss=3.376, nll_loss=1.454, ppl=2.74, wps=1182.9, ups=0.1, wpb=11722.9, bsz=535.1, num_updates=320, lr=2.65263e-05, gnorm=2.813, clip=100, loss_scale=0.125, train_wall=95, gb_free=9.8, wall=1985
2023-05-20 12:33:55 | INFO | train_inner | epoch 006:     25 / 65 loss=3.307, nll_loss=1.384, ppl=2.61, wps=2621.4, ups=0.21, wpb=12231.2, bsz=581.1, num_updates=340, lr=2.62105e-05, gnorm=2.089, clip=100, loss_scale=0.125, train_wall=93, gb_free=8.7, wall=2078
2023-05-20 12:35:29 | INFO | train_inner | epoch 006:     45 / 65 loss=3.299, nll_loss=1.379, ppl=2.6, wps=2571.9, ups=0.21, wpb=12127.8, bsz=541.9, num_updates=360, lr=2.58947e-05, gnorm=2.426, clip=100, loss_scale=0.125, train_wall=94, gb_free=9.7, wall=2172
2023-05-20 12:37:02 | INFO | train_inner | epoch 006:     65 / 65 loss=3.289, nll_loss=1.371, ppl=2.59, wps=2550.2, ups=0.22, wpb=11789.9, bsz=561.1, num_updates=380, lr=2.55789e-05, gnorm=3.414, clip=100, loss_scale=0.125, train_wall=92, gb_free=10, wall=2265
2023-05-20 12:37:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:37:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:37:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.486 | nll_loss 1.554 | ppl 2.94 | bleu 38.42 | wps 1786.9 | wpb 2785 | bsz 105.2 | num_updates 380 | best_loss 3.486
2023-05-20 12:37:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 380 updates
2023-05-20 12:37:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint6.pt
2023-05-20 12:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint6.pt
2023-05-20 12:38:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint6.pt (epoch 6 @ 380 updates, score 3.486) (writing took 84.26895944029093 seconds)
2023-05-20 12:38:45 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-20 12:38:46 | INFO | train | epoch 006 | loss 3.299 | nll_loss 1.377 | ppl 2.6 | wps 1918.7 | ups 0.16 | wpb 12054.7 | bsz 559.9 | num_updates 380 | lr 2.55789e-05 | gnorm 2.678 | clip 100 | loss_scale 0.125 | train_wall 304 | gb_free 10 | wall 2369
2023-05-20 12:38:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:38:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:38:46 | INFO | fairseq.trainer | begin training epoch 7
2023-05-20 12:38:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:40:18 | INFO | train_inner | epoch 007:     20 / 65 loss=3.191, nll_loss=1.261, ppl=2.4, wps=1238.6, ups=0.1, wpb=12186.5, bsz=554.4, num_updates=400, lr=2.52632e-05, gnorm=3.297, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.9, wall=2461
2023-05-20 12:41:52 | INFO | train_inner | epoch 007:     40 / 65 loss=3.194, nll_loss=1.267, ppl=2.41, wps=2642.3, ups=0.21, wpb=12299.5, bsz=578.6, num_updates=420, lr=2.49474e-05, gnorm=2.965, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.8, wall=2555
2023-05-20 12:43:25 | INFO | train_inner | epoch 007:     60 / 65 loss=3.172, nll_loss=1.247, ppl=2.37, wps=2586, ups=0.21, wpb=12067.7, bsz=567.1, num_updates=440, lr=2.46316e-05, gnorm=2.022, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.7, wall=2648
2023-05-20 12:43:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:43:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:44:03 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.453 | nll_loss 1.534 | ppl 2.89 | bleu 40 | wps 2044 | wpb 2785 | bsz 105.2 | num_updates 445 | best_loss 3.453
2023-05-20 12:44:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 445 updates
2023-05-20 12:44:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint7.pt
2023-05-20 12:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint7.pt
2023-05-20 12:44:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint7.pt (epoch 7 @ 445 updates, score 3.453) (writing took 56.386653792113066 seconds)
2023-05-20 12:45:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-20 12:45:01 | INFO | train | epoch 007 | loss 3.185 | nll_loss 1.257 | ppl 2.39 | wps 2086.5 | ups 0.17 | wpb 12050.8 | bsz 559.8 | num_updates 445 | lr 2.45526e-05 | gnorm 3.036 | clip 100 | loss_scale 0.125 | train_wall 299 | gb_free 9.4 | wall 2744
2023-05-20 12:45:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:45:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:45:01 | INFO | fairseq.trainer | begin training epoch 8
2023-05-20 12:45:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:46:11 | INFO | train_inner | epoch 008:     15 / 65 loss=3.117, nll_loss=1.182, ppl=2.27, wps=1422.1, ups=0.12, wpb=11812, bsz=536.4, num_updates=460, lr=2.43158e-05, gnorm=3.683, clip=100, loss_scale=0.125, train_wall=90, gb_free=9.4, wall=2814
2023-05-20 12:47:46 | INFO | train_inner | epoch 008:     35 / 65 loss=3.108, nll_loss=1.18, ppl=2.27, wps=2558.5, ups=0.21, wpb=12171.6, bsz=557.6, num_updates=480, lr=2.4e-05, gnorm=3.179, clip=100, loss_scale=0.125, train_wall=95, gb_free=9.2, wall=2909
2023-05-20 12:49:17 | INFO | train_inner | epoch 008:     55 / 65 loss=3.092, nll_loss=1.162, ppl=2.24, wps=2660.7, ups=0.22, wpb=12108.9, bsz=579.9, num_updates=500, lr=2.36842e-05, gnorm=2.336, clip=100, loss_scale=0.125, train_wall=91, gb_free=9.4, wall=3000
2023-05-20 12:50:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:50:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:50:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.426 | nll_loss 1.498 | ppl 2.82 | bleu 40.82 | wps 1820.2 | wpb 2785 | bsz 105.2 | num_updates 510 | best_loss 3.426
2023-05-20 12:50:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 510 updates
2023-05-20 12:50:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint8.pt
2023-05-20 12:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint8.pt
2023-05-20 12:51:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint8.pt (epoch 8 @ 510 updates, score 3.426) (writing took 83.44906559959054 seconds)
2023-05-20 12:51:45 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-20 12:51:45 | INFO | train | epoch 008 | loss 3.103 | nll_loss 1.172 | ppl 2.25 | wps 1939.8 | ups 0.16 | wpb 12057.1 | bsz 559.6 | num_updates 510 | lr 2.35263e-05 | gnorm 2.816 | clip 100 | loss_scale 0.125 | train_wall 301 | gb_free 9.4 | wall 3148
2023-05-20 12:51:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:51:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:51:45 | INFO | fairseq.trainer | begin training epoch 9
2023-05-20 12:51:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:52:30 | INFO | train_inner | epoch 009:     10 / 65 loss=3.062, nll_loss=1.127, ppl=2.18, wps=1223.8, ups=0.1, wpb=11824.7, bsz=541.6, num_updates=520, lr=2.33684e-05, gnorm=2.318, clip=100, loss_scale=0.125, train_wall=90, gb_free=10.1, wall=3193
2023-05-20 12:54:06 | INFO | train_inner | epoch 009:     30 / 65 loss=3.037, nll_loss=1.102, ppl=2.15, wps=2533.9, ups=0.21, wpb=12058.4, bsz=553.1, num_updates=540, lr=2.30526e-05, gnorm=2.528, clip=100, loss_scale=0.125, train_wall=95, gb_free=9.4, wall=3289
2023-05-20 12:55:39 | INFO | train_inner | epoch 009:     50 / 65 loss=3.032, nll_loss=1.099, ppl=2.14, wps=2575.2, ups=0.21, wpb=12064.5, bsz=558.4, num_updates=560, lr=2.27368e-05, gnorm=1.912, clip=100, loss_scale=0.125, train_wall=94, gb_free=10, wall=3382
2023-05-20 12:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 12:56:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:57:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.39 | nll_loss 1.466 | ppl 2.76 | bleu 42.03 | wps 1884.4 | wpb 2785 | bsz 105.2 | num_updates 575 | best_loss 3.39
2023-05-20 12:57:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 575 updates
2023-05-20 12:57:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint9.pt
2023-05-20 12:57:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint9.pt
2023-05-20 12:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint9.pt (epoch 9 @ 575 updates, score 3.39) (writing took 89.50167663767934 seconds)
2023-05-20 12:58:35 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-20 12:58:35 | INFO | train | epoch 009 | loss 3.033 | nll_loss 1.1 | ppl 2.14 | wps 1911.3 | ups 0.16 | wpb 12052 | bsz 558.8 | num_updates 575 | lr 2.25e-05 | gnorm 3.266 | clip 100 | loss_scale 0.125 | train_wall 301 | gb_free 9.8 | wall 3558
2023-05-20 12:58:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 12:58:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 12:58:35 | INFO | fairseq.trainer | begin training epoch 10
2023-05-20 12:58:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 12:59:00 | INFO | train_inner | epoch 010:      5 / 65 loss=3.019, nll_loss=1.087, ppl=2.12, wps=1196.8, ups=0.1, wpb=11988.1, bsz=580.1, num_updates=580, lr=2.24211e-05, gnorm=7.049, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.7, wall=3583
2023-05-20 13:00:32 | INFO | train_inner | epoch 010:     25 / 65 loss=2.945, nll_loss=1.004, ppl=2.01, wps=2639.9, ups=0.22, wpb=12217.2, bsz=537.1, num_updates=600, lr=2.21053e-05, gnorm=1.67, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.8, wall=3675
2023-05-20 13:02:04 | INFO | train_inner | epoch 010:     45 / 65 loss=2.975, nll_loss=1.039, ppl=2.05, wps=2639, ups=0.22, wpb=12140.4, bsz=547, num_updates=620, lr=2.17895e-05, gnorm=3.589, clip=100, loss_scale=0.125, train_wall=92, gb_free=10, wall=3767
2023-05-20 13:03:37 | INFO | train_inner | epoch 010:     65 / 65 loss=3, nll_loss=1.064, ppl=2.09, wps=2526.1, ups=0.21, wpb=11759.1, bsz=581.3, num_updates=640, lr=2.14737e-05, gnorm=2.405, clip=100, loss_scale=0.125, train_wall=93, gb_free=10.1, wall=3860
2023-05-20 13:03:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:03:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:03:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.395 | nll_loss 1.477 | ppl 2.78 | bleu 42.09 | wps 1914.8 | wpb 2785 | bsz 105.2 | num_updates 640 | best_loss 3.39
2023-05-20 13:03:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 640 updates
2023-05-20 13:03:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint10.pt
2023-05-20 13:04:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint10.pt
2023-05-20 13:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint10.pt (epoch 10 @ 640 updates, score 3.395) (writing took 59.903447825461626 seconds)
2023-05-20 13:04:55 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-20 13:04:55 | INFO | train | epoch 010 | loss 2.971 | nll_loss 1.033 | ppl 2.05 | wps 2059.9 | ups 0.17 | wpb 12055 | bsz 559.5 | num_updates 640 | lr 2.14737e-05 | gnorm 2.868 | clip 100 | loss_scale 0.125 | train_wall 302 | gb_free 10.1 | wall 3938
2023-05-20 13:04:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:04:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:04:55 | INFO | fairseq.trainer | begin training epoch 11
2023-05-20 13:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:06:28 | INFO | train_inner | epoch 011:     20 / 65 loss=2.925, nll_loss=0.982, ppl=1.98, wps=1430.7, ups=0.12, wpb=12211.6, bsz=555.8, num_updates=660, lr=2.11579e-05, gnorm=2.968, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.8, wall=4031
2023-05-20 13:08:02 | INFO | train_inner | epoch 011:     40 / 65 loss=2.915, nll_loss=0.974, ppl=1.96, wps=2602.4, ups=0.21, wpb=12172.7, bsz=564, num_updates=680, lr=2.08421e-05, gnorm=2.602, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.7, wall=4125
2023-05-20 13:09:35 | INFO | train_inner | epoch 011:     60 / 65 loss=2.928, nll_loss=0.987, ppl=1.98, wps=2614.7, ups=0.22, wpb=12160, bsz=578.4, num_updates=700, lr=2.05263e-05, gnorm=3.055, clip=100, loss_scale=0.25, train_wall=93, gb_free=8.9, wall=4218
2023-05-20 13:09:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:09:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:10:15 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.395 | nll_loss 1.487 | ppl 2.8 | bleu 42.46 | wps 1829.4 | wpb 2785 | bsz 105.2 | num_updates 705 | best_loss 3.39
2023-05-20 13:10:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 705 updates
2023-05-20 13:10:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint11.pt
2023-05-20 13:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint11.pt
2023-05-20 13:11:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint11.pt (epoch 11 @ 705 updates, score 3.395) (writing took 50.951883248984814 seconds)
2023-05-20 13:11:06 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-20 13:11:06 | INFO | train | epoch 011 | loss 2.922 | nll_loss 0.98 | ppl 1.97 | wps 2111.6 | ups 0.18 | wpb 12043.3 | bsz 557.6 | num_updates 705 | lr 2.04474e-05 | gnorm 3.275 | clip 100 | loss_scale 0.25 | train_wall 300 | gb_free 9.7 | wall 4309
2023-05-20 13:11:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:11:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:11:06 | INFO | fairseq.trainer | begin training epoch 12
2023-05-20 13:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:12:15 | INFO | train_inner | epoch 012:     15 / 65 loss=2.876, nll_loss=0.929, ppl=1.9, wps=1484.3, ups=0.12, wpb=11881.2, bsz=559.9, num_updates=720, lr=2.02105e-05, gnorm=6.886, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.5, wall=4378
2023-05-20 13:13:50 | INFO | train_inner | epoch 012:     35 / 65 loss=2.872, nll_loss=0.927, ppl=1.9, wps=2552.9, ups=0.21, wpb=12110.2, bsz=558.9, num_updates=740, lr=1.98947e-05, gnorm=3.467, clip=100, loss_scale=0.25, train_wall=95, gb_free=9.9, wall=4473
2023-05-20 13:15:22 | INFO | train_inner | epoch 012:     55 / 65 loss=2.883, nll_loss=0.939, ppl=1.92, wps=2626.1, ups=0.22, wpb=12142.2, bsz=560, num_updates=760, lr=1.95789e-05, gnorm=3.005, clip=100, loss_scale=0.25, train_wall=92, gb_free=8.1, wall=4565
2023-05-20 13:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:16:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:16:22 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.401 | nll_loss 1.483 | ppl 2.79 | bleu 42.62 | wps 1926.3 | wpb 2785 | bsz 105.2 | num_updates 770 | best_loss 3.39
2023-05-20 13:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 770 updates
2023-05-20 13:16:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint12.pt
2023-05-20 13:16:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint12.pt
2023-05-20 13:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint12.pt (epoch 12 @ 770 updates, score 3.401) (writing took 39.66028096526861 seconds)
2023-05-20 13:17:02 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-20 13:17:02 | INFO | train | epoch 012 | loss 2.872 | nll_loss 0.926 | ppl 1.9 | wps 2200 | ups 0.18 | wpb 12044.3 | bsz 556.3 | num_updates 770 | lr 1.94211e-05 | gnorm 4.771 | clip 100 | loss_scale 0.25 | train_wall 297 | gb_free 9.6 | wall 4665
2023-05-20 13:17:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:17:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:17:02 | INFO | fairseq.trainer | begin training epoch 13
2023-05-20 13:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:17:50 | INFO | train_inner | epoch 013:     10 / 65 loss=2.85, nll_loss=0.903, ppl=1.87, wps=1593.9, ups=0.14, wpb=11787.8, bsz=529.9, num_updates=780, lr=1.92632e-05, gnorm=6.253, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.6, wall=4713
2023-05-20 13:18:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 13:19:26 | INFO | train_inner | epoch 013:     31 / 65 loss=2.826, nll_loss=0.874, ppl=1.83, wps=2471.7, ups=0.21, wpb=11867.5, bsz=523.6, num_updates=800, lr=1.89474e-05, gnorm=9.535, clip=100, loss_scale=0.125, train_wall=96, gb_free=9.7, wall=4809
2023-05-20 13:21:00 | INFO | train_inner | epoch 013:     51 / 65 loss=2.854, nll_loss=0.907, ppl=1.87, wps=2589, ups=0.21, wpb=12232.7, bsz=581.4, num_updates=820, lr=1.86316e-05, gnorm=3.643, clip=100, loss_scale=0.125, train_wall=94, gb_free=9.9, wall=4903
2023-05-20 13:22:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:22:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:22:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.402 | nll_loss 1.495 | ppl 2.82 | bleu 42.82 | wps 1928.9 | wpb 2785 | bsz 105.2 | num_updates 834 | best_loss 3.39
2023-05-20 13:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 834 updates
2023-05-20 13:22:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint13.pt
2023-05-20 13:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint13.pt
2023-05-20 13:23:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint13.pt (epoch 13 @ 834 updates, score 3.402) (writing took 42.408700563013554 seconds)
2023-05-20 13:23:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-20 13:23:02 | INFO | train | epoch 013 | loss 2.842 | nll_loss 0.893 | ppl 1.86 | wps 2138 | ups 0.18 | wpb 12026.2 | bsz 553.9 | num_updates 834 | lr 1.84105e-05 | gnorm 5.496 | clip 100 | loss_scale 0.125 | train_wall 299 | gb_free 10 | wall 5025
2023-05-20 13:23:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:23:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:23:02 | INFO | fairseq.trainer | begin training epoch 14
2023-05-20 13:23:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:23:30 | INFO | train_inner | epoch 014:      6 / 65 loss=2.83, nll_loss=0.882, ppl=1.84, wps=1587.1, ups=0.13, wpb=11829.8, bsz=532.8, num_updates=840, lr=1.83158e-05, gnorm=3.512, clip=100, loss_scale=0.125, train_wall=88, gb_free=9.6, wall=5053
2023-05-20 13:25:00 | INFO | train_inner | epoch 014:     26 / 65 loss=2.839, nll_loss=0.888, ppl=1.85, wps=2727.5, ups=0.22, wpb=12373.2, bsz=578.8, num_updates=860, lr=1.8e-05, gnorm=5.28, clip=100, loss_scale=0.125, train_wall=91, gb_free=9.4, wall=5143
2023-05-20 13:26:35 | INFO | train_inner | epoch 014:     46 / 65 loss=2.853, nll_loss=0.903, ppl=1.87, wps=2552.2, ups=0.21, wpb=12087.2, bsz=549.8, num_updates=880, lr=1.76842e-05, gnorm=3.181, clip=100, loss_scale=0.125, train_wall=95, gb_free=9.5, wall=5238
2023-05-20 13:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:28:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:28:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.382 | nll_loss 1.47 | ppl 2.77 | bleu 43.09 | wps 1890.2 | wpb 2785 | bsz 105.2 | num_updates 899 | best_loss 3.382
2023-05-20 13:28:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 899 updates
2023-05-20 13:28:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint14.pt
2023-05-20 13:28:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint14.pt
2023-05-20 13:29:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint14.pt (epoch 14 @ 899 updates, score 3.382) (writing took 79.63805541023612 seconds)
2023-05-20 13:29:39 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-20 13:29:39 | INFO | train | epoch 014 | loss 2.841 | nll_loss 0.891 | ppl 1.85 | wps 1971.9 | ups 0.16 | wpb 12050.6 | bsz 558 | num_updates 899 | lr 1.73842e-05 | gnorm 4.001 | clip 100 | loss_scale 0.125 | train_wall 299 | gb_free 9.6 | wall 5422
2023-05-20 13:29:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:29:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:29:39 | INFO | fairseq.trainer | begin training epoch 15
2023-05-20 13:29:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:29:43 | INFO | train_inner | epoch 015:      1 / 65 loss=2.838, nll_loss=0.89, ppl=1.85, wps=1240.5, ups=0.11, wpb=11681.8, bsz=565, num_updates=900, lr=1.73684e-05, gnorm=3.425, clip=100, loss_scale=0.125, train_wall=90, gb_free=9.6, wall=5426
2023-05-20 13:31:19 | INFO | train_inner | epoch 015:     21 / 65 loss=2.797, nll_loss=0.841, ppl=1.79, wps=2540.3, ups=0.21, wpb=12105.9, bsz=552.6, num_updates=920, lr=1.70526e-05, gnorm=1.959, clip=100, loss_scale=0.125, train_wall=95, gb_free=8.9, wall=5522
2023-05-20 13:32:52 | INFO | train_inner | epoch 015:     41 / 65 loss=2.81, nll_loss=0.861, ppl=1.82, wps=2634, ups=0.21, wpb=12319.5, bsz=581.9, num_updates=940, lr=1.67368e-05, gnorm=1.968, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.9, wall=5615
2023-05-20 13:34:25 | INFO | train_inner | epoch 015:     61 / 65 loss=2.795, nll_loss=0.844, ppl=1.8, wps=2639.4, ups=0.22, wpb=12205.2, bsz=556.6, num_updates=960, lr=1.64211e-05, gnorm=2.57, clip=100, loss_scale=0.125, train_wall=92, gb_free=10, wall=5708
2023-05-20 13:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:34:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:34:58 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.386 | nll_loss 1.479 | ppl 2.79 | bleu 42.97 | wps 1890.7 | wpb 2785 | bsz 105.2 | num_updates 964 | best_loss 3.382
2023-05-20 13:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 964 updates
2023-05-20 13:34:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint15.pt
2023-05-20 13:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint15.pt
2023-05-20 13:35:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint15.pt (epoch 15 @ 964 updates, score 3.386) (writing took 36.42784018069506 seconds)
2023-05-20 13:35:35 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-20 13:35:35 | INFO | train | epoch 015 | loss 2.799 | nll_loss 0.847 | ppl 1.8 | wps 2206.1 | ups 0.18 | wpb 12062.8 | bsz 559.4 | num_updates 964 | lr 1.63579e-05 | gnorm 2.129 | clip 100 | loss_scale 0.125 | train_wall 300 | gb_free 9.7 | wall 5778
2023-05-20 13:35:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:35:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:35:35 | INFO | fairseq.trainer | begin training epoch 16
2023-05-20 13:35:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:36:48 | INFO | train_inner | epoch 016:     16 / 65 loss=2.773, nll_loss=0.818, ppl=1.76, wps=1631.5, ups=0.14, wpb=11692.2, bsz=562.9, num_updates=980, lr=1.61053e-05, gnorm=1.619, clip=100, loss_scale=0.125, train_wall=88, gb_free=9.8, wall=5851
2023-05-20 13:38:22 | INFO | train_inner | epoch 016:     36 / 65 loss=2.761, nll_loss=0.807, ppl=1.75, wps=2589.6, ups=0.21, wpb=12184, bsz=587.8, num_updates=1000, lr=1.57895e-05, gnorm=3.84, clip=100, loss_scale=0.125, train_wall=94, gb_free=9.2, wall=5945
2023-05-20 13:39:54 | INFO | train_inner | epoch 016:     56 / 65 loss=2.756, nll_loss=0.799, ppl=1.74, wps=2656.6, ups=0.22, wpb=12241.2, bsz=549.9, num_updates=1020, lr=1.54737e-05, gnorm=1.695, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.3, wall=6037
2023-05-20 13:40:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:40:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:40:52 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.397 | nll_loss 1.467 | ppl 2.76 | bleu 43.2 | wps 1913.9 | wpb 2785 | bsz 105.2 | num_updates 1029 | best_loss 3.382
2023-05-20 13:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1029 updates
2023-05-20 13:40:52 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint16.pt
2023-05-20 13:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint16.pt
2023-05-20 13:41:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint16.pt (epoch 16 @ 1029 updates, score 3.397) (writing took 27.43593878671527 seconds)
2023-05-20 13:41:19 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-20 13:41:19 | INFO | train | epoch 016 | loss 2.761 | nll_loss 0.807 | ppl 1.75 | wps 2273.9 | ups 0.19 | wpb 12053 | bsz 559.7 | num_updates 1029 | lr 1.53316e-05 | gnorm 3.045 | clip 100 | loss_scale 0.125 | train_wall 298 | gb_free 9.4 | wall 6122
2023-05-20 13:41:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:41:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:41:19 | INFO | fairseq.trainer | begin training epoch 17
2023-05-20 13:41:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:42:11 | INFO | train_inner | epoch 017:     11 / 65 loss=2.738, nll_loss=0.783, ppl=1.72, wps=1716.1, ups=0.15, wpb=11753.7, bsz=527.3, num_updates=1040, lr=1.51579e-05, gnorm=4.087, clip=100, loss_scale=0.125, train_wall=91, gb_free=9.2, wall=6174
2023-05-20 13:43:43 | INFO | train_inner | epoch 017:     31 / 65 loss=2.727, nll_loss=0.77, ppl=1.71, wps=2667.7, ups=0.22, wpb=12276.2, bsz=585.4, num_updates=1060, lr=1.48421e-05, gnorm=1.782, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.9, wall=6266
2023-05-20 13:45:16 | INFO | train_inner | epoch 017:     51 / 65 loss=2.73, nll_loss=0.773, ppl=1.71, wps=2646.3, ups=0.22, wpb=12216.2, bsz=553.6, num_updates=1080, lr=1.45263e-05, gnorm=1.704, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.7, wall=6359
2023-05-20 13:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:46:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:46:37 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.405 | nll_loss 1.495 | ppl 2.82 | bleu 43.4 | wps 1865.8 | wpb 2785 | bsz 105.2 | num_updates 1094 | best_loss 3.382
2023-05-20 13:46:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1094 updates
2023-05-20 13:46:37 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint17.pt
2023-05-20 13:46:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint17.pt
2023-05-20 13:47:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint17.pt (epoch 17 @ 1094 updates, score 3.405) (writing took 25.889871433377266 seconds)
2023-05-20 13:47:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-20 13:47:03 | INFO | train | epoch 017 | loss 2.725 | nll_loss 0.768 | ppl 1.7 | wps 2281.1 | ups 0.19 | wpb 12054.2 | bsz 559.6 | num_updates 1094 | lr 1.43053e-05 | gnorm 1.788 | clip 100 | loss_scale 0.125 | train_wall 298 | gb_free 9.4 | wall 6466
2023-05-20 13:47:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:47:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:47:03 | INFO | fairseq.trainer | begin training epoch 18
2023-05-20 13:47:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:47:31 | INFO | train_inner | epoch 018:      6 / 65 loss=2.71, nll_loss=0.755, ppl=1.69, wps=1717.7, ups=0.15, wpb=11597, bsz=523.6, num_updates=1100, lr=1.42105e-05, gnorm=1.806, clip=100, loss_scale=0.125, train_wall=90, gb_free=9.7, wall=6494
2023-05-20 13:49:05 | INFO | train_inner | epoch 018:     26 / 65 loss=2.691, nll_loss=0.731, ppl=1.66, wps=2573.5, ups=0.21, wpb=12189.2, bsz=566.1, num_updates=1120, lr=1.38947e-05, gnorm=4.511, clip=100, loss_scale=0.125, train_wall=95, gb_free=10.1, wall=6588
2023-05-20 13:50:38 | INFO | train_inner | epoch 018:     46 / 65 loss=2.693, nll_loss=0.733, ppl=1.66, wps=2644.9, ups=0.22, wpb=12231.3, bsz=567.4, num_updates=1140, lr=1.35789e-05, gnorm=1.968, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.6, wall=6681
2023-05-20 13:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:52:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:52:22 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.403 | nll_loss 1.504 | ppl 2.84 | bleu 43.65 | wps 1885.3 | wpb 2785 | bsz 105.2 | num_updates 1159 | best_loss 3.382
2023-05-20 13:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1159 updates
2023-05-20 13:52:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint18.pt
2023-05-20 13:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint18.pt
2023-05-20 13:53:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint18.pt (epoch 18 @ 1159 updates, score 3.403) (writing took 41.291423209011555 seconds)
2023-05-20 13:53:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-20 13:53:03 | INFO | train | epoch 018 | loss 2.696 | nll_loss 0.736 | ppl 1.67 | wps 2170.8 | ups 0.18 | wpb 12049.9 | bsz 559.4 | num_updates 1159 | lr 1.32789e-05 | gnorm 2.519 | clip 100 | loss_scale 0.125 | train_wall 300 | gb_free 9.9 | wall 6826
2023-05-20 13:53:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:53:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:53:03 | INFO | fairseq.trainer | begin training epoch 19
2023-05-20 13:53:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 13:53:08 | INFO | train_inner | epoch 019:      1 / 65 loss=2.703, nll_loss=0.745, ppl=1.68, wps=1557, ups=0.13, wpb=11721, bsz=559.2, num_updates=1160, lr=1.32632e-05, gnorm=1.283, clip=100, loss_scale=0.125, train_wall=90, gb_free=10.1, wall=6831
2023-05-20 13:54:43 | INFO | train_inner | epoch 019:     21 / 65 loss=2.68, nll_loss=0.718, ppl=1.65, wps=2596.9, ups=0.21, wpb=12270.8, bsz=579, num_updates=1180, lr=1.29474e-05, gnorm=1.795, clip=100, loss_scale=0.125, train_wall=94, gb_free=9.4, wall=6926
2023-05-20 13:56:16 | INFO | train_inner | epoch 019:     41 / 65 loss=2.674, nll_loss=0.714, ppl=1.64, wps=2602, ups=0.21, wpb=12150.4, bsz=580.1, num_updates=1200, lr=1.26316e-05, gnorm=1.408, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.6, wall=7019
2023-05-20 13:57:49 | INFO | train_inner | epoch 019:     61 / 65 loss=2.658, nll_loss=0.697, ppl=1.62, wps=2645.6, ups=0.22, wpb=12241.1, bsz=546.5, num_updates=1220, lr=1.23158e-05, gnorm=1.556, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.4, wall=7112
2023-05-20 13:58:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 13:58:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:58:23 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.408 | nll_loss 1.516 | ppl 2.86 | bleu 43.78 | wps 1941.5 | wpb 2785 | bsz 105.2 | num_updates 1224 | best_loss 3.382
2023-05-20 13:58:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1224 updates
2023-05-20 13:58:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint19.pt
2023-05-20 13:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint19.pt
2023-05-20 13:59:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint19.pt (epoch 19 @ 1224 updates, score 3.408) (writing took 38.622568808496 seconds)
2023-05-20 13:59:13 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-20 13:59:13 | INFO | train | epoch 019 | loss 2.669 | nll_loss 0.708 | ppl 1.63 | wps 2118 | ups 0.18 | wpb 12055 | bsz 559.7 | num_updates 1224 | lr 1.22526e-05 | gnorm 1.623 | clip 100 | loss_scale 0.125 | train_wall 301 | gb_free 9.7 | wall 7196
2023-05-20 13:59:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 13:59:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 13:59:14 | INFO | fairseq.trainer | begin training epoch 20
2023-05-20 13:59:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:00:30 | INFO | train_inner | epoch 020:     16 / 65 loss=2.631, nll_loss=0.664, ppl=1.58, wps=1424.1, ups=0.12, wpb=11479.2, bsz=512.9, num_updates=1240, lr=1.2e-05, gnorm=1.506, clip=100, loss_scale=0.125, train_wall=93, gb_free=9.8, wall=7273
2023-05-20 14:02:00 | INFO | train_inner | epoch 020:     36 / 65 loss=2.646, nll_loss=0.683, ppl=1.61, wps=2706, ups=0.22, wpb=12230.2, bsz=561.6, num_updates=1260, lr=1.16842e-05, gnorm=1.794, clip=100, loss_scale=0.125, train_wall=90, gb_free=9.2, wall=7363
2023-05-20 14:03:33 | INFO | train_inner | epoch 020:     56 / 65 loss=2.637, nll_loss=0.673, ppl=1.59, wps=2660.5, ups=0.22, wpb=12290.1, bsz=557, num_updates=1280, lr=1.13684e-05, gnorm=2.007, clip=100, loss_scale=0.125, train_wall=92, gb_free=9.8, wall=7456
2023-05-20 14:04:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:04:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:04:33 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.433 | nll_loss 1.54 | ppl 2.91 | bleu 43.32 | wps 1937.7 | wpb 2785 | bsz 105.2 | num_updates 1289 | best_loss 3.382
2023-05-20 14:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1289 updates
2023-05-20 14:04:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint20.pt
2023-05-20 14:04:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint20.pt
2023-05-20 14:05:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint20.pt (epoch 20 @ 1289 updates, score 3.433) (writing took 34.65360464528203 seconds)
2023-05-20 14:05:08 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-20 14:05:08 | INFO | train | epoch 020 | loss 2.644 | nll_loss 0.681 | ppl 1.6 | wps 2208.5 | ups 0.18 | wpb 12046 | bsz 558.3 | num_updates 1289 | lr 1.12263e-05 | gnorm 1.748 | clip 100 | loss_scale 0.125 | train_wall 301 | gb_free 9.9 | wall 7551
2023-05-20 14:05:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:05:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:05:08 | INFO | fairseq.trainer | begin training epoch 21
2023-05-20 14:05:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:05:57 | INFO | train_inner | epoch 021:     11 / 65 loss=2.655, nll_loss=0.694, ppl=1.62, wps=1634.7, ups=0.14, wpb=11817.2, bsz=580.8, num_updates=1300, lr=1.10526e-05, gnorm=1.556, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.3, wall=7600
2023-05-20 14:07:29 | INFO | train_inner | epoch 021:     31 / 65 loss=2.624, nll_loss=0.659, ppl=1.58, wps=2678.3, ups=0.22, wpb=12308.7, bsz=577.9, num_updates=1320, lr=1.07368e-05, gnorm=1.379, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.5, wall=7692
2023-05-20 14:09:02 | INFO | train_inner | epoch 021:     51 / 65 loss=2.616, nll_loss=0.651, ppl=1.57, wps=2572, ups=0.21, wpb=11971.9, bsz=557.5, num_updates=1340, lr=1.04211e-05, gnorm=2.388, clip=100, loss_scale=0.25, train_wall=93, gb_free=10.1, wall=7785
2023-05-20 14:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:10:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:10:25 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.429 | nll_loss 1.542 | ppl 2.91 | bleu 43.47 | wps 1775.2 | wpb 2785 | bsz 105.2 | num_updates 1354 | best_loss 3.382
2023-05-20 14:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1354 updates
2023-05-20 14:10:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint21.pt
2023-05-20 14:10:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint21.pt
2023-05-20 14:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint21.pt (epoch 21 @ 1354 updates, score 3.429) (writing took 29.184223137795925 seconds)
2023-05-20 14:10:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-05-20 14:10:55 | INFO | train | epoch 021 | loss 2.623 | nll_loss 0.658 | ppl 1.58 | wps 2259.8 | ups 0.19 | wpb 12054.7 | bsz 559.9 | num_updates 1354 | lr 1.02e-05 | gnorm 1.842 | clip 100 | loss_scale 0.25 | train_wall 297 | gb_free 9.7 | wall 7898
2023-05-20 14:10:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:10:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:10:55 | INFO | fairseq.trainer | begin training epoch 22
2023-05-20 14:10:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:11:22 | INFO | train_inner | epoch 022:      6 / 65 loss=2.614, nll_loss=0.65, ppl=1.57, wps=1686.3, ups=0.14, wpb=11781.6, bsz=534.4, num_updates=1360, lr=1.01053e-05, gnorm=3.344, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.8, wall=7925
2023-05-20 14:12:54 | INFO | train_inner | epoch 022:     26 / 65 loss=2.606, nll_loss=0.64, ppl=1.56, wps=2667.1, ups=0.22, wpb=12234.9, bsz=567.6, num_updates=1380, lr=9.78947e-06, gnorm=2.056, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.1, wall=8017
2023-05-20 14:14:29 | INFO | train_inner | epoch 022:     46 / 65 loss=2.605, nll_loss=0.638, ppl=1.56, wps=2549.9, ups=0.21, wpb=12146.7, bsz=558.8, num_updates=1400, lr=9.47368e-06, gnorm=2.771, clip=100, loss_scale=0.25, train_wall=95, gb_free=8.2, wall=8112
2023-05-20 14:15:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:15:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:16:13 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.435 | nll_loss 1.542 | ppl 2.91 | bleu 43.89 | wps 1883.8 | wpb 2785 | bsz 105.2 | num_updates 1419 | best_loss 3.382
2023-05-20 14:16:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1419 updates
2023-05-20 14:16:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint22.pt
2023-05-20 14:16:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint22.pt
2023-05-20 14:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint22.pt (epoch 22 @ 1419 updates, score 3.435) (writing took 64.70772501453757 seconds)
2023-05-20 14:17:18 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-05-20 14:17:18 | INFO | train | epoch 022 | loss 2.607 | nll_loss 0.642 | ppl 1.56 | wps 2045.3 | ups 0.17 | wpb 12059.9 | bsz 558.9 | num_updates 1419 | lr 9.17368e-06 | gnorm 3.537 | clip 100 | loss_scale 0.25 | train_wall 300 | gb_free 10 | wall 8281
2023-05-20 14:17:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:17:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:17:18 | INFO | fairseq.trainer | begin training epoch 23
2023-05-20 14:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:17:22 | INFO | train_inner | epoch 023:      1 / 65 loss=2.611, nll_loss=0.646, ppl=1.57, wps=1360.5, ups=0.12, wpb=11767.2, bsz=557.6, num_updates=1420, lr=9.15789e-06, gnorm=4.871, clip=100, loss_scale=0.25, train_wall=89, gb_free=9.6, wall=8285
2023-05-20 14:18:58 | INFO | train_inner | epoch 023:     21 / 65 loss=2.587, nll_loss=0.619, ppl=1.54, wps=2566, ups=0.21, wpb=12227, bsz=538.2, num_updates=1440, lr=8.84211e-06, gnorm=1.241, clip=100, loss_scale=0.25, train_wall=95, gb_free=8.8, wall=8380
2023-05-20 14:20:29 | INFO | train_inner | epoch 023:     41 / 65 loss=2.599, nll_loss=0.633, ppl=1.55, wps=2657.7, ups=0.22, wpb=12111.8, bsz=569.8, num_updates=1460, lr=8.52632e-06, gnorm=1.414, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.8, wall=8472
2023-05-20 14:22:01 | INFO | train_inner | epoch 023:     61 / 65 loss=2.586, nll_loss=0.619, ppl=1.54, wps=2642.5, ups=0.22, wpb=12150.6, bsz=572, num_updates=1480, lr=8.21053e-06, gnorm=1.349, clip=100, loss_scale=0.25, train_wall=92, gb_free=10.1, wall=8564
2023-05-20 14:22:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:22:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:22:35 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.435 | nll_loss 1.548 | ppl 2.92 | bleu 44.39 | wps 1819.8 | wpb 2785 | bsz 105.2 | num_updates 1484 | best_loss 3.382
2023-05-20 14:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1484 updates
2023-05-20 14:22:35 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint23.pt
2023-05-20 14:22:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint23.pt
2023-05-20 14:23:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint23.pt (epoch 23 @ 1484 updates, score 3.435) (writing took 51.98166370764375 seconds)
2023-05-20 14:23:29 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-05-20 14:23:29 | INFO | train | epoch 023 | loss 2.591 | nll_loss 0.624 | ppl 1.54 | wps 2113.1 | ups 0.18 | wpb 12054.2 | bsz 559.6 | num_updates 1484 | lr 8.14737e-06 | gnorm 1.364 | clip 100 | loss_scale 0.25 | train_wall 298 | gb_free 10.1 | wall 8652
2023-05-20 14:23:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:23:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:23:29 | INFO | fairseq.trainer | begin training epoch 24
2023-05-20 14:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:24:44 | INFO | train_inner | epoch 024:     16 / 65 loss=2.57, nll_loss=0.601, ppl=1.52, wps=1423.3, ups=0.12, wpb=11640.4, bsz=539.6, num_updates=1500, lr=7.89474e-06, gnorm=1.566, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.7, wall=8727
2023-05-20 14:26:17 | INFO | train_inner | epoch 024:     36 / 65 loss=2.576, nll_loss=0.609, ppl=1.52, wps=2643.5, ups=0.21, wpb=12299.1, bsz=559.2, num_updates=1520, lr=7.57895e-06, gnorm=1.626, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.3, wall=8820
2023-05-20 14:27:53 | INFO | train_inner | epoch 024:     56 / 65 loss=2.575, nll_loss=0.607, ppl=1.52, wps=2534.4, ups=0.21, wpb=12151.6, bsz=580.4, num_updates=1540, lr=7.26316e-06, gnorm=1.699, clip=100, loss_scale=0.25, train_wall=96, gb_free=9.6, wall=8916
2023-05-20 14:28:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:28:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:28:52 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.457 | nll_loss 1.566 | ppl 2.96 | bleu 43.96 | wps 1889.5 | wpb 2785 | bsz 105.2 | num_updates 1549 | best_loss 3.382
2023-05-20 14:28:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1549 updates
2023-05-20 14:28:52 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint24.pt
2023-05-20 14:29:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint24.pt
2023-05-20 14:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint24.pt (epoch 24 @ 1549 updates, score 3.457) (writing took 39.37251132726669 seconds)
2023-05-20 14:29:56 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-05-20 14:29:56 | INFO | train | epoch 024 | loss 2.574 | nll_loss 0.606 | ppl 1.52 | wps 2024.3 | ups 0.17 | wpb 12057.5 | bsz 560 | num_updates 1549 | lr 7.12105e-06 | gnorm 2.183 | clip 100 | loss_scale 0.25 | train_wall 303 | gb_free 10.1 | wall 9039
2023-05-20 14:29:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:29:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:29:56 | INFO | fairseq.trainer | begin training epoch 25
2023-05-20 14:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:30:47 | INFO | train_inner | epoch 025:     11 / 65 loss=2.574, nll_loss=0.607, ppl=1.52, wps=1356.2, ups=0.12, wpb=11792, bsz=559.6, num_updates=1560, lr=6.94737e-06, gnorm=3.284, clip=100, loss_scale=0.25, train_wall=90, gb_free=10, wall=9090
2023-05-20 14:32:19 | INFO | train_inner | epoch 025:     31 / 65 loss=2.543, nll_loss=0.572, ppl=1.49, wps=2613.1, ups=0.22, wpb=11987.1, bsz=531, num_updates=1580, lr=6.63158e-06, gnorm=1.409, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.3, wall=9182
2023-05-20 14:33:53 | INFO | train_inner | epoch 025:     51 / 65 loss=2.557, nll_loss=0.588, ppl=1.5, wps=2602.6, ups=0.21, wpb=12262.2, bsz=559.7, num_updates=1600, lr=6.31579e-06, gnorm=1.313, clip=100, loss_scale=0.25, train_wall=94, gb_free=9.8, wall=9276
2023-05-20 14:34:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:34:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:35:18 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.455 | nll_loss 1.571 | ppl 2.97 | bleu 43.94 | wps 1851.7 | wpb 2785 | bsz 105.2 | num_updates 1614 | best_loss 3.382
2023-05-20 14:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1614 updates
2023-05-20 14:35:18 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint25.pt
2023-05-20 14:35:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint25.pt
2023-05-20 14:36:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint25.pt (epoch 25 @ 1614 updates, score 3.455) (writing took 44.76385059580207 seconds)
2023-05-20 14:36:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-05-20 14:36:03 | INFO | train | epoch 025 | loss 2.561 | nll_loss 0.592 | ppl 1.51 | wps 2137.5 | ups 0.18 | wpb 12057.2 | bsz 560.3 | num_updates 1614 | lr 6.09474e-06 | gnorm 1.378 | clip 100 | loss_scale 0.25 | train_wall 303 | gb_free 9.2 | wall 9406
2023-05-20 14:36:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:36:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:36:03 | INFO | fairseq.trainer | begin training epoch 26
2023-05-20 14:36:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:36:31 | INFO | train_inner | epoch 026:      6 / 65 loss=2.589, nll_loss=0.624, ppl=1.54, wps=1539.2, ups=0.13, wpb=12146.2, bsz=624.6, num_updates=1620, lr=6e-06, gnorm=4.755, clip=100, loss_scale=0.25, train_wall=94, gb_free=7.4, wall=9434
2023-05-20 14:38:06 | INFO | train_inner | epoch 026:     26 / 65 loss=2.537, nll_loss=0.565, ppl=1.48, wps=2528.9, ups=0.21, wpb=12010.1, bsz=551.7, num_updates=1640, lr=5.68421e-06, gnorm=2.132, clip=100, loss_scale=0.25, train_wall=95, gb_free=10.1, wall=9529
2023-05-20 14:39:42 | INFO | train_inner | epoch 026:     46 / 65 loss=2.551, nll_loss=0.582, ppl=1.5, wps=2531.5, ups=0.21, wpb=12166.5, bsz=556.2, num_updates=1660, lr=5.36842e-06, gnorm=1.867, clip=100, loss_scale=0.25, train_wall=96, gb_free=10, wall=9625
2023-05-20 14:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:41:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:41:26 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.459 | nll_loss 1.579 | ppl 2.99 | bleu 44.02 | wps 1933.9 | wpb 2785 | bsz 105.2 | num_updates 1679 | best_loss 3.382
2023-05-20 14:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1679 updates
2023-05-20 14:41:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint26.pt
2023-05-20 14:41:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint26.pt
2023-05-20 14:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint26.pt (epoch 26 @ 1679 updates, score 3.459) (writing took 33.09314622357488 seconds)
2023-05-20 14:41:59 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-05-20 14:41:59 | INFO | train | epoch 026 | loss 2.55 | nll_loss 0.58 | ppl 1.49 | wps 2198.8 | ups 0.18 | wpb 12056.4 | bsz 559.7 | num_updates 1679 | lr 5.06842e-06 | gnorm 2.733 | clip 100 | loss_scale 0.25 | train_wall 305 | gb_free 10 | wall 9762
2023-05-20 14:41:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:41:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:41:59 | INFO | fairseq.trainer | begin training epoch 27
2023-05-20 14:41:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:42:03 | INFO | train_inner | epoch 027:      1 / 65 loss=2.55, nll_loss=0.58, ppl=1.49, wps=1662.8, ups=0.14, wpb=11766.3, bsz=528, num_updates=1680, lr=5.05263e-06, gnorm=1.236, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.8, wall=9766
2023-05-20 14:43:36 | INFO | train_inner | epoch 027:     21 / 65 loss=2.539, nll_loss=0.568, ppl=1.48, wps=2633.4, ups=0.21, wpb=12249.9, bsz=578.4, num_updates=1700, lr=4.73684e-06, gnorm=1.075, clip=100, loss_scale=0.25, train_wall=93, gb_free=7.5, wall=9859
2023-05-20 14:45:08 | INFO | train_inner | epoch 027:     41 / 65 loss=2.534, nll_loss=0.563, ppl=1.48, wps=2632.6, ups=0.22, wpb=12096.9, bsz=528.7, num_updates=1720, lr=4.42105e-06, gnorm=1.433, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.1, wall=9951
2023-05-20 14:46:42 | INFO | train_inner | epoch 027:     61 / 65 loss=2.552, nll_loss=0.584, ppl=1.5, wps=2614.1, ups=0.21, wpb=12218.9, bsz=598.1, num_updates=1740, lr=4.10526e-06, gnorm=3.825, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.8, wall=10045
2023-05-20 14:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:47:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:47:19 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.463 | nll_loss 1.582 | ppl 2.99 | bleu 44.4 | wps 1836.9 | wpb 2785 | bsz 105.2 | num_updates 1744 | best_loss 3.382
2023-05-20 14:47:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1744 updates
2023-05-20 14:47:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint27.pt
2023-05-20 14:47:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint27.pt
2023-05-20 14:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint27.pt (epoch 27 @ 1744 updates, score 3.463) (writing took 34.21847070381045 seconds)
2023-05-20 14:47:53 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-05-20 14:47:53 | INFO | train | epoch 027 | loss 2.541 | nll_loss 0.571 | ppl 1.49 | wps 2211 | ups 0.18 | wpb 12044.6 | bsz 557.2 | num_updates 1744 | lr 4.04211e-06 | gnorm 2.051 | clip 100 | loss_scale 0.25 | train_wall 300 | gb_free 9 | wall 10116
2023-05-20 14:47:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:47:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:47:53 | INFO | fairseq.trainer | begin training epoch 28
2023-05-20 14:47:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:49:08 | INFO | train_inner | epoch 028:     16 / 65 loss=2.542, nll_loss=0.572, ppl=1.49, wps=1615.1, ups=0.14, wpb=11790.9, bsz=538, num_updates=1760, lr=3.78947e-06, gnorm=1.38, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.7, wall=10191
2023-05-20 14:50:45 | INFO | train_inner | epoch 028:     36 / 65 loss=2.528, nll_loss=0.556, ppl=1.47, wps=2464, ups=0.2, wpb=12023.7, bsz=544.2, num_updates=1780, lr=3.47368e-06, gnorm=1.581, clip=100, loss_scale=0.25, train_wall=97, gb_free=9.7, wall=10288
2023-05-20 14:52:18 | INFO | train_inner | epoch 028:     56 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=2644.9, ups=0.22, wpb=12274.8, bsz=575.6, num_updates=1800, lr=3.15789e-06, gnorm=1.221, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.8, wall=10381
2023-05-20 14:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:52:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:53:16 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.455 | nll_loss 1.575 | ppl 2.98 | bleu 44.34 | wps 1784.8 | wpb 2785 | bsz 105.2 | num_updates 1809 | best_loss 3.382
2023-05-20 14:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1809 updates
2023-05-20 14:53:16 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint28.pt
2023-05-20 14:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint28.pt
2023-05-20 14:53:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint28.pt (epoch 28 @ 1809 updates, score 3.455) (writing took 40.572237096726894 seconds)
2023-05-20 14:53:56 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-05-20 14:53:56 | INFO | train | epoch 028 | loss 2.533 | nll_loss 0.562 | ppl 1.48 | wps 2156.9 | ups 0.18 | wpb 12048.1 | bsz 557.8 | num_updates 1809 | lr 3.01579e-06 | gnorm 1.509 | clip 100 | loss_scale 0.5 | train_wall 302 | gb_free 10.1 | wall 10479
2023-05-20 14:53:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:53:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 14:53:56 | INFO | fairseq.trainer | begin training epoch 29
2023-05-20 14:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:54:48 | INFO | train_inner | epoch 029:     11 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=1556.5, ups=0.13, wpb=11627.2, bsz=518.6, num_updates=1820, lr=2.84211e-06, gnorm=1.607, clip=100, loss_scale=0.5, train_wall=89, gb_free=9.3, wall=10531
2023-05-20 14:56:21 | INFO | train_inner | epoch 029:     31 / 65 loss=2.534, nll_loss=0.565, ppl=1.48, wps=2616.6, ups=0.21, wpb=12244, bsz=574.4, num_updates=1840, lr=2.52632e-06, gnorm=1.439, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.6, wall=10624
2023-05-20 14:57:54 | INFO | train_inner | epoch 029:     51 / 65 loss=2.526, nll_loss=0.555, ppl=1.47, wps=2658.3, ups=0.22, wpb=12280.4, bsz=576, num_updates=1860, lr=2.21053e-06, gnorm=1.531, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.3, wall=10717
2023-05-20 14:58:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:58:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:59:15 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.466 | nll_loss 1.586 | ppl 3 | bleu 44.39 | wps 1900.9 | wpb 2785 | bsz 105.2 | num_updates 1874 | best_loss 3.382
2023-05-20 14:59:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1874 updates
2023-05-20 14:59:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint29.pt
2023-05-20 14:59:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint29.pt
2023-05-20 15:00:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint29.pt (epoch 29 @ 1874 updates, score 3.466) (writing took 57.69363218173385 seconds)
2023-05-20 15:00:12 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-05-20 15:00:12 | INFO | train | epoch 029 | loss 2.528 | nll_loss 0.557 | ppl 1.47 | wps 2083 | ups 0.17 | wpb 12058.9 | bsz 559.8 | num_updates 1874 | lr 1.98947e-06 | gnorm 1.822 | clip 100 | loss_scale 0.5 | train_wall 300 | gb_free 10 | wall 10855
2023-05-20 15:00:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:00:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:00:12 | INFO | fairseq.trainer | begin training epoch 30
2023-05-20 15:00:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:00:38 | INFO | train_inner | epoch 030:      6 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=1415.8, ups=0.12, wpb=11656.9, bsz=552.4, num_updates=1880, lr=1.89474e-06, gnorm=2.887, clip=100, loss_scale=0.5, train_wall=88, gb_free=9.7, wall=10881
2023-05-20 15:02:09 | INFO | train_inner | epoch 030:     26 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=2684.8, ups=0.22, wpb=12155.2, bsz=559.1, num_updates=1900, lr=1.57895e-06, gnorm=1.158, clip=100, loss_scale=0.5, train_wall=90, gb_free=8.7, wall=10972
2023-05-20 15:03:44 | INFO | train_inner | epoch 030:     46 / 65 loss=2.528, nll_loss=0.555, ppl=1.47, wps=2599.6, ups=0.21, wpb=12406, bsz=580, num_updates=1920, lr=1.26316e-06, gnorm=1.694, clip=100, loss_scale=0.5, train_wall=95, gb_free=9.6, wall=11067
2023-05-20 15:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:05:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:05:26 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.467 | nll_loss 1.587 | ppl 3 | bleu 44.46 | wps 1966.5 | wpb 2785 | bsz 105.2 | num_updates 1939 | best_loss 3.382
2023-05-20 15:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1939 updates
2023-05-20 15:05:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint30.pt
2023-05-20 15:05:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint30.pt
2023-05-20 15:06:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint30.pt (epoch 30 @ 1939 updates, score 3.467) (writing took 53.03449431806803 seconds)
2023-05-20 15:06:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-05-20 15:06:19 | INFO | train | epoch 030 | loss 2.522 | nll_loss 0.55 | ppl 1.46 | wps 2138.8 | ups 0.18 | wpb 12054.9 | bsz 558.9 | num_updates 1939 | lr 9.63158e-07 | gnorm 1.363 | clip 100 | loss_scale 0.5 | train_wall 295 | gb_free 9.8 | wall 11222
2023-05-20 15:06:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:06:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:06:19 | INFO | fairseq.trainer | begin training epoch 31
2023-05-20 15:06:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:06:23 | INFO | train_inner | epoch 031:      1 / 65 loss=2.52, nll_loss=0.551, ppl=1.46, wps=1475, ups=0.13, wpb=11684.9, bsz=537.1, num_updates=1940, lr=9.47368e-07, gnorm=1.167, clip=100, loss_scale=0.5, train_wall=87, gb_free=9.5, wall=11226
2023-05-20 15:07:57 | INFO | train_inner | epoch 031:     21 / 65 loss=2.507, nll_loss=0.534, ppl=1.45, wps=2588.4, ups=0.21, wpb=12175.8, bsz=554.5, num_updates=1960, lr=6.31579e-07, gnorm=1.473, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.9, wall=11320
2023-05-20 15:09:30 | INFO | train_inner | epoch 031:     41 / 65 loss=2.523, nll_loss=0.552, ppl=1.47, wps=2624.7, ups=0.22, wpb=12196.6, bsz=556.6, num_updates=1980, lr=3.15789e-07, gnorm=1.258, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.1, wall=11413
2023-05-20 15:11:01 | INFO | train_inner | epoch 031:     61 / 65 loss=2.524, nll_loss=0.553, ppl=1.47, wps=2666, ups=0.22, wpb=12153.2, bsz=576.8, num_updates=2000, lr=0, gnorm=1.432, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.3, wall=11504
2023-05-20 15:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:11:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:11:35 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1828.9 | wpb 2785 | bsz 105.2 | num_updates 2004 | best_loss 3.382
2023-05-20 15:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2004 updates
2023-05-20 15:11:35 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint31.pt
2023-05-20 15:11:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint31.pt
2023-05-20 15:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint31.pt (epoch 31 @ 2004 updates, score 3.469) (writing took 49.51918267458677 seconds)
2023-05-20 15:12:25 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-05-20 15:12:25 | INFO | train | epoch 031 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2140.8 | ups 0.18 | wpb 12054.2 | bsz 557.9 | num_updates 2004 | lr 0 | gnorm 1.404 | clip 100 | loss_scale 0.5 | train_wall 297 | gb_free 8.8 | wall 11588
2023-05-20 15:12:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:12:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:12:25 | INFO | fairseq.trainer | begin training epoch 32
2023-05-20 15:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:13:37 | INFO | train_inner | epoch 032:     16 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=1511.8, ups=0.13, wpb=11809.2, bsz=545.2, num_updates=2020, lr=0, gnorm=1.283, clip=100, loss_scale=0.5, train_wall=87, gb_free=9.6, wall=11660
2023-05-20 15:15:11 | INFO | train_inner | epoch 032:     36 / 65 loss=2.503, nll_loss=0.53, ppl=1.44, wps=2558.7, ups=0.21, wpb=12059.2, bsz=528.3, num_updates=2040, lr=0, gnorm=1.716, clip=100, loss_scale=0.5, train_wall=94, gb_free=10.1, wall=11754
2023-05-20 15:16:48 | INFO | train_inner | epoch 032:     56 / 65 loss=2.526, nll_loss=0.555, ppl=1.47, wps=2528.8, ups=0.21, wpb=12202, bsz=577.2, num_updates=2060, lr=0, gnorm=1.815, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.1, wall=11851
2023-05-20 15:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:17:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:17:47 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1849.4 | wpb 2785 | bsz 105.2 | num_updates 2069 | best_loss 3.382
2023-05-20 15:17:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2069 updates
2023-05-20 15:17:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint32.pt
2023-05-20 15:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint32.pt
2023-05-20 15:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint32.pt (epoch 32 @ 2069 updates, score 3.469) (writing took 32.618776232004166 seconds)
2023-05-20 15:18:20 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-05-20 15:18:20 | INFO | train | epoch 032 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2207 | ups 0.18 | wpb 12057.6 | bsz 560 | num_updates 2069 | lr 0 | gnorm 1.625 | clip 100 | loss_scale 0.5 | train_wall 303 | gb_free 9.7 | wall 11943
2023-05-20 15:18:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:18:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:18:20 | INFO | fairseq.trainer | begin training epoch 33
2023-05-20 15:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:19:12 | INFO | train_inner | epoch 033:     11 / 65 loss=2.521, nll_loss=0.551, ppl=1.46, wps=1642.6, ups=0.14, wpb=11797.2, bsz=567.6, num_updates=2080, lr=0, gnorm=1.374, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.8, wall=11995
2023-05-20 15:20:42 | INFO | train_inner | epoch 033:     31 / 65 loss=2.524, nll_loss=0.553, ppl=1.47, wps=2718.3, ups=0.22, wpb=12221.8, bsz=579.2, num_updates=2100, lr=0, gnorm=1.481, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.2, wall=12085
2023-05-20 15:22:14 | INFO | train_inner | epoch 033:     51 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=2613.3, ups=0.22, wpb=12121, bsz=557.1, num_updates=2120, lr=0, gnorm=1.099, clip=100, loss_scale=0.5, train_wall=93, gb_free=8.8, wall=12177
2023-05-20 15:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:23:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:23:36 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1783 | wpb 2785 | bsz 105.2 | num_updates 2134 | best_loss 3.382
2023-05-20 15:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2134 updates
2023-05-20 15:23:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint33.pt
2023-05-20 15:23:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint33.pt
2023-05-20 15:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint33.pt (epoch 33 @ 2134 updates, score 3.469) (writing took 51.63811310753226 seconds)
2023-05-20 15:24:27 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-05-20 15:24:27 | INFO | train | epoch 033 | loss 2.518 | nll_loss 0.547 | ppl 1.46 | wps 2132.9 | ups 0.18 | wpb 12052.8 | bsz 559.9 | num_updates 2134 | lr 0 | gnorm 1.233 | clip 100 | loss_scale 0.5 | train_wall 296 | gb_free 9 | wall 12310
2023-05-20 15:24:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:24:27 | INFO | fairseq.trainer | begin training epoch 34
2023-05-20 15:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:24:54 | INFO | train_inner | epoch 034:      6 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=1474.2, ups=0.12, wpb=11795.6, bsz=549, num_updates=2140, lr=0, gnorm=1.134, clip=100, loss_scale=0.5, train_wall=88, gb_free=9.3, wall=12337
2023-05-20 15:26:27 | INFO | train_inner | epoch 034:     26 / 65 loss=2.523, nll_loss=0.552, ppl=1.47, wps=2605.3, ups=0.21, wpb=12134.2, bsz=578.2, num_updates=2160, lr=0, gnorm=4.111, clip=100, loss_scale=0.5, train_wall=93, gb_free=9, wall=12430
2023-05-20 15:27:58 | INFO | train_inner | epoch 034:     46 / 65 loss=2.524, nll_loss=0.553, ppl=1.47, wps=2733.4, ups=0.22, wpb=12383.5, bsz=586.2, num_updates=2180, lr=0, gnorm=1.576, clip=100, loss_scale=0.5, train_wall=90, gb_free=10.1, wall=12521
2023-05-20 15:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:29:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:29:48 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1795.2 | wpb 2785 | bsz 105.2 | num_updates 2199 | best_loss 3.382
2023-05-20 15:29:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2199 updates
2023-05-20 15:29:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint34.pt
2023-05-20 15:30:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint34.pt
2023-05-20 15:30:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint34.pt (epoch 34 @ 2199 updates, score 3.469) (writing took 46.79536531493068 seconds)
2023-05-20 15:30:35 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-05-20 15:30:35 | INFO | train | epoch 034 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2132.1 | ups 0.18 | wpb 12056.4 | bsz 559.1 | num_updates 2199 | lr 0 | gnorm 2.218 | clip 100 | loss_scale 0.5 | train_wall 300 | gb_free 10 | wall 12678
2023-05-20 15:30:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:30:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:30:35 | INFO | fairseq.trainer | begin training epoch 35
2023-05-20 15:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:30:39 | INFO | train_inner | epoch 035:      1 / 65 loss=2.504, nll_loss=0.531, ppl=1.44, wps=1442.8, ups=0.12, wpb=11616.6, bsz=511.2, num_updates=2200, lr=0, gnorm=1.245, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.5, wall=12682
2023-05-20 15:32:11 | INFO | train_inner | epoch 035:     21 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=2665, ups=0.22, wpb=12194.2, bsz=566.1, num_updates=2220, lr=0, gnorm=2.293, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.2, wall=12774
2023-05-20 15:33:45 | INFO | train_inner | epoch 035:     41 / 65 loss=2.512, nll_loss=0.539, ppl=1.45, wps=2586.9, ups=0.21, wpb=12162.2, bsz=547.2, num_updates=2240, lr=0, gnorm=2.209, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.3, wall=12868
2023-05-20 15:35:20 | INFO | train_inner | epoch 035:     61 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2594, ups=0.21, wpb=12307.5, bsz=594.2, num_updates=2260, lr=0, gnorm=1.583, clip=100, loss_scale=0.5, train_wall=95, gb_free=9.9, wall=12963
2023-05-20 15:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:35:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:35:55 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1919.4 | wpb 2785 | bsz 105.2 | num_updates 2264 | best_loss 3.382
2023-05-20 15:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2264 updates
2023-05-20 15:35:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint35.pt
2023-05-20 15:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint35.pt
2023-05-20 15:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint35.pt (epoch 35 @ 2264 updates, score 3.469) (writing took 51.79012627154589 seconds)
2023-05-20 15:36:46 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2023-05-20 15:36:46 | INFO | train | epoch 035 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2108.7 | ups 0.17 | wpb 12054.5 | bsz 559.5 | num_updates 2264 | lr 0 | gnorm 1.971 | clip 100 | loss_scale 0.5 | train_wall 301 | gb_free 9.7 | wall 13049
2023-05-20 15:36:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:36:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:36:46 | INFO | fairseq.trainer | begin training epoch 36
2023-05-20 15:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:37:59 | INFO | train_inner | epoch 036:     16 / 65 loss=2.522, nll_loss=0.551, ppl=1.47, wps=1464.1, ups=0.13, wpb=11653.1, bsz=553.1, num_updates=2280, lr=0, gnorm=3.209, clip=100, loss_scale=0.5, train_wall=89, gb_free=9.1, wall=13122
2023-05-20 15:39:32 | INFO | train_inner | epoch 036:     36 / 65 loss=2.516, nll_loss=0.545, ppl=1.46, wps=2603.1, ups=0.21, wpb=12159.7, bsz=558.9, num_updates=2300, lr=0, gnorm=2.135, clip=100, loss_scale=0.5, train_wall=93, gb_free=8.6, wall=13215
2023-05-20 15:41:09 | INFO | train_inner | epoch 036:     56 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=2522.1, ups=0.21, wpb=12250.6, bsz=550.6, num_updates=2320, lr=0, gnorm=2.259, clip=100, loss_scale=1, train_wall=97, gb_free=9.8, wall=13312
2023-05-20 15:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:41:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:42:07 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1915.4 | wpb 2785 | bsz 105.2 | num_updates 2329 | best_loss 3.382
2023-05-20 15:42:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2329 updates
2023-05-20 15:42:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint36.pt
2023-05-20 15:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint36.pt
2023-05-20 15:43:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint36.pt (epoch 36 @ 2329 updates, score 3.469) (writing took 58.87060099467635 seconds)
2023-05-20 15:43:06 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2023-05-20 15:43:06 | INFO | train | epoch 036 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2066.2 | ups 0.17 | wpb 12055.9 | bsz 559.2 | num_updates 2329 | lr 0 | gnorm 2.491 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 8.8 | wall 13429
2023-05-20 15:43:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:43:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:43:06 | INFO | fairseq.trainer | begin training epoch 37
2023-05-20 15:43:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:43:58 | INFO | train_inner | epoch 037:     11 / 65 loss=2.529, nll_loss=0.557, ppl=1.47, wps=1413.4, ups=0.12, wpb=11888.1, bsz=566.8, num_updates=2340, lr=0, gnorm=1.555, clip=100, loss_scale=1, train_wall=91, gb_free=9.4, wall=13481
2023-05-20 15:44:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 15:45:36 | INFO | train_inner | epoch 037:     32 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=2472.9, ups=0.2, wpb=12160.1, bsz=563, num_updates=2360, lr=0, gnorm=1.612, clip=100, loss_scale=0.5, train_wall=98, gb_free=9, wall=13579
2023-05-20 15:47:12 | INFO | train_inner | epoch 037:     52 / 65 loss=2.509, nll_loss=0.537, ppl=1.45, wps=2520.9, ups=0.21, wpb=12111.5, bsz=553.1, num_updates=2380, lr=0, gnorm=2.452, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.4, wall=13675
2023-05-20 15:48:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:48:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:48:29 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1750.2 | wpb 2785 | bsz 105.2 | num_updates 2393 | best_loss 3.382
2023-05-20 15:48:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2393 updates
2023-05-20 15:48:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint37.pt
2023-05-20 15:48:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint37.pt
2023-05-20 15:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint37.pt (epoch 37 @ 2393 updates, score 3.469) (writing took 50.267698768526316 seconds)
2023-05-20 15:49:20 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2023-05-20 15:49:20 | INFO | train | epoch 037 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2062.2 | ups 0.17 | wpb 12056.3 | bsz 561.4 | num_updates 2393 | lr 0 | gnorm 1.802 | clip 100 | loss_scale 0.5 | train_wall 303 | gb_free 9.6 | wall 13803
2023-05-20 15:49:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:49:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:49:20 | INFO | fairseq.trainer | begin training epoch 38
2023-05-20 15:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:49:51 | INFO | train_inner | epoch 038:      7 / 65 loss=2.522, nll_loss=0.551, ppl=1.46, wps=1474.7, ups=0.13, wpb=11754.3, bsz=558, num_updates=2400, lr=0, gnorm=1.399, clip=100, loss_scale=0.5, train_wall=89, gb_free=10, wall=13834
2023-05-20 15:51:25 | INFO | train_inner | epoch 038:     27 / 65 loss=2.521, nll_loss=0.549, ppl=1.46, wps=2590.1, ups=0.21, wpb=12181.8, bsz=574, num_updates=2420, lr=0, gnorm=1.397, clip=100, loss_scale=0.5, train_wall=94, gb_free=9, wall=13928
2023-05-20 15:52:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 15:53:05 | INFO | train_inner | epoch 038:     48 / 65 loss=2.515, nll_loss=0.542, ppl=1.46, wps=2450.5, ups=0.2, wpb=12150.3, bsz=561.9, num_updates=2440, lr=0, gnorm=1.011, clip=100, loss_scale=0.25, train_wall=99, gb_free=9.9, wall=14028
2023-05-20 15:54:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:54:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:54:41 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1877.2 | wpb 2785 | bsz 105.2 | num_updates 2457 | best_loss 3.382
2023-05-20 15:54:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2457 updates
2023-05-20 15:54:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint38.pt
2023-05-20 15:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint38.pt
2023-05-20 15:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint38.pt (epoch 38 @ 2457 updates, score 3.469) (writing took 48.97889935225248 seconds)
2023-05-20 15:55:30 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2023-05-20 15:55:30 | INFO | train | epoch 038 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2082.4 | ups 0.17 | wpb 12049.2 | bsz 558.8 | num_updates 2457 | lr 0 | gnorm 1.2 | clip 100 | loss_scale 0.25 | train_wall 302 | gb_free 10.1 | wall 14173
2023-05-20 15:55:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:55:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 15:55:30 | INFO | fairseq.trainer | begin training epoch 39
2023-05-20 15:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:55:43 | INFO | train_inner | epoch 039:      3 / 65 loss=2.51, nll_loss=0.539, ppl=1.45, wps=1473.2, ups=0.13, wpb=11690.7, bsz=525.9, num_updates=2460, lr=0, gnorm=1.095, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.4, wall=14186
2023-05-20 15:57:14 | INFO | train_inner | epoch 039:     23 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2676.8, ups=0.22, wpb=12198.5, bsz=583.6, num_updates=2480, lr=0, gnorm=2.088, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.9, wall=14277
2023-05-20 15:58:50 | INFO | train_inner | epoch 039:     43 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2536.7, ups=0.21, wpb=12088.4, bsz=558.6, num_updates=2500, lr=0, gnorm=1.524, clip=100, loss_scale=0.25, train_wall=95, gb_free=10.1, wall=14373
2023-05-20 16:00:23 | INFO | train_inner | epoch 039:     63 / 65 loss=2.515, nll_loss=0.544, ppl=1.46, wps=2642.3, ups=0.21, wpb=12375.2, bsz=568.6, num_updates=2520, lr=0, gnorm=1.101, clip=100, loss_scale=0.25, train_wall=94, gb_free=9.6, wall=14466
2023-05-20 16:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:00:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:00:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1933.2 | wpb 2785 | bsz 105.2 | num_updates 2522 | best_loss 3.382
2023-05-20 16:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2522 updates
2023-05-20 16:00:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint39.pt
2023-05-20 16:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint39.pt
2023-05-20 16:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint39.pt (epoch 39 @ 2522 updates, score 3.469) (writing took 35.385514453053474 seconds)
2023-05-20 16:01:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2023-05-20 16:01:40 | INFO | train | epoch 039 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2120.5 | ups 0.18 | wpb 12054.7 | bsz 559.6 | num_updates 2522 | lr 0 | gnorm 1.628 | clip 100 | loss_scale 0.25 | train_wall 301 | gb_free 9.7 | wall 14543
2023-05-20 16:01:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:01:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:01:40 | INFO | fairseq.trainer | begin training epoch 40
2023-05-20 16:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:03:05 | INFO | train_inner | epoch 040:     18 / 65 loss=2.515, nll_loss=0.541, ppl=1.46, wps=1450.5, ups=0.12, wpb=11688.1, bsz=525.1, num_updates=2540, lr=0, gnorm=1.619, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.6, wall=14628
2023-05-20 16:04:33 | INFO | train_inner | epoch 040:     38 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2769.2, ups=0.23, wpb=12261.8, bsz=564.1, num_updates=2560, lr=0, gnorm=1.477, clip=100, loss_scale=0.25, train_wall=88, gb_free=10, wall=14716
2023-05-20 16:06:06 | INFO | train_inner | epoch 040:     58 / 65 loss=2.513, nll_loss=0.542, ppl=1.46, wps=2611.1, ups=0.21, wpb=12181.1, bsz=581.8, num_updates=2580, lr=0, gnorm=3.396, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.8, wall=14809
2023-05-20 16:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:06:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:06:56 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1857 | wpb 2785 | bsz 105.2 | num_updates 2587 | best_loss 3.382
2023-05-20 16:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2587 updates
2023-05-20 16:06:56 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint40.pt
2023-05-20 16:07:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint40.pt
2023-05-20 16:07:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint40.pt (epoch 40 @ 2587 updates, score 3.469) (writing took 57.96688625589013 seconds)
2023-05-20 16:07:54 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2023-05-20 16:07:54 | INFO | train | epoch 040 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2091.4 | ups 0.17 | wpb 12044.2 | bsz 558.3 | num_updates 2587 | lr 0 | gnorm 1.989 | clip 100 | loss_scale 0.25 | train_wall 297 | gb_free 10 | wall 14917
2023-05-20 16:07:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:07:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:07:54 | INFO | fairseq.trainer | begin training epoch 41
2023-05-20 16:07:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:08:55 | INFO | train_inner | epoch 041:     13 / 65 loss=2.512, nll_loss=0.539, ppl=1.45, wps=1396.5, ups=0.12, wpb=11788.6, bsz=540, num_updates=2600, lr=0, gnorm=1.652, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.5, wall=14978
2023-05-20 16:10:29 | INFO | train_inner | epoch 041:     33 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=2596.4, ups=0.21, wpb=12212.2, bsz=543.8, num_updates=2620, lr=0, gnorm=1.134, clip=100, loss_scale=0.25, train_wall=94, gb_free=9.1, wall=15072
2023-05-20 16:12:02 | INFO | train_inner | epoch 041:     53 / 65 loss=2.529, nll_loss=0.559, ppl=1.47, wps=2636.3, ups=0.22, wpb=12201.8, bsz=606.6, num_updates=2640, lr=0, gnorm=1.497, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.6, wall=15165
2023-05-20 16:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:12:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:13:15 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1965.4 | wpb 2785 | bsz 105.2 | num_updates 2652 | best_loss 3.382
2023-05-20 16:13:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2652 updates
2023-05-20 16:13:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint41.pt
2023-05-20 16:13:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint41.pt
2023-05-20 16:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint41.pt (epoch 41 @ 2652 updates, score 3.469) (writing took 61.59522194415331 seconds)
2023-05-20 16:14:16 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2023-05-20 16:14:16 | INFO | train | epoch 041 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2049.2 | ups 0.17 | wpb 12053.7 | bsz 559.7 | num_updates 2652 | lr 0 | gnorm 1.497 | clip 100 | loss_scale 0.25 | train_wall 302 | gb_free 9.6 | wall 15299
2023-05-20 16:14:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:14:16 | INFO | fairseq.trainer | begin training epoch 42
2023-05-20 16:14:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:14:54 | INFO | train_inner | epoch 042:      8 / 65 loss=2.508, nll_loss=0.536, ppl=1.45, wps=1335.9, ups=0.12, wpb=11508.8, bsz=516.8, num_updates=2660, lr=0, gnorm=1.618, clip=100, loss_scale=0.25, train_wall=92, gb_free=10.1, wall=15337
2023-05-20 16:16:26 | INFO | train_inner | epoch 042:     28 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2639, ups=0.22, wpb=12152.1, bsz=567, num_updates=2680, lr=0, gnorm=1.411, clip=100, loss_scale=0.25, train_wall=92, gb_free=8.5, wall=15429
2023-05-20 16:17:57 | INFO | train_inner | epoch 042:     48 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2696.5, ups=0.22, wpb=12267.6, bsz=576.6, num_updates=2700, lr=0, gnorm=1.114, clip=100, loss_scale=0.25, train_wall=91, gb_free=8.7, wall=15520
2023-05-20 16:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:19:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:19:34 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1917.6 | wpb 2785 | bsz 105.2 | num_updates 2717 | best_loss 3.382
2023-05-20 16:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2717 updates
2023-05-20 16:19:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint42.pt
2023-05-20 16:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint42.pt
2023-05-20 16:20:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint42.pt (epoch 42 @ 2717 updates, score 3.469) (writing took 27.900630187243223 seconds)
2023-05-20 16:20:01 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2023-05-20 16:20:01 | INFO | train | epoch 042 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2269.1 | ups 0.19 | wpb 12051.8 | bsz 559.6 | num_updates 2717 | lr 0 | gnorm 1.449 | clip 100 | loss_scale 0.25 | train_wall 298 | gb_free 9.7 | wall 15644
2023-05-20 16:20:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:20:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:20:02 | INFO | fairseq.trainer | begin training epoch 43
2023-05-20 16:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:20:15 | INFO | train_inner | epoch 043:      3 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=1715.4, ups=0.15, wpb=11803.2, bsz=537.8, num_updates=2720, lr=0, gnorm=1.92, clip=100, loss_scale=0.25, train_wall=91, gb_free=8.1, wall=15658
2023-05-20 16:21:46 | INFO | train_inner | epoch 043:     23 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2644, ups=0.22, wpb=12053.6, bsz=573.2, num_updates=2740, lr=0, gnorm=3.012, clip=100, loss_scale=0.25, train_wall=91, gb_free=9.3, wall=15749
2023-05-20 16:23:23 | INFO | train_inner | epoch 043:     43 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2551.5, ups=0.21, wpb=12321.3, bsz=578.8, num_updates=2760, lr=0, gnorm=2.138, clip=100, loss_scale=0.25, train_wall=96, gb_free=9.6, wall=15846
2023-05-20 16:24:54 | INFO | train_inner | epoch 043:     63 / 65 loss=2.509, nll_loss=0.536, ppl=1.45, wps=2658.2, ups=0.22, wpb=12193.7, bsz=557.1, num_updates=2780, lr=0, gnorm=1.528, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.6, wall=15937
2023-05-20 16:25:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:25:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:25:20 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1908.9 | wpb 2785 | bsz 105.2 | num_updates 2782 | best_loss 3.382
2023-05-20 16:25:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2782 updates
2023-05-20 16:25:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint43.pt
2023-05-20 16:25:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint43.pt
2023-05-20 16:25:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint43.pt (epoch 43 @ 2782 updates, score 3.469) (writing took 29.747819997370243 seconds)
2023-05-20 16:25:50 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2023-05-20 16:25:50 | INFO | train | epoch 043 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2247.1 | ups 0.19 | wpb 12046.1 | bsz 558.5 | num_updates 2782 | lr 0 | gnorm 2.228 | clip 100 | loss_scale 0.25 | train_wall 300 | gb_free 10.1 | wall 15993
2023-05-20 16:25:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:25:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:25:50 | INFO | fairseq.trainer | begin training epoch 44
2023-05-20 16:25:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:27:12 | INFO | train_inner | epoch 044:     18 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=1716.2, ups=0.15, wpb=11826.7, bsz=570.2, num_updates=2800, lr=0, gnorm=1.75, clip=100, loss_scale=0.25, train_wall=89, gb_free=9.2, wall=16075
2023-05-20 16:28:46 | INFO | train_inner | epoch 044:     38 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=2599.1, ups=0.21, wpb=12155.9, bsz=555.9, num_updates=2820, lr=0, gnorm=1.165, clip=100, loss_scale=0.25, train_wall=93, gb_free=9.8, wall=16169
2023-05-20 16:30:20 | INFO | train_inner | epoch 044:     58 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=2582.2, ups=0.21, wpb=12170.2, bsz=546.9, num_updates=2840, lr=0, gnorm=3.473, clip=100, loss_scale=0.25, train_wall=94, gb_free=9, wall=16263
2023-05-20 16:30:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:30:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:31:09 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1936.1 | wpb 2785 | bsz 105.2 | num_updates 2847 | best_loss 3.382
2023-05-20 16:31:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2847 updates
2023-05-20 16:31:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint44.pt
2023-05-20 16:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint44.pt
2023-05-20 16:31:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint44.pt (epoch 44 @ 2847 updates, score 3.469) (writing took 33.88676220551133 seconds)
2023-05-20 16:31:43 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2023-05-20 16:31:43 | INFO | train | epoch 044 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2216.1 | ups 0.18 | wpb 12040.4 | bsz 557 | num_updates 2847 | lr 0 | gnorm 2.042 | clip 100 | loss_scale 0.25 | train_wall 301 | gb_free 10.1 | wall 16346
2023-05-20 16:31:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:31:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:31:43 | INFO | fairseq.trainer | begin training epoch 45
2023-05-20 16:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:32:45 | INFO | train_inner | epoch 045:     13 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=1603.6, ups=0.14, wpb=11605.5, bsz=535.8, num_updates=2860, lr=0, gnorm=1.444, clip=100, loss_scale=0.25, train_wall=92, gb_free=8.8, wall=16408
2023-05-20 16:34:19 | INFO | train_inner | epoch 045:     33 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2577.2, ups=0.21, wpb=12189.8, bsz=581.8, num_updates=2880, lr=0, gnorm=1.698, clip=100, loss_scale=0.25, train_wall=94, gb_free=9.3, wall=16502
2023-05-20 16:35:52 | INFO | train_inner | epoch 045:     53 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2631.8, ups=0.22, wpb=12198.1, bsz=551.3, num_updates=2900, lr=0, gnorm=2.06, clip=100, loss_scale=0.25, train_wall=93, gb_free=8, wall=16595
2023-05-20 16:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:36:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:37:05 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1943.1 | wpb 2785 | bsz 105.2 | num_updates 2912 | best_loss 3.382
2023-05-20 16:37:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2912 updates
2023-05-20 16:37:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint45.pt
2023-05-20 16:37:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint45.pt
2023-05-20 16:37:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint45.pt (epoch 45 @ 2912 updates, score 3.469) (writing took 42.7092698328197 seconds)
2023-05-20 16:37:47 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2023-05-20 16:37:47 | INFO | train | epoch 045 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2152.6 | ups 0.18 | wpb 12059 | bsz 559.4 | num_updates 2912 | lr 0 | gnorm 1.728 | clip 100 | loss_scale 0.25 | train_wall 303 | gb_free 9.5 | wall 16710
2023-05-20 16:37:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:37:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:37:47 | INFO | fairseq.trainer | begin training epoch 46
2023-05-20 16:37:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:38:23 | INFO | train_inner | epoch 046:      8 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=1558.5, ups=0.13, wpb=11769.1, bsz=556.6, num_updates=2920, lr=0, gnorm=1.464, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.1, wall=16746
2023-05-20 16:39:55 | INFO | train_inner | epoch 046:     28 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2634.5, ups=0.22, wpb=12077.9, bsz=553.5, num_updates=2940, lr=0, gnorm=1.336, clip=100, loss_scale=0.25, train_wall=92, gb_free=9.4, wall=16838
2023-05-20 16:41:30 | INFO | train_inner | epoch 046:     48 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2571, ups=0.21, wpb=12184.2, bsz=567.5, num_updates=2960, lr=0, gnorm=1.938, clip=100, loss_scale=0.5, train_wall=95, gb_free=8.4, wall=16933
2023-05-20 16:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:42:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:43:09 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1921 | wpb 2785 | bsz 105.2 | num_updates 2977 | best_loss 3.382
2023-05-20 16:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2977 updates
2023-05-20 16:43:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint46.pt
2023-05-20 16:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint46.pt
2023-05-20 16:43:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint46.pt (epoch 46 @ 2977 updates, score 3.469) (writing took 32.525781743228436 seconds)
2023-05-20 16:43:42 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2023-05-20 16:43:42 | INFO | train | epoch 046 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2206.5 | ups 0.18 | wpb 12041.7 | bsz 559 | num_updates 2977 | lr 0 | gnorm 1.634 | clip 100 | loss_scale 0.5 | train_wall 304 | gb_free 9.5 | wall 17065
2023-05-20 16:43:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:43:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:43:42 | INFO | fairseq.trainer | begin training epoch 47
2023-05-20 16:43:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:43:56 | INFO | train_inner | epoch 047:      3 / 65 loss=2.515, nll_loss=0.542, ppl=1.46, wps=1607.9, ups=0.14, wpb=11811.4, bsz=529.6, num_updates=2980, lr=0, gnorm=1.574, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.3, wall=17079
2023-05-20 16:45:27 | INFO | train_inner | epoch 047:     23 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2673.6, ups=0.22, wpb=12164.1, bsz=554, num_updates=3000, lr=0, gnorm=1.549, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.2, wall=17170
2023-05-20 16:47:02 | INFO | train_inner | epoch 047:     43 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2601.7, ups=0.21, wpb=12240.1, bsz=579.6, num_updates=3020, lr=0, gnorm=1.79, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.1, wall=17265
2023-05-20 16:48:33 | INFO | train_inner | epoch 047:     63 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2687.6, ups=0.22, wpb=12270.8, bsz=574.8, num_updates=3040, lr=0, gnorm=1.221, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.7, wall=17356
2023-05-20 16:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:48:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:48:58 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1941.5 | wpb 2785 | bsz 105.2 | num_updates 3042 | best_loss 3.382
2023-05-20 16:48:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3042 updates
2023-05-20 16:48:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint47.pt
2023-05-20 16:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint47.pt
2023-05-20 16:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint47.pt (epoch 47 @ 3042 updates, score 3.469) (writing took 23.30453696101904 seconds)
2023-05-20 16:49:22 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2023-05-20 16:49:22 | INFO | train | epoch 047 | loss 2.518 | nll_loss 0.547 | ppl 1.46 | wps 2304.7 | ups 0.19 | wpb 12059.9 | bsz 559.5 | num_updates 3042 | lr 0 | gnorm 1.506 | clip 100 | loss_scale 0.5 | train_wall 297 | gb_free 9.3 | wall 17405
2023-05-20 16:49:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:49:22 | INFO | fairseq.trainer | begin training epoch 48
2023-05-20 16:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:50:45 | INFO | train_inner | epoch 048:     18 / 65 loss=2.522, nll_loss=0.551, ppl=1.46, wps=1778.5, ups=0.15, wpb=11755.7, bsz=551.6, num_updates=3060, lr=0, gnorm=1.647, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.7, wall=17488
2023-05-20 16:52:21 | INFO | train_inner | epoch 048:     38 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=2532.5, ups=0.21, wpb=12106.2, bsz=527.9, num_updates=3080, lr=0, gnorm=1.383, clip=100, loss_scale=0.5, train_wall=95, gb_free=8.7, wall=17584
2023-05-20 16:53:53 | INFO | train_inner | epoch 048:     58 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2648.2, ups=0.22, wpb=12190.1, bsz=586.2, num_updates=3100, lr=0, gnorm=1.932, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.5, wall=17676
2023-05-20 16:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:54:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:54:40 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1945.9 | wpb 2785 | bsz 105.2 | num_updates 3107 | best_loss 3.382
2023-05-20 16:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3107 updates
2023-05-20 16:54:40 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint48.pt
2023-05-20 16:54:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint48.pt
2023-05-20 16:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint48.pt (epoch 48 @ 3107 updates, score 3.469) (writing took 26.19791942089796 seconds)
2023-05-20 16:55:06 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2023-05-20 16:55:06 | INFO | train | epoch 048 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2277.8 | ups 0.19 | wpb 12056.3 | bsz 559.5 | num_updates 3107 | lr 0 | gnorm 1.607 | clip 100 | loss_scale 0.5 | train_wall 299 | gb_free 9.6 | wall 17749
2023-05-20 16:55:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:55:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 16:55:06 | INFO | fairseq.trainer | begin training epoch 49
2023-05-20 16:55:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:56:04 | INFO | train_inner | epoch 049:     13 / 65 loss=2.52, nll_loss=0.547, ppl=1.46, wps=1796.9, ups=0.15, wpb=11761.5, bsz=554.2, num_updates=3120, lr=0, gnorm=1.192, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.5, wall=17807
2023-05-20 16:57:39 | INFO | train_inner | epoch 049:     33 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2547.7, ups=0.21, wpb=12126.2, bsz=571.7, num_updates=3140, lr=0, gnorm=1.855, clip=100, loss_scale=0.5, train_wall=95, gb_free=9.5, wall=17902
2023-05-20 16:59:12 | INFO | train_inner | epoch 049:     53 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2626.9, ups=0.21, wpb=12223.4, bsz=555, num_updates=3160, lr=0, gnorm=1.349, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.2, wall=17995
2023-05-20 17:00:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:00:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:00:24 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1848.4 | wpb 2785 | bsz 105.2 | num_updates 3172 | best_loss 3.382
2023-05-20 17:00:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3172 updates
2023-05-20 17:00:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint49.pt
2023-05-20 17:00:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint49.pt
2023-05-20 17:00:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint49.pt (epoch 49 @ 3172 updates, score 3.469) (writing took 27.931510381400585 seconds)
2023-05-20 17:00:52 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2023-05-20 17:00:52 | INFO | train | epoch 049 | loss 2.516 | nll_loss 0.544 | ppl 1.46 | wps 2263 | ups 0.19 | wpb 12044.9 | bsz 557.7 | num_updates 3172 | lr 0 | gnorm 1.453 | clip 100 | loss_scale 0.5 | train_wall 299 | gb_free 9.3 | wall 18095
2023-05-20 17:00:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:00:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:00:52 | INFO | fairseq.trainer | begin training epoch 50
2023-05-20 17:00:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:01:28 | INFO | train_inner | epoch 050:      8 / 65 loss=2.516, nll_loss=0.545, ppl=1.46, wps=1737.6, ups=0.15, wpb=11829.8, bsz=549.5, num_updates=3180, lr=0, gnorm=1.852, clip=100, loss_scale=0.5, train_wall=89, gb_free=9.6, wall=18131
2023-05-20 17:03:06 | INFO | train_inner | epoch 050:     28 / 65 loss=2.508, nll_loss=0.537, ppl=1.45, wps=2508.2, ups=0.2, wpb=12249.9, bsz=555.9, num_updates=3200, lr=0, gnorm=1.152, clip=100, loss_scale=0.5, train_wall=98, gb_free=9.6, wall=18229
2023-05-20 17:04:36 | INFO | train_inner | epoch 050:     48 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=2712.3, ups=0.22, wpb=12186.8, bsz=576, num_updates=3220, lr=0, gnorm=2.555, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.3, wall=18319
2023-05-20 17:05:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:05:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:06:08 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1834.5 | wpb 2785 | bsz 105.2 | num_updates 3237 | best_loss 3.382
2023-05-20 17:06:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3237 updates
2023-05-20 17:06:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint50.pt
2023-05-20 17:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint50.pt
2023-05-20 17:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint50.pt (epoch 50 @ 3237 updates, score 3.469) (writing took 33.41195373982191 seconds)
2023-05-20 17:06:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2023-05-20 17:06:41 | INFO | train | epoch 050 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2243.5 | ups 0.19 | wpb 12056.7 | bsz 559.7 | num_updates 3237 | lr 0 | gnorm 1.858 | clip 100 | loss_scale 0.5 | train_wall 296 | gb_free 9.6 | wall 18444
2023-05-20 17:06:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:06:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:06:41 | INFO | fairseq.trainer | begin training epoch 51
2023-05-20 17:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:06:54 | INFO | train_inner | epoch 051:      3 / 65 loss=2.518, nll_loss=0.545, ppl=1.46, wps=1678.7, ups=0.14, wpb=11654.7, bsz=540.9, num_updates=3240, lr=0, gnorm=1.39, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.8, wall=18457
2023-05-20 17:08:26 | INFO | train_inner | epoch 051:     23 / 65 loss=2.522, nll_loss=0.551, ppl=1.47, wps=2649.9, ups=0.22, wpb=12190.9, bsz=576.5, num_updates=3260, lr=0, gnorm=1.288, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.2, wall=18549
2023-05-20 17:10:01 | INFO | train_inner | epoch 051:     43 / 65 loss=2.516, nll_loss=0.543, ppl=1.46, wps=2596.8, ups=0.21, wpb=12274, bsz=552.2, num_updates=3280, lr=0, gnorm=1.65, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.5, wall=18644
2023-05-20 17:11:36 | INFO | train_inner | epoch 051:     63 / 65 loss=2.511, nll_loss=0.539, ppl=1.45, wps=2557, ups=0.21, wpb=12087.6, bsz=565.5, num_updates=3300, lr=0, gnorm=1.998, clip=100, loss_scale=0.5, train_wall=94, gb_free=8.7, wall=18739
2023-05-20 17:11:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:11:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:12:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1945.6 | wpb 2785 | bsz 105.2 | num_updates 3302 | best_loss 3.382
2023-05-20 17:12:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3302 updates
2023-05-20 17:12:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint51.pt
2023-05-20 17:12:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint51.pt
2023-05-20 17:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint51.pt (epoch 51 @ 3302 updates, score 3.469) (writing took 20.794447645545006 seconds)
2023-05-20 17:12:22 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2023-05-20 17:12:22 | INFO | train | epoch 051 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2295.2 | ups 0.19 | wpb 12042.7 | bsz 558.3 | num_updates 3302 | lr 0 | gnorm 1.617 | clip 100 | loss_scale 0.5 | train_wall 302 | gb_free 10.1 | wall 18785
2023-05-20 17:12:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:12:23 | INFO | fairseq.trainer | begin training epoch 52
2023-05-20 17:12:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:13:49 | INFO | train_inner | epoch 052:     18 / 65 loss=2.505, nll_loss=0.533, ppl=1.45, wps=1742.5, ups=0.15, wpb=11643.1, bsz=522.8, num_updates=3320, lr=0, gnorm=1.818, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.2, wall=18872
2023-05-20 17:15:22 | INFO | train_inner | epoch 052:     38 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2600.9, ups=0.21, wpb=12121.5, bsz=551, num_updates=3340, lr=0, gnorm=1.456, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.1, wall=18965
2023-05-20 17:16:58 | INFO | train_inner | epoch 052:     58 / 65 loss=2.523, nll_loss=0.551, ppl=1.47, wps=2561, ups=0.21, wpb=12245.9, bsz=575.2, num_updates=3360, lr=0, gnorm=1.328, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.8, wall=19061
2023-05-20 17:17:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:17:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:17:45 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1954.4 | wpb 2785 | bsz 105.2 | num_updates 3367 | best_loss 3.382
2023-05-20 17:17:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3367 updates
2023-05-20 17:17:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint52.pt
2023-05-20 17:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint52.pt
2023-05-20 17:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint52.pt (epoch 52 @ 3367 updates, score 3.469) (writing took 26.236405558884144 seconds)
2023-05-20 17:18:12 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2023-05-20 17:18:12 | INFO | train | epoch 052 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2244.7 | ups 0.19 | wpb 12054.4 | bsz 559 | num_updates 3367 | lr 0 | gnorm 1.522 | clip 100 | loss_scale 0.5 | train_wall 304 | gb_free 10 | wall 19135
2023-05-20 17:18:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:18:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:18:12 | INFO | fairseq.trainer | begin training epoch 53
2023-05-20 17:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:19:14 | INFO | train_inner | epoch 053:     13 / 65 loss=2.532, nll_loss=0.561, ppl=1.48, wps=1762.6, ups=0.15, wpb=11973.2, bsz=592.6, num_updates=3380, lr=0, gnorm=1.459, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.2, wall=19197
2023-05-20 17:20:48 | INFO | train_inner | epoch 053:     33 / 65 loss=2.508, nll_loss=0.536, ppl=1.45, wps=2576, ups=0.21, wpb=12136.9, bsz=529.1, num_updates=3400, lr=0, gnorm=1.617, clip=100, loss_scale=0.5, train_wall=94, gb_free=8.9, wall=19291
2023-05-20 17:22:21 | INFO | train_inner | epoch 053:     53 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2645, ups=0.22, wpb=12230.5, bsz=578.4, num_updates=3420, lr=0, gnorm=1.992, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.8, wall=19384
2023-05-20 17:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:23:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:23:32 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1848.7 | wpb 2785 | bsz 105.2 | num_updates 3432 | best_loss 3.382
2023-05-20 17:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 3432 updates
2023-05-20 17:23:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint53.pt
2023-05-20 17:23:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint53.pt
2023-05-20 17:23:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint53.pt (epoch 53 @ 3432 updates, score 3.469) (writing took 20.82576386258006 seconds)
2023-05-20 17:23:53 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2023-05-20 17:23:53 | INFO | train | epoch 053 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2294.4 | ups 0.19 | wpb 12054.2 | bsz 558.7 | num_updates 3432 | lr 0 | gnorm 1.809 | clip 100 | loss_scale 0.5 | train_wall 301 | gb_free 9.6 | wall 19476
2023-05-20 17:23:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:23:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:23:53 | INFO | fairseq.trainer | begin training epoch 54
2023-05-20 17:23:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:24:31 | INFO | train_inner | epoch 054:      8 / 65 loss=2.507, nll_loss=0.533, ppl=1.45, wps=1778, ups=0.15, wpb=11573.9, bsz=538.2, num_updates=3440, lr=0, gnorm=1.804, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.8, wall=19514
2023-05-20 17:26:04 | INFO | train_inner | epoch 054:     28 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2607.1, ups=0.22, wpb=12117.2, bsz=558.7, num_updates=3460, lr=0, gnorm=1.999, clip=100, loss_scale=1, train_wall=93, gb_free=9.9, wall=19607
2023-05-20 17:27:36 | INFO | train_inner | epoch 054:     48 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2632.5, ups=0.22, wpb=12177.2, bsz=583.6, num_updates=3480, lr=0, gnorm=1.313, clip=100, loss_scale=1, train_wall=92, gb_free=9.5, wall=19699
2023-05-20 17:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:28:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:29:11 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1931.9 | wpb 2785 | bsz 105.2 | num_updates 3497 | best_loss 3.382
2023-05-20 17:29:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 3497 updates
2023-05-20 17:29:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint54.pt
2023-05-20 17:29:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint54.pt
2023-05-20 17:29:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint54.pt (epoch 54 @ 3497 updates, score 3.469) (writing took 37.44280003011227 seconds)
2023-05-20 17:29:49 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2023-05-20 17:29:49 | INFO | train | epoch 054 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2202.3 | ups 0.18 | wpb 12057.9 | bsz 560.1 | num_updates 3497 | lr 0 | gnorm 1.612 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 10.1 | wall 19832
2023-05-20 17:29:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:29:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:29:49 | INFO | fairseq.trainer | begin training epoch 55
2023-05-20 17:29:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:30:04 | INFO | train_inner | epoch 055:      3 / 65 loss=2.524, nll_loss=0.552, ppl=1.47, wps=1610.8, ups=0.14, wpb=11927.8, bsz=545.1, num_updates=3500, lr=0, gnorm=1.592, clip=100, loss_scale=1, train_wall=92, gb_free=9.6, wall=19847
2023-05-20 17:31:39 | INFO | train_inner | epoch 055:     23 / 65 loss=2.508, nll_loss=0.536, ppl=1.45, wps=2570.7, ups=0.21, wpb=12224.7, bsz=555.1, num_updates=3520, lr=0, gnorm=1.174, clip=100, loss_scale=1, train_wall=95, gb_free=9.8, wall=19942
2023-05-20 17:33:10 | INFO | train_inner | epoch 055:     43 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2694.8, ups=0.22, wpb=12134.1, bsz=570.6, num_updates=3540, lr=0, gnorm=1.317, clip=100, loss_scale=1, train_wall=90, gb_free=9.8, wall=20033
2023-05-20 17:34:40 | INFO | train_inner | epoch 055:     63 / 65 loss=2.524, nll_loss=0.553, ppl=1.47, wps=2709.6, ups=0.22, wpb=12224.9, bsz=570.2, num_updates=3560, lr=0, gnorm=1.468, clip=100, loss_scale=1, train_wall=90, gb_free=9.5, wall=20123
2023-05-20 17:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:34:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:35:07 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1809.7 | wpb 2785 | bsz 105.2 | num_updates 3562 | best_loss 3.382
2023-05-20 17:35:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 3562 updates
2023-05-20 17:35:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint55.pt
2023-05-20 17:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint55.pt
2023-05-20 17:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint55.pt (epoch 55 @ 3562 updates, score 3.469) (writing took 32.160637222230434 seconds)
2023-05-20 17:35:39 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2023-05-20 17:35:39 | INFO | train | epoch 055 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2237.2 | ups 0.19 | wpb 12053.6 | bsz 558.5 | num_updates 3562 | lr 0 | gnorm 1.401 | clip 100 | loss_scale 1 | train_wall 298 | gb_free 10 | wall 20182
2023-05-20 17:35:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:35:39 | INFO | fairseq.trainer | begin training epoch 56
2023-05-20 17:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:37:03 | INFO | train_inner | epoch 056:     18 / 65 loss=2.506, nll_loss=0.532, ppl=1.45, wps=1647.2, ups=0.14, wpb=11773.1, bsz=538.8, num_updates=3580, lr=0, gnorm=1.509, clip=100, loss_scale=1, train_wall=91, gb_free=9.3, wall=20266
2023-05-20 17:38:35 | INFO | train_inner | epoch 056:     38 / 65 loss=2.529, nll_loss=0.559, ppl=1.47, wps=2682.1, ups=0.22, wpb=12332.9, bsz=594.1, num_updates=3600, lr=0, gnorm=1.287, clip=100, loss_scale=1, train_wall=92, gb_free=9.4, wall=20358
2023-05-20 17:40:08 | INFO | train_inner | epoch 056:     58 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2604.5, ups=0.21, wpb=12173.5, bsz=551.2, num_updates=3620, lr=0, gnorm=1.564, clip=100, loss_scale=1, train_wall=93, gb_free=9.8, wall=20451
2023-05-20 17:40:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:40:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:40:58 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1928 | wpb 2785 | bsz 105.2 | num_updates 3627 | best_loss 3.382
2023-05-20 17:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 3627 updates
2023-05-20 17:40:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint56.pt
2023-05-20 17:41:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint56.pt
2023-05-20 17:41:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint56.pt (epoch 56 @ 3627 updates, score 3.469) (writing took 37.174254182726145 seconds)
2023-05-20 17:41:35 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2023-05-20 17:41:35 | INFO | train | epoch 056 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2203.1 | ups 0.18 | wpb 12055.3 | bsz 560 | num_updates 3627 | lr 0 | gnorm 1.342 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.6 | wall 20538
2023-05-20 17:41:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:41:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:41:35 | INFO | fairseq.trainer | begin training epoch 57
2023-05-20 17:41:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:42:35 | INFO | train_inner | epoch 057:     13 / 65 loss=2.527, nll_loss=0.557, ppl=1.47, wps=1590.6, ups=0.14, wpb=11716.2, bsz=564.7, num_updates=3640, lr=0, gnorm=2.902, clip=100, loss_scale=1, train_wall=92, gb_free=9.7, wall=20598
2023-05-20 17:44:10 | INFO | train_inner | epoch 057:     33 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2589.2, ups=0.21, wpb=12231, bsz=562.7, num_updates=3660, lr=0, gnorm=1.193, clip=100, loss_scale=1, train_wall=94, gb_free=9.2, wall=20693
2023-05-20 17:45:40 | INFO | train_inner | epoch 057:     53 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2672.2, ups=0.22, wpb=12082.2, bsz=564.1, num_updates=3680, lr=0, gnorm=1.277, clip=100, loss_scale=1, train_wall=90, gb_free=9.1, wall=20783
2023-05-20 17:46:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:46:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:46:53 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1939.4 | wpb 2785 | bsz 105.2 | num_updates 3692 | best_loss 3.382
2023-05-20 17:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 3692 updates
2023-05-20 17:46:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint57.pt
2023-05-20 17:47:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint57.pt
2023-05-20 17:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint57.pt (epoch 57 @ 3692 updates, score 3.469) (writing took 36.48453191295266 seconds)
2023-05-20 17:47:30 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2023-05-20 17:47:30 | INFO | train | epoch 057 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2206.5 | ups 0.18 | wpb 12056.3 | bsz 559.9 | num_updates 3692 | lr 0 | gnorm 1.763 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.4 | wall 20893
2023-05-20 17:47:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:47:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:47:30 | INFO | fairseq.trainer | begin training epoch 58
2023-05-20 17:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:48:04 | INFO | train_inner | epoch 058:      8 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=1636.4, ups=0.14, wpb=11757.2, bsz=553.1, num_updates=3700, lr=0, gnorm=1.339, clip=100, loss_scale=1, train_wall=89, gb_free=10.1, wall=20927
2023-05-20 17:49:44 | INFO | train_inner | epoch 058:     28 / 65 loss=2.505, nll_loss=0.532, ppl=1.45, wps=2415.3, ups=0.2, wpb=12021.5, bsz=551.9, num_updates=3720, lr=0, gnorm=1.754, clip=100, loss_scale=1, train_wall=99, gb_free=9.6, wall=21027
2023-05-20 17:51:19 | INFO | train_inner | epoch 058:     48 / 65 loss=2.519, nll_loss=0.546, ppl=1.46, wps=2575.8, ups=0.21, wpb=12216, bsz=556.9, num_updates=3740, lr=0, gnorm=2.174, clip=100, loss_scale=1, train_wall=95, gb_free=9.6, wall=21121
2023-05-20 17:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:52:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:52:54 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1951.2 | wpb 2785 | bsz 105.2 | num_updates 3757 | best_loss 3.382
2023-05-20 17:52:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 3757 updates
2023-05-20 17:52:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint58.pt
2023-05-20 17:53:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint58.pt
2023-05-20 17:53:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint58.pt (epoch 58 @ 3757 updates, score 3.469) (writing took 54.217948105186224 seconds)
2023-05-20 17:53:48 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2023-05-20 17:53:48 | INFO | train | epoch 058 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2070.1 | ups 0.17 | wpb 12050.4 | bsz 559.6 | num_updates 3757 | lr 0 | gnorm 1.788 | clip 100 | loss_scale 1 | train_wall 305 | gb_free 9.8 | wall 21271
2023-05-20 17:53:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:53:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 17:53:48 | INFO | fairseq.trainer | begin training epoch 59
2023-05-20 17:53:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:54:03 | INFO | train_inner | epoch 059:      3 / 65 loss=2.519, nll_loss=0.548, ppl=1.46, wps=1440.7, ups=0.12, wpb=11828.6, bsz=551.2, num_updates=3760, lr=0, gnorm=1.515, clip=100, loss_scale=1, train_wall=92, gb_free=9.9, wall=21286
2023-05-20 17:55:38 | INFO | train_inner | epoch 059:     23 / 65 loss=2.517, nll_loss=0.544, ppl=1.46, wps=2523.3, ups=0.21, wpb=12043.4, bsz=531.6, num_updates=3780, lr=0, gnorm=1.281, clip=100, loss_scale=1, train_wall=95, gb_free=9.5, wall=21381
2023-05-20 17:57:11 | INFO | train_inner | epoch 059:     43 / 65 loss=2.516, nll_loss=0.545, ppl=1.46, wps=2604.8, ups=0.22, wpb=12064.8, bsz=553.2, num_updates=3800, lr=0, gnorm=1.253, clip=100, loss_scale=1, train_wall=93, gb_free=8.8, wall=21474
2023-05-20 17:58:41 | INFO | train_inner | epoch 059:     63 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2739.5, ups=0.22, wpb=12394.1, bsz=599.4, num_updates=3820, lr=0, gnorm=2.886, clip=100, loss_scale=1, train_wall=90, gb_free=9.8, wall=21564
2023-05-20 17:58:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:58:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:59:09 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1775.6 | wpb 2785 | bsz 105.2 | num_updates 3822 | best_loss 3.382
2023-05-20 17:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 3822 updates
2023-05-20 17:59:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint59.pt
2023-05-20 17:59:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint59.pt
2023-05-20 18:00:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint59.pt (epoch 59 @ 3822 updates, score 3.469) (writing took 95.62439522147179 seconds)
2023-05-20 18:00:44 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2023-05-20 18:00:44 | INFO | train | epoch 059 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 1882.2 | ups 0.16 | wpb 12047.8 | bsz 558.5 | num_updates 3822 | lr 0 | gnorm 1.769 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.4 | wall 21687
2023-05-20 18:00:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:00:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:00:45 | INFO | fairseq.trainer | begin training epoch 60
2023-05-20 18:00:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:02:09 | INFO | train_inner | epoch 060:     18 / 65 loss=2.513, nll_loss=0.54, ppl=1.45, wps=1136.2, ups=0.1, wpb=11788.6, bsz=540.9, num_updates=3840, lr=0, gnorm=1.301, clip=100, loss_scale=1, train_wall=92, gb_free=9.2, wall=21772
2023-05-20 18:03:45 | INFO | train_inner | epoch 060:     38 / 65 loss=2.531, nll_loss=0.561, ppl=1.47, wps=2568.8, ups=0.21, wpb=12366.8, bsz=588.8, num_updates=3860, lr=0, gnorm=1.697, clip=100, loss_scale=1, train_wall=96, gb_free=8.9, wall=21868
2023-05-20 18:05:19 | INFO | train_inner | epoch 060:     58 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=2572.1, ups=0.21, wpb=12052.7, bsz=554.6, num_updates=3880, lr=0, gnorm=1.151, clip=100, loss_scale=1, train_wall=94, gb_free=9.3, wall=21962
2023-05-20 18:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:06:08 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1957.5 | wpb 2785 | bsz 105.2 | num_updates 3887 | best_loss 3.382
2023-05-20 18:06:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 3887 updates
2023-05-20 18:06:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint60.pt
2023-05-20 18:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint60.pt
2023-05-20 18:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint60.pt (epoch 60 @ 3887 updates, score 3.469) (writing took 19.051403272897005 seconds)
2023-05-20 18:06:27 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2023-05-20 18:06:27 | INFO | train | epoch 060 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2289.8 | ups 0.19 | wpb 12056.3 | bsz 559.7 | num_updates 3887 | lr 0 | gnorm 1.512 | clip 100 | loss_scale 1 | train_wall 304 | gb_free 9.4 | wall 22030
2023-05-20 18:06:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:06:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:06:27 | INFO | fairseq.trainer | begin training epoch 61
2023-05-20 18:06:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:07:26 | INFO | train_inner | epoch 061:     13 / 65 loss=2.51, nll_loss=0.538, ppl=1.45, wps=1835.7, ups=0.16, wpb=11663.8, bsz=531.9, num_updates=3900, lr=0, gnorm=1.697, clip=100, loss_scale=1, train_wall=90, gb_free=10.1, wall=22089
2023-05-20 18:09:00 | INFO | train_inner | epoch 061:     33 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2612.5, ups=0.21, wpb=12256, bsz=559.6, num_updates=3920, lr=0, gnorm=1.621, clip=100, loss_scale=1, train_wall=94, gb_free=9.2, wall=22183
2023-05-20 18:10:33 | INFO | train_inner | epoch 061:     53 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2594, ups=0.21, wpb=12097.7, bsz=563.8, num_updates=3940, lr=0, gnorm=1.052, clip=100, loss_scale=1, train_wall=93, gb_free=9.2, wall=22276
2023-05-20 18:11:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:11:46 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1936 | wpb 2785 | bsz 105.2 | num_updates 3952 | best_loss 3.382
2023-05-20 18:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3952 updates
2023-05-20 18:11:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint61.pt
2023-05-20 18:11:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint61.pt
2023-05-20 18:12:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint61.pt (epoch 61 @ 3952 updates, score 3.469) (writing took 19.273387268185616 seconds)
2023-05-20 18:12:05 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2023-05-20 18:12:05 | INFO | train | epoch 061 | loss 2.516 | nll_loss 0.543 | ppl 1.46 | wps 2316.3 | ups 0.19 | wpb 12051.7 | bsz 557.1 | num_updates 3952 | lr 0 | gnorm 1.324 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.8 | wall 22368
2023-05-20 18:12:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:12:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:12:05 | INFO | fairseq.trainer | begin training epoch 62
2023-05-20 18:12:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:12:42 | INFO | train_inner | epoch 062:      8 / 65 loss=2.516, nll_loss=0.543, ppl=1.46, wps=1832.3, ups=0.15, wpb=11862, bsz=538.8, num_updates=3960, lr=0, gnorm=1.29, clip=100, loss_scale=1, train_wall=92, gb_free=9.4, wall=22405
2023-05-20 18:14:14 | INFO | train_inner | epoch 062:     28 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2672.6, ups=0.22, wpb=12246.7, bsz=585.8, num_updates=3980, lr=0, gnorm=1.224, clip=100, loss_scale=2, train_wall=92, gb_free=9.5, wall=22497
2023-05-20 18:15:46 | INFO | train_inner | epoch 062:     48 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2633.5, ups=0.22, wpb=12130, bsz=553.9, num_updates=4000, lr=0, gnorm=1.205, clip=100, loss_scale=2, train_wall=92, gb_free=9.8, wall=22589
2023-05-20 18:16:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 18:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:17:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:17:24 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1924.3 | wpb 2785 | bsz 105.2 | num_updates 4016 | best_loss 3.382
2023-05-20 18:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4016 updates
2023-05-20 18:17:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint62.pt
2023-05-20 18:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint62.pt
2023-05-20 18:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint62.pt (epoch 62 @ 4016 updates, score 3.469) (writing took 18.990856297314167 seconds)
2023-05-20 18:17:43 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2023-05-20 18:17:43 | INFO | train | epoch 062 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2282.4 | ups 0.19 | wpb 12050.8 | bsz 557.7 | num_updates 4016 | lr 0 | gnorm 1.201 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.8 | wall 22706
2023-05-20 18:17:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:17:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:17:43 | INFO | fairseq.trainer | begin training epoch 63
2023-05-20 18:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:17:59 | INFO | train_inner | epoch 063:      4 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=1766.7, ups=0.15, wpb=11766.2, bsz=550.5, num_updates=4020, lr=0, gnorm=1.177, clip=100, loss_scale=1, train_wall=96, gb_free=9.1, wall=22722
2023-05-20 18:19:31 | INFO | train_inner | epoch 063:     24 / 65 loss=2.504, nll_loss=0.532, ppl=1.45, wps=2607.5, ups=0.22, wpb=11963.9, bsz=551, num_updates=4040, lr=0, gnorm=1.071, clip=100, loss_scale=1, train_wall=92, gb_free=9.8, wall=22814
2023-05-20 18:21:08 | INFO | train_inner | epoch 063:     44 / 65 loss=2.526, nll_loss=0.555, ppl=1.47, wps=2536.6, ups=0.21, wpb=12299.6, bsz=570.6, num_updates=4060, lr=0, gnorm=1.682, clip=100, loss_scale=1, train_wall=97, gb_free=9.4, wall=22911
2023-05-20 18:22:42 | INFO | train_inner | epoch 063:     64 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2615.9, ups=0.21, wpb=12247.8, bsz=570.5, num_updates=4080, lr=0, gnorm=1.272, clip=100, loss_scale=1, train_wall=94, gb_free=9.6, wall=23005
2023-05-20 18:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:23:05 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1764.7 | wpb 2785 | bsz 105.2 | num_updates 4081 | best_loss 3.382
2023-05-20 18:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 4081 updates
2023-05-20 18:23:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint63.pt
2023-05-20 18:23:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint63.pt
2023-05-20 18:23:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint63.pt (epoch 63 @ 4081 updates, score 3.469) (writing took 20.269292403012514 seconds)
2023-05-20 18:23:25 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2023-05-20 18:23:25 | INFO | train | epoch 063 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2290.6 | ups 0.19 | wpb 12057.1 | bsz 559.1 | num_updates 4081 | lr 0 | gnorm 1.332 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 9.5 | wall 23048
2023-05-20 18:23:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:23:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:23:25 | INFO | fairseq.trainer | begin training epoch 64
2023-05-20 18:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:24:51 | INFO | train_inner | epoch 064:     19 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=1833.2, ups=0.16, wpb=11801.2, bsz=541.6, num_updates=4100, lr=0, gnorm=2.05, clip=100, loss_scale=1, train_wall=88, gb_free=9.7, wall=23134
2023-05-20 18:26:23 | INFO | train_inner | epoch 064:     39 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=2632.8, ups=0.22, wpb=12182.3, bsz=571, num_updates=4120, lr=0, gnorm=1.172, clip=100, loss_scale=1, train_wall=92, gb_free=8.1, wall=23226
2023-05-20 18:27:58 | INFO | train_inner | epoch 064:     59 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2562.8, ups=0.21, wpb=12160.1, bsz=569.6, num_updates=4140, lr=0, gnorm=1.903, clip=100, loss_scale=1, train_wall=95, gb_free=9.8, wall=23321
2023-05-20 18:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:28:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:28:43 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1775.4 | wpb 2785 | bsz 105.2 | num_updates 4146 | best_loss 3.382
2023-05-20 18:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 4146 updates
2023-05-20 18:28:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint64.pt
2023-05-20 18:28:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint64.pt
2023-05-20 18:29:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint64.pt (epoch 64 @ 4146 updates, score 3.469) (writing took 21.213911343365908 seconds)
2023-05-20 18:29:04 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2023-05-20 18:29:04 | INFO | train | epoch 064 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2306.8 | ups 0.19 | wpb 12051.9 | bsz 559.4 | num_updates 4146 | lr 0 | gnorm 1.653 | clip 100 | loss_scale 1 | train_wall 298 | gb_free 10 | wall 23387
2023-05-20 18:29:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:29:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:29:05 | INFO | fairseq.trainer | begin training epoch 65
2023-05-20 18:29:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:30:09 | INFO | train_inner | epoch 065:     14 / 65 loss=2.524, nll_loss=0.552, ppl=1.47, wps=1812, ups=0.15, wpb=11848.8, bsz=538.8, num_updates=4160, lr=0, gnorm=1.321, clip=100, loss_scale=1, train_wall=90, gb_free=9.6, wall=23452
2023-05-20 18:31:46 | INFO | train_inner | epoch 065:     34 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2532.3, ups=0.21, wpb=12273, bsz=585.8, num_updates=4180, lr=0, gnorm=2.047, clip=100, loss_scale=1, train_wall=97, gb_free=9.4, wall=23549
2023-05-20 18:33:16 | INFO | train_inner | epoch 065:     54 / 65 loss=2.516, nll_loss=0.545, ppl=1.46, wps=2684.7, ups=0.22, wpb=12128.1, bsz=559.6, num_updates=4200, lr=0, gnorm=1.176, clip=100, loss_scale=1, train_wall=90, gb_free=8.8, wall=23639
2023-05-20 18:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:34:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:34:24 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1942.9 | wpb 2785 | bsz 105.2 | num_updates 4211 | best_loss 3.382
2023-05-20 18:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 4211 updates
2023-05-20 18:34:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint65.pt
2023-05-20 18:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint65.pt
2023-05-20 18:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint65.pt (epoch 65 @ 4211 updates, score 3.469) (writing took 24.85974483191967 seconds)
2023-05-20 18:34:49 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2023-05-20 18:34:49 | INFO | train | epoch 065 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2274.1 | ups 0.19 | wpb 12044.6 | bsz 558 | num_updates 4211 | lr 0 | gnorm 1.534 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 10.1 | wall 23732
2023-05-20 18:34:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:34:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:34:49 | INFO | fairseq.trainer | begin training epoch 66
2023-05-20 18:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:35:33 | INFO | train_inner | epoch 066:      9 / 65 loss=2.506, nll_loss=0.532, ppl=1.45, wps=1679.1, ups=0.15, wpb=11514.5, bsz=523.2, num_updates=4220, lr=0, gnorm=1.522, clip=100, loss_scale=1, train_wall=94, gb_free=8.9, wall=23776
2023-05-20 18:37:06 | INFO | train_inner | epoch 066:     29 / 65 loss=2.527, nll_loss=0.557, ppl=1.47, wps=2642.5, ups=0.22, wpb=12201.2, bsz=599.5, num_updates=4240, lr=0, gnorm=2.317, clip=100, loss_scale=1, train_wall=92, gb_free=8.8, wall=23869
2023-05-20 18:38:39 | INFO | train_inner | epoch 066:     49 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=2590.9, ups=0.22, wpb=12043.7, bsz=532.6, num_updates=4260, lr=0, gnorm=3.427, clip=100, loss_scale=1, train_wall=93, gb_free=9.8, wall=23962
2023-05-20 18:39:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:39:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:40:10 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1862.6 | wpb 2785 | bsz 105.2 | num_updates 4276 | best_loss 3.382
2023-05-20 18:40:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 4276 updates
2023-05-20 18:40:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint66.pt
2023-05-20 18:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint66.pt
2023-05-20 18:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint66.pt (epoch 66 @ 4276 updates, score 3.469) (writing took 32.98173261061311 seconds)
2023-05-20 18:40:42 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2023-05-20 18:40:43 | INFO | train | epoch 066 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2215.2 | ups 0.18 | wpb 12057.1 | bsz 559.9 | num_updates 4276 | lr 0 | gnorm 2.408 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 7.7 | wall 24085
2023-05-20 18:40:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:40:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:40:43 | INFO | fairseq.trainer | begin training epoch 67
2023-05-20 18:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:41:02 | INFO | train_inner | epoch 067:      4 / 65 loss=2.515, nll_loss=0.544, ppl=1.46, wps=1667.3, ups=0.14, wpb=11924.2, bsz=563.1, num_updates=4280, lr=0, gnorm=1.577, clip=100, loss_scale=1, train_wall=91, gb_free=9.5, wall=24105
2023-05-20 18:42:35 | INFO | train_inner | epoch 067:     24 / 65 loss=2.519, nll_loss=0.546, ppl=1.46, wps=2606.7, ups=0.21, wpb=12195.4, bsz=578.5, num_updates=4300, lr=0, gnorm=1.548, clip=100, loss_scale=1, train_wall=93, gb_free=9.2, wall=24198
2023-05-20 18:44:09 | INFO | train_inner | epoch 067:     44 / 65 loss=2.515, nll_loss=0.544, ppl=1.46, wps=2586.6, ups=0.21, wpb=12099.4, bsz=549.8, num_updates=4320, lr=0, gnorm=1.313, clip=100, loss_scale=1, train_wall=93, gb_free=10.1, wall=24292
2023-05-20 18:45:39 | INFO | train_inner | epoch 067:     64 / 65 loss=2.515, nll_loss=0.542, ppl=1.46, wps=2739, ups=0.22, wpb=12318.6, bsz=559.4, num_updates=4340, lr=0, gnorm=1.219, clip=100, loss_scale=1, train_wall=90, gb_free=9.5, wall=24382
2023-05-20 18:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:45:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:46:00 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1872.7 | wpb 2785 | bsz 105.2 | num_updates 4341 | best_loss 3.382
2023-05-20 18:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 4341 updates
2023-05-20 18:46:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint67.pt
2023-05-20 18:46:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint67.pt
2023-05-20 18:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint67.pt (epoch 67 @ 4341 updates, score 3.469) (writing took 19.510612066835165 seconds)
2023-05-20 18:46:20 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2023-05-20 18:46:20 | INFO | train | epoch 067 | loss 2.517 | nll_loss 0.544 | ppl 1.46 | wps 2324.3 | ups 0.19 | wpb 12058.4 | bsz 559.6 | num_updates 4341 | lr 0 | gnorm 1.354 | clip 100 | loss_scale 1 | train_wall 299 | gb_free 9.6 | wall 24423
2023-05-20 18:46:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:46:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:46:20 | INFO | fairseq.trainer | begin training epoch 68
2023-05-20 18:46:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:47:52 | INFO | train_inner | epoch 068:     19 / 65 loss=2.533, nll_loss=0.563, ppl=1.48, wps=1791.5, ups=0.15, wpb=11985.2, bsz=589.7, num_updates=4360, lr=0, gnorm=1.295, clip=100, loss_scale=1, train_wall=95, gb_free=9.3, wall=24515
2023-05-20 18:49:23 | INFO | train_inner | epoch 068:     39 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=2667.8, ups=0.22, wpb=12127.1, bsz=545.1, num_updates=4380, lr=0, gnorm=1.087, clip=100, loss_scale=1, train_wall=91, gb_free=9.4, wall=24606
2023-05-20 18:50:54 | INFO | train_inner | epoch 068:     59 / 65 loss=2.509, nll_loss=0.536, ppl=1.45, wps=2656.2, ups=0.22, wpb=12069.8, bsz=545.8, num_updates=4400, lr=0, gnorm=1.586, clip=100, loss_scale=1, train_wall=91, gb_free=10, wall=24697
2023-05-20 18:51:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:51:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:51:41 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1759.1 | wpb 2785 | bsz 105.2 | num_updates 4406 | best_loss 3.382
2023-05-20 18:51:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 4406 updates
2023-05-20 18:51:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint68.pt
2023-05-20 18:51:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint68.pt
2023-05-20 18:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint68.pt (epoch 68 @ 4406 updates, score 3.469) (writing took 20.484035301953554 seconds)
2023-05-20 18:52:01 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2023-05-20 18:52:01 | INFO | train | epoch 068 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2291.9 | ups 0.19 | wpb 12048.3 | bsz 559.7 | num_updates 4406 | lr 0 | gnorm 1.296 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 9.7 | wall 24764
2023-05-20 18:52:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:52:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:52:01 | INFO | fairseq.trainer | begin training epoch 69
2023-05-20 18:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:53:08 | INFO | train_inner | epoch 069:     14 / 65 loss=2.503, nll_loss=0.529, ppl=1.44, wps=1744.1, ups=0.15, wpb=11663.2, bsz=524.8, num_updates=4420, lr=0, gnorm=1.316, clip=100, loss_scale=1, train_wall=93, gb_free=6.6, wall=24831
2023-05-20 18:54:37 | INFO | train_inner | epoch 069:     34 / 65 loss=2.515, nll_loss=0.543, ppl=1.46, wps=2740.1, ups=0.22, wpb=12200.6, bsz=585.6, num_updates=4440, lr=0, gnorm=1.403, clip=100, loss_scale=1, train_wall=89, gb_free=9.2, wall=24920
2023-05-20 18:56:09 | INFO | train_inner | epoch 069:     54 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2668.4, ups=0.22, wpb=12265.3, bsz=568.6, num_updates=4460, lr=0, gnorm=3.635, clip=100, loss_scale=1, train_wall=92, gb_free=9.6, wall=25012
2023-05-20 18:57:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:57:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:57:21 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1955.2 | wpb 2785 | bsz 105.2 | num_updates 4471 | best_loss 3.382
2023-05-20 18:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 4471 updates
2023-05-20 18:57:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint69.pt
2023-05-20 18:57:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint69.pt
2023-05-20 18:57:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint69.pt (epoch 69 @ 4471 updates, score 3.469) (writing took 23.122804030776024 seconds)
2023-05-20 18:57:45 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2023-05-20 18:57:45 | INFO | train | epoch 069 | loss 2.516 | nll_loss 0.544 | ppl 1.46 | wps 2282.8 | ups 0.19 | wpb 12052.1 | bsz 559.4 | num_updates 4471 | lr 0 | gnorm 2.08 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 8.4 | wall 25108
2023-05-20 18:57:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:57:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 18:57:45 | INFO | fairseq.trainer | begin training epoch 70
2023-05-20 18:57:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:58:26 | INFO | train_inner | epoch 070:      9 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=1685.4, ups=0.15, wpb=11513.4, bsz=527.9, num_updates=4480, lr=0, gnorm=1.205, clip=100, loss_scale=1, train_wall=95, gb_free=9.7, wall=25149
2023-05-20 18:59:56 | INFO | train_inner | epoch 070:     29 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2705, ups=0.22, wpb=12223.9, bsz=579.5, num_updates=4500, lr=0, gnorm=1.202, clip=100, loss_scale=1, train_wall=90, gb_free=9.2, wall=25239
2023-05-20 19:01:31 | INFO | train_inner | epoch 070:     49 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2576.3, ups=0.21, wpb=12200.2, bsz=555.4, num_updates=4520, lr=0, gnorm=1.818, clip=100, loss_scale=1, train_wall=95, gb_free=10, wall=25334
2023-05-20 19:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:02:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:03:00 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1913.9 | wpb 2785 | bsz 105.2 | num_updates 4536 | best_loss 3.382
2023-05-20 19:03:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 4536 updates
2023-05-20 19:03:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint70.pt
2023-05-20 19:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint70.pt
2023-05-20 19:03:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint70.pt (epoch 70 @ 4536 updates, score 3.469) (writing took 35.66753267869353 seconds)
2023-05-20 19:03:36 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2023-05-20 19:03:36 | INFO | train | epoch 070 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2229.8 | ups 0.18 | wpb 12060.8 | bsz 558.8 | num_updates 4536 | lr 0 | gnorm 1.469 | clip 100 | loss_scale 2 | train_wall 297 | gb_free 9.6 | wall 25459
2023-05-20 19:03:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:03:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:03:36 | INFO | fairseq.trainer | begin training epoch 71
2023-05-20 19:03:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:03:54 | INFO | train_inner | epoch 071:      4 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=1655.7, ups=0.14, wpb=11867.7, bsz=547.8, num_updates=4540, lr=0, gnorm=1.536, clip=100, loss_scale=2, train_wall=89, gb_free=9.5, wall=25477
2023-05-20 19:05:23 | INFO | train_inner | epoch 071:     24 / 65 loss=2.515, nll_loss=0.543, ppl=1.46, wps=2731.1, ups=0.22, wpb=12185.2, bsz=545.6, num_updates=4560, lr=0, gnorm=1.133, clip=100, loss_scale=2, train_wall=89, gb_free=9.2, wall=25566
2023-05-20 19:06:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 19:07:07 | INFO | train_inner | epoch 071:     45 / 65 loss=2.515, nll_loss=0.543, ppl=1.46, wps=2333.6, ups=0.19, wpb=12091, bsz=563.2, num_updates=4580, lr=0, gnorm=1.092, clip=100, loss_scale=1, train_wall=104, gb_free=10.1, wall=25670
2023-05-20 19:08:35 | INFO | train_inner | epoch 071:     65 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2681.9, ups=0.23, wpb=11869.8, bsz=570.1, num_updates=4600, lr=0, gnorm=4.111, clip=100, loss_scale=1, train_wall=88, gb_free=10, wall=25758
2023-05-20 19:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:08:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:08:53 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1946.7 | wpb 2785 | bsz 105.2 | num_updates 4600 | best_loss 3.382
2023-05-20 19:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 4600 updates
2023-05-20 19:08:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint71.pt
2023-05-20 19:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint71.pt
2023-05-20 19:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint71.pt (epoch 71 @ 4600 updates, score 3.469) (writing took 23.846320018172264 seconds)
2023-05-20 19:09:17 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2023-05-20 19:09:17 | INFO | train | epoch 071 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2262 | ups 0.19 | wpb 12053.3 | bsz 559.2 | num_updates 4600 | lr 0 | gnorm 2.049 | clip 100 | loss_scale 1 | train_wall 299 | gb_free 10 | wall 25800
2023-05-20 19:09:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:09:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:09:17 | INFO | fairseq.trainer | begin training epoch 72
2023-05-20 19:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:10:53 | INFO | train_inner | epoch 072:     20 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=1777.2, ups=0.15, wpb=12212.8, bsz=566.1, num_updates=4620, lr=0, gnorm=1.216, clip=100, loss_scale=1, train_wall=95, gb_free=9.5, wall=25896
2023-05-20 19:12:26 | INFO | train_inner | epoch 072:     40 / 65 loss=2.507, nll_loss=0.534, ppl=1.45, wps=2616.3, ups=0.22, wpb=12163.1, bsz=551.9, num_updates=4640, lr=0, gnorm=1.636, clip=100, loss_scale=1, train_wall=93, gb_free=9.5, wall=25989
2023-05-20 19:13:58 | INFO | train_inner | epoch 072:     60 / 65 loss=2.532, nll_loss=0.562, ppl=1.48, wps=2660.5, ups=0.22, wpb=12283.8, bsz=590.4, num_updates=4660, lr=0, gnorm=1.707, clip=100, loss_scale=1, train_wall=92, gb_free=8.1, wall=26081
2023-05-20 19:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:14:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:14:39 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1786.9 | wpb 2785 | bsz 105.2 | num_updates 4665 | best_loss 3.382
2023-05-20 19:14:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 4665 updates
2023-05-20 19:14:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint72.pt
2023-05-20 19:14:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint72.pt
2023-05-20 19:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint72.pt (epoch 72 @ 4665 updates, score 3.469) (writing took 28.75944232940674 seconds)
2023-05-20 19:15:08 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2023-05-20 19:15:08 | INFO | train | epoch 072 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2231.8 | ups 0.19 | wpb 12051.9 | bsz 559.4 | num_updates 4665 | lr 0 | gnorm 1.505 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 10.1 | wall 26151
2023-05-20 19:15:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:15:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:15:08 | INFO | fairseq.trainer | begin training epoch 73
2023-05-20 19:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:16:18 | INFO | train_inner | epoch 073:     15 / 65 loss=2.499, nll_loss=0.526, ppl=1.44, wps=1654.9, ups=0.14, wpb=11529, bsz=505.6, num_updates=4680, lr=0, gnorm=1.13, clip=100, loss_scale=1, train_wall=91, gb_free=9.7, wall=26221
2023-05-20 19:17:53 | INFO | train_inner | epoch 073:     35 / 65 loss=2.521, nll_loss=0.549, ppl=1.46, wps=2550.5, ups=0.21, wpb=12197.7, bsz=566.4, num_updates=4700, lr=0, gnorm=1.807, clip=100, loss_scale=1, train_wall=96, gb_free=9.8, wall=26316
2023-05-20 19:19:26 | INFO | train_inner | epoch 073:     55 / 65 loss=2.532, nll_loss=0.562, ppl=1.48, wps=2650.8, ups=0.21, wpb=12337, bsz=609, num_updates=4720, lr=0, gnorm=1.626, clip=100, loss_scale=1, train_wall=93, gb_free=9.4, wall=26409
2023-05-20 19:20:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:20:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:20:28 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1942.6 | wpb 2785 | bsz 105.2 | num_updates 4730 | best_loss 3.382
2023-05-20 19:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 4730 updates
2023-05-20 19:20:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint73.pt
2023-05-20 19:20:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint73.pt
2023-05-20 19:20:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint73.pt (epoch 73 @ 4730 updates, score 3.469) (writing took 23.990886338055134 seconds)
2023-05-20 19:20:52 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2023-05-20 19:20:52 | INFO | train | epoch 073 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2277.7 | ups 0.19 | wpb 12056.5 | bsz 559.8 | num_updates 4730 | lr 0 | gnorm 1.5 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 10.1 | wall 26495
2023-05-20 19:20:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:20:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:20:52 | INFO | fairseq.trainer | begin training epoch 74
2023-05-20 19:20:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:21:40 | INFO | train_inner | epoch 074:     10 / 65 loss=2.515, nll_loss=0.544, ppl=1.46, wps=1761.4, ups=0.15, wpb=11777.5, bsz=539.3, num_updates=4740, lr=0, gnorm=1.174, clip=100, loss_scale=1, train_wall=91, gb_free=9.4, wall=26543
2023-05-20 19:23:12 | INFO | train_inner | epoch 074:     30 / 65 loss=2.509, nll_loss=0.537, ppl=1.45, wps=2676.5, ups=0.22, wpb=12278.5, bsz=568.7, num_updates=4760, lr=0, gnorm=1.509, clip=100, loss_scale=1, train_wall=92, gb_free=10, wall=26635
2023-05-20 19:24:43 | INFO | train_inner | epoch 074:     50 / 65 loss=2.524, nll_loss=0.552, ppl=1.47, wps=2679.8, ups=0.22, wpb=12223.4, bsz=558.4, num_updates=4780, lr=0, gnorm=1.3, clip=100, loss_scale=1, train_wall=91, gb_free=10.1, wall=26726
2023-05-20 19:25:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:25:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:26:10 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1790.3 | wpb 2785 | bsz 105.2 | num_updates 4795 | best_loss 3.382
2023-05-20 19:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 4795 updates
2023-05-20 19:26:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint74.pt
2023-05-20 19:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint74.pt
2023-05-20 19:26:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint74.pt (epoch 74 @ 4795 updates, score 3.469) (writing took 20.680113896727562 seconds)
2023-05-20 19:26:30 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2023-05-20 19:26:30 | INFO | train | epoch 074 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2318.3 | ups 0.19 | wpb 12060.5 | bsz 559.6 | num_updates 4795 | lr 0 | gnorm 1.318 | clip 100 | loss_scale 1 | train_wall 297 | gb_free 9.9 | wall 26833
2023-05-20 19:26:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:26:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:26:30 | INFO | fairseq.trainer | begin training epoch 75
2023-05-20 19:26:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:26:54 | INFO | train_inner | epoch 075:      5 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=1773.3, ups=0.15, wpb=11632.2, bsz=546.6, num_updates=4800, lr=0, gnorm=1.175, clip=100, loss_scale=1, train_wall=91, gb_free=9.3, wall=26857
2023-05-20 19:28:24 | INFO | train_inner | epoch 075:     25 / 65 loss=2.511, nll_loss=0.539, ppl=1.45, wps=2697.4, ups=0.22, wpb=12182.8, bsz=549.5, num_updates=4820, lr=0, gnorm=1.551, clip=100, loss_scale=1, train_wall=90, gb_free=9.4, wall=26947
2023-05-20 19:29:57 | INFO | train_inner | epoch 075:     45 / 65 loss=2.516, nll_loss=0.543, ppl=1.46, wps=2627.4, ups=0.22, wpb=12141.8, bsz=558.1, num_updates=4840, lr=0, gnorm=1.227, clip=100, loss_scale=1, train_wall=92, gb_free=9.9, wall=27040
2023-05-20 19:31:22 | INFO | train_inner | epoch 075:     65 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2745.8, ups=0.23, wpb=11745.5, bsz=561.6, num_updates=4860, lr=0, gnorm=1.522, clip=100, loss_scale=1, train_wall=85, gb_free=9.5, wall=27125
2023-05-20 19:31:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:31:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:31:42 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1779 | wpb 2785 | bsz 105.2 | num_updates 4860 | best_loss 3.382
2023-05-20 19:31:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 4860 updates
2023-05-20 19:31:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint75.pt
2023-05-20 19:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint75.pt
2023-05-20 19:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint75.pt (epoch 75 @ 4860 updates, score 3.469) (writing took 25.02671904861927 seconds)
2023-05-20 19:32:07 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2023-05-20 19:32:07 | INFO | train | epoch 075 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2326.8 | ups 0.19 | wpb 12047.9 | bsz 558.6 | num_updates 4860 | lr 0 | gnorm 1.394 | clip 100 | loss_scale 1 | train_wall 291 | gb_free 9.5 | wall 27170
2023-05-20 19:32:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:32:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:32:07 | INFO | fairseq.trainer | begin training epoch 76
2023-05-20 19:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:33:40 | INFO | train_inner | epoch 076:     20 / 65 loss=2.514, nll_loss=0.543, ppl=1.46, wps=1783.1, ups=0.15, wpb=12274.6, bsz=580.1, num_updates=4880, lr=0, gnorm=1.281, clip=100, loss_scale=1, train_wall=93, gb_free=9.9, wall=27263
2023-05-20 19:35:15 | INFO | train_inner | epoch 076:     40 / 65 loss=2.528, nll_loss=0.557, ppl=1.47, wps=2608.4, ups=0.21, wpb=12331.1, bsz=573.6, num_updates=4900, lr=0, gnorm=1.585, clip=100, loss_scale=1, train_wall=94, gb_free=9.9, wall=27358
2023-05-20 19:36:49 | INFO | train_inner | epoch 076:     60 / 65 loss=2.511, nll_loss=0.538, ppl=1.45, wps=2547.5, ups=0.21, wpb=11984.6, bsz=538.6, num_updates=4920, lr=0, gnorm=1.815, clip=100, loss_scale=1, train_wall=94, gb_free=9.5, wall=27452
2023-05-20 19:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:37:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:37:28 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1926.5 | wpb 2785 | bsz 105.2 | num_updates 4925 | best_loss 3.382
2023-05-20 19:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 4925 updates
2023-05-20 19:37:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint76.pt
2023-05-20 19:37:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint76.pt
2023-05-20 19:37:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint76.pt (epoch 76 @ 4925 updates, score 3.469) (writing took 20.418537016958 seconds)
2023-05-20 19:37:48 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2023-05-20 19:37:48 | INFO | train | epoch 076 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2297.1 | ups 0.19 | wpb 12057.4 | bsz 559.4 | num_updates 4925 | lr 0 | gnorm 1.885 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 10 | wall 27511
2023-05-20 19:37:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:37:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:37:48 | INFO | fairseq.trainer | begin training epoch 77
2023-05-20 19:37:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:38:57 | INFO | train_inner | epoch 077:     15 / 65 loss=2.507, nll_loss=0.534, ppl=1.45, wps=1821.4, ups=0.16, wpb=11679.9, bsz=531.5, num_updates=4940, lr=0, gnorm=2.218, clip=100, loss_scale=1, train_wall=89, gb_free=9.5, wall=27580
2023-05-20 19:40:34 | INFO | train_inner | epoch 077:     35 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2529.9, ups=0.21, wpb=12219, bsz=567.5, num_updates=4960, lr=0, gnorm=1.374, clip=100, loss_scale=1, train_wall=96, gb_free=9.2, wall=27677
2023-05-20 19:42:05 | INFO | train_inner | epoch 077:     55 / 65 loss=2.529, nll_loss=0.559, ppl=1.47, wps=2655.9, ups=0.22, wpb=12173.1, bsz=589, num_updates=4980, lr=0, gnorm=1.199, clip=100, loss_scale=1, train_wall=92, gb_free=9.4, wall=27768
2023-05-20 19:42:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:42:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:43:08 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1789.6 | wpb 2785 | bsz 105.2 | num_updates 4990 | best_loss 3.382
2023-05-20 19:43:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 4990 updates
2023-05-20 19:43:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint77.pt
2023-05-20 19:43:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint77.pt
2023-05-20 19:43:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint77.pt (epoch 77 @ 4990 updates, score 3.469) (writing took 24.227949809283018 seconds)
2023-05-20 19:43:33 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2023-05-20 19:43:33 | INFO | train | epoch 077 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2273.6 | ups 0.19 | wpb 12051.7 | bsz 558.9 | num_updates 4990 | lr 0 | gnorm 1.368 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 10.1 | wall 27856
2023-05-20 19:43:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:43:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:43:33 | INFO | fairseq.trainer | begin training epoch 78
2023-05-20 19:43:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:44:22 | INFO | train_inner | epoch 078:     10 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=1722, ups=0.15, wpb=11759.5, bsz=535.1, num_updates=5000, lr=0, gnorm=1.829, clip=100, loss_scale=1, train_wall=93, gb_free=10.1, wall=27905
2023-05-20 19:45:55 | INFO | train_inner | epoch 078:     30 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=2648.6, ups=0.21, wpb=12336.4, bsz=564.3, num_updates=5020, lr=0, gnorm=1.536, clip=100, loss_scale=1, train_wall=93, gb_free=9.5, wall=27998
2023-05-20 19:47:29 | INFO | train_inner | epoch 078:     50 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2622.3, ups=0.21, wpb=12270.4, bsz=571.6, num_updates=5040, lr=0, gnorm=1.651, clip=100, loss_scale=1, train_wall=93, gb_free=9.6, wall=28092
2023-05-20 19:48:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:48:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:48:51 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1936.5 | wpb 2785 | bsz 105.2 | num_updates 5055 | best_loss 3.382
2023-05-20 19:48:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 5055 updates
2023-05-20 19:48:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint78.pt
2023-05-20 19:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint78.pt
2023-05-20 19:49:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint78.pt (epoch 78 @ 5055 updates, score 3.469) (writing took 21.387463241815567 seconds)
2023-05-20 19:49:13 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2023-05-20 19:49:13 | INFO | train | epoch 078 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2303.8 | ups 0.19 | wpb 12052.9 | bsz 558.9 | num_updates 5055 | lr 0 | gnorm 1.508 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 10 | wall 28196
2023-05-20 19:49:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:49:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:49:13 | INFO | fairseq.trainer | begin training epoch 79
2023-05-20 19:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:49:36 | INFO | train_inner | epoch 079:      5 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=1815.5, ups=0.16, wpb=11538.9, bsz=538.7, num_updates=5060, lr=0, gnorm=1.418, clip=100, loss_scale=1, train_wall=88, gb_free=9.8, wall=28219
2023-05-20 19:51:10 | INFO | train_inner | epoch 079:     25 / 65 loss=2.519, nll_loss=0.548, ppl=1.46, wps=2562.4, ups=0.21, wpb=12044.7, bsz=554.8, num_updates=5080, lr=0, gnorm=1.717, clip=100, loss_scale=1, train_wall=94, gb_free=9.5, wall=28313
2023-05-20 19:52:44 | INFO | train_inner | epoch 079:     45 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2599.2, ups=0.21, wpb=12266.9, bsz=568.3, num_updates=5100, lr=0, gnorm=1.083, clip=100, loss_scale=2, train_wall=94, gb_free=9.8, wall=28407
2023-05-20 19:54:14 | INFO | train_inner | epoch 079:     65 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2638, ups=0.22, wpb=11798.8, bsz=556, num_updates=5120, lr=0, gnorm=2.224, clip=100, loss_scale=2, train_wall=89, gb_free=10.1, wall=28497
2023-05-20 19:54:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:54:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:54:31 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1950.8 | wpb 2785 | bsz 105.2 | num_updates 5120 | best_loss 3.382
2023-05-20 19:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 5120 updates
2023-05-20 19:54:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint79.pt
2023-05-20 19:54:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint79.pt
2023-05-20 19:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint79.pt (epoch 79 @ 5120 updates, score 3.469) (writing took 25.748525872826576 seconds)
2023-05-20 19:54:57 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2023-05-20 19:54:57 | INFO | train | epoch 079 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2273 | ups 0.19 | wpb 12046.4 | bsz 559.4 | num_updates 5120 | lr 0 | gnorm 1.678 | clip 100 | loss_scale 2 | train_wall 300 | gb_free 10.1 | wall 28540
2023-05-20 19:54:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:54:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 19:54:57 | INFO | fairseq.trainer | begin training epoch 80
2023-05-20 19:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:56:32 | INFO | train_inner | epoch 080:     20 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=1765.9, ups=0.14, wpb=12257, bsz=587.1, num_updates=5140, lr=0, gnorm=1.36, clip=100, loss_scale=2, train_wall=95, gb_free=9.5, wall=28635
2023-05-20 19:58:07 | INFO | train_inner | epoch 080:     40 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2558.8, ups=0.21, wpb=12092.1, bsz=555.6, num_updates=5160, lr=0, gnorm=1.34, clip=100, loss_scale=2, train_wall=94, gb_free=9.9, wall=28730
2023-05-20 19:59:37 | INFO | train_inner | epoch 080:     60 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=2726.3, ups=0.22, wpb=12251.5, bsz=557.1, num_updates=5180, lr=0, gnorm=1.361, clip=100, loss_scale=2, train_wall=90, gb_free=9.5, wall=28820
2023-05-20 19:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:59:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:00:17 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1906.5 | wpb 2785 | bsz 105.2 | num_updates 5185 | best_loss 3.382
2023-05-20 20:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5185 updates
2023-05-20 20:00:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint80.pt
2023-05-20 20:00:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint80.pt
2023-05-20 20:00:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint80.pt (epoch 80 @ 5185 updates, score 3.469) (writing took 36.210459284484386 seconds)
2023-05-20 20:00:53 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2023-05-20 20:00:53 | INFO | train | epoch 080 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2200.8 | ups 0.18 | wpb 12052.9 | bsz 559.5 | num_updates 5185 | lr 0 | gnorm 1.569 | clip 100 | loss_scale 2 | train_wall 301 | gb_free 10.1 | wall 28896
2023-05-20 20:00:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:00:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:00:53 | INFO | fairseq.trainer | begin training epoch 81
2023-05-20 20:00:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:02:02 | INFO | train_inner | epoch 081:     15 / 65 loss=2.506, nll_loss=0.534, ppl=1.45, wps=1605.4, ups=0.14, wpb=11693.7, bsz=534, num_updates=5200, lr=0, gnorm=2.691, clip=100, loss_scale=2, train_wall=91, gb_free=9.4, wall=28965
2023-05-20 20:03:38 | INFO | train_inner | epoch 081:     35 / 65 loss=2.528, nll_loss=0.557, ppl=1.47, wps=2571.3, ups=0.21, wpb=12259.1, bsz=593.8, num_updates=5220, lr=0, gnorm=1.95, clip=100, loss_scale=2, train_wall=95, gb_free=10, wall=29061
2023-05-20 20:04:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 20:05:14 | INFO | train_inner | epoch 081:     56 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=2533.1, ups=0.21, wpb=12189.4, bsz=560.1, num_updates=5240, lr=0, gnorm=2.212, clip=100, loss_scale=1, train_wall=96, gb_free=9.5, wall=29157
2023-05-20 20:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:05:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:06:13 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1936.4 | wpb 2785 | bsz 105.2 | num_updates 5249 | best_loss 3.382
2023-05-20 20:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 5249 updates
2023-05-20 20:06:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint81.pt
2023-05-20 20:06:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint81.pt
2023-05-20 20:06:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint81.pt (epoch 81 @ 5249 updates, score 3.469) (writing took 21.596348512917757 seconds)
2023-05-20 20:06:34 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2023-05-20 20:06:34 | INFO | train | epoch 081 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2264.8 | ups 0.19 | wpb 12064.9 | bsz 561.9 | num_updates 5249 | lr 0 | gnorm 2.073 | clip 100 | loss_scale 1 | train_wall 301 | gb_free 8.1 | wall 29237
2023-05-20 20:06:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:06:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:06:34 | INFO | fairseq.trainer | begin training epoch 82
2023-05-20 20:06:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:07:26 | INFO | train_inner | epoch 082:     11 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=1786.1, ups=0.15, wpb=11773.2, bsz=537.3, num_updates=5260, lr=0, gnorm=1.658, clip=100, loss_scale=1, train_wall=92, gb_free=10, wall=29289
2023-05-20 20:09:00 | INFO | train_inner | epoch 082:     31 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2586.8, ups=0.21, wpb=12113.9, bsz=555.1, num_updates=5280, lr=0, gnorm=1.956, clip=100, loss_scale=1, train_wall=94, gb_free=9.6, wall=29383
2023-05-20 20:10:38 | INFO | train_inner | epoch 082:     51 / 65 loss=2.509, nll_loss=0.537, ppl=1.45, wps=2466, ups=0.2, wpb=12179.2, bsz=561, num_updates=5300, lr=0, gnorm=1.126, clip=100, loss_scale=1, train_wall=99, gb_free=9.8, wall=29481
2023-05-20 20:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:11:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:11:58 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1926 | wpb 2785 | bsz 105.2 | num_updates 5314 | best_loss 3.382
2023-05-20 20:11:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 5314 updates
2023-05-20 20:11:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint82.pt
2023-05-20 20:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint82.pt
2023-05-20 20:12:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint82.pt (epoch 82 @ 5314 updates, score 3.469) (writing took 19.740679554641247 seconds)
2023-05-20 20:12:18 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2023-05-20 20:12:18 | INFO | train | epoch 082 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2279.5 | ups 0.19 | wpb 12044.4 | bsz 557.8 | num_updates 5314 | lr 0 | gnorm 1.607 | clip 100 | loss_scale 1 | train_wall 305 | gb_free 9.5 | wall 29581
2023-05-20 20:12:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:12:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:12:18 | INFO | fairseq.trainer | begin training epoch 83
2023-05-20 20:12:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:12:46 | INFO | train_inner | epoch 083:      6 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=1825.6, ups=0.16, wpb=11663, bsz=551.6, num_updates=5320, lr=0, gnorm=1.942, clip=100, loss_scale=1, train_wall=90, gb_free=9.3, wall=29609
2023-05-20 20:14:22 | INFO | train_inner | epoch 083:     26 / 65 loss=2.523, nll_loss=0.552, ppl=1.47, wps=2546.9, ups=0.21, wpb=12229.4, bsz=582.3, num_updates=5340, lr=0, gnorm=1.102, clip=100, loss_scale=1, train_wall=96, gb_free=10, wall=29705
2023-05-20 20:15:52 | INFO | train_inner | epoch 083:     46 / 65 loss=2.513, nll_loss=0.54, ppl=1.45, wps=2692.5, ups=0.22, wpb=12096.5, bsz=560.1, num_updates=5360, lr=0, gnorm=0.935, clip=100, loss_scale=1, train_wall=90, gb_free=10, wall=29795
2023-05-20 20:17:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:17:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:17:39 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1799 | wpb 2785 | bsz 105.2 | num_updates 5379 | best_loss 3.382
2023-05-20 20:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 5379 updates
2023-05-20 20:17:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint83.pt
2023-05-20 20:17:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint83.pt
2023-05-20 20:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint83.pt (epoch 83 @ 5379 updates, score 3.469) (writing took 19.756287202239037 seconds)
2023-05-20 20:17:59 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2023-05-20 20:17:59 | INFO | train | epoch 083 | loss 2.516 | nll_loss 0.544 | ppl 1.46 | wps 2294.1 | ups 0.19 | wpb 12055.5 | bsz 558.9 | num_updates 5379 | lr 0 | gnorm 1.199 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 10.1 | wall 29922
2023-05-20 20:17:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:17:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:17:59 | INFO | fairseq.trainer | begin training epoch 84
2023-05-20 20:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:18:04 | INFO | train_inner | epoch 084:      1 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=1792.4, ups=0.15, wpb=11863.1, bsz=532, num_updates=5380, lr=0, gnorm=1.265, clip=100, loss_scale=1, train_wall=93, gb_free=8.8, wall=29927
2023-05-20 20:19:36 | INFO | train_inner | epoch 084:     21 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=2666.4, ups=0.22, wpb=12230.9, bsz=577, num_updates=5400, lr=0, gnorm=1.634, clip=100, loss_scale=1, train_wall=92, gb_free=10.1, wall=30019
2023-05-20 20:21:10 | INFO | train_inner | epoch 084:     41 / 65 loss=2.521, nll_loss=0.549, ppl=1.46, wps=2612.9, ups=0.21, wpb=12218.8, bsz=554.5, num_updates=5420, lr=0, gnorm=1.61, clip=100, loss_scale=1, train_wall=93, gb_free=9.9, wall=30113
2023-05-20 20:22:44 | INFO | train_inner | epoch 084:     61 / 65 loss=2.514, nll_loss=0.543, ppl=1.46, wps=2575.5, ups=0.21, wpb=12193.2, bsz=575, num_updates=5440, lr=0, gnorm=1.408, clip=100, loss_scale=1, train_wall=95, gb_free=9.7, wall=30207
2023-05-20 20:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:23:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:23:21 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1788.2 | wpb 2785 | bsz 105.2 | num_updates 5444 | best_loss 3.382
2023-05-20 20:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 5444 updates
2023-05-20 20:23:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint84.pt
2023-05-20 20:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint84.pt
2023-05-20 20:23:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint84.pt (epoch 84 @ 5444 updates, score 3.469) (writing took 20.775082238018513 seconds)
2023-05-20 20:23:41 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2023-05-20 20:23:41 | INFO | train | epoch 084 | loss 2.516 | nll_loss 0.545 | ppl 1.46 | wps 2290.9 | ups 0.19 | wpb 12058.6 | bsz 559.8 | num_updates 5444 | lr 0 | gnorm 1.539 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 9.6 | wall 30264
2023-05-20 20:23:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:23:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:23:41 | INFO | fairseq.trainer | begin training epoch 85
2023-05-20 20:23:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:24:57 | INFO | train_inner | epoch 085:     16 / 65 loss=2.527, nll_loss=0.558, ppl=1.47, wps=1799.3, ups=0.15, wpb=11928.2, bsz=579.2, num_updates=5460, lr=0, gnorm=1.284, clip=100, loss_scale=1, train_wall=92, gb_free=9.5, wall=30340
2023-05-20 20:26:33 | INFO | train_inner | epoch 085:     36 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2533.6, ups=0.21, wpb=12111.5, bsz=552.1, num_updates=5480, lr=0, gnorm=1.332, clip=100, loss_scale=1, train_wall=96, gb_free=9.6, wall=30436
2023-05-20 20:28:06 | INFO | train_inner | epoch 085:     56 / 65 loss=2.515, nll_loss=0.542, ppl=1.46, wps=2604, ups=0.21, wpb=12179.6, bsz=561.5, num_updates=5500, lr=0, gnorm=1.557, clip=100, loss_scale=1, train_wall=93, gb_free=9.5, wall=30529
2023-05-20 20:28:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:28:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:29:02 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1926.2 | wpb 2785 | bsz 105.2 | num_updates 5509 | best_loss 3.382
2023-05-20 20:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 5509 updates
2023-05-20 20:29:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint85.pt
2023-05-20 20:29:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint85.pt
2023-05-20 20:29:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint85.pt (epoch 85 @ 5509 updates, score 3.469) (writing took 24.119659647345543 seconds)
2023-05-20 20:29:26 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2023-05-20 20:29:26 | INFO | train | epoch 085 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2272.5 | ups 0.19 | wpb 12060.5 | bsz 560.4 | num_updates 5509 | lr 0 | gnorm 1.342 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 10.1 | wall 30609
2023-05-20 20:29:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:29:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:29:26 | INFO | fairseq.trainer | begin training epoch 86
2023-05-20 20:29:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:30:16 | INFO | train_inner | epoch 086:     11 / 65 loss=2.507, nll_loss=0.534, ppl=1.45, wps=1795, ups=0.15, wpb=11621, bsz=503.1, num_updates=5520, lr=0, gnorm=1.696, clip=100, loss_scale=1, train_wall=87, gb_free=9.7, wall=30659
2023-05-20 20:31:52 | INFO | train_inner | epoch 086:     31 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=2505, ups=0.21, wpb=12087.8, bsz=544.7, num_updates=5540, lr=0, gnorm=1.334, clip=100, loss_scale=1, train_wall=96, gb_free=9.1, wall=30755
2023-05-20 20:33:25 | INFO | train_inner | epoch 086:     51 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2645.8, ups=0.22, wpb=12280.3, bsz=579.9, num_updates=5560, lr=0, gnorm=1.786, clip=100, loss_scale=1, train_wall=93, gb_free=9.7, wall=30848
2023-05-20 20:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:34:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:34:45 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1935.2 | wpb 2785 | bsz 105.2 | num_updates 5574 | best_loss 3.382
2023-05-20 20:34:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 5574 updates
2023-05-20 20:34:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint86.pt
2023-05-20 20:35:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint86.pt
2023-05-20 20:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint86.pt (epoch 86 @ 5574 updates, score 3.469) (writing took 21.076398190110922 seconds)
2023-05-20 20:35:06 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2023-05-20 20:35:06 | INFO | train | epoch 086 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2307.1 | ups 0.19 | wpb 12059.5 | bsz 559.8 | num_updates 5574 | lr 0 | gnorm 1.746 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 9.8 | wall 30949
2023-05-20 20:35:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:35:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:35:06 | INFO | fairseq.trainer | begin training epoch 87
2023-05-20 20:35:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:35:34 | INFO | train_inner | epoch 087:      6 / 65 loss=2.528, nll_loss=0.558, ppl=1.47, wps=1827.1, ups=0.15, wpb=11804.5, bsz=575.5, num_updates=5580, lr=0, gnorm=2.689, clip=100, loss_scale=1, train_wall=90, gb_free=10, wall=30977
2023-05-20 20:37:09 | INFO | train_inner | epoch 087:     26 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2550.5, ups=0.21, wpb=12162.2, bsz=564.7, num_updates=5600, lr=0, gnorm=2.063, clip=100, loss_scale=1, train_wall=95, gb_free=9.6, wall=31072
2023-05-20 20:38:44 | INFO | train_inner | epoch 087:     46 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=2559.6, ups=0.21, wpb=12144.2, bsz=557.3, num_updates=5620, lr=0, gnorm=1.806, clip=100, loss_scale=1, train_wall=95, gb_free=10.1, wall=31167
2023-05-20 20:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:40:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:40:27 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1949 | wpb 2785 | bsz 105.2 | num_updates 5639 | best_loss 3.382
2023-05-20 20:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 5639 updates
2023-05-20 20:40:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint87.pt
2023-05-20 20:40:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint87.pt
2023-05-20 20:41:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint87.pt (epoch 87 @ 5639 updates, score 3.469) (writing took 33.0989286378026 seconds)
2023-05-20 20:41:00 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2023-05-20 20:41:00 | INFO | train | epoch 087 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2215 | ups 0.18 | wpb 12054 | bsz 559.4 | num_updates 5639 | lr 0 | gnorm 2.217 | clip 100 | loss_scale 1 | train_wall 302 | gb_free 10 | wall 31303
2023-05-20 20:41:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:41:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:41:00 | INFO | fairseq.trainer | begin training epoch 88
2023-05-20 20:41:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:41:04 | INFO | train_inner | epoch 088:      1 / 65 loss=2.514, nll_loss=0.543, ppl=1.46, wps=1697.4, ups=0.14, wpb=11834.6, bsz=559.1, num_updates=5640, lr=0, gnorm=2.04, clip=100, loss_scale=1, train_wall=88, gb_free=9.2, wall=31307
2023-05-20 20:42:41 | INFO | train_inner | epoch 088:     21 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2517, ups=0.21, wpb=12170.4, bsz=558.9, num_updates=5660, lr=0, gnorm=1.672, clip=100, loss_scale=1, train_wall=97, gb_free=9.6, wall=31404
2023-05-20 20:44:12 | INFO | train_inner | epoch 088:     41 / 65 loss=2.525, nll_loss=0.555, ppl=1.47, wps=2674.6, ups=0.22, wpb=12290.6, bsz=588.5, num_updates=5680, lr=0, gnorm=1.428, clip=100, loss_scale=1, train_wall=92, gb_free=9.1, wall=31495
2023-05-20 20:45:50 | INFO | train_inner | epoch 088:     61 / 65 loss=2.506, nll_loss=0.533, ppl=1.45, wps=2469.2, ups=0.21, wpb=12045, bsz=538.8, num_updates=5700, lr=0, gnorm=2.274, clip=100, loss_scale=1, train_wall=97, gb_free=9.3, wall=31593
2023-05-20 20:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:46:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:46:24 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1776.8 | wpb 2785 | bsz 105.2 | num_updates 5704 | best_loss 3.382
2023-05-20 20:46:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 5704 updates
2023-05-20 20:46:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint88.pt
2023-05-20 20:46:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint88.pt
2023-05-20 20:46:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint88.pt (epoch 88 @ 5704 updates, score 3.469) (writing took 20.92252877354622 seconds)
2023-05-20 20:46:45 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2023-05-20 20:46:45 | INFO | train | epoch 088 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2268.8 | ups 0.19 | wpb 12053.5 | bsz 559.6 | num_updates 5704 | lr 0 | gnorm 1.79 | clip 100 | loss_scale 1 | train_wall 304 | gb_free 9.9 | wall 31648
2023-05-20 20:46:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:46:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:46:45 | INFO | fairseq.trainer | begin training epoch 89
2023-05-20 20:46:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:48:03 | INFO | train_inner | epoch 089:     16 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=1761.4, ups=0.15, wpb=11728.6, bsz=540.6, num_updates=5720, lr=0, gnorm=2.088, clip=100, loss_scale=1, train_wall=92, gb_free=9.5, wall=31726
2023-05-20 20:49:33 | INFO | train_inner | epoch 089:     36 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2702.5, ups=0.22, wpb=12186.8, bsz=567.8, num_updates=5740, lr=0, gnorm=1.857, clip=100, loss_scale=1, train_wall=90, gb_free=9.7, wall=31816
2023-05-20 20:51:08 | INFO | train_inner | epoch 089:     56 / 65 loss=2.506, nll_loss=0.532, ppl=1.45, wps=2544.8, ups=0.21, wpb=12094, bsz=544.6, num_updates=5760, lr=0, gnorm=1.321, clip=100, loss_scale=2, train_wall=95, gb_free=9.8, wall=31911
2023-05-20 20:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:51:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:52:07 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1797.4 | wpb 2785 | bsz 105.2 | num_updates 5769 | best_loss 3.382
2023-05-20 20:52:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 5769 updates
2023-05-20 20:52:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint89.pt
2023-05-20 20:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint89.pt
2023-05-20 20:52:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint89.pt (epoch 89 @ 5769 updates, score 3.469) (writing took 21.01180337369442 seconds)
2023-05-20 20:52:28 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2023-05-20 20:52:28 | INFO | train | epoch 089 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2284.3 | ups 0.19 | wpb 12053.4 | bsz 559.4 | num_updates 5769 | lr 0 | gnorm 1.777 | clip 100 | loss_scale 2 | train_wall 302 | gb_free 10.1 | wall 31991
2023-05-20 20:52:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:52:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:52:28 | INFO | fairseq.trainer | begin training epoch 90
2023-05-20 20:52:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:53:21 | INFO | train_inner | epoch 090:     11 / 65 loss=2.519, nll_loss=0.548, ppl=1.46, wps=1797.1, ups=0.15, wpb=11900.2, bsz=561.2, num_updates=5780, lr=0, gnorm=1.544, clip=100, loss_scale=2, train_wall=92, gb_free=9.8, wall=32044
2023-05-20 20:54:53 | INFO | train_inner | epoch 090:     31 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2649.4, ups=0.22, wpb=12188.9, bsz=566.8, num_updates=5800, lr=0, gnorm=2.03, clip=100, loss_scale=2, train_wall=92, gb_free=9.4, wall=32136
2023-05-20 20:56:24 | INFO | train_inner | epoch 090:     51 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2692.6, ups=0.22, wpb=12261.9, bsz=571.2, num_updates=5820, lr=0, gnorm=2.142, clip=100, loss_scale=2, train_wall=91, gb_free=9.7, wall=32227
2023-05-20 20:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:57:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:57:48 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1790.2 | wpb 2785 | bsz 105.2 | num_updates 5834 | best_loss 3.382
2023-05-20 20:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 5834 updates
2023-05-20 20:57:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint90.pt
2023-05-20 20:58:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint90.pt
2023-05-20 20:58:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint90.pt (epoch 90 @ 5834 updates, score 3.469) (writing took 20.821331065148115 seconds)
2023-05-20 20:58:09 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2023-05-20 20:58:09 | INFO | train | epoch 090 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2300.2 | ups 0.19 | wpb 12049.1 | bsz 559.8 | num_updates 5834 | lr 0 | gnorm 1.78 | clip 100 | loss_scale 2 | train_wall 300 | gb_free 9.2 | wall 32332
2023-05-20 20:58:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:58:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 20:58:09 | INFO | fairseq.trainer | begin training epoch 91
2023-05-20 20:58:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:58:35 | INFO | train_inner | epoch 091:      6 / 65 loss=2.526, nll_loss=0.555, ppl=1.47, wps=1798.8, ups=0.15, wpb=11833.3, bsz=578.7, num_updates=5840, lr=0, gnorm=1.263, clip=100, loss_scale=2, train_wall=91, gb_free=9.5, wall=32358
2023-05-20 21:00:11 | INFO | train_inner | epoch 091:     26 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=2546.4, ups=0.21, wpb=12149.6, bsz=560.5, num_updates=5860, lr=0, gnorm=1.731, clip=100, loss_scale=2, train_wall=95, gb_free=9.3, wall=32454
2023-05-20 21:01:45 | INFO | train_inner | epoch 091:     46 / 65 loss=2.526, nll_loss=0.555, ppl=1.47, wps=2591.5, ups=0.21, wpb=12218.8, bsz=575.3, num_updates=5880, lr=0, gnorm=2.367, clip=100, loss_scale=2, train_wall=94, gb_free=8.6, wall=32548
2023-05-20 21:03:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:03:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:03:30 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1873.6 | wpb 2785 | bsz 105.2 | num_updates 5899 | best_loss 3.382
2023-05-20 21:03:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 5899 updates
2023-05-20 21:03:30 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint91.pt
2023-05-20 21:03:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint91.pt
2023-05-20 21:03:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint91.pt (epoch 91 @ 5899 updates, score 3.469) (writing took 20.461650010198355 seconds)
2023-05-20 21:03:50 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2023-05-20 21:03:50 | INFO | train | epoch 091 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2294.3 | ups 0.19 | wpb 12061.5 | bsz 559.6 | num_updates 5899 | lr 0 | gnorm 1.775 | clip 100 | loss_scale 2 | train_wall 302 | gb_free 10.1 | wall 32673
2023-05-20 21:03:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:03:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:03:50 | INFO | fairseq.trainer | begin training epoch 92
2023-05-20 21:03:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:03:55 | INFO | train_inner | epoch 092:      1 / 65 loss=2.511, nll_loss=0.538, ppl=1.45, wps=1795.8, ups=0.15, wpb=11689.4, bsz=524.9, num_updates=5900, lr=0, gnorm=1.43, clip=100, loss_scale=2, train_wall=91, gb_free=8.8, wall=32678
2023-05-20 21:05:26 | INFO | train_inner | epoch 092:     21 / 65 loss=2.529, nll_loss=0.559, ppl=1.47, wps=2710, ups=0.22, wpb=12253.4, bsz=600.6, num_updates=5920, lr=0, gnorm=1.285, clip=100, loss_scale=2, train_wall=90, gb_free=9.3, wall=32769
2023-05-20 21:07:01 | INFO | train_inner | epoch 092:     41 / 65 loss=2.508, nll_loss=0.535, ppl=1.45, wps=2541.1, ups=0.21, wpb=12072.5, bsz=529.4, num_updates=5940, lr=0, gnorm=1.315, clip=100, loss_scale=2, train_wall=95, gb_free=9.4, wall=32864
2023-05-20 21:08:38 | INFO | train_inner | epoch 092:     61 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=2514.4, ups=0.21, wpb=12230.8, bsz=566.1, num_updates=5960, lr=0, gnorm=1.386, clip=100, loss_scale=2, train_wall=97, gb_free=9.7, wall=32961
2023-05-20 21:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:08:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:09:13 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1932.1 | wpb 2785 | bsz 105.2 | num_updates 5964 | best_loss 3.382
2023-05-20 21:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 5964 updates
2023-05-20 21:09:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint92.pt
2023-05-20 21:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint92.pt
2023-05-20 21:09:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint92.pt (epoch 92 @ 5964 updates, score 3.469) (writing took 22.875984471291304 seconds)
2023-05-20 21:09:35 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2023-05-20 21:09:35 | INFO | train | epoch 092 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2270.7 | ups 0.19 | wpb 12057.9 | bsz 559.8 | num_updates 5964 | lr 0 | gnorm 1.33 | clip 100 | loss_scale 2 | train_wall 304 | gb_free 10 | wall 33018
2023-05-20 21:09:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:09:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:09:35 | INFO | fairseq.trainer | begin training epoch 93
2023-05-20 21:09:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:10:49 | INFO | train_inner | epoch 093:     16 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=1789, ups=0.15, wpb=11688.8, bsz=550.1, num_updates=5980, lr=0, gnorm=1.348, clip=100, loss_scale=2, train_wall=89, gb_free=9.2, wall=33092
2023-05-20 21:12:21 | INFO | train_inner | epoch 093:     36 / 65 loss=2.526, nll_loss=0.556, ppl=1.47, wps=2650.3, ups=0.22, wpb=12215.9, bsz=586.1, num_updates=6000, lr=0, gnorm=1.297, clip=100, loss_scale=2, train_wall=92, gb_free=9.5, wall=33184
2023-05-20 21:13:55 | INFO | train_inner | epoch 093:     56 / 65 loss=2.502, nll_loss=0.529, ppl=1.44, wps=2576, ups=0.21, wpb=12141.3, bsz=533.1, num_updates=6020, lr=0, gnorm=2.164, clip=100, loss_scale=2, train_wall=94, gb_free=9.5, wall=33278
2023-05-20 21:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:14:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:14:54 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1926.8 | wpb 2785 | bsz 105.2 | num_updates 6029 | best_loss 3.382
2023-05-20 21:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 6029 updates
2023-05-20 21:14:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint93.pt
2023-05-20 21:15:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint93.pt
2023-05-20 21:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint93.pt (epoch 93 @ 6029 updates, score 3.469) (writing took 27.407283261418343 seconds)
2023-05-20 21:15:22 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2023-05-20 21:15:22 | INFO | train | epoch 093 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2261 | ups 0.19 | wpb 12052.4 | bsz 559.3 | num_updates 6029 | lr 0 | gnorm 2.462 | clip 100 | loss_scale 2 | train_wall 300 | gb_free 8.9 | wall 33365
2023-05-20 21:15:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:15:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:15:22 | INFO | fairseq.trainer | begin training epoch 94
2023-05-20 21:15:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:16:12 | INFO | train_inner | epoch 094:     11 / 65 loss=2.524, nll_loss=0.554, ppl=1.47, wps=1730.9, ups=0.15, wpb=11819.9, bsz=559.8, num_updates=6040, lr=0, gnorm=4.169, clip=100, loss_scale=2, train_wall=91, gb_free=9.2, wall=33415
2023-05-20 21:17:50 | INFO | train_inner | epoch 094:     31 / 65 loss=2.503, nll_loss=0.53, ppl=1.44, wps=2440.8, ups=0.2, wpb=12007, bsz=513.3, num_updates=6060, lr=0, gnorm=1.998, clip=100, loss_scale=2, train_wall=98, gb_free=9.6, wall=33513
2023-05-20 21:19:22 | INFO | train_inner | epoch 094:     51 / 65 loss=2.528, nll_loss=0.557, ppl=1.47, wps=2656.7, ups=0.22, wpb=12205.9, bsz=587.4, num_updates=6080, lr=0, gnorm=1.104, clip=100, loss_scale=2, train_wall=92, gb_free=8.3, wall=33605
2023-05-20 21:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:20:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:20:45 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1944.5 | wpb 2785 | bsz 105.2 | num_updates 6094 | best_loss 3.382
2023-05-20 21:20:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 6094 updates
2023-05-20 21:20:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint94.pt
2023-05-20 21:21:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint94.pt
2023-05-20 21:21:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint94.pt (epoch 94 @ 6094 updates, score 3.469) (writing took 28.792230539023876 seconds)
2023-05-20 21:21:14 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2023-05-20 21:21:14 | INFO | train | epoch 094 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2225.3 | ups 0.18 | wpb 12057.4 | bsz 558.8 | num_updates 6094 | lr 0 | gnorm 1.524 | clip 100 | loss_scale 2 | train_wall 305 | gb_free 9 | wall 33717
2023-05-20 21:21:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:21:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:21:14 | INFO | fairseq.trainer | begin training epoch 95
2023-05-20 21:21:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:21:43 | INFO | train_inner | epoch 095:      6 / 65 loss=2.519, nll_loss=0.548, ppl=1.46, wps=1716.4, ups=0.14, wpb=12049, bsz=571.7, num_updates=6100, lr=0, gnorm=1.467, clip=100, loss_scale=2, train_wall=93, gb_free=9.5, wall=33746
2023-05-20 21:23:19 | INFO | train_inner | epoch 095:     26 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2510, ups=0.21, wpb=12169.6, bsz=575.3, num_updates=6120, lr=0, gnorm=1.272, clip=100, loss_scale=2, train_wall=97, gb_free=8.9, wall=33842
2023-05-20 21:24:54 | INFO | train_inner | epoch 095:     46 / 65 loss=2.51, nll_loss=0.538, ppl=1.45, wps=2537.1, ups=0.21, wpb=12040.4, bsz=548.2, num_updates=6140, lr=0, gnorm=1.543, clip=100, loss_scale=2, train_wall=95, gb_free=9.4, wall=33937
2023-05-20 21:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:26:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:26:39 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1931.1 | wpb 2785 | bsz 105.2 | num_updates 6159 | best_loss 3.382
2023-05-20 21:26:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 6159 updates
2023-05-20 21:26:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint95.pt
2023-05-20 21:26:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint95.pt
2023-05-20 21:27:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint95.pt (epoch 95 @ 6159 updates, score 3.469) (writing took 24.052877221256495 seconds)
2023-05-20 21:27:03 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2023-05-20 21:27:03 | INFO | train | epoch 095 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2242.9 | ups 0.19 | wpb 12049.4 | bsz 559.6 | num_updates 6159 | lr 0 | gnorm 1.557 | clip 100 | loss_scale 2 | train_wall 306 | gb_free 9.7 | wall 34066
2023-05-20 21:27:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:27:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:27:03 | INFO | fairseq.trainer | begin training epoch 96
2023-05-20 21:27:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:27:08 | INFO | train_inner | epoch 096:      1 / 65 loss=2.522, nll_loss=0.551, ppl=1.46, wps=1758.1, ups=0.15, wpb=11742.8, bsz=542.4, num_updates=6160, lr=0, gnorm=1.996, clip=100, loss_scale=2, train_wall=91, gb_free=9.2, wall=34071
2023-05-20 21:28:46 | INFO | train_inner | epoch 096:     21 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=2486.3, ups=0.2, wpb=12164.8, bsz=563.6, num_updates=6180, lr=0, gnorm=1.703, clip=100, loss_scale=2, train_wall=98, gb_free=9.6, wall=34169
2023-05-20 21:30:14 | INFO | train_inner | epoch 096:     41 / 65 loss=2.527, nll_loss=0.556, ppl=1.47, wps=2768.9, ups=0.23, wpb=12235.2, bsz=595.2, num_updates=6200, lr=0, gnorm=1.507, clip=100, loss_scale=2, train_wall=88, gb_free=9.5, wall=34257
2023-05-20 21:31:52 | INFO | train_inner | epoch 096:     61 / 65 loss=2.513, nll_loss=0.54, ppl=1.45, wps=2500.3, ups=0.2, wpb=12253.4, bsz=544.3, num_updates=6220, lr=0, gnorm=1.08, clip=100, loss_scale=2, train_wall=98, gb_free=8.8, wall=34355
2023-05-20 21:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:32:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:32:27 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1927.5 | wpb 2785 | bsz 105.2 | num_updates 6224 | best_loss 3.382
2023-05-20 21:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 6224 updates
2023-05-20 21:32:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint96.pt
2023-05-20 21:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint96.pt
2023-05-20 21:32:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint96.pt (epoch 96 @ 6224 updates, score 3.469) (writing took 19.11315804719925 seconds)
2023-05-20 21:32:46 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2023-05-20 21:32:46 | INFO | train | epoch 096 | loss 2.518 | nll_loss 0.547 | ppl 1.46 | wps 2288.5 | ups 0.19 | wpb 12056.1 | bsz 560.1 | num_updates 6224 | lr 0 | gnorm 1.57 | clip 100 | loss_scale 2 | train_wall 305 | gb_free 10 | wall 34409
2023-05-20 21:32:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:32:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:32:46 | INFO | fairseq.trainer | begin training epoch 97
2023-05-20 21:32:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:34:02 | INFO | train_inner | epoch 097:     16 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=1803.6, ups=0.15, wpb=11726.9, bsz=545.6, num_updates=6240, lr=0, gnorm=1.669, clip=100, loss_scale=2, train_wall=92, gb_free=8.3, wall=34485
2023-05-20 21:35:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-20 21:35:39 | INFO | train_inner | epoch 097:     37 / 65 loss=2.523, nll_loss=0.553, ppl=1.47, wps=2497.5, ups=0.21, wpb=12091.4, bsz=574.4, num_updates=6260, lr=0, gnorm=1.253, clip=100, loss_scale=2, train_wall=97, gb_free=10, wall=34582
2023-05-20 21:37:14 | INFO | train_inner | epoch 097:     57 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=2597.8, ups=0.21, wpb=12278.6, bsz=566.7, num_updates=6280, lr=0, gnorm=2.263, clip=100, loss_scale=2, train_wall=94, gb_free=9.9, wall=34677
2023-05-20 21:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:37:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:38:07 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1784.9 | wpb 2785 | bsz 105.2 | num_updates 6288 | best_loss 3.382
2023-05-20 21:38:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 6288 updates
2023-05-20 21:38:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint97.pt
2023-05-20 21:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint97.pt
2023-05-20 21:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint97.pt (epoch 97 @ 6288 updates, score 3.469) (writing took 20.714287128299475 seconds)
2023-05-20 21:38:28 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2023-05-20 21:38:28 | INFO | train | epoch 097 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2256.6 | ups 0.19 | wpb 12062.8 | bsz 558.6 | num_updates 6288 | lr 0 | gnorm 1.622 | clip 100 | loss_scale 2 | train_wall 301 | gb_free 9 | wall 34751
2023-05-20 21:38:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:38:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:38:28 | INFO | fairseq.trainer | begin training epoch 98
2023-05-20 21:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:39:23 | INFO | train_inner | epoch 098:     12 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=1813.8, ups=0.15, wpb=11765.4, bsz=560.6, num_updates=6300, lr=0, gnorm=1.433, clip=100, loss_scale=2, train_wall=89, gb_free=9.5, wall=34806
2023-05-20 21:40:57 | INFO | train_inner | epoch 098:     32 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=2631.6, ups=0.21, wpb=12259.4, bsz=565.7, num_updates=6320, lr=0, gnorm=1.816, clip=100, loss_scale=2, train_wall=93, gb_free=8.9, wall=34900
2023-05-20 21:42:29 | INFO | train_inner | epoch 098:     52 / 65 loss=2.522, nll_loss=0.551, ppl=1.46, wps=2641.8, ups=0.22, wpb=12153.4, bsz=554.3, num_updates=6340, lr=0, gnorm=1.924, clip=100, loss_scale=2, train_wall=92, gb_free=9.3, wall=34992
2023-05-20 21:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:43:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:43:46 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1852 | wpb 2785 | bsz 105.2 | num_updates 6353 | best_loss 3.382
2023-05-20 21:43:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 6353 updates
2023-05-20 21:43:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint98.pt
2023-05-20 21:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint98.pt
2023-05-20 21:44:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint98.pt (epoch 98 @ 6353 updates, score 3.469) (writing took 19.799157604575157 seconds)
2023-05-20 21:44:05 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2023-05-20 21:44:05 | INFO | train | epoch 098 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2320.9 | ups 0.19 | wpb 12055.9 | bsz 559.3 | num_updates 6353 | lr 0 | gnorm 1.57 | clip 100 | loss_scale 2 | train_wall 298 | gb_free 9.7 | wall 35088
2023-05-20 21:44:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:44:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:44:06 | INFO | fairseq.trainer | begin training epoch 99
2023-05-20 21:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:44:39 | INFO | train_inner | epoch 099:      7 / 65 loss=2.498, nll_loss=0.523, ppl=1.44, wps=1791.4, ups=0.15, wpb=11689.1, bsz=512.8, num_updates=6360, lr=0, gnorm=1.029, clip=100, loss_scale=2, train_wall=91, gb_free=8.8, wall=35122
2023-05-20 21:46:11 | INFO | train_inner | epoch 099:     27 / 65 loss=2.524, nll_loss=0.553, ppl=1.47, wps=2687.2, ups=0.22, wpb=12382.7, bsz=577.2, num_updates=6380, lr=0, gnorm=1.64, clip=100, loss_scale=2, train_wall=92, gb_free=9.4, wall=35214
2023-05-20 21:47:44 | INFO | train_inner | epoch 099:     47 / 65 loss=2.511, nll_loss=0.538, ppl=1.45, wps=2587.9, ups=0.22, wpb=12030.2, bsz=545.3, num_updates=6400, lr=0, gnorm=1.418, clip=100, loss_scale=2, train_wall=93, gb_free=9.9, wall=35307
2023-05-20 21:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:49:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:49:22 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1790.4 | wpb 2785 | bsz 105.2 | num_updates 6418 | best_loss 3.382
2023-05-20 21:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 6418 updates
2023-05-20 21:49:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint99.pt
2023-05-20 21:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint99.pt
2023-05-20 21:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint99.pt (epoch 99 @ 6418 updates, score 3.469) (writing took 20.94065348058939 seconds)
2023-05-20 21:49:43 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2023-05-20 21:49:43 | INFO | train | epoch 099 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2321.9 | ups 0.19 | wpb 12054.1 | bsz 559.1 | num_updates 6418 | lr 0 | gnorm 1.595 | clip 100 | loss_scale 2 | train_wall 296 | gb_free 10.1 | wall 35426
2023-05-20 21:49:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:49:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:49:43 | INFO | fairseq.trainer | begin training epoch 100
2023-05-20 21:49:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:49:53 | INFO | train_inner | epoch 100:      2 / 65 loss=2.529, nll_loss=0.559, ppl=1.47, wps=1824, ups=0.16, wpb=11706.2, bsz=566.1, num_updates=6420, lr=0, gnorm=2.072, clip=100, loss_scale=2, train_wall=88, gb_free=10, wall=35436
2023-05-20 21:51:25 | INFO | train_inner | epoch 100:     22 / 65 loss=2.525, nll_loss=0.555, ppl=1.47, wps=2670.4, ups=0.22, wpb=12305.1, bsz=582.1, num_updates=6440, lr=0, gnorm=1.507, clip=100, loss_scale=2, train_wall=92, gb_free=9.7, wall=35528
2023-05-20 21:53:01 | INFO | train_inner | epoch 100:     42 / 65 loss=2.511, nll_loss=0.539, ppl=1.45, wps=2516.6, ups=0.21, wpb=12169.4, bsz=549.1, num_updates=6460, lr=0, gnorm=1.226, clip=100, loss_scale=2, train_wall=97, gb_free=9.5, wall=35624
2023-05-20 21:54:37 | INFO | train_inner | epoch 100:     62 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=2552.9, ups=0.21, wpb=12142.2, bsz=579.8, num_updates=6480, lr=0, gnorm=1.046, clip=100, loss_scale=2, train_wall=95, gb_free=9.6, wall=35720
2023-05-20 21:54:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:54:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:55:08 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1769.3 | wpb 2785 | bsz 105.2 | num_updates 6483 | best_loss 3.382
2023-05-20 21:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 6483 updates
2023-05-20 21:55:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint100.pt
2023-05-20 21:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint100.pt
2023-05-20 21:55:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint100.pt (epoch 100 @ 6483 updates, score 3.469) (writing took 22.24256258457899 seconds)
2023-05-20 21:55:30 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2023-05-20 21:55:30 | INFO | train | epoch 100 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2255 | ups 0.19 | wpb 12052.6 | bsz 558.3 | num_updates 6483 | lr 0 | gnorm 1.33 | clip 100 | loss_scale 2 | train_wall 305 | gb_free 9.9 | wall 35773
2023-05-20 21:55:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:55:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 21:55:30 | INFO | fairseq.trainer | begin training epoch 101
2023-05-20 21:55:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:56:50 | INFO | train_inner | epoch 101:     17 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=1778.6, ups=0.15, wpb=11870.3, bsz=546.7, num_updates=6500, lr=0, gnorm=1.641, clip=100, loss_scale=2, train_wall=91, gb_free=9.3, wall=35853
2023-05-20 21:56:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 21:58:27 | INFO | train_inner | epoch 101:     38 / 65 loss=2.513, nll_loss=0.54, ppl=1.45, wps=2505.1, ups=0.21, wpb=12097.9, bsz=543.8, num_updates=6520, lr=0, gnorm=1.548, clip=100, loss_scale=1, train_wall=96, gb_free=8.8, wall=35950
2023-05-20 21:59:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 22:00:06 | INFO | train_inner | epoch 101:     59 / 65 loss=2.51, nll_loss=0.537, ppl=1.45, wps=2453.3, ups=0.2, wpb=12130.1, bsz=534.6, num_updates=6540, lr=0, gnorm=1.819, clip=100, loss_scale=0.5, train_wall=99, gb_free=10.1, wall=36048
2023-05-20 22:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:00:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:00:50 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1936.1 | wpb 2785 | bsz 105.2 | num_updates 6546 | best_loss 3.382
2023-05-20 22:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 6546 updates
2023-05-20 22:00:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint101.pt
2023-05-20 22:01:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint101.pt
2023-05-20 22:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint101.pt (epoch 101 @ 6546 updates, score 3.469) (writing took 20.584782484918833 seconds)
2023-05-20 22:01:10 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2023-05-20 22:01:10 | INFO | train | epoch 101 | loss 2.515 | nll_loss 0.543 | ppl 1.46 | wps 2229.6 | ups 0.19 | wpb 12028.9 | bsz 547.7 | num_updates 6546 | lr 0 | gnorm 1.644 | clip 100 | loss_scale 0.5 | train_wall 301 | gb_free 9.9 | wall 36113
2023-05-20 22:01:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:01:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:01:10 | INFO | fairseq.trainer | begin training epoch 102
2023-05-20 22:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:02:17 | INFO | train_inner | epoch 102:     14 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=1765, ups=0.15, wpb=11638.2, bsz=562.5, num_updates=6560, lr=0, gnorm=1.435, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.8, wall=36180
2023-05-20 22:03:50 | INFO | train_inner | epoch 102:     34 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=2676.2, ups=0.21, wpb=12451, bsz=579.4, num_updates=6580, lr=0, gnorm=1.6, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.6, wall=36273
2023-05-20 22:05:24 | INFO | train_inner | epoch 102:     54 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2575, ups=0.21, wpb=12067.5, bsz=556.2, num_updates=6600, lr=0, gnorm=2.032, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.9, wall=36367
2023-05-20 22:06:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:06:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:06:33 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1927.4 | wpb 2785 | bsz 105.2 | num_updates 6611 | best_loss 3.382
2023-05-20 22:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 6611 updates
2023-05-20 22:06:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint102.pt
2023-05-20 22:06:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint102.pt
2023-05-20 22:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint102.pt (epoch 102 @ 6611 updates, score 3.469) (writing took 22.962058424949646 seconds)
2023-05-20 22:06:56 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2023-05-20 22:06:56 | INFO | train | epoch 102 | loss 2.518 | nll_loss 0.547 | ppl 1.46 | wps 2265.5 | ups 0.19 | wpb 12055.3 | bsz 558.8 | num_updates 6611 | lr 0 | gnorm 1.693 | clip 100 | loss_scale 0.5 | train_wall 304 | gb_free 9.4 | wall 36459
2023-05-20 22:06:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:06:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:06:56 | INFO | fairseq.trainer | begin training epoch 103
2023-05-20 22:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:07:36 | INFO | train_inner | epoch 103:      9 / 65 loss=2.508, nll_loss=0.536, ppl=1.45, wps=1769, ups=0.15, wpb=11669.5, bsz=522.9, num_updates=6620, lr=0, gnorm=1.452, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.5, wall=36499
2023-05-20 22:09:12 | INFO | train_inner | epoch 103:     29 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=2542.1, ups=0.21, wpb=12173.4, bsz=562.3, num_updates=6640, lr=0, gnorm=1.207, clip=100, loss_scale=0.5, train_wall=96, gb_free=10.1, wall=36595
2023-05-20 22:10:44 | INFO | train_inner | epoch 103:     49 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2648.6, ups=0.22, wpb=12203.5, bsz=571.1, num_updates=6660, lr=0, gnorm=1.242, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.4, wall=36687
2023-05-20 22:11:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:11:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:12:18 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1809.6 | wpb 2785 | bsz 105.2 | num_updates 6676 | best_loss 3.382
2023-05-20 22:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 6676 updates
2023-05-20 22:12:18 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint103.pt
2023-05-20 22:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint103.pt
2023-05-20 22:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint103.pt (epoch 103 @ 6676 updates, score 3.469) (writing took 21.712627667933702 seconds)
2023-05-20 22:12:39 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2023-05-20 22:12:39 | INFO | train | epoch 103 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2284.4 | ups 0.19 | wpb 12060.2 | bsz 559.4 | num_updates 6676 | lr 0 | gnorm 1.32 | clip 100 | loss_scale 0.5 | train_wall 302 | gb_free 9.9 | wall 36802
2023-05-20 22:12:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:12:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:12:39 | INFO | fairseq.trainer | begin training epoch 104
2023-05-20 22:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:12:59 | INFO | train_inner | epoch 104:      4 / 65 loss=2.517, nll_loss=0.545, ppl=1.46, wps=1754.7, ups=0.15, wpb=11855.8, bsz=558.2, num_updates=6680, lr=0, gnorm=1.45, clip=100, loss_scale=0.5, train_wall=94, gb_free=10, wall=36822
2023-05-20 22:14:32 | INFO | train_inner | epoch 104:     24 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2597.3, ups=0.21, wpb=12099.4, bsz=556.2, num_updates=6700, lr=0, gnorm=1.725, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.1, wall=36915
2023-05-20 22:16:06 | INFO | train_inner | epoch 104:     44 / 65 loss=2.515, nll_loss=0.542, ppl=1.46, wps=2630.5, ups=0.21, wpb=12315.8, bsz=562, num_updates=6720, lr=0, gnorm=1.579, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.6, wall=37009
2023-05-20 22:17:39 | INFO | train_inner | epoch 104:     64 / 65 loss=2.525, nll_loss=0.554, ppl=1.47, wps=2600, ups=0.21, wpb=12111.9, bsz=569.6, num_updates=6740, lr=0, gnorm=0.981, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.8, wall=37102
2023-05-20 22:17:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:17:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:18:00 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1956.4 | wpb 2785 | bsz 105.2 | num_updates 6741 | best_loss 3.382
2023-05-20 22:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 6741 updates
2023-05-20 22:18:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint104.pt
2023-05-20 22:18:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint104.pt
2023-05-20 22:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint104.pt (epoch 104 @ 6741 updates, score 3.469) (writing took 20.348037373274565 seconds)
2023-05-20 22:18:21 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2023-05-20 22:18:21 | INFO | train | epoch 104 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2295.3 | ups 0.19 | wpb 12058.1 | bsz 560 | num_updates 6741 | lr 0 | gnorm 1.415 | clip 100 | loss_scale 0.5 | train_wall 303 | gb_free 9.4 | wall 37144
2023-05-20 22:18:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:18:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:18:21 | INFO | fairseq.trainer | begin training epoch 105
2023-05-20 22:18:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:19:48 | INFO | train_inner | epoch 105:     19 / 65 loss=2.534, nll_loss=0.565, ppl=1.48, wps=1823.7, ups=0.16, wpb=11753.4, bsz=571.1, num_updates=6760, lr=0, gnorm=1.684, clip=100, loss_scale=0.5, train_wall=90, gb_free=10.1, wall=37231
2023-05-20 22:21:24 | INFO | train_inner | epoch 105:     39 / 65 loss=2.509, nll_loss=0.536, ppl=1.45, wps=2524.5, ups=0.21, wpb=12159.9, bsz=548.4, num_updates=6780, lr=0, gnorm=1.862, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.2, wall=37327
2023-05-20 22:22:54 | INFO | train_inner | epoch 105:     59 / 65 loss=2.507, nll_loss=0.534, ppl=1.45, wps=2708.1, ups=0.22, wpb=12188.9, bsz=563.6, num_updates=6800, lr=0, gnorm=1.752, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.7, wall=37417
2023-05-20 22:23:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:23:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:23:40 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1761.3 | wpb 2785 | bsz 105.2 | num_updates 6806 | best_loss 3.382
2023-05-20 22:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 6806 updates
2023-05-20 22:23:40 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint105.pt
2023-05-20 22:23:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint105.pt
2023-05-20 22:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint105.pt (epoch 105 @ 6806 updates, score 3.469) (writing took 18.689568255096674 seconds)
2023-05-20 22:23:59 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2023-05-20 22:23:59 | INFO | train | epoch 105 | loss 2.516 | nll_loss 0.544 | ppl 1.46 | wps 2318.3 | ups 0.19 | wpb 12046.5 | bsz 557.2 | num_updates 6806 | lr 0 | gnorm 1.77 | clip 100 | loss_scale 0.5 | train_wall 299 | gb_free 9.1 | wall 37482
2023-05-20 22:23:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:23:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:23:59 | INFO | fairseq.trainer | begin training epoch 106
2023-05-20 22:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:25:06 | INFO | train_inner | epoch 106:     14 / 65 loss=2.504, nll_loss=0.531, ppl=1.45, wps=1791.2, ups=0.15, wpb=11799.2, bsz=517, num_updates=6820, lr=0, gnorm=1.968, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.2, wall=37549
2023-05-20 22:26:37 | INFO | train_inner | epoch 106:     34 / 65 loss=2.529, nll_loss=0.558, ppl=1.47, wps=2693.2, ups=0.22, wpb=12199.2, bsz=574.3, num_updates=6840, lr=0, gnorm=1.527, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.3, wall=37640
2023-05-20 22:28:10 | INFO | train_inner | epoch 106:     54 / 65 loss=2.513, nll_loss=0.541, ppl=1.46, wps=2597.2, ups=0.21, wpb=12115.2, bsz=574, num_updates=6860, lr=0, gnorm=1.327, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.5, wall=37733
2023-05-20 22:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:28:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:29:17 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1865.5 | wpb 2785 | bsz 105.2 | num_updates 6871 | best_loss 3.382
2023-05-20 22:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 6871 updates
2023-05-20 22:29:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint106.pt
2023-05-20 22:29:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint106.pt
2023-05-20 22:29:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint106.pt (epoch 106 @ 6871 updates, score 3.469) (writing took 20.00412867218256 seconds)
2023-05-20 22:29:37 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2023-05-20 22:29:37 | INFO | train | epoch 106 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2311.3 | ups 0.19 | wpb 12053.2 | bsz 559.5 | num_updates 6871 | lr 0 | gnorm 1.561 | clip 100 | loss_scale 0.5 | train_wall 300 | gb_free 9.5 | wall 37820
2023-05-20 22:29:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:29:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:29:38 | INFO | fairseq.trainer | begin training epoch 107
2023-05-20 22:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:30:18 | INFO | train_inner | epoch 107:      9 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=1829.7, ups=0.16, wpb=11747.2, bsz=539.4, num_updates=6880, lr=0, gnorm=1.29, clip=100, loss_scale=0.5, train_wall=90, gb_free=9, wall=37861
2023-05-20 22:31:53 | INFO | train_inner | epoch 107:     29 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=2589.8, ups=0.21, wpb=12280.8, bsz=584, num_updates=6900, lr=0, gnorm=1.994, clip=100, loss_scale=0.5, train_wall=95, gb_free=8.7, wall=37956
2023-05-20 22:33:25 | INFO | train_inner | epoch 107:     49 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=2618.4, ups=0.22, wpb=12043, bsz=553.1, num_updates=6920, lr=0, gnorm=1.695, clip=100, loss_scale=0.5, train_wall=92, gb_free=8.1, wall=38048
2023-05-20 22:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:34:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:34:58 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1790.2 | wpb 2785 | bsz 105.2 | num_updates 6936 | best_loss 3.382
2023-05-20 22:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 6936 updates
2023-05-20 22:34:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint107.pt
2023-05-20 22:35:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint107.pt
2023-05-20 22:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint107.pt (epoch 107 @ 6936 updates, score 3.469) (writing took 40.12915638834238 seconds)
2023-05-20 22:35:38 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2023-05-20 22:35:38 | INFO | train | epoch 107 | loss 2.515 | nll_loss 0.544 | ppl 1.46 | wps 2172.5 | ups 0.18 | wpb 12051.7 | bsz 558.6 | num_updates 6936 | lr 0 | gnorm 1.837 | clip 100 | loss_scale 0.5 | train_wall 300 | gb_free 9.8 | wall 38181
2023-05-20 22:35:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:35:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:35:38 | INFO | fairseq.trainer | begin training epoch 108
2023-05-20 22:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:35:56 | INFO | train_inner | epoch 108:      4 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=1560.2, ups=0.13, wpb=11771.4, bsz=547.1, num_updates=6940, lr=0, gnorm=2.189, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.5, wall=38199
2023-05-20 22:37:27 | INFO | train_inner | epoch 108:     24 / 65 loss=2.506, nll_loss=0.532, ppl=1.45, wps=2662, ups=0.22, wpb=12095, bsz=530.7, num_updates=6960, lr=0, gnorm=1.165, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.7, wall=38290
2023-05-20 22:39:02 | INFO | train_inner | epoch 108:     44 / 65 loss=2.511, nll_loss=0.539, ppl=1.45, wps=2567.6, ups=0.21, wpb=12227.4, bsz=570.9, num_updates=6980, lr=0, gnorm=1.637, clip=100, loss_scale=0.5, train_wall=95, gb_free=8.7, wall=38385
2023-05-20 22:40:36 | INFO | train_inner | epoch 108:     64 / 65 loss=2.532, nll_loss=0.562, ppl=1.48, wps=2628.3, ups=0.21, wpb=12261.3, bsz=586.7, num_updates=7000, lr=0, gnorm=2.526, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.5, wall=38479
2023-05-20 22:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:40:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:40:58 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1790.5 | wpb 2785 | bsz 105.2 | num_updates 7001 | best_loss 3.382
2023-05-20 22:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 7001 updates
2023-05-20 22:40:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint108.pt
2023-05-20 22:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint108.pt
2023-05-20 22:41:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint108.pt (epoch 108 @ 7001 updates, score 3.469) (writing took 23.66302704066038 seconds)
2023-05-20 22:41:21 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2023-05-20 22:41:21 | INFO | train | epoch 108 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2282 | ups 0.19 | wpb 12051.6 | bsz 559 | num_updates 7001 | lr 0 | gnorm 1.789 | clip 100 | loss_scale 0.5 | train_wall 300 | gb_free 10 | wall 38524
2023-05-20 22:41:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:41:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:41:21 | INFO | fairseq.trainer | begin training epoch 109
2023-05-20 22:41:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:42:49 | INFO | train_inner | epoch 109:     19 / 65 loss=2.518, nll_loss=0.547, ppl=1.46, wps=1782.4, ups=0.15, wpb=11936.2, bsz=568.1, num_updates=7020, lr=0, gnorm=1.317, clip=100, loss_scale=0.5, train_wall=90, gb_free=10.1, wall=38612
2023-05-20 22:44:26 | INFO | train_inner | epoch 109:     39 / 65 loss=2.513, nll_loss=0.54, ppl=1.45, wps=2519.7, ups=0.21, wpb=12157.2, bsz=558.6, num_updates=7040, lr=0, gnorm=1.142, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.4, wall=38709
2023-05-20 22:46:00 | INFO | train_inner | epoch 109:     59 / 65 loss=2.514, nll_loss=0.541, ppl=1.46, wps=2563.8, ups=0.21, wpb=12070.8, bsz=552.3, num_updates=7060, lr=0, gnorm=1.643, clip=100, loss_scale=1, train_wall=94, gb_free=9, wall=38803
2023-05-20 22:46:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:46:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:46:45 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1753.5 | wpb 2785 | bsz 105.2 | num_updates 7066 | best_loss 3.382
2023-05-20 22:46:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 7066 updates
2023-05-20 22:46:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint109.pt
2023-05-20 22:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint109.pt
2023-05-20 22:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint109.pt (epoch 109 @ 7066 updates, score 3.469) (writing took 20.45520192757249 seconds)
2023-05-20 22:47:05 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2023-05-20 22:47:05 | INFO | train | epoch 109 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2279.3 | ups 0.19 | wpb 12056.8 | bsz 560.1 | num_updates 7066 | lr 0 | gnorm 1.343 | clip 100 | loss_scale 1 | train_wall 303 | gb_free 9.4 | wall 38868
2023-05-20 22:47:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:47:05 | INFO | fairseq.trainer | begin training epoch 110
2023-05-20 22:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:48:10 | INFO | train_inner | epoch 110:     14 / 65 loss=2.519, nll_loss=0.548, ppl=1.46, wps=1772.4, ups=0.15, wpb=11516.4, bsz=532.9, num_updates=7080, lr=0, gnorm=1.371, clip=100, loss_scale=1, train_wall=89, gb_free=9.8, wall=38933
2023-05-20 22:49:45 | INFO | train_inner | epoch 110:     34 / 65 loss=2.515, nll_loss=0.543, ppl=1.46, wps=2557.8, ups=0.21, wpb=12116.2, bsz=536, num_updates=7100, lr=0, gnorm=1.334, clip=100, loss_scale=1, train_wall=95, gb_free=9.5, wall=39028
2023-05-20 22:51:15 | INFO | train_inner | epoch 110:     54 / 65 loss=2.526, nll_loss=0.556, ppl=1.47, wps=2782.6, ups=0.22, wpb=12557.2, bsz=617.2, num_updates=7120, lr=0, gnorm=1.823, clip=100, loss_scale=1, train_wall=90, gb_free=10, wall=39118
2023-05-20 22:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:52:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:52:24 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1802.1 | wpb 2785 | bsz 105.2 | num_updates 7131 | best_loss 3.382
2023-05-20 22:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 7131 updates
2023-05-20 22:52:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint110.pt
2023-05-20 22:52:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint110.pt
2023-05-20 22:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint110.pt (epoch 110 @ 7131 updates, score 3.469) (writing took 23.189243894070387 seconds)
2023-05-20 22:52:47 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2023-05-20 22:52:47 | INFO | train | epoch 110 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2289.3 | ups 0.19 | wpb 12055.4 | bsz 559.6 | num_updates 7131 | lr 0 | gnorm 1.613 | clip 100 | loss_scale 1 | train_wall 299 | gb_free 9.9 | wall 39210
2023-05-20 22:52:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:52:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:52:47 | INFO | fairseq.trainer | begin training epoch 111
2023-05-20 22:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:53:26 | INFO | train_inner | epoch 111:      9 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=1778.7, ups=0.15, wpb=11658.7, bsz=537.9, num_updates=7140, lr=0, gnorm=2.001, clip=100, loss_scale=1, train_wall=88, gb_free=9.1, wall=39249
2023-05-20 22:54:59 | INFO | train_inner | epoch 111:     29 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2641.5, ups=0.21, wpb=12315.4, bsz=567.4, num_updates=7160, lr=0, gnorm=1.258, clip=100, loss_scale=1, train_wall=93, gb_free=9.5, wall=39342
2023-05-20 22:56:33 | INFO | train_inner | epoch 111:     49 / 65 loss=2.515, nll_loss=0.543, ppl=1.46, wps=2583.3, ups=0.21, wpb=12035.8, bsz=542.1, num_updates=7180, lr=0, gnorm=1.771, clip=100, loss_scale=1, train_wall=93, gb_free=10, wall=39436
2023-05-20 22:56:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 22:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:57:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:58:06 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1794.6 | wpb 2785 | bsz 105.2 | num_updates 7195 | best_loss 3.382
2023-05-20 22:58:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 7195 updates
2023-05-20 22:58:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint111.pt
2023-05-20 22:58:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint111.pt
2023-05-20 22:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint111.pt (epoch 111 @ 7195 updates, score 3.469) (writing took 21.113821174949408 seconds)
2023-05-20 22:58:27 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2023-05-20 22:58:27 | INFO | train | epoch 111 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2270.9 | ups 0.19 | wpb 12047.8 | bsz 559.7 | num_updates 7195 | lr 0 | gnorm 1.592 | clip 100 | loss_scale 0.5 | train_wall 298 | gb_free 9.8 | wall 39550
2023-05-20 22:58:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:58:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 22:58:27 | INFO | fairseq.trainer | begin training epoch 112
2023-05-20 22:58:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:58:51 | INFO | train_inner | epoch 112:      5 / 65 loss=2.531, nll_loss=0.561, ppl=1.48, wps=1705.7, ups=0.14, wpb=11785.4, bsz=586.6, num_updates=7200, lr=0, gnorm=1.854, clip=100, loss_scale=0.5, train_wall=97, gb_free=9.8, wall=39574
2023-05-20 23:00:20 | INFO | train_inner | epoch 112:     25 / 65 loss=2.519, nll_loss=0.547, ppl=1.46, wps=2702, ups=0.22, wpb=12114.5, bsz=557.3, num_updates=7220, lr=0, gnorm=1.383, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.5, wall=39663
2023-05-20 23:01:55 | INFO | train_inner | epoch 112:     45 / 65 loss=2.512, nll_loss=0.54, ppl=1.45, wps=2545.8, ups=0.21, wpb=12056.8, bsz=537.5, num_updates=7240, lr=0, gnorm=1.716, clip=100, loss_scale=0.5, train_wall=95, gb_free=9.8, wall=39758
2023-05-20 23:03:29 | INFO | train_inner | epoch 112:     65 / 65 loss=2.514, nll_loss=0.542, ppl=1.46, wps=2558.9, ups=0.21, wpb=11966.2, bsz=564.2, num_updates=7260, lr=0, gnorm=1.789, clip=100, loss_scale=0.5, train_wall=93, gb_free=10.1, wall=39852
2023-05-20 23:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:03:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:03:48 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1777.4 | wpb 2785 | bsz 105.2 | num_updates 7260 | best_loss 3.382
2023-05-20 23:03:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 7260 updates
2023-05-20 23:03:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint112.pt
2023-05-20 23:04:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint112.pt
2023-05-20 23:04:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint112.pt (epoch 112 @ 7260 updates, score 3.469) (writing took 20.889933094382286 seconds)
2023-05-20 23:04:09 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2023-05-20 23:04:09 | INFO | train | epoch 112 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2291.7 | ups 0.19 | wpb 12061.6 | bsz 559.8 | num_updates 7260 | lr 0 | gnorm 1.716 | clip 100 | loss_scale 0.5 | train_wall 301 | gb_free 10.1 | wall 39892
2023-05-20 23:04:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:04:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:04:09 | INFO | fairseq.trainer | begin training epoch 113
2023-05-20 23:04:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:05:40 | INFO | train_inner | epoch 113:     20 / 65 loss=2.523, nll_loss=0.55, ppl=1.46, wps=1852.6, ups=0.15, wpb=12158.9, bsz=557.1, num_updates=7280, lr=0, gnorm=1.428, clip=100, loss_scale=0.5, train_wall=91, gb_free=8.9, wall=39983
2023-05-20 23:07:13 | INFO | train_inner | epoch 113:     40 / 65 loss=2.511, nll_loss=0.54, ppl=1.45, wps=2637.5, ups=0.21, wpb=12282.4, bsz=575.5, num_updates=7300, lr=0, gnorm=1.662, clip=100, loss_scale=0.5, train_wall=93, gb_free=10.1, wall=40076
2023-05-20 23:08:45 | INFO | train_inner | epoch 113:     60 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2649.8, ups=0.22, wpb=12194.5, bsz=580.6, num_updates=7320, lr=0, gnorm=2.24, clip=100, loss_scale=0.5, train_wall=92, gb_free=9.1, wall=40168
2023-05-20 23:09:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:09:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:09:28 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1798.2 | wpb 2785 | bsz 105.2 | num_updates 7325 | best_loss 3.382
2023-05-20 23:09:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 7325 updates
2023-05-20 23:09:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint113.pt
2023-05-20 23:09:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint113.pt
2023-05-20 23:09:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint113.pt (epoch 113 @ 7325 updates, score 3.469) (writing took 20.34406415373087 seconds)
2023-05-20 23:09:48 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2023-05-20 23:09:48 | INFO | train | epoch 113 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2311.7 | ups 0.19 | wpb 12054.6 | bsz 558.9 | num_updates 7325 | lr 0 | gnorm 1.742 | clip 100 | loss_scale 0.5 | train_wall 299 | gb_free 9.8 | wall 40231
2023-05-20 23:09:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:09:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:09:48 | INFO | fairseq.trainer | begin training epoch 114
2023-05-20 23:09:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:10:57 | INFO | train_inner | epoch 114:     15 / 65 loss=2.504, nll_loss=0.53, ppl=1.44, wps=1768.9, ups=0.15, wpb=11620.6, bsz=513.8, num_updates=7340, lr=0, gnorm=1.248, clip=100, loss_scale=0.5, train_wall=91, gb_free=10, wall=40300
2023-05-20 23:12:33 | INFO | train_inner | epoch 114:     35 / 65 loss=2.518, nll_loss=0.545, ppl=1.46, wps=2511.6, ups=0.21, wpb=12085.7, bsz=556, num_updates=7360, lr=0, gnorm=1.408, clip=100, loss_scale=0.5, train_wall=96, gb_free=9.3, wall=40396
2023-05-20 23:14:07 | INFO | train_inner | epoch 114:     55 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2623.6, ups=0.21, wpb=12351.1, bsz=559.7, num_updates=7380, lr=0, gnorm=1.054, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.1, wall=40490
2023-05-20 23:14:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:14:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:15:12 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1762.7 | wpb 2785 | bsz 105.2 | num_updates 7390 | best_loss 3.382
2023-05-20 23:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 7390 updates
2023-05-20 23:15:12 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint114.pt
2023-05-20 23:15:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint114.pt
2023-05-20 23:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint114.pt (epoch 114 @ 7390 updates, score 3.469) (writing took 20.044162053614855 seconds)
2023-05-20 23:15:32 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2023-05-20 23:15:32 | INFO | train | epoch 114 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2276.4 | ups 0.19 | wpb 12052.3 | bsz 559.4 | num_updates 7390 | lr 0 | gnorm 1.35 | clip 100 | loss_scale 0.5 | train_wall 304 | gb_free 9.8 | wall 40575
2023-05-20 23:15:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:15:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:15:32 | INFO | fairseq.trainer | begin training epoch 115
2023-05-20 23:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:16:17 | INFO | train_inner | epoch 115:     10 / 65 loss=2.528, nll_loss=0.557, ppl=1.47, wps=1788.6, ups=0.15, wpb=11672.2, bsz=591.8, num_updates=7400, lr=0, gnorm=1.823, clip=100, loss_scale=0.5, train_wall=90, gb_free=9.7, wall=40620
2023-05-20 23:17:51 | INFO | train_inner | epoch 115:     30 / 65 loss=2.511, nll_loss=0.539, ppl=1.45, wps=2639.5, ups=0.21, wpb=12299.7, bsz=553, num_updates=7420, lr=0, gnorm=1.022, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.5, wall=40714
2023-05-20 23:19:25 | INFO | train_inner | epoch 115:     50 / 65 loss=2.506, nll_loss=0.534, ppl=1.45, wps=2564.2, ups=0.21, wpb=12044.9, bsz=539.6, num_updates=7440, lr=0, gnorm=1.434, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.4, wall=40808
2023-05-20 23:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:20:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:20:49 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1820.2 | wpb 2785 | bsz 105.2 | num_updates 7455 | best_loss 3.382
2023-05-20 23:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 7455 updates
2023-05-20 23:20:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint115.pt
2023-05-20 23:21:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint115.pt
2023-05-20 23:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint115.pt (epoch 115 @ 7455 updates, score 3.469) (writing took 19.958020854741335 seconds)
2023-05-20 23:21:09 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2023-05-20 23:21:09 | INFO | train | epoch 115 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2325.5 | ups 0.19 | wpb 12058.6 | bsz 559.8 | num_updates 7455 | lr 0 | gnorm 1.577 | clip 100 | loss_scale 0.5 | train_wall 297 | gb_free 9.8 | wall 40912
2023-05-20 23:21:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:21:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:21:09 | INFO | fairseq.trainer | begin training epoch 116
2023-05-20 23:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:21:32 | INFO | train_inner | epoch 116:      5 / 65 loss=2.527, nll_loss=0.557, ppl=1.47, wps=1872.5, ups=0.16, wpb=11897.2, bsz=565.5, num_updates=7460, lr=0, gnorm=2.544, clip=100, loss_scale=0.5, train_wall=88, gb_free=10.1, wall=40935
2023-05-20 23:23:05 | INFO | train_inner | epoch 116:     25 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=2622.1, ups=0.21, wpb=12273.8, bsz=579.6, num_updates=7480, lr=0, gnorm=1.302, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.8, wall=41028
2023-05-20 23:24:38 | INFO | train_inner | epoch 116:     45 / 65 loss=2.515, nll_loss=0.544, ppl=1.46, wps=2625.8, ups=0.22, wpb=12183.9, bsz=552.9, num_updates=7500, lr=0, gnorm=1.467, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.5, wall=41121
2023-05-20 23:26:09 | INFO | train_inner | epoch 116:     65 / 65 loss=2.518, nll_loss=0.546, ppl=1.46, wps=2563.2, ups=0.22, wpb=11677.2, bsz=549.1, num_updates=7520, lr=0, gnorm=1.607, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.4, wall=41212
2023-05-20 23:26:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:26:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:26:27 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1945.5 | wpb 2785 | bsz 105.2 | num_updates 7520 | best_loss 3.382
2023-05-20 23:26:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 7520 updates
2023-05-20 23:26:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint116.pt
2023-05-20 23:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint116.pt
2023-05-20 23:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint116.pt (epoch 116 @ 7520 updates, score 3.469) (writing took 19.243718095123768 seconds)
2023-05-20 23:26:46 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2023-05-20 23:26:46 | INFO | train | epoch 116 | loss 2.518 | nll_loss 0.546 | ppl 1.46 | wps 2326 | ups 0.19 | wpb 12062.8 | bsz 560.1 | num_updates 7520 | lr 0 | gnorm 1.559 | clip 100 | loss_scale 0.5 | train_wall 299 | gb_free 9.4 | wall 41249
2023-05-20 23:26:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:26:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:26:46 | INFO | fairseq.trainer | begin training epoch 117
2023-05-20 23:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:28:19 | INFO | train_inner | epoch 117:     20 / 65 loss=2.521, nll_loss=0.55, ppl=1.46, wps=1888.3, ups=0.15, wpb=12263.6, bsz=586, num_updates=7540, lr=0, gnorm=1.569, clip=100, loss_scale=0.5, train_wall=92, gb_free=9, wall=41342
2023-05-20 23:29:54 | INFO | train_inner | epoch 117:     40 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=2538.3, ups=0.21, wpb=12059.1, bsz=535.4, num_updates=7560, lr=0, gnorm=1.706, clip=100, loss_scale=0.5, train_wall=95, gb_free=9.8, wall=41437
2023-05-20 23:31:29 | INFO | train_inner | epoch 117:     60 / 65 loss=2.522, nll_loss=0.55, ppl=1.46, wps=2607.6, ups=0.21, wpb=12319.8, bsz=581.7, num_updates=7580, lr=0, gnorm=1.175, clip=100, loss_scale=0.5, train_wall=94, gb_free=9.2, wall=41532
2023-05-20 23:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:31:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:32:09 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1780.8 | wpb 2785 | bsz 105.2 | num_updates 7585 | best_loss 3.382
2023-05-20 23:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 7585 updates
2023-05-20 23:32:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint117.pt
2023-05-20 23:32:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint117.pt
2023-05-20 23:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint117.pt (epoch 117 @ 7585 updates, score 3.469) (writing took 20.938829723745584 seconds)
2023-05-20 23:32:30 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2023-05-20 23:32:30 | INFO | train | epoch 117 | loss 2.517 | nll_loss 0.546 | ppl 1.46 | wps 2281.9 | ups 0.19 | wpb 12056.5 | bsz 560.3 | num_updates 7585 | lr 0 | gnorm 1.767 | clip 100 | loss_scale 0.5 | train_wall 302 | gb_free 8.5 | wall 41593
2023-05-20 23:32:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:32:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:32:30 | INFO | fairseq.trainer | begin training epoch 118
2023-05-20 23:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:33:41 | INFO | train_inner | epoch 118:     15 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=1783, ups=0.15, wpb=11794.7, bsz=530.9, num_updates=7600, lr=0, gnorm=2.44, clip=100, loss_scale=0.5, train_wall=91, gb_free=10.1, wall=41664
2023-05-20 23:35:14 | INFO | train_inner | epoch 118:     35 / 65 loss=2.496, nll_loss=0.523, ppl=1.44, wps=2562.6, ups=0.22, wpb=11881.5, bsz=515.1, num_updates=7620, lr=0, gnorm=2.017, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.7, wall=41757
2023-05-20 23:36:45 | INFO | train_inner | epoch 118:     55 / 65 loss=2.536, nll_loss=0.566, ppl=1.48, wps=2694.8, ups=0.22, wpb=12263.3, bsz=612.6, num_updates=7640, lr=0, gnorm=2.85, clip=100, loss_scale=0.5, train_wall=91, gb_free=9.7, wall=41848
2023-05-20 23:37:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:37:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:37:44 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1939.9 | wpb 2785 | bsz 105.2 | num_updates 7650 | best_loss 3.382
2023-05-20 23:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 7650 updates
2023-05-20 23:37:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint118.pt
2023-05-20 23:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint118.pt
2023-05-20 23:38:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint118.pt (epoch 118 @ 7650 updates, score 3.469) (writing took 24.519202046096325 seconds)
2023-05-20 23:38:08 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2023-05-20 23:38:08 | INFO | train | epoch 118 | loss 2.516 | nll_loss 0.544 | ppl 1.46 | wps 2315.1 | ups 0.19 | wpb 12053.5 | bsz 557.3 | num_updates 7650 | lr 0 | gnorm 2.046 | clip 100 | loss_scale 0.5 | train_wall 295 | gb_free 10 | wall 41931
2023-05-20 23:38:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:38:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:38:08 | INFO | fairseq.trainer | begin training epoch 119
2023-05-20 23:38:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:38:56 | INFO | train_inner | epoch 119:     10 / 65 loss=2.508, nll_loss=0.535, ppl=1.45, wps=1794.1, ups=0.15, wpb=11746.4, bsz=528.3, num_updates=7660, lr=0, gnorm=1.643, clip=100, loss_scale=0.5, train_wall=88, gb_free=9.4, wall=41979
2023-05-20 23:40:28 | INFO | train_inner | epoch 119:     30 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2631.1, ups=0.22, wpb=12167.1, bsz=567.1, num_updates=7680, lr=0, gnorm=3.753, clip=100, loss_scale=0.5, train_wall=92, gb_free=10.1, wall=42071
2023-05-20 23:42:02 | INFO | train_inner | epoch 119:     50 / 65 loss=2.517, nll_loss=0.546, ppl=1.46, wps=2592, ups=0.21, wpb=12190.1, bsz=577.2, num_updates=7700, lr=0, gnorm=1.556, clip=100, loss_scale=1, train_wall=94, gb_free=9.6, wall=42165
2023-05-20 23:43:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:43:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:43:28 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1803.6 | wpb 2785 | bsz 105.2 | num_updates 7715 | best_loss 3.382
2023-05-20 23:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 7715 updates
2023-05-20 23:43:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint119.pt
2023-05-20 23:43:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint119.pt
2023-05-20 23:43:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint119.pt (epoch 119 @ 7715 updates, score 3.469) (writing took 20.199589889496565 seconds)
2023-05-20 23:43:48 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2023-05-20 23:43:48 | INFO | train | epoch 119 | loss 2.517 | nll_loss 0.545 | ppl 1.46 | wps 2303.9 | ups 0.19 | wpb 12052.3 | bsz 558.4 | num_updates 7715 | lr 0 | gnorm 2.386 | clip 100 | loss_scale 1 | train_wall 300 | gb_free 10.1 | wall 42271
2023-05-20 23:43:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:43:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:43:48 | INFO | fairseq.trainer | begin training epoch 120
2023-05-20 23:43:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:43:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 23:44:15 | INFO | train_inner | epoch 120:      6 / 65 loss=2.52, nll_loss=0.548, ppl=1.46, wps=1799.2, ups=0.15, wpb=11915.6, bsz=549.8, num_updates=7720, lr=0, gnorm=1.724, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.5, wall=42298
2023-05-20 23:45:48 | INFO | train_inner | epoch 120:     26 / 65 loss=2.513, nll_loss=0.541, ppl=1.45, wps=2575.8, ups=0.21, wpb=12046, bsz=562.6, num_updates=7740, lr=0, gnorm=1.104, clip=100, loss_scale=0.5, train_wall=93, gb_free=9.8, wall=42391
2023-05-20 23:47:25 | INFO | train_inner | epoch 120:     46 / 65 loss=2.516, nll_loss=0.544, ppl=1.46, wps=2515.5, ups=0.21, wpb=12182.2, bsz=545.5, num_updates=7760, lr=0, gnorm=1.19, clip=100, loss_scale=0.5, train_wall=97, gb_free=9.5, wall=42488
2023-05-20 23:48:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:48:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:49:08 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 3.469 | nll_loss 1.589 | ppl 3.01 | bleu 44.55 | wps 1939.3 | wpb 2785 | bsz 105.2 | num_updates 7779 | best_loss 3.382
2023-05-20 23:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7779 updates
2023-05-20 23:49:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint120.pt
2023-05-20 23:49:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3/checkpoint120.pt
2023-05-20 23:49:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3/checkpoint120.pt (epoch 120 @ 7779 updates, score 3.469) (writing took 26.474415134638548 seconds)
2023-05-20 23:49:34 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2023-05-20 23:49:34 | INFO | train | epoch 120 | loss 2.515 | nll_loss 0.543 | ppl 1.46 | wps 2228.5 | ups 0.19 | wpb 12045.2 | bsz 556.8 | num_updates 7779 | lr 0 | gnorm 1.18 | clip 100 | loss_scale 0.5 | train_wall 301 | gb_free 9.6 | wall 42617
2023-05-20 23:49:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:49:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 23:49:34 | INFO | fairseq.trainer | begin training epoch 121
2023-05-20 23:49:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:49:38 | INFO | train_inner | epoch 121:      1 / 65 loss=2.52, nll_loss=0.549, ppl=1.46, wps=1771.3, ups=0.15, wpb=11808.5, bsz=562.2, num_updates=7780, lr=0, gnorm=1.235, clip=100, loss_scale=0.5, train_wall=89, gb_free=9.2, wall=42621
slurmstepd: error: *** JOB 116923 ON 99server CANCELLED AT 2023-05-20T23:49:44 ***
