2023-04-25 10:00:50 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:22148
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:22148
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:22148
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:22148
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | initialized host 99server as rank 1
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | initialized host 99server as rank 0
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-04-25 10:00:52 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | initialized host 99server as rank 2
2023-04-25 10:00:52 | INFO | fairseq.distributed.utils | initialized host 99server as rank 3
2023-04-25 10:00:56 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:22148', 'distributed_port': 22148, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='4000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=80, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='4000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=80, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 80, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 4000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-04-25 10:00:56 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-04-25 10:01:06 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-04-25 10:01:06 | INFO | fairseq_cli.train | task: GraphToSeq
2023-04-25 10:01:06 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-04-25 10:01:06 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-04-25 10:01:06 | INFO | fairseq_cli.train | num. shared model params: 557,445,120 (num. trained: 557,445,120)
2023-04-25 10:01:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-04-25 10:01:06 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-04-25 10:01:06 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-04-25 10:01:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-04-25 10:01:15 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2023-04-25 10:01:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-04-25 10:01:15 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-04-25 10:01:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-04-25 10:01:16 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-04-25 10:01:16 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-04-25 10:01:16 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-04-25 10:01:16 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-04-25 10:01:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-04-25 10:01:16 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2023-04-25 10:01:16 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-04-25 10:01:16 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt
2023-04-25 10:01:45 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt (epoch 41 @ 0 updates)
2023-04-25 10:01:45 | INFO | fairseq.trainer | loading train data for epoch 1
2023-04-25 10:01:45 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-04-25 10:01:45 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-04-25 10:01:45 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-04-25 10:01:46 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-04-25 10:01:46 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-04-25 10:01:46 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-04-25 10:01:46 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-04-25 10:01:46 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-04-25 10:01:46 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-04-25 10:01:46 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-04-25 10:01:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:01:56 | INFO | fairseq.trainer | begin training epoch 1
2023-04-25 10:01:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:02:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-04-25 10:02:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-04-25 10:02:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-04-25 10:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-04-25 10:02:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-04-25 10:02:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-04-25 10:02:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-04-25 10:02:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-04-25 10:02:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-04-25 10:02:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-04-25 10:02:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-04-25 10:02:32 | INFO | train_inner | epoch 001:     31 / 196 loss=9.904, nll_loss=8.362, ppl=328.9, wps=3621.3, ups=0.88, wpb=4074.2, bsz=188.6, num_updates=20, lr=7.5e-06, gnorm=124.739, clip=100, loss_scale=0.0625, train_wall=34, gb_free=10.4, wall=76
2023-04-25 10:02:52 | INFO | train_inner | epoch 001:     51 / 196 loss=6.301, nll_loss=4.535, ppl=23.18, wps=3846.4, ups=0.99, wpb=3879.4, bsz=171.9, num_updates=40, lr=1.5e-05, gnorm=11.619, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=97
2023-04-25 10:03:12 | INFO | train_inner | epoch 001:     71 / 196 loss=5.063, nll_loss=3.233, ppl=9.4, wps=3987.6, ups=0.99, wpb=4038.2, bsz=182.9, num_updates=60, lr=2.25e-05, gnorm=4.656, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=117
2023-04-25 10:03:33 | INFO | train_inner | epoch 001:     91 / 196 loss=4.6, nll_loss=2.75, ppl=6.73, wps=3949.4, ups=0.97, wpb=4057.7, bsz=188.5, num_updates=80, lr=3e-05, gnorm=2.834, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=137
2023-04-25 10:03:54 | INFO | train_inner | epoch 001:    111 / 196 loss=4.339, nll_loss=2.486, ppl=5.6, wps=3858.9, ups=0.96, wpb=3999.5, bsz=194.1, num_updates=100, lr=2.98469e-05, gnorm=2.77, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=158
2023-04-25 10:04:14 | INFO | train_inner | epoch 001:    131 / 196 loss=4.194, nll_loss=2.337, ppl=5.05, wps=3976.4, ups=0.97, wpb=4082.4, bsz=188.9, num_updates=120, lr=2.96939e-05, gnorm=2.347, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=179
2023-04-25 10:04:35 | INFO | train_inner | epoch 001:    151 / 196 loss=4.155, nll_loss=2.306, ppl=4.94, wps=3935.8, ups=0.96, wpb=4082.8, bsz=196.2, num_updates=140, lr=2.95408e-05, gnorm=4.996, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=199
2023-04-25 10:04:56 | INFO | train_inner | epoch 001:    171 / 196 loss=3.988, nll_loss=2.124, ppl=4.36, wps=3925.3, ups=0.97, wpb=4036.2, bsz=197.4, num_updates=160, lr=2.93878e-05, gnorm=3.625, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=220
2023-04-25 10:05:16 | INFO | train_inner | epoch 001:    191 / 196 loss=3.909, nll_loss=2.04, ppl=4.11, wps=3965.5, ups=0.98, wpb=4038, bsz=180.9, num_updates=180, lr=2.92347e-05, gnorm=2.681, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=240
2023-04-25 10:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:05:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:05:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.517 | nll_loss 1.549 | ppl 2.93 | bleu 36.63 | wps 1253.2 | wpb 905.1 | bsz 34.2 | num_updates 185
2023-04-25 10:05:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 185 updates
2023-04-25 10:05:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint1.pt
2023-04-25 10:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint1.pt
2023-04-25 10:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 185 updates, score 3.517) (writing took 62.07790547981858 seconds)
2023-04-25 10:06:51 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-04-25 10:06:52 | INFO | train | epoch 001 | loss 5.129 | nll_loss 3.318 | ppl 9.98 | wps 2630.2 | ups 0.65 | wpb 4017.1 | bsz 186.7 | num_updates 185 | lr 2.91964e-05 | gnorm 17.425 | clip 100 | loss_scale 0.0625 | train_wall 201 | gb_free 10.4 | wall 336
2023-04-25 10:06:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:06:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:06:52 | INFO | fairseq.trainer | begin training epoch 2
2023-04-25 10:06:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:07:08 | INFO | train_inner | epoch 002:     15 / 196 loss=3.868, nll_loss=1.999, ppl=4, wps=692.9, ups=0.18, wpb=3883, bsz=188.5, num_updates=200, lr=2.90816e-05, gnorm=5.061, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.7, wall=352
2023-04-25 10:07:29 | INFO | train_inner | epoch 002:     35 / 196 loss=3.785, nll_loss=1.91, ppl=3.76, wps=3962.4, ups=0.96, wpb=4144.9, bsz=203.1, num_updates=220, lr=2.89286e-05, gnorm=2.804, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=373
2023-04-25 10:07:49 | INFO | train_inner | epoch 002:     55 / 196 loss=3.732, nll_loss=1.851, ppl=3.61, wps=4003.6, ups=0.99, wpb=4029, bsz=168.5, num_updates=240, lr=2.87755e-05, gnorm=3.322, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=393
2023-04-25 10:08:10 | INFO | train_inner | epoch 002:     75 / 196 loss=3.724, nll_loss=1.852, ppl=3.61, wps=3988.5, ups=0.98, wpb=4080, bsz=193.2, num_updates=260, lr=2.86224e-05, gnorm=3.388, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=414
2023-04-25 10:08:30 | INFO | train_inner | epoch 002:     95 / 196 loss=3.819, nll_loss=1.955, ppl=3.88, wps=3849.3, ups=0.96, wpb=4022.6, bsz=177.6, num_updates=280, lr=2.84694e-05, gnorm=17.799, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=435
2023-04-25 10:08:51 | INFO | train_inner | epoch 002:    115 / 196 loss=3.668, nll_loss=1.792, ppl=3.46, wps=3902.3, ups=0.98, wpb=3995.6, bsz=180.2, num_updates=300, lr=2.83163e-05, gnorm=2.469, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=455
2023-04-25 10:09:11 | INFO | train_inner | epoch 002:    135 / 196 loss=3.715, nll_loss=1.848, ppl=3.6, wps=4073.2, ups=0.98, wpb=4158.6, bsz=199.6, num_updates=320, lr=2.81633e-05, gnorm=2.696, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=476
2023-04-25 10:09:32 | INFO | train_inner | epoch 002:    155 / 196 loss=3.628, nll_loss=1.751, ppl=3.37, wps=3863.6, ups=0.98, wpb=3955, bsz=186.6, num_updates=340, lr=2.80102e-05, gnorm=1.953, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=496
2023-04-25 10:09:52 | INFO | train_inner | epoch 002:    175 / 196 loss=3.617, nll_loss=1.742, ppl=3.34, wps=3879.8, ups=0.97, wpb=4006.2, bsz=193.1, num_updates=360, lr=2.78571e-05, gnorm=2.039, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=517
2023-04-25 10:10:12 | INFO | train_inner | epoch 002:    195 / 196 loss=3.578, nll_loss=1.701, ppl=3.25, wps=3917.2, ups=1, wpb=3922.8, bsz=172.8, num_updates=380, lr=2.77041e-05, gnorm=2.462, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=537
2023-04-25 10:10:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:10:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:10:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.333 | nll_loss 1.358 | ppl 2.56 | bleu 42.43 | wps 1311.9 | wpb 905.1 | bsz 34.2 | num_updates 381 | best_loss 3.333
2023-04-25 10:10:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 381 updates
2023-04-25 10:10:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint2.pt
2023-04-25 10:10:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint2.pt
2023-04-25 10:11:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 381 updates, score 3.333) (writing took 59.88593688607216 seconds)
2023-04-25 10:11:40 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-04-25 10:11:40 | INFO | train | epoch 002 | loss 3.71 | nll_loss 1.837 | ppl 3.57 | wps 2726.2 | ups 0.68 | wpb 4016.8 | bsz 186.3 | num_updates 381 | lr 2.76964e-05 | gnorm 4.417 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 625
2023-04-25 10:11:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:11:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:11:40 | INFO | fairseq.trainer | begin training epoch 3
2023-04-25 10:11:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:12:01 | INFO | train_inner | epoch 003:     19 / 196 loss=3.436, nll_loss=1.538, ppl=2.9, wps=696.1, ups=0.19, wpb=3760.7, bsz=170.5, num_updates=400, lr=2.7551e-05, gnorm=1.917, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=645
2023-04-25 10:12:21 | INFO | train_inner | epoch 003:     39 / 196 loss=3.485, nll_loss=1.594, ppl=3.02, wps=4106, ups=0.97, wpb=4215.5, bsz=206.8, num_updates=420, lr=2.7398e-05, gnorm=1.796, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.1, wall=665
2023-04-25 10:12:41 | INFO | train_inner | epoch 003:     59 / 196 loss=3.439, nll_loss=1.546, ppl=2.92, wps=3883.8, ups=0.98, wpb=3966.1, bsz=179.8, num_updates=440, lr=2.72449e-05, gnorm=1.803, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=686
2023-04-25 10:13:02 | INFO | train_inner | epoch 003:     79 / 196 loss=3.453, nll_loss=1.562, ppl=2.95, wps=3874.4, ups=0.97, wpb=3981.8, bsz=191.4, num_updates=460, lr=2.70918e-05, gnorm=3.825, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=706
2023-04-25 10:13:23 | INFO | train_inner | epoch 003:     99 / 196 loss=3.412, nll_loss=1.521, ppl=2.87, wps=3975.9, ups=0.97, wpb=4115.9, bsz=178.9, num_updates=480, lr=2.69388e-05, gnorm=1.716, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=727
2023-04-25 10:13:43 | INFO | train_inner | epoch 003:    119 / 196 loss=3.455, nll_loss=1.572, ppl=2.97, wps=3939.7, ups=0.98, wpb=4033.3, bsz=179.7, num_updates=500, lr=2.67857e-05, gnorm=1.826, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=748
2023-04-25 10:14:04 | INFO | train_inner | epoch 003:    139 / 196 loss=3.427, nll_loss=1.541, ppl=2.91, wps=3864.3, ups=0.97, wpb=3988.4, bsz=168.1, num_updates=520, lr=2.66327e-05, gnorm=1.747, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=768
2023-04-25 10:14:25 | INFO | train_inner | epoch 003:    159 / 196 loss=3.438, nll_loss=1.554, ppl=2.94, wps=3902.5, ups=0.97, wpb=4036, bsz=189.4, num_updates=540, lr=2.64796e-05, gnorm=1.752, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=789
2023-04-25 10:14:45 | INFO | train_inner | epoch 003:    179 / 196 loss=3.464, nll_loss=1.588, ppl=3.01, wps=3964.9, ups=0.97, wpb=4101.7, bsz=221.6, num_updates=560, lr=2.63265e-05, gnorm=2.086, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=810
2023-04-25 10:15:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:15:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:15:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.238 | nll_loss 1.289 | ppl 2.44 | bleu 45.48 | wps 1226.4 | wpb 905.1 | bsz 34.2 | num_updates 577 | best_loss 3.238
2023-04-25 10:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 577 updates
2023-04-25 10:15:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint3.pt
2023-04-25 10:15:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint3.pt
2023-04-25 10:16:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint3.pt (epoch 3 @ 577 updates, score 3.238) (writing took 67.23241240903735 seconds)
2023-04-25 10:16:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-04-25 10:16:39 | INFO | train | epoch 003 | loss 3.441 | nll_loss 1.554 | ppl 2.94 | wps 2640.7 | ups 0.66 | wpb 4016.8 | bsz 186.3 | num_updates 577 | lr 2.61964e-05 | gnorm 2.025 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 923
2023-04-25 10:16:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:16:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:16:39 | INFO | fairseq.trainer | begin training epoch 4
2023-04-25 10:16:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:16:42 | INFO | train_inner | epoch 004:      3 / 196 loss=3.402, nll_loss=1.518, ppl=2.86, wps=662, ups=0.17, wpb=3875.2, bsz=183.1, num_updates=580, lr=2.61735e-05, gnorm=1.994, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=927
2023-04-25 10:17:02 | INFO | train_inner | epoch 004:     23 / 196 loss=3.269, nll_loss=1.362, ppl=2.57, wps=3946.7, ups=0.99, wpb=3987.7, bsz=178.1, num_updates=600, lr=2.60204e-05, gnorm=1.839, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=947
2023-04-25 10:17:23 | INFO | train_inner | epoch 004:     43 / 196 loss=3.264, nll_loss=1.361, ppl=2.57, wps=3952.3, ups=0.97, wpb=4091.9, bsz=191.8, num_updates=620, lr=2.58673e-05, gnorm=2.417, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=968
2023-04-25 10:17:44 | INFO | train_inner | epoch 004:     63 / 196 loss=3.277, nll_loss=1.375, ppl=2.59, wps=3912.3, ups=0.97, wpb=4022.2, bsz=185.3, num_updates=640, lr=2.57143e-05, gnorm=1.696, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.8, wall=988
2023-04-25 10:18:04 | INFO | train_inner | epoch 004:     83 / 196 loss=3.262, nll_loss=1.359, ppl=2.56, wps=3953.5, ups=0.97, wpb=4062.6, bsz=193, num_updates=660, lr=2.55612e-05, gnorm=1.719, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1009
2023-04-25 10:18:25 | INFO | train_inner | epoch 004:    103 / 196 loss=3.27, nll_loss=1.372, ppl=2.59, wps=3907.9, ups=0.98, wpb=4000.9, bsz=181.8, num_updates=680, lr=2.54082e-05, gnorm=1.713, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1029
2023-04-25 10:18:45 | INFO | train_inner | epoch 004:    123 / 196 loss=3.278, nll_loss=1.38, ppl=2.6, wps=3948.7, ups=0.97, wpb=4054, bsz=192.7, num_updates=700, lr=2.52551e-05, gnorm=1.656, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1050
2023-04-25 10:19:06 | INFO | train_inner | epoch 004:    143 / 196 loss=3.245, nll_loss=1.345, ppl=2.54, wps=3926.2, ups=0.98, wpb=4017.4, bsz=173.6, num_updates=720, lr=2.5102e-05, gnorm=1.642, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1070
2023-04-25 10:19:27 | INFO | train_inner | epoch 004:    163 / 196 loss=3.243, nll_loss=1.344, ppl=2.54, wps=3849.5, ups=0.96, wpb=4020.5, bsz=181.1, num_updates=740, lr=2.4949e-05, gnorm=1.628, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=1091
2023-04-25 10:19:47 | INFO | train_inner | epoch 004:    183 / 196 loss=3.239, nll_loss=1.34, ppl=2.53, wps=3881.6, ups=0.97, wpb=3985.2, bsz=182.2, num_updates=760, lr=2.47959e-05, gnorm=1.619, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1112
2023-04-25 10:20:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:20:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:20:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.195 | nll_loss 1.265 | ppl 2.4 | bleu 46.39 | wps 1242.9 | wpb 905.1 | bsz 34.2 | num_updates 773 | best_loss 3.195
2023-04-25 10:20:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 773 updates
2023-04-25 10:20:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint4.pt
2023-04-25 10:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint4.pt
2023-04-25 10:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint4.pt (epoch 4 @ 773 updates, score 3.195) (writing took 79.40810996294022 seconds)
2023-04-25 10:21:48 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-04-25 10:21:48 | INFO | train | epoch 004 | loss 3.262 | nll_loss 1.362 | ppl 2.57 | wps 2542.2 | ups 0.63 | wpb 4016.8 | bsz 186.3 | num_updates 773 | lr 2.46964e-05 | gnorm 1.789 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 1233
2023-04-25 10:21:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:21:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:21:48 | INFO | fairseq.trainer | begin training epoch 5
2023-04-25 10:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:21:56 | INFO | train_inner | epoch 005:      7 / 196 loss=3.209, nll_loss=1.306, ppl=2.47, wps=599.6, ups=0.16, wpb=3864.4, bsz=192.8, num_updates=780, lr=2.46429e-05, gnorm=1.765, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.6, wall=1240
2023-04-25 10:22:17 | INFO | train_inner | epoch 005:     27 / 196 loss=3.136, nll_loss=1.219, ppl=2.33, wps=3904.7, ups=0.98, wpb=3981.9, bsz=182.7, num_updates=800, lr=2.44898e-05, gnorm=1.63, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1261
2023-04-25 10:22:37 | INFO | train_inner | epoch 005:     47 / 196 loss=3.129, nll_loss=1.217, ppl=2.32, wps=4011.5, ups=0.98, wpb=4104.5, bsz=177.7, num_updates=820, lr=2.43367e-05, gnorm=1.678, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1281
2023-04-25 10:22:58 | INFO | train_inner | epoch 005:     67 / 196 loss=3.136, nll_loss=1.223, ppl=2.33, wps=3876.9, ups=0.97, wpb=3986.5, bsz=185.8, num_updates=840, lr=2.41837e-05, gnorm=1.775, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1302
2023-04-25 10:23:18 | INFO | train_inner | epoch 005:     87 / 196 loss=3.151, nll_loss=1.242, ppl=2.37, wps=3993.6, ups=0.97, wpb=4112.3, bsz=195, num_updates=860, lr=2.40306e-05, gnorm=1.842, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=1323
2023-04-25 10:23:39 | INFO | train_inner | epoch 005:    107 / 196 loss=3.124, nll_loss=1.211, ppl=2.32, wps=3947.2, ups=0.98, wpb=4042.9, bsz=178.5, num_updates=880, lr=2.38776e-05, gnorm=1.556, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1343
2023-04-25 10:23:59 | INFO | train_inner | epoch 005:    127 / 196 loss=3.13, nll_loss=1.222, ppl=2.33, wps=3925.1, ups=0.97, wpb=4042.8, bsz=189.6, num_updates=900, lr=2.37245e-05, gnorm=1.622, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1364
2023-04-25 10:24:20 | INFO | train_inner | epoch 005:    147 / 196 loss=3.141, nll_loss=1.232, ppl=2.35, wps=3894.3, ups=0.97, wpb=4034, bsz=186.6, num_updates=920, lr=2.35714e-05, gnorm=1.609, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=1384
2023-04-25 10:24:41 | INFO | train_inner | epoch 005:    167 / 196 loss=3.12, nll_loss=1.211, ppl=2.31, wps=3822, ups=0.96, wpb=3963.6, bsz=192.7, num_updates=940, lr=2.34184e-05, gnorm=1.654, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=1405
2023-04-25 10:25:01 | INFO | train_inner | epoch 005:    187 / 196 loss=3.103, nll_loss=1.196, ppl=2.29, wps=3964.1, ups=0.98, wpb=4045.3, bsz=196.1, num_updates=960, lr=2.32653e-05, gnorm=1.57, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1425
2023-04-25 10:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:25:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:25:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.185 | nll_loss 1.242 | ppl 2.37 | bleu 47.16 | wps 1216.7 | wpb 905.1 | bsz 34.2 | num_updates 969 | best_loss 3.185
2023-04-25 10:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 969 updates
2023-04-25 10:25:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint5.pt
2023-04-25 10:25:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint5.pt
2023-04-25 10:26:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint5.pt (epoch 5 @ 969 updates, score 3.185) (writing took 58.55828481167555 seconds)
2023-04-25 10:26:44 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-04-25 10:26:52 | INFO | train | epoch 005 | loss 3.13 | nll_loss 1.219 | ppl 2.33 | wps 2594.8 | ups 0.65 | wpb 4016.8 | bsz 186.3 | num_updates 969 | lr 2.31964e-05 | gnorm 1.663 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 1536
2023-04-25 10:26:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:26:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:26:52 | INFO | fairseq.trainer | begin training epoch 6
2023-04-25 10:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:27:04 | INFO | train_inner | epoch 006:     11 / 196 loss=3.031, nll_loss=1.11, ppl=2.16, wps=613, ups=0.16, wpb=3765.7, bsz=164.2, num_updates=980, lr=2.31122e-05, gnorm=1.634, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1548
2023-04-25 10:27:24 | INFO | train_inner | epoch 006:     31 / 196 loss=3.02, nll_loss=1.096, ppl=2.14, wps=3883.1, ups=0.98, wpb=3951.6, bsz=171.7, num_updates=1000, lr=2.29592e-05, gnorm=3.11, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1569
2023-04-25 10:27:45 | INFO | train_inner | epoch 006:     51 / 196 loss=3.046, nll_loss=1.127, ppl=2.18, wps=4138.3, ups=0.99, wpb=4199.5, bsz=208.5, num_updates=1020, lr=2.28061e-05, gnorm=1.621, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10, wall=1589
2023-04-25 10:28:05 | INFO | train_inner | epoch 006:     71 / 196 loss=3.043, nll_loss=1.125, ppl=2.18, wps=3944.8, ups=0.97, wpb=4079.5, bsz=188, num_updates=1040, lr=2.26531e-05, gnorm=1.779, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1610
2023-04-25 10:28:26 | INFO | train_inner | epoch 006:     91 / 196 loss=3, nll_loss=1.077, ppl=2.11, wps=3885.1, ups=0.97, wpb=4025.2, bsz=181.3, num_updates=1060, lr=2.25e-05, gnorm=1.499, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1630
2023-04-25 10:28:47 | INFO | train_inner | epoch 006:    111 / 196 loss=3.044, nll_loss=1.125, ppl=2.18, wps=3908.3, ups=0.96, wpb=4061.7, bsz=206.2, num_updates=1080, lr=2.23469e-05, gnorm=1.527, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1651
2023-04-25 10:29:07 | INFO | train_inner | epoch 006:    131 / 196 loss=3.031, nll_loss=1.115, ppl=2.17, wps=3887.9, ups=0.97, wpb=4004.6, bsz=184, num_updates=1100, lr=2.21939e-05, gnorm=1.586, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1672
2023-04-25 10:29:28 | INFO | train_inner | epoch 006:    151 / 196 loss=3.019, nll_loss=1.098, ppl=2.14, wps=3864.8, ups=0.97, wpb=4000.7, bsz=178.4, num_updates=1120, lr=2.20408e-05, gnorm=1.556, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1692
2023-04-25 10:29:49 | INFO | train_inner | epoch 006:    171 / 196 loss=3.028, nll_loss=1.111, ppl=2.16, wps=3951, ups=0.98, wpb=4035.9, bsz=186.5, num_updates=1140, lr=2.18878e-05, gnorm=1.521, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1713
2023-04-25 10:30:09 | INFO | train_inner | epoch 006:    191 / 196 loss=3.015, nll_loss=1.096, ppl=2.14, wps=3896.7, ups=0.96, wpb=4038.2, bsz=195.2, num_updates=1160, lr=2.17347e-05, gnorm=1.546, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1734
2023-04-25 10:30:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:30:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:30:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.179 | nll_loss 1.24 | ppl 2.36 | bleu 47.74 | wps 1207.2 | wpb 905.1 | bsz 34.2 | num_updates 1165 | best_loss 3.179
2023-04-25 10:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1165 updates
2023-04-25 10:30:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint6.pt
2023-04-25 10:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint6.pt
2023-04-25 10:31:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint6.pt (epoch 6 @ 1165 updates, score 3.179) (writing took 68.5784685574472 seconds)
2023-04-25 10:31:52 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-04-25 10:31:52 | INFO | train | epoch 006 | loss 3.024 | nll_loss 1.104 | ppl 2.15 | wps 2620.7 | ups 0.65 | wpb 4016.8 | bsz 186.3 | num_updates 1165 | lr 2.16964e-05 | gnorm 1.742 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 1836
2023-04-25 10:31:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:31:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:31:52 | INFO | fairseq.trainer | begin training epoch 7
2023-04-25 10:31:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:32:08 | INFO | train_inner | epoch 007:     15 / 196 loss=2.945, nll_loss=1.016, ppl=2.02, wps=645.2, ups=0.17, wpb=3831.9, bsz=169.8, num_updates=1180, lr=2.15816e-05, gnorm=1.592, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1852
2023-04-25 10:32:29 | INFO | train_inner | epoch 007:     35 / 196 loss=2.921, nll_loss=0.989, ppl=1.98, wps=3754.9, ups=0.97, wpb=3867.9, bsz=184.6, num_updates=1200, lr=2.14286e-05, gnorm=1.542, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1873
2023-04-25 10:32:49 | INFO | train_inner | epoch 007:     55 / 196 loss=2.911, nll_loss=0.979, ppl=1.97, wps=3958.9, ups=0.99, wpb=4014.2, bsz=181.2, num_updates=1220, lr=2.12755e-05, gnorm=1.495, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.1, wall=1893
2023-04-25 10:33:09 | INFO | train_inner | epoch 007:     75 / 196 loss=2.925, nll_loss=0.993, ppl=1.99, wps=4025.7, ups=0.98, wpb=4117.8, bsz=182, num_updates=1240, lr=2.11224e-05, gnorm=1.444, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1914
2023-04-25 10:33:30 | INFO | train_inner | epoch 007:     95 / 196 loss=2.95, nll_loss=1.022, ppl=2.03, wps=3933.4, ups=0.97, wpb=4039.6, bsz=202.2, num_updates=1260, lr=2.09694e-05, gnorm=1.531, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1934
2023-04-25 10:33:51 | INFO | train_inner | epoch 007:    115 / 196 loss=2.916, nll_loss=0.986, ppl=1.98, wps=3958.7, ups=0.97, wpb=4091.2, bsz=193.7, num_updates=1280, lr=2.08163e-05, gnorm=1.467, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1955
2023-04-25 10:34:11 | INFO | train_inner | epoch 007:    135 / 196 loss=2.972, nll_loss=1.049, ppl=2.07, wps=3872, ups=0.96, wpb=4048.7, bsz=189.2, num_updates=1300, lr=2.06633e-05, gnorm=1.945, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1976
2023-04-25 10:34:32 | INFO | train_inner | epoch 007:    155 / 196 loss=2.942, nll_loss=1.018, ppl=2.02, wps=4006.9, ups=0.97, wpb=4129.9, bsz=190.5, num_updates=1320, lr=2.05102e-05, gnorm=1.508, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1996
2023-04-25 10:34:53 | INFO | train_inner | epoch 007:    175 / 196 loss=2.958, nll_loss=1.034, ppl=2.05, wps=3915.1, ups=0.96, wpb=4067.8, bsz=204.2, num_updates=1340, lr=2.03571e-05, gnorm=1.532, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2017
2023-04-25 10:35:13 | INFO | train_inner | epoch 007:    195 / 196 loss=2.93, nll_loss=1.002, ppl=2, wps=3957.8, ups=1, wpb=3961, bsz=163.1, num_updates=1360, lr=2.02041e-05, gnorm=1.552, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2037
2023-04-25 10:35:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:35:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:35:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.192 | nll_loss 1.253 | ppl 2.38 | bleu 47.84 | wps 1239.5 | wpb 905.1 | bsz 34.2 | num_updates 1361 | best_loss 3.179
2023-04-25 10:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2023-04-25 10:35:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint7.pt
2023-04-25 10:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint7.pt
2023-04-25 10:36:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint7.pt (epoch 7 @ 1361 updates, score 3.192) (writing took 29.20718654245138 seconds)
2023-04-25 10:36:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-04-25 10:36:12 | INFO | train | epoch 007 | loss 2.935 | nll_loss 1.007 | ppl 2.01 | wps 3032.4 | ups 0.75 | wpb 4016.8 | bsz 186.3 | num_updates 1361 | lr 2.01964e-05 | gnorm 1.559 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 2096
2023-04-25 10:36:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:36:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:36:12 | INFO | fairseq.trainer | begin training epoch 8
2023-04-25 10:36:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:36:32 | INFO | train_inner | epoch 008:     19 / 196 loss=2.847, nll_loss=0.908, ppl=1.88, wps=989.5, ups=0.25, wpb=3896.9, bsz=179.4, num_updates=1380, lr=2.0051e-05, gnorm=1.525, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2116
2023-04-25 10:36:52 | INFO | train_inner | epoch 008:     39 / 196 loss=2.842, nll_loss=0.903, ppl=1.87, wps=3922.6, ups=0.96, wpb=4078.2, bsz=180, num_updates=1400, lr=1.9898e-05, gnorm=1.458, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2137
2023-04-25 10:37:13 | INFO | train_inner | epoch 008:     59 / 196 loss=2.871, nll_loss=0.936, ppl=1.91, wps=3883.1, ups=0.97, wpb=4014.7, bsz=193.4, num_updates=1420, lr=1.97449e-05, gnorm=1.473, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2158
2023-04-25 10:37:33 | INFO | train_inner | epoch 008:     79 / 196 loss=2.837, nll_loss=0.899, ppl=1.86, wps=3944.3, ups=0.98, wpb=4010.1, bsz=182.7, num_updates=1440, lr=1.95918e-05, gnorm=1.572, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2178
2023-04-25 10:37:54 | INFO | train_inner | epoch 008:     99 / 196 loss=2.853, nll_loss=0.916, ppl=1.89, wps=3937, ups=0.98, wpb=4034.8, bsz=187.9, num_updates=1460, lr=1.94388e-05, gnorm=1.496, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2198
2023-04-25 10:38:14 | INFO | train_inner | epoch 008:    119 / 196 loss=2.853, nll_loss=0.917, ppl=1.89, wps=3903.3, ups=0.98, wpb=3997.1, bsz=185.7, num_updates=1480, lr=1.92857e-05, gnorm=2.011, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2219
2023-04-25 10:38:35 | INFO | train_inner | epoch 008:    139 / 196 loss=2.843, nll_loss=0.905, ppl=1.87, wps=3867.2, ups=0.96, wpb=4010.2, bsz=174.9, num_updates=1500, lr=1.91327e-05, gnorm=1.479, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2240
2023-04-25 10:38:56 | INFO | train_inner | epoch 008:    159 / 196 loss=2.881, nll_loss=0.95, ppl=1.93, wps=3895.7, ups=0.96, wpb=4041.6, bsz=195.4, num_updates=1520, lr=1.89796e-05, gnorm=1.665, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2260
2023-04-25 10:39:16 | INFO | train_inner | epoch 008:    179 / 196 loss=2.856, nll_loss=0.922, ppl=1.89, wps=3978.4, ups=0.98, wpb=4053.8, bsz=187.2, num_updates=1540, lr=1.88265e-05, gnorm=1.488, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2281
2023-04-25 10:39:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:39:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:40:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.193 | nll_loss 1.261 | ppl 2.4 | bleu 47.8 | wps 1207.9 | wpb 905.1 | bsz 34.2 | num_updates 1557 | best_loss 3.179
2023-04-25 10:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1557 updates
2023-04-25 10:40:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint8.pt
2023-04-25 10:40:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint8.pt
2023-04-25 10:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint8.pt (epoch 8 @ 1557 updates, score 3.193) (writing took 22.545239008963108 seconds)
2023-04-25 10:40:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-04-25 10:40:25 | INFO | train | epoch 008 | loss 2.858 | nll_loss 0.922 | ppl 1.89 | wps 3104.7 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 1557 | lr 1.86964e-05 | gnorm 1.579 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 2350
2023-04-25 10:40:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:40:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:40:25 | INFO | fairseq.trainer | begin training epoch 9
2023-04-25 10:40:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:40:29 | INFO | train_inner | epoch 009:      3 / 196 loss=2.884, nll_loss=0.954, ppl=1.94, wps=1067.5, ups=0.28, wpb=3860.4, bsz=185.6, num_updates=1560, lr=1.86735e-05, gnorm=1.671, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2353
2023-04-25 10:40:59 | INFO | train_inner | epoch 009:     23 / 196 loss=2.797, nll_loss=0.852, ppl=1.8, wps=3891.6, ups=0.97, wpb=4027.3, bsz=192.1, num_updates=1580, lr=1.85204e-05, gnorm=1.448, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2383
2023-04-25 10:41:19 | INFO | train_inner | epoch 009:     43 / 196 loss=2.789, nll_loss=0.844, ppl=1.8, wps=3941.5, ups=0.98, wpb=4021.2, bsz=185.6, num_updates=1600, lr=1.83673e-05, gnorm=1.469, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2403
2023-04-25 10:41:40 | INFO | train_inner | epoch 009:     63 / 196 loss=2.801, nll_loss=0.859, ppl=1.81, wps=3915.4, ups=0.97, wpb=4044.6, bsz=202.1, num_updates=1620, lr=1.82143e-05, gnorm=1.467, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2424
2023-04-25 10:42:00 | INFO | train_inner | epoch 009:     83 / 196 loss=2.787, nll_loss=0.844, ppl=1.79, wps=3976.9, ups=0.97, wpb=4089.5, bsz=193, num_updates=1640, lr=1.80612e-05, gnorm=1.472, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2445
2023-04-25 10:42:21 | INFO | train_inner | epoch 009:    103 / 196 loss=2.788, nll_loss=0.846, ppl=1.8, wps=3828.1, ups=0.97, wpb=3939.5, bsz=177.6, num_updates=1660, lr=1.79082e-05, gnorm=1.517, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.1, wall=2465
2023-04-25 10:42:41 | INFO | train_inner | epoch 009:    123 / 196 loss=2.788, nll_loss=0.844, ppl=1.8, wps=3977.2, ups=0.98, wpb=4058.1, bsz=179, num_updates=1680, lr=1.77551e-05, gnorm=1.513, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2486
2023-04-25 10:43:02 | INFO | train_inner | epoch 009:    143 / 196 loss=2.792, nll_loss=0.852, ppl=1.8, wps=3991.1, ups=0.97, wpb=4113.8, bsz=188.4, num_updates=1700, lr=1.7602e-05, gnorm=1.446, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2506
2023-04-25 10:43:22 | INFO | train_inner | epoch 009:    163 / 196 loss=2.778, nll_loss=0.835, ppl=1.78, wps=3872.5, ups=0.98, wpb=3935.2, bsz=164.4, num_updates=1720, lr=1.7449e-05, gnorm=1.499, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2527
2023-04-25 10:43:43 | INFO | train_inner | epoch 009:    183 / 196 loss=2.828, nll_loss=0.892, ppl=1.86, wps=3949.8, ups=0.97, wpb=4079.4, bsz=211.4, num_updates=1740, lr=1.72959e-05, gnorm=1.589, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2547
2023-04-25 10:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:43:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:44:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.214 | nll_loss 1.294 | ppl 2.45 | bleu 47.61 | wps 1227.7 | wpb 905.1 | bsz 34.2 | num_updates 1753 | best_loss 3.179
2023-04-25 10:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1753 updates
2023-04-25 10:44:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint9.pt
2023-04-25 10:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint9.pt
2023-04-25 10:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint9.pt (epoch 9 @ 1753 updates, score 3.214) (writing took 24.74894053861499 seconds)
2023-04-25 10:44:57 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-04-25 10:45:00 | INFO | train | epoch 009 | loss 2.793 | nll_loss 0.851 | ppl 1.8 | wps 2867.6 | ups 0.71 | wpb 4016.8 | bsz 186.3 | num_updates 1753 | lr 1.71964e-05 | gnorm 1.496 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 2624
2023-04-25 10:45:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:45:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:45:00 | INFO | fairseq.trainer | begin training epoch 10
2023-04-25 10:45:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:45:07 | INFO | train_inner | epoch 010:      7 / 196 loss=2.763, nll_loss=0.819, ppl=1.76, wps=915.6, ups=0.24, wpb=3869.1, bsz=176, num_updates=1760, lr=1.71429e-05, gnorm=1.539, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2632
2023-04-25 10:45:28 | INFO | train_inner | epoch 010:     27 / 196 loss=2.73, nll_loss=0.778, ppl=1.71, wps=3897.9, ups=0.98, wpb=3987.9, bsz=186.2, num_updates=1780, lr=1.69898e-05, gnorm=1.496, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2652
2023-04-25 10:45:48 | INFO | train_inner | epoch 010:     47 / 196 loss=2.727, nll_loss=0.777, ppl=1.71, wps=3970.5, ups=0.98, wpb=4049.4, bsz=183, num_updates=1800, lr=1.68367e-05, gnorm=1.412, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2673
2023-04-25 10:46:09 | INFO | train_inner | epoch 010:     67 / 196 loss=2.731, nll_loss=0.781, ppl=1.72, wps=3929.1, ups=0.97, wpb=4031, bsz=179.3, num_updates=1820, lr=1.66837e-05, gnorm=1.439, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2693
2023-04-25 10:46:29 | INFO | train_inner | epoch 010:     87 / 196 loss=2.722, nll_loss=0.774, ppl=1.71, wps=3864.9, ups=0.97, wpb=4003.2, bsz=191.5, num_updates=1840, lr=1.65306e-05, gnorm=1.433, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2714
2023-04-25 10:46:50 | INFO | train_inner | epoch 010:    107 / 196 loss=2.727, nll_loss=0.779, ppl=1.72, wps=3935.6, ups=0.98, wpb=4020.2, bsz=174.1, num_updates=1860, lr=1.63776e-05, gnorm=1.699, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2734
2023-04-25 10:47:10 | INFO | train_inner | epoch 010:    127 / 196 loss=2.726, nll_loss=0.777, ppl=1.71, wps=3982.6, ups=0.97, wpb=4096.6, bsz=188.2, num_updates=1880, lr=1.62245e-05, gnorm=1.432, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2755
2023-04-25 10:47:31 | INFO | train_inner | epoch 010:    147 / 196 loss=2.744, nll_loss=0.798, ppl=1.74, wps=3910.7, ups=0.97, wpb=4017.4, bsz=187.9, num_updates=1900, lr=1.60714e-05, gnorm=1.505, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2775
2023-04-25 10:47:52 | INFO | train_inner | epoch 010:    167 / 196 loss=2.739, nll_loss=0.792, ppl=1.73, wps=3825, ups=0.96, wpb=3977.8, bsz=184.2, num_updates=1920, lr=1.59184e-05, gnorm=1.466, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2796
2023-04-25 10:48:13 | INFO | train_inner | epoch 010:    187 / 196 loss=2.782, nll_loss=0.843, ppl=1.79, wps=3928.3, ups=0.96, wpb=4072.8, bsz=205.4, num_updates=1940, lr=1.57653e-05, gnorm=1.995, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2817
2023-04-25 10:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:48:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:48:51 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.224 | nll_loss 1.301 | ppl 2.46 | bleu 47.99 | wps 1193.6 | wpb 905.1 | bsz 34.2 | num_updates 1949 | best_loss 3.179
2023-04-25 10:48:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1949 updates
2023-04-25 10:48:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint10.pt
2023-04-25 10:49:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint10.pt
2023-04-25 10:49:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint10.pt (epoch 10 @ 1949 updates, score 3.224) (writing took 27.62245834618807 seconds)
2023-04-25 10:49:19 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-04-25 10:49:19 | INFO | train | epoch 010 | loss 2.737 | nll_loss 0.79 | ppl 1.73 | wps 3042.1 | ups 0.76 | wpb 4016.8 | bsz 186.3 | num_updates 1949 | lr 1.56964e-05 | gnorm 1.541 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 2883
2023-04-25 10:49:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:49:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:49:19 | INFO | fairseq.trainer | begin training epoch 11
2023-04-25 10:49:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:49:30 | INFO | train_inner | epoch 011:     11 / 196 loss=2.714, nll_loss=0.766, ppl=1.7, wps=998.8, ups=0.26, wpb=3874.5, bsz=186, num_updates=1960, lr=1.56122e-05, gnorm=1.487, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2894
2023-04-25 10:49:51 | INFO | train_inner | epoch 011:     31 / 196 loss=2.676, nll_loss=0.72, ppl=1.65, wps=3907.3, ups=0.97, wpb=4042.4, bsz=178.2, num_updates=1980, lr=1.54592e-05, gnorm=1.385, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2915
2023-04-25 10:50:11 | INFO | train_inner | epoch 011:     51 / 196 loss=2.672, nll_loss=0.716, ppl=1.64, wps=3862.6, ups=0.97, wpb=3973.2, bsz=176.3, num_updates=2000, lr=1.53061e-05, gnorm=1.418, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2936
2023-04-25 10:50:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-04-25 10:50:33 | INFO | train_inner | epoch 011:     72 / 196 loss=2.713, nll_loss=0.764, ppl=1.7, wps=3752.5, ups=0.93, wpb=4023.9, bsz=186.7, num_updates=2020, lr=1.51531e-05, gnorm=1.592, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=2957
2023-04-25 10:50:53 | INFO | train_inner | epoch 011:     92 / 196 loss=2.683, nll_loss=0.732, ppl=1.66, wps=3925.4, ups=0.97, wpb=4031, bsz=188.4, num_updates=2040, lr=1.5e-05, gnorm=1.45, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=2978
2023-04-25 10:51:14 | INFO | train_inner | epoch 011:    112 / 196 loss=2.699, nll_loss=0.749, ppl=1.68, wps=3974.3, ups=0.96, wpb=4140.2, bsz=207.9, num_updates=2060, lr=1.48469e-05, gnorm=1.457, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=2999
2023-04-25 10:51:35 | INFO | train_inner | epoch 011:    132 / 196 loss=2.693, nll_loss=0.741, ppl=1.67, wps=3895.4, ups=0.96, wpb=4042.7, bsz=188.7, num_updates=2080, lr=1.46939e-05, gnorm=1.422, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3019
2023-04-25 10:51:55 | INFO | train_inner | epoch 011:    152 / 196 loss=2.705, nll_loss=0.755, ppl=1.69, wps=3970, ups=0.98, wpb=4053.1, bsz=190, num_updates=2100, lr=1.45408e-05, gnorm=1.44, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3040
2023-04-25 10:52:16 | INFO | train_inner | epoch 011:    172 / 196 loss=2.683, nll_loss=0.733, ppl=1.66, wps=3967.4, ups=0.97, wpb=4098.3, bsz=180.8, num_updates=2120, lr=1.43878e-05, gnorm=1.409, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3060
2023-04-25 10:52:36 | INFO | train_inner | epoch 011:    192 / 196 loss=2.685, nll_loss=0.733, ppl=1.66, wps=3865.3, ups=0.98, wpb=3951.4, bsz=179.8, num_updates=2140, lr=1.42347e-05, gnorm=1.455, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3081
2023-04-25 10:52:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:52:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:53:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.247 | nll_loss 1.332 | ppl 2.52 | bleu 47.47 | wps 1219.5 | wpb 905.1 | bsz 34.2 | num_updates 2144 | best_loss 3.179
2023-04-25 10:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2144 updates
2023-04-25 10:53:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint11.pt
2023-04-25 10:53:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint11.pt
2023-04-25 10:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint11.pt (epoch 11 @ 2144 updates, score 3.247) (writing took 23.106051608920097 seconds)
2023-04-25 10:53:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-04-25 10:53:32 | INFO | train | epoch 011 | loss 2.689 | nll_loss 0.738 | ppl 1.67 | wps 3086 | ups 0.77 | wpb 4017.5 | bsz 186.5 | num_updates 2144 | lr 1.42041e-05 | gnorm 1.454 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 3137
2023-04-25 10:53:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:53:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:53:33 | INFO | fairseq.trainer | begin training epoch 12
2023-04-25 10:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:53:49 | INFO | train_inner | epoch 012:     16 / 196 loss=2.647, nll_loss=0.692, ppl=1.62, wps=1049.6, ups=0.27, wpb=3824.2, bsz=177.5, num_updates=2160, lr=1.40816e-05, gnorm=1.503, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3154
2023-04-25 10:54:10 | INFO | train_inner | epoch 012:     36 / 196 loss=2.654, nll_loss=0.698, ppl=1.62, wps=3843.8, ups=0.95, wpb=4026.8, bsz=201.4, num_updates=2180, lr=1.39286e-05, gnorm=1.532, clip=100, loss_scale=0.0625, train_wall=21, gb_free=9.7, wall=3175
2023-04-25 10:54:31 | INFO | train_inner | epoch 012:     56 / 196 loss=2.639, nll_loss=0.682, ppl=1.6, wps=3995.6, ups=0.98, wpb=4094.5, bsz=193.1, num_updates=2200, lr=1.37755e-05, gnorm=1.381, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3195
2023-04-25 10:54:51 | INFO | train_inner | epoch 012:     76 / 196 loss=2.652, nll_loss=0.698, ppl=1.62, wps=3961.1, ups=0.97, wpb=4093.2, bsz=193.1, num_updates=2220, lr=1.36224e-05, gnorm=1.397, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3216
2023-04-25 10:55:12 | INFO | train_inner | epoch 012:     96 / 196 loss=2.64, nll_loss=0.684, ppl=1.61, wps=3960.1, ups=0.98, wpb=4056.1, bsz=183.9, num_updates=2240, lr=1.34694e-05, gnorm=1.437, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3236
2023-04-25 10:55:33 | INFO | train_inner | epoch 012:    116 / 196 loss=2.645, nll_loss=0.688, ppl=1.61, wps=3904.9, ups=0.97, wpb=4043.5, bsz=176.2, num_updates=2260, lr=1.33163e-05, gnorm=1.39, clip=100, loss_scale=0.0625, train_wall=21, gb_free=9.8, wall=3257
2023-04-25 10:55:53 | INFO | train_inner | epoch 012:    136 / 196 loss=2.653, nll_loss=0.698, ppl=1.62, wps=3862.4, ups=0.96, wpb=4004.3, bsz=184.1, num_updates=2280, lr=1.31633e-05, gnorm=1.409, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3278
2023-04-25 10:56:14 | INFO | train_inner | epoch 012:    156 / 196 loss=2.636, nll_loss=0.68, ppl=1.6, wps=3863.3, ups=0.97, wpb=3967.5, bsz=186.8, num_updates=2300, lr=1.30102e-05, gnorm=1.395, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3298
2023-04-25 10:56:35 | INFO | train_inner | epoch 012:    176 / 196 loss=2.648, nll_loss=0.692, ppl=1.62, wps=3939.4, ups=0.97, wpb=4067.8, bsz=185.8, num_updates=2320, lr=1.28571e-05, gnorm=1.441, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3319
2023-04-25 10:56:55 | INFO | train_inner | epoch 012:    196 / 196 loss=2.663, nll_loss=0.71, ppl=1.64, wps=3836.4, ups=1, wpb=3826.7, bsz=178.6, num_updates=2340, lr=1.27041e-05, gnorm=1.529, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3339
2023-04-25 10:56:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 10:56:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:57:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.256 | nll_loss 1.346 | ppl 2.54 | bleu 47.61 | wps 1191.2 | wpb 905.1 | bsz 34.2 | num_updates 2340 | best_loss 3.179
2023-04-25 10:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2340 updates
2023-04-25 10:57:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint12.pt
2023-04-25 10:57:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint12.pt
2023-04-25 10:57:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint12.pt (epoch 12 @ 2340 updates, score 3.256) (writing took 21.824088770896196 seconds)
2023-04-25 10:57:46 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-04-25 10:57:46 | INFO | train | epoch 012 | loss 2.647 | nll_loss 0.691 | ppl 1.61 | wps 3100.6 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 2340 | lr 1.27041e-05 | gnorm 1.432 | clip 100 | loss_scale 0.0625 | train_wall 201 | gb_free 10.4 | wall 3391
2023-04-25 10:57:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 10:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 10:57:46 | INFO | fairseq.trainer | begin training epoch 13
2023-04-25 10:57:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 10:58:07 | INFO | train_inner | epoch 013:     20 / 196 loss=2.602, nll_loss=0.643, ppl=1.56, wps=1106, ups=0.27, wpb=4028.2, bsz=187.2, num_updates=2360, lr=1.2551e-05, gnorm=1.363, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3412
2023-04-25 10:58:28 | INFO | train_inner | epoch 013:     40 / 196 loss=2.601, nll_loss=0.64, ppl=1.56, wps=3877.2, ups=0.97, wpb=4012.4, bsz=178.1, num_updates=2380, lr=1.2398e-05, gnorm=1.383, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3432
2023-04-25 10:58:49 | INFO | train_inner | epoch 013:     60 / 196 loss=2.612, nll_loss=0.655, ppl=1.57, wps=3933.8, ups=0.95, wpb=4128.5, bsz=202.3, num_updates=2400, lr=1.22449e-05, gnorm=1.394, clip=100, loss_scale=0.0625, train_wall=21, gb_free=9.4, wall=3453
2023-04-25 10:59:10 | INFO | train_inner | epoch 013:     80 / 196 loss=2.601, nll_loss=0.642, ppl=1.56, wps=3861.3, ups=0.98, wpb=3950.8, bsz=178.5, num_updates=2420, lr=1.20918e-05, gnorm=1.374, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3474
2023-04-25 10:59:31 | INFO | train_inner | epoch 013:    100 / 196 loss=2.625, nll_loss=0.667, ppl=1.59, wps=3811.4, ups=0.95, wpb=4007.7, bsz=189.2, num_updates=2440, lr=1.19388e-05, gnorm=1.675, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3495
2023-04-25 10:59:51 | INFO | train_inner | epoch 013:    120 / 196 loss=2.597, nll_loss=0.637, ppl=1.56, wps=3937.5, ups=0.98, wpb=4014.6, bsz=182.7, num_updates=2460, lr=1.17857e-05, gnorm=1.385, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3515
2023-04-25 11:00:12 | INFO | train_inner | epoch 013:    140 / 196 loss=2.626, nll_loss=0.668, ppl=1.59, wps=3971.7, ups=0.97, wpb=4105.4, bsz=191.2, num_updates=2480, lr=1.16327e-05, gnorm=1.419, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3536
2023-04-25 11:00:32 | INFO | train_inner | epoch 013:    160 / 196 loss=2.632, nll_loss=0.675, ppl=1.6, wps=3847.4, ups=0.96, wpb=3997.9, bsz=203.2, num_updates=2500, lr=1.14796e-05, gnorm=1.428, clip=100, loss_scale=0.0625, train_wall=21, gb_free=9.9, wall=3557
2023-04-25 11:00:53 | INFO | train_inner | epoch 013:    180 / 196 loss=2.61, nll_loss=0.651, ppl=1.57, wps=3890.5, ups=0.97, wpb=4011.5, bsz=177.1, num_updates=2520, lr=1.13265e-05, gnorm=1.42, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3577
2023-04-25 11:01:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:01:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:01:39 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.274 | nll_loss 1.358 | ppl 2.56 | bleu 47.97 | wps 1186.8 | wpb 905.1 | bsz 34.2 | num_updates 2536 | best_loss 3.179
2023-04-25 11:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2536 updates
2023-04-25 11:01:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint13.pt
2023-04-25 11:01:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint13.pt
2023-04-25 11:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint13.pt (epoch 13 @ 2536 updates, score 3.274) (writing took 28.58617029339075 seconds)
2023-04-25 11:02:07 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-04-25 11:02:08 | INFO | train | epoch 013 | loss 2.611 | nll_loss 0.653 | ppl 1.57 | wps 3015.2 | ups 0.75 | wpb 4016.8 | bsz 186.3 | num_updates 2536 | lr 1.12041e-05 | gnorm 1.435 | clip 100 | loss_scale 0.0625 | train_wall 201 | gb_free 10.4 | wall 3652
2023-04-25 11:02:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:02:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:02:08 | INFO | fairseq.trainer | begin training epoch 14
2023-04-25 11:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:02:12 | INFO | train_inner | epoch 014:      4 / 196 loss=2.596, nll_loss=0.636, ppl=1.55, wps=993, ups=0.25, wpb=3925, bsz=171.9, num_updates=2540, lr=1.11735e-05, gnorm=1.485, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3656
2023-04-25 11:02:32 | INFO | train_inner | epoch 014:     24 / 196 loss=2.569, nll_loss=0.605, ppl=1.52, wps=4001.5, ups=0.98, wpb=4070.1, bsz=182.6, num_updates=2560, lr=1.10204e-05, gnorm=1.361, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.8, wall=3677
2023-04-25 11:02:53 | INFO | train_inner | epoch 014:     44 / 196 loss=2.591, nll_loss=0.628, ppl=1.55, wps=3994.7, ups=0.97, wpb=4124.4, bsz=192.2, num_updates=2580, lr=1.08673e-05, gnorm=1.365, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3697
2023-04-25 11:03:14 | INFO | train_inner | epoch 014:     64 / 196 loss=2.579, nll_loss=0.618, ppl=1.53, wps=3941.3, ups=0.98, wpb=4025.1, bsz=201.2, num_updates=2600, lr=1.07143e-05, gnorm=1.365, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3718
2023-04-25 11:03:34 | INFO | train_inner | epoch 014:     84 / 196 loss=2.597, nll_loss=0.639, ppl=1.56, wps=3861.8, ups=0.97, wpb=3969.9, bsz=192.2, num_updates=2620, lr=1.05612e-05, gnorm=1.544, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3738
2023-04-25 11:03:55 | INFO | train_inner | epoch 014:    104 / 196 loss=2.574, nll_loss=0.613, ppl=1.53, wps=4008.3, ups=0.97, wpb=4116.6, bsz=190.4, num_updates=2640, lr=1.04082e-05, gnorm=1.358, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3759
2023-04-25 11:04:15 | INFO | train_inner | epoch 014:    124 / 196 loss=2.587, nll_loss=0.625, ppl=1.54, wps=3944.7, ups=0.98, wpb=4030.9, bsz=182, num_updates=2660, lr=1.02551e-05, gnorm=1.411, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3779
2023-04-25 11:04:36 | INFO | train_inner | epoch 014:    144 / 196 loss=2.569, nll_loss=0.608, ppl=1.52, wps=3899, ups=0.97, wpb=4020.5, bsz=176.4, num_updates=2680, lr=1.0102e-05, gnorm=1.358, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3800
2023-04-25 11:04:56 | INFO | train_inner | epoch 014:    164 / 196 loss=2.586, nll_loss=0.626, ppl=1.54, wps=3867.2, ups=0.96, wpb=4013.9, bsz=188.9, num_updates=2700, lr=9.94898e-06, gnorm=1.401, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3821
2023-04-25 11:05:17 | INFO | train_inner | epoch 014:    184 / 196 loss=2.558, nll_loss=0.596, ppl=1.51, wps=3845.7, ups=0.97, wpb=3949.8, bsz=167, num_updates=2720, lr=9.79592e-06, gnorm=1.352, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3841
2023-04-25 11:05:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:05:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:05:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.296 | nll_loss 1.389 | ppl 2.62 | bleu 47.62 | wps 1210 | wpb 905.1 | bsz 34.2 | num_updates 2732 | best_loss 3.179
2023-04-25 11:05:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2732 updates
2023-04-25 11:05:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint14.pt
2023-04-25 11:06:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint14.pt
2023-04-25 11:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint14.pt (epoch 14 @ 2732 updates, score 3.296) (writing took 24.11390332505107 seconds)
2023-04-25 11:06:22 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-04-25 11:06:22 | INFO | train | epoch 014 | loss 2.579 | nll_loss 0.618 | ppl 1.53 | wps 3088.7 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 2732 | lr 9.70408e-06 | gnorm 1.395 | clip 100 | loss_scale 0.0625 | train_wall 200 | gb_free 10.4 | wall 3907
2023-04-25 11:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:06:22 | INFO | fairseq.trainer | begin training epoch 15
2023-04-25 11:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:06:31 | INFO | train_inner | epoch 015:      8 / 196 loss=2.571, nll_loss=0.611, ppl=1.53, wps=1054.8, ups=0.27, wpb=3894.6, bsz=195.5, num_updates=2740, lr=9.64286e-06, gnorm=1.441, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3915
2023-04-25 11:06:52 | INFO | train_inner | epoch 015:     28 / 196 loss=2.557, nll_loss=0.591, ppl=1.51, wps=3845.7, ups=0.96, wpb=4002.1, bsz=193.2, num_updates=2760, lr=9.4898e-06, gnorm=6.897, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3936
2023-04-25 11:07:12 | INFO | train_inner | epoch 015:     48 / 196 loss=2.549, nll_loss=0.584, ppl=1.5, wps=3990.4, ups=0.98, wpb=4069.9, bsz=189.2, num_updates=2780, lr=9.33673e-06, gnorm=1.353, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=3956
2023-04-25 11:07:33 | INFO | train_inner | epoch 015:     68 / 196 loss=2.545, nll_loss=0.581, ppl=1.5, wps=3931.9, ups=0.95, wpb=4123.6, bsz=183.9, num_updates=2800, lr=9.18367e-06, gnorm=1.324, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10, wall=3977
2023-04-25 11:07:54 | INFO | train_inner | epoch 015:     88 / 196 loss=2.539, nll_loss=0.575, ppl=1.49, wps=3815.9, ups=0.97, wpb=3933.2, bsz=185.9, num_updates=2820, lr=9.03061e-06, gnorm=1.352, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=3998
2023-04-25 11:08:14 | INFO | train_inner | epoch 015:    108 / 196 loss=2.547, nll_loss=0.582, ppl=1.5, wps=3925.7, ups=0.97, wpb=4033.8, bsz=176.3, num_updates=2840, lr=8.87755e-06, gnorm=1.349, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4019
2023-04-25 11:08:35 | INFO | train_inner | epoch 015:    128 / 196 loss=2.583, nll_loss=0.624, ppl=1.54, wps=3916.9, ups=0.95, wpb=4106.9, bsz=201.9, num_updates=2860, lr=8.72449e-06, gnorm=1.445, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=4040
2023-04-25 11:08:56 | INFO | train_inner | epoch 015:    148 / 196 loss=2.536, nll_loss=0.572, ppl=1.49, wps=3876, ups=0.98, wpb=3955.2, bsz=171.5, num_updates=2880, lr=8.57143e-06, gnorm=1.711, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4060
2023-04-25 11:09:16 | INFO | train_inner | epoch 015:    168 / 196 loss=2.55, nll_loss=0.588, ppl=1.5, wps=3930.5, ups=0.98, wpb=4026.1, bsz=189, num_updates=2900, lr=8.41837e-06, gnorm=1.34, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4080
2023-04-25 11:09:37 | INFO | train_inner | epoch 015:    188 / 196 loss=2.558, nll_loss=0.594, ppl=1.51, wps=3862, ups=0.97, wpb=3977.4, bsz=188.5, num_updates=2920, lr=8.26531e-06, gnorm=1.387, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4101
2023-04-25 11:09:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:09:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:10:15 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.301 | nll_loss 1.398 | ppl 2.64 | bleu 47.81 | wps 1176.7 | wpb 905.1 | bsz 34.2 | num_updates 2928 | best_loss 3.179
2023-04-25 11:10:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2928 updates
2023-04-25 11:10:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint15.pt
2023-04-25 11:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint15.pt
2023-04-25 11:10:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint15.pt (epoch 15 @ 2928 updates, score 3.301) (writing took 23.045573119074106 seconds)
2023-04-25 11:10:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-04-25 11:10:38 | INFO | train | epoch 015 | loss 2.552 | nll_loss 0.588 | ppl 1.5 | wps 3083.4 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 2928 | lr 8.20408e-06 | gnorm 1.974 | clip 100 | loss_scale 0.0625 | train_wall 201 | gb_free 10.4 | wall 4162
2023-04-25 11:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:10:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:10:38 | INFO | fairseq.trainer | begin training epoch 16
2023-04-25 11:10:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:10:51 | INFO | train_inner | epoch 016:     12 / 196 loss=2.541, nll_loss=0.577, ppl=1.49, wps=1049.5, ups=0.27, wpb=3880.2, bsz=182.8, num_updates=2940, lr=8.11224e-06, gnorm=1.45, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4175
2023-04-25 11:11:11 | INFO | train_inner | epoch 016:     32 / 196 loss=2.52, nll_loss=0.553, ppl=1.47, wps=4018.7, ups=0.98, wpb=4088.7, bsz=183.2, num_updates=2960, lr=7.95918e-06, gnorm=1.29, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4195
2023-04-25 11:11:32 | INFO | train_inner | epoch 016:     52 / 196 loss=2.534, nll_loss=0.569, ppl=1.48, wps=3955, ups=0.97, wpb=4078.5, bsz=190.3, num_updates=2980, lr=7.80612e-06, gnorm=1.362, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=4216
2023-04-25 11:11:52 | INFO | train_inner | epoch 016:     72 / 196 loss=2.514, nll_loss=0.547, ppl=1.46, wps=3884.4, ups=0.98, wpb=3963.9, bsz=175.4, num_updates=3000, lr=7.65306e-06, gnorm=1.344, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4236
2023-04-25 11:12:13 | INFO | train_inner | epoch 016:     92 / 196 loss=2.537, nll_loss=0.573, ppl=1.49, wps=3996.7, ups=0.97, wpb=4116.8, bsz=200.2, num_updates=3020, lr=7.5e-06, gnorm=1.326, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=4257
2023-04-25 11:12:33 | INFO | train_inner | epoch 016:    112 / 196 loss=2.525, nll_loss=0.559, ppl=1.47, wps=3773.8, ups=0.96, wpb=3925.9, bsz=174.7, num_updates=3040, lr=7.34694e-06, gnorm=1.348, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4278
2023-04-25 11:12:54 | INFO | train_inner | epoch 016:    132 / 196 loss=2.541, nll_loss=0.576, ppl=1.49, wps=3929.9, ups=0.96, wpb=4095.7, bsz=192.4, num_updates=3060, lr=7.19388e-06, gnorm=1.373, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4299
2023-04-25 11:13:15 | INFO | train_inner | epoch 016:    152 / 196 loss=2.536, nll_loss=0.572, ppl=1.49, wps=3867, ups=0.95, wpb=4052.8, bsz=201.8, num_updates=3080, lr=7.04082e-06, gnorm=1.345, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4320
2023-04-25 11:13:35 | INFO | train_inner | epoch 016:    172 / 196 loss=2.527, nll_loss=0.561, ppl=1.48, wps=3958.1, ups=0.99, wpb=4007.4, bsz=178, num_updates=3100, lr=6.88776e-06, gnorm=1.344, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4340
2023-04-25 11:13:56 | INFO | train_inner | epoch 016:    192 / 196 loss=2.529, nll_loss=0.564, ppl=1.48, wps=3888.4, ups=0.97, wpb=4028.9, bsz=190.5, num_updates=3120, lr=6.73469e-06, gnorm=1.361, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4361
2023-04-25 11:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:14:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:14:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.317 | nll_loss 1.417 | ppl 2.67 | bleu 47.72 | wps 1186.4 | wpb 905.1 | bsz 34.2 | num_updates 3124 | best_loss 3.179
2023-04-25 11:14:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3124 updates
2023-04-25 11:14:30 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint16.pt
2023-04-25 11:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint16.pt
2023-04-25 11:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint16.pt (epoch 16 @ 3124 updates, score 3.317) (writing took 23.01071548834443 seconds)
2023-04-25 11:14:53 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-04-25 11:14:53 | INFO | train | epoch 016 | loss 2.529 | nll_loss 0.563 | ppl 1.48 | wps 3087.5 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 3124 | lr 6.70408e-06 | gnorm 1.352 | clip 100 | loss_scale 0.125 | train_wall 201 | gb_free 10.4 | wall 4417
2023-04-25 11:14:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:14:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:14:53 | INFO | fairseq.trainer | begin training epoch 17
2023-04-25 11:14:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:15:10 | INFO | train_inner | epoch 017:     16 / 196 loss=2.505, nll_loss=0.537, ppl=1.45, wps=1047.7, ups=0.27, wpb=3845.9, bsz=165.4, num_updates=3140, lr=6.58163e-06, gnorm=1.376, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4434
2023-04-25 11:15:30 | INFO | train_inner | epoch 017:     36 / 196 loss=2.504, nll_loss=0.535, ppl=1.45, wps=3950.5, ups=0.98, wpb=4011.2, bsz=190.2, num_updates=3160, lr=6.42857e-06, gnorm=1.316, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4454
2023-04-25 11:15:50 | INFO | train_inner | epoch 017:     56 / 196 loss=2.53, nll_loss=0.565, ppl=1.48, wps=3934.4, ups=0.98, wpb=4016.9, bsz=186.9, num_updates=3180, lr=6.27551e-06, gnorm=1.364, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4475
2023-04-25 11:16:11 | INFO | train_inner | epoch 017:     76 / 196 loss=2.519, nll_loss=0.554, ppl=1.47, wps=3996, ups=0.97, wpb=4122.8, bsz=212, num_updates=3200, lr=6.12245e-06, gnorm=1.297, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4495
2023-04-25 11:16:32 | INFO | train_inner | epoch 017:     96 / 196 loss=2.51, nll_loss=0.544, ppl=1.46, wps=3925.9, ups=0.97, wpb=4045.2, bsz=189.5, num_updates=3220, lr=5.96939e-06, gnorm=1.309, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4516
2023-04-25 11:16:53 | INFO | train_inner | epoch 017:    116 / 196 loss=2.515, nll_loss=0.548, ppl=1.46, wps=3843.2, ups=0.95, wpb=4035.7, bsz=193.1, num_updates=3240, lr=5.81633e-06, gnorm=1.325, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4537
2023-04-25 11:17:13 | INFO | train_inner | epoch 017:    136 / 196 loss=2.506, nll_loss=0.54, ppl=1.45, wps=3932.2, ups=0.97, wpb=4043.5, bsz=195.2, num_updates=3260, lr=5.66327e-06, gnorm=1.317, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4558
2023-04-25 11:17:34 | INFO | train_inner | epoch 017:    156 / 196 loss=2.511, nll_loss=0.544, ppl=1.46, wps=3943.8, ups=0.97, wpb=4066.4, bsz=181.9, num_updates=3280, lr=5.5102e-06, gnorm=1.309, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4578
2023-04-25 11:17:54 | INFO | train_inner | epoch 017:    176 / 196 loss=2.494, nll_loss=0.524, ppl=1.44, wps=3888.5, ups=0.97, wpb=3993.2, bsz=170.8, num_updates=3300, lr=5.35714e-06, gnorm=1.309, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4599
2023-04-25 11:18:14 | INFO | train_inner | epoch 017:    196 / 196 loss=2.504, nll_loss=0.537, ppl=1.45, wps=3826.6, ups=1.01, wpb=3780.5, bsz=167.8, num_updates=3320, lr=5.20408e-06, gnorm=1.385, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4618
2023-04-25 11:18:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:18:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:18:44 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.334 | nll_loss 1.437 | ppl 2.71 | bleu 47.76 | wps 1193.6 | wpb 905.1 | bsz 34.2 | num_updates 3320 | best_loss 3.179
2023-04-25 11:18:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3320 updates
2023-04-25 11:18:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint17.pt
2023-04-25 11:18:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint17.pt
2023-04-25 11:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint17.pt (epoch 17 @ 3320 updates, score 3.334) (writing took 21.673648342490196 seconds)
2023-04-25 11:19:06 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-04-25 11:19:06 | INFO | train | epoch 017 | loss 2.51 | nll_loss 0.543 | ppl 1.46 | wps 3112.6 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 3320 | lr 5.20408e-06 | gnorm 1.323 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 4670
2023-04-25 11:19:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:19:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:19:06 | INFO | fairseq.trainer | begin training epoch 18
2023-04-25 11:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:19:26 | INFO | train_inner | epoch 018:     20 / 196 loss=2.488, nll_loss=0.518, ppl=1.43, wps=1109.4, ups=0.28, wpb=4011, bsz=188.1, num_updates=3340, lr=5.05102e-06, gnorm=1.294, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4691
2023-04-25 11:19:47 | INFO | train_inner | epoch 018:     40 / 196 loss=2.482, nll_loss=0.511, ppl=1.42, wps=3904.6, ups=0.97, wpb=4038, bsz=167.7, num_updates=3360, lr=4.89796e-06, gnorm=1.256, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4711
2023-04-25 11:20:08 | INFO | train_inner | epoch 018:     60 / 196 loss=2.481, nll_loss=0.511, ppl=1.42, wps=3895.5, ups=0.97, wpb=4021.6, bsz=187.5, num_updates=3380, lr=4.7449e-06, gnorm=1.291, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4732
2023-04-25 11:20:28 | INFO | train_inner | epoch 018:     80 / 196 loss=2.504, nll_loss=0.538, ppl=1.45, wps=3867.4, ups=0.96, wpb=4017.3, bsz=200.5, num_updates=3400, lr=4.59184e-06, gnorm=1.314, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4753
2023-04-25 11:20:49 | INFO | train_inner | epoch 018:    100 / 196 loss=2.49, nll_loss=0.52, ppl=1.43, wps=3930.6, ups=0.98, wpb=4028.4, bsz=177.5, num_updates=3420, lr=4.43878e-06, gnorm=1.313, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4773
2023-04-25 11:21:10 | INFO | train_inner | epoch 018:    120 / 196 loss=2.515, nll_loss=0.552, ppl=1.47, wps=3913.8, ups=0.96, wpb=4080.9, bsz=196.5, num_updates=3440, lr=4.28571e-06, gnorm=1.498, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4794
2023-04-25 11:21:30 | INFO | train_inner | epoch 018:    140 / 196 loss=2.497, nll_loss=0.529, ppl=1.44, wps=3896.2, ups=0.98, wpb=3977.9, bsz=178.7, num_updates=3460, lr=4.13265e-06, gnorm=1.346, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4815
2023-04-25 11:21:51 | INFO | train_inner | epoch 018:    160 / 196 loss=2.493, nll_loss=0.525, ppl=1.44, wps=3939.8, ups=0.98, wpb=4037.7, bsz=182.9, num_updates=3480, lr=3.97959e-06, gnorm=1.312, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4835
2023-04-25 11:22:11 | INFO | train_inner | epoch 018:    180 / 196 loss=2.497, nll_loss=0.529, ppl=1.44, wps=3964.5, ups=0.97, wpb=4103.7, bsz=197.3, num_updates=3500, lr=3.82653e-06, gnorm=1.293, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4856
2023-04-25 11:22:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:22:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:22:58 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.334 | nll_loss 1.437 | ppl 2.71 | bleu 47.83 | wps 1181.4 | wpb 905.1 | bsz 34.2 | num_updates 3516 | best_loss 3.179
2023-04-25 11:22:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3516 updates
2023-04-25 11:22:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint18.pt
2023-04-25 11:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint18.pt
2023-04-25 11:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint18.pt (epoch 18 @ 3516 updates, score 3.334) (writing took 24.535609051585197 seconds)
2023-04-25 11:23:22 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-04-25 11:23:22 | INFO | train | epoch 018 | loss 2.496 | nll_loss 0.528 | ppl 1.44 | wps 3068.6 | ups 0.76 | wpb 4016.8 | bsz 186.3 | num_updates 3516 | lr 3.70408e-06 | gnorm 1.332 | clip 100 | loss_scale 0.125 | train_wall 201 | gb_free 10.4 | wall 4927
2023-04-25 11:23:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:23:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:23:22 | INFO | fairseq.trainer | begin training epoch 19
2023-04-25 11:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:23:27 | INFO | train_inner | epoch 019:      4 / 196 loss=2.508, nll_loss=0.542, ppl=1.46, wps=1031.5, ups=0.27, wpb=3872.5, bsz=190.4, num_updates=3520, lr=3.67347e-06, gnorm=1.384, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4931
2023-04-25 11:23:47 | INFO | train_inner | epoch 019:     24 / 196 loss=2.465, nll_loss=0.496, ppl=1.41, wps=3837.9, ups=0.97, wpb=3959, bsz=176.8, num_updates=3540, lr=3.52041e-06, gnorm=1.273, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=4952
2023-04-25 11:24:07 | INFO | train_inner | epoch 019:     44 / 196 loss=2.484, nll_loss=0.514, ppl=1.43, wps=3903.5, ups=0.98, wpb=3969.4, bsz=182.6, num_updates=3560, lr=3.36735e-06, gnorm=1.287, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4972
2023-04-25 11:24:28 | INFO | train_inner | epoch 019:     64 / 196 loss=2.481, nll_loss=0.512, ppl=1.43, wps=3920.6, ups=0.98, wpb=3998.7, bsz=180.9, num_updates=3580, lr=3.21429e-06, gnorm=1.29, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=4992
2023-04-25 11:24:49 | INFO | train_inner | epoch 019:     84 / 196 loss=2.505, nll_loss=0.537, ppl=1.45, wps=3868.3, ups=0.97, wpb=3995.4, bsz=199, num_updates=3600, lr=3.06122e-06, gnorm=1.339, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.1, wall=5013
2023-04-25 11:25:09 | INFO | train_inner | epoch 019:    104 / 196 loss=2.462, nll_loss=0.492, ppl=1.41, wps=3910.7, ups=0.99, wpb=3967.9, bsz=174.8, num_updates=3620, lr=2.90816e-06, gnorm=1.259, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5033
2023-04-25 11:25:30 | INFO | train_inner | epoch 019:    124 / 196 loss=2.51, nll_loss=0.545, ppl=1.46, wps=3985.5, ups=0.96, wpb=4152.4, bsz=204.9, num_updates=3640, lr=2.7551e-06, gnorm=1.341, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5054
2023-04-25 11:25:50 | INFO | train_inner | epoch 019:    144 / 196 loss=2.469, nll_loss=0.499, ppl=1.41, wps=3965.6, ups=0.97, wpb=4086.8, bsz=186.2, num_updates=3660, lr=2.60204e-06, gnorm=1.256, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5075
2023-04-25 11:26:11 | INFO | train_inner | epoch 019:    164 / 196 loss=2.497, nll_loss=0.528, ppl=1.44, wps=4027.8, ups=0.98, wpb=4113.5, bsz=199.8, num_updates=3680, lr=2.44898e-06, gnorm=1.286, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5095
2023-04-25 11:26:31 | INFO | train_inner | epoch 019:    184 / 196 loss=2.477, nll_loss=0.509, ppl=1.42, wps=3924.8, ups=0.97, wpb=4049.6, bsz=178.9, num_updates=3700, lr=2.29592e-06, gnorm=1.253, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5116
2023-04-25 11:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:26:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:27:13 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.344 | nll_loss 1.448 | ppl 2.73 | bleu 48.03 | wps 1190.2 | wpb 905.1 | bsz 34.2 | num_updates 3712 | best_loss 3.179
2023-04-25 11:27:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3712 updates
2023-04-25 11:27:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint19.pt
2023-04-25 11:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint19.pt
2023-04-25 11:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint19.pt (epoch 19 @ 3712 updates, score 3.344) (writing took 23.227617103606462 seconds)
2023-04-25 11:27:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-04-25 11:27:50 | INFO | train | epoch 019 | loss 2.483 | nll_loss 0.514 | ppl 1.43 | wps 2937.7 | ups 0.73 | wpb 4016.8 | bsz 186.3 | num_updates 3712 | lr 2.20408e-06 | gnorm 1.292 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 5195
2023-04-25 11:27:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:27:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:27:52 | INFO | fairseq.trainer | begin training epoch 20
2023-04-25 11:27:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:28:01 | INFO | train_inner | epoch 020:      8 / 196 loss=2.469, nll_loss=0.499, ppl=1.41, wps=861.2, ups=0.22, wpb=3857.8, bsz=170.1, num_updates=3720, lr=2.14286e-06, gnorm=1.329, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5205
2023-04-25 11:28:21 | INFO | train_inner | epoch 020:     28 / 196 loss=2.463, nll_loss=0.494, ppl=1.41, wps=3949.5, ups=0.98, wpb=4026.1, bsz=183.7, num_updates=3740, lr=1.9898e-06, gnorm=1.223, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5226
2023-04-25 11:28:42 | INFO | train_inner | epoch 020:     48 / 196 loss=2.472, nll_loss=0.503, ppl=1.42, wps=3947.7, ups=0.98, wpb=4047.2, bsz=194.8, num_updates=3760, lr=1.83673e-06, gnorm=1.263, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5246
2023-04-25 11:29:02 | INFO | train_inner | epoch 020:     68 / 196 loss=2.458, nll_loss=0.485, ppl=1.4, wps=3944.7, ups=0.99, wpb=3996.4, bsz=166.6, num_updates=3780, lr=1.68367e-06, gnorm=1.248, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5266
2023-04-25 11:29:23 | INFO | train_inner | epoch 020:     88 / 196 loss=2.479, nll_loss=0.508, ppl=1.42, wps=3965.7, ups=0.96, wpb=4118.9, bsz=195.6, num_updates=3800, lr=1.53061e-06, gnorm=1.261, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.3, wall=5287
2023-04-25 11:29:43 | INFO | train_inner | epoch 020:    108 / 196 loss=2.459, nll_loss=0.489, ppl=1.4, wps=3848.5, ups=0.98, wpb=3924.6, bsz=178, num_updates=3820, lr=1.37755e-06, gnorm=1.258, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5308
2023-04-25 11:30:04 | INFO | train_inner | epoch 020:    128 / 196 loss=2.484, nll_loss=0.515, ppl=1.43, wps=3916.3, ups=0.96, wpb=4085.8, bsz=205.3, num_updates=3840, lr=1.22449e-06, gnorm=1.251, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5329
2023-04-25 11:30:25 | INFO | train_inner | epoch 020:    148 / 196 loss=2.469, nll_loss=0.5, ppl=1.41, wps=3824.9, ups=0.97, wpb=3961.6, bsz=172.2, num_updates=3860, lr=1.07143e-06, gnorm=1.27, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5349
2023-04-25 11:30:45 | INFO | train_inner | epoch 020:    168 / 196 loss=2.471, nll_loss=0.502, ppl=1.42, wps=3975.2, ups=0.98, wpb=4061.1, bsz=182.1, num_updates=3880, lr=9.18367e-07, gnorm=1.256, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5370
2023-04-25 11:31:06 | INFO | train_inner | epoch 020:    188 / 196 loss=2.505, nll_loss=0.538, ppl=1.45, wps=3960.6, ups=0.97, wpb=4098.7, bsz=206.7, num_updates=3900, lr=7.65306e-07, gnorm=1.316, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5390
2023-04-25 11:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:31:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:31:44 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.347 | nll_loss 1.455 | ppl 2.74 | bleu 47.92 | wps 1189.8 | wpb 905.1 | bsz 34.2 | num_updates 3908 | best_loss 3.179
2023-04-25 11:31:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3908 updates
2023-04-25 11:31:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint20.pt
2023-04-25 11:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint20.pt
2023-04-25 11:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint20.pt (epoch 20 @ 3908 updates, score 3.347) (writing took 23.006205558776855 seconds)
2023-04-25 11:32:16 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-04-25 11:32:16 | INFO | train | epoch 020 | loss 2.474 | nll_loss 0.505 | ppl 1.42 | wps 2970.6 | ups 0.74 | wpb 4016.8 | bsz 186.3 | num_updates 3908 | lr 7.04082e-07 | gnorm 1.273 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 10.4 | wall 5460
2023-04-25 11:32:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:32:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:32:16 | INFO | fairseq.trainer | begin training epoch 21
2023-04-25 11:32:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:32:29 | INFO | train_inner | epoch 021:     12 / 196 loss=2.519, nll_loss=0.556, ppl=1.47, wps=945.4, ups=0.24, wpb=3904.2, bsz=209.9, num_updates=3920, lr=6.12245e-07, gnorm=1.414, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5473
2023-04-25 11:32:49 | INFO | train_inner | epoch 021:     32 / 196 loss=2.468, nll_loss=0.497, ppl=1.41, wps=3939.3, ups=0.99, wpb=3998.9, bsz=186.2, num_updates=3940, lr=4.59184e-07, gnorm=1.23, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5493
2023-04-25 11:33:10 | INFO | train_inner | epoch 021:     52 / 196 loss=2.458, nll_loss=0.488, ppl=1.4, wps=3950.2, ups=0.97, wpb=4076.6, bsz=183.5, num_updates=3960, lr=3.06122e-07, gnorm=1.221, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5514
2023-04-25 11:33:30 | INFO | train_inner | epoch 021:     72 / 196 loss=2.472, nll_loss=0.503, ppl=1.42, wps=3972, ups=0.96, wpb=4137.4, bsz=196.6, num_updates=3980, lr=1.53061e-07, gnorm=1.241, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=5535
2023-04-25 11:33:51 | INFO | train_inner | epoch 021:     92 / 196 loss=2.47, nll_loss=0.501, ppl=1.42, wps=3886.9, ups=0.97, wpb=3999.6, bsz=190.9, num_updates=4000, lr=0, gnorm=1.25, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5555
2023-04-25 11:34:12 | INFO | train_inner | epoch 021:    112 / 196 loss=2.471, nll_loss=0.502, ppl=1.42, wps=3932, ups=0.97, wpb=4041, bsz=194, num_updates=4020, lr=0, gnorm=1.253, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5576
2023-04-25 11:34:32 | INFO | train_inner | epoch 021:    132 / 196 loss=2.45, nll_loss=0.478, ppl=1.39, wps=3838.6, ups=0.97, wpb=3941.9, bsz=168.9, num_updates=4040, lr=0, gnorm=1.238, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=5596
2023-04-25 11:34:53 | INFO | train_inner | epoch 021:    152 / 196 loss=2.451, nll_loss=0.48, ppl=1.39, wps=3911.4, ups=0.98, wpb=4007.7, bsz=168.9, num_updates=4060, lr=0, gnorm=1.238, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5617
2023-04-25 11:35:13 | INFO | train_inner | epoch 021:    172 / 196 loss=2.472, nll_loss=0.502, ppl=1.42, wps=3917.3, ups=0.97, wpb=4018.9, bsz=181.9, num_updates=4080, lr=0, gnorm=1.262, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5637
2023-04-25 11:35:34 | INFO | train_inner | epoch 021:    192 / 196 loss=2.47, nll_loss=0.499, ppl=1.41, wps=3923.7, ups=0.97, wpb=4030.5, bsz=196.6, num_updates=4100, lr=0, gnorm=1.312, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5658
2023-04-25 11:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:35:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:36:07 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1205.3 | wpb 905.1 | bsz 34.2 | num_updates 4104 | best_loss 3.179
2023-04-25 11:36:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4104 updates
2023-04-25 11:36:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint21.pt
2023-04-25 11:36:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint21.pt
2023-04-25 11:36:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint21.pt (epoch 21 @ 4104 updates, score 3.349) (writing took 27.258106883615255 seconds)
2023-04-25 11:36:34 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-04-25 11:36:34 | INFO | train | epoch 021 | loss 2.468 | nll_loss 0.499 | ppl 1.41 | wps 3045.7 | ups 0.76 | wpb 4016.8 | bsz 186.3 | num_updates 4104 | lr 0 | gnorm 1.262 | clip 100 | loss_scale 0.25 | train_wall 200 | gb_free 10.4 | wall 5719
2023-04-25 11:36:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:36:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:36:34 | INFO | fairseq.trainer | begin training epoch 22
2023-04-25 11:36:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:36:51 | INFO | train_inner | epoch 022:     16 / 196 loss=2.463, nll_loss=0.492, ppl=1.41, wps=988.9, ups=0.26, wpb=3811.9, bsz=166, num_updates=4120, lr=0, gnorm=1.331, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5735
2023-04-25 11:37:11 | INFO | train_inner | epoch 022:     36 / 196 loss=2.47, nll_loss=0.499, ppl=1.41, wps=3879, ups=0.98, wpb=3964, bsz=184.6, num_updates=4140, lr=0, gnorm=1.252, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.3, wall=5756
2023-04-25 11:37:32 | INFO | train_inner | epoch 022:     56 / 196 loss=2.463, nll_loss=0.494, ppl=1.41, wps=3883.8, ups=0.98, wpb=3981.4, bsz=189.4, num_updates=4160, lr=0, gnorm=1.243, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5776
2023-04-25 11:37:52 | INFO | train_inner | epoch 022:     76 / 196 loss=2.463, nll_loss=0.492, ppl=1.41, wps=3951.6, ups=0.99, wpb=3988.5, bsz=170.2, num_updates=4180, lr=0, gnorm=1.245, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5796
2023-04-25 11:38:13 | INFO | train_inner | epoch 022:     96 / 196 loss=2.454, nll_loss=0.484, ppl=1.4, wps=3949.6, ups=0.97, wpb=4083, bsz=190.9, num_updates=4200, lr=0, gnorm=1.2, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=5817
2023-04-25 11:38:33 | INFO | train_inner | epoch 022:    116 / 196 loss=2.463, nll_loss=0.494, ppl=1.41, wps=3883, ups=0.98, wpb=3977.3, bsz=183.4, num_updates=4220, lr=0, gnorm=1.271, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5837
2023-04-25 11:38:54 | INFO | train_inner | epoch 022:    136 / 196 loss=2.495, nll_loss=0.53, ppl=1.44, wps=3949.6, ups=0.96, wpb=4106.8, bsz=216, num_updates=4240, lr=0, gnorm=1.242, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=5858
2023-04-25 11:39:15 | INFO | train_inner | epoch 022:    156 / 196 loss=2.468, nll_loss=0.497, ppl=1.41, wps=3943.9, ups=0.96, wpb=4095.6, bsz=183.5, num_updates=4260, lr=0, gnorm=1.24, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=5879
2023-04-25 11:39:35 | INFO | train_inner | epoch 022:    176 / 196 loss=2.467, nll_loss=0.497, ppl=1.41, wps=3885.3, ups=0.96, wpb=4041.4, bsz=183.1, num_updates=4280, lr=0, gnorm=1.304, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=5900
2023-04-25 11:39:56 | INFO | train_inner | epoch 022:    196 / 196 loss=2.47, nll_loss=0.5, ppl=1.41, wps=3932, ups=0.99, wpb=3959.8, bsz=180.6, num_updates=4300, lr=0, gnorm=1.315, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=5920
2023-04-25 11:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:39:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:40:25 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1195 | wpb 905.1 | bsz 34.2 | num_updates 4300 | best_loss 3.179
2023-04-25 11:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4300 updates
2023-04-25 11:40:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint22.pt
2023-04-25 11:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint22.pt
2023-04-25 11:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint22.pt (epoch 22 @ 4300 updates, score 3.349) (writing took 23.523470625281334 seconds)
2023-04-25 11:40:49 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-04-25 11:40:49 | INFO | train | epoch 022 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 3088.9 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 4300 | lr 0 | gnorm 1.257 | clip 100 | loss_scale 0.25 | train_wall 200 | gb_free 10.4 | wall 5973
2023-04-25 11:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:40:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:40:49 | INFO | fairseq.trainer | begin training epoch 23
2023-04-25 11:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:41:10 | INFO | train_inner | epoch 023:     20 / 196 loss=2.474, nll_loss=0.504, ppl=1.42, wps=1082.3, ups=0.27, wpb=4023.9, bsz=198, num_updates=4320, lr=0, gnorm=1.251, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=5994
2023-04-25 11:41:30 | INFO | train_inner | epoch 023:     40 / 196 loss=2.457, nll_loss=0.486, ppl=1.4, wps=3885.3, ups=0.98, wpb=3962, bsz=173.2, num_updates=4340, lr=0, gnorm=1.229, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6015
2023-04-25 11:41:51 | INFO | train_inner | epoch 023:     60 / 196 loss=2.457, nll_loss=0.486, ppl=1.4, wps=3961.4, ups=0.97, wpb=4066.2, bsz=182.6, num_updates=4360, lr=0, gnorm=1.212, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6035
2023-04-25 11:42:11 | INFO | train_inner | epoch 023:     80 / 196 loss=2.457, nll_loss=0.485, ppl=1.4, wps=3825.1, ups=0.98, wpb=3911.2, bsz=177.9, num_updates=4380, lr=0, gnorm=1.253, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6056
2023-04-25 11:42:32 | INFO | train_inner | epoch 023:    100 / 196 loss=2.468, nll_loss=0.498, ppl=1.41, wps=3931, ups=0.96, wpb=4081.9, bsz=188.2, num_updates=4400, lr=0, gnorm=1.221, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6076
2023-04-25 11:42:53 | INFO | train_inner | epoch 023:    120 / 196 loss=2.47, nll_loss=0.5, ppl=1.41, wps=3925.6, ups=0.97, wpb=4046.8, bsz=182.4, num_updates=4420, lr=0, gnorm=1.253, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6097
2023-04-25 11:43:13 | INFO | train_inner | epoch 023:    140 / 196 loss=2.463, nll_loss=0.493, ppl=1.41, wps=3899.8, ups=0.97, wpb=4019.7, bsz=187.8, num_updates=4440, lr=0, gnorm=1.221, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6118
2023-04-25 11:43:34 | INFO | train_inner | epoch 023:    160 / 196 loss=2.49, nll_loss=0.524, ppl=1.44, wps=3868.6, ups=0.97, wpb=3999, bsz=198.5, num_updates=4460, lr=0, gnorm=1.284, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6138
2023-04-25 11:43:55 | INFO | train_inner | epoch 023:    180 / 196 loss=2.472, nll_loss=0.502, ppl=1.42, wps=3962.7, ups=0.97, wpb=4104.8, bsz=183.2, num_updates=4480, lr=0, gnorm=1.543, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6159
2023-04-25 11:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:44:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:44:41 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1178.5 | wpb 905.1 | bsz 34.2 | num_updates 4496 | best_loss 3.179
2023-04-25 11:44:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4496 updates
2023-04-25 11:44:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint23.pt
2023-04-25 11:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint23.pt
2023-04-25 11:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint23.pt (epoch 23 @ 4496 updates, score 3.349) (writing took 22.09468111395836 seconds)
2023-04-25 11:45:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-04-25 11:45:03 | INFO | train | epoch 023 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 3100 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 4496 | lr 0 | gnorm 1.281 | clip 100 | loss_scale 0.25 | train_wall 200 | gb_free 10.4 | wall 6227
2023-04-25 11:45:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:45:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:45:03 | INFO | fairseq.trainer | begin training epoch 24
2023-04-25 11:45:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:45:07 | INFO | train_inner | epoch 024:      4 / 196 loss=2.469, nll_loss=0.5, ppl=1.41, wps=1080, ups=0.28, wpb=3921.7, bsz=191.8, num_updates=4500, lr=0, gnorm=1.336, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6232
2023-04-25 11:45:35 | INFO | train_inner | epoch 024:     24 / 196 loss=2.455, nll_loss=0.485, ppl=1.4, wps=3921.4, ups=0.98, wpb=3993, bsz=176.2, num_updates=4520, lr=0, gnorm=1.225, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6259
2023-04-25 11:45:56 | INFO | train_inner | epoch 024:     44 / 196 loss=2.473, nll_loss=0.503, ppl=1.42, wps=3915.4, ups=0.97, wpb=4042.7, bsz=181.9, num_updates=4540, lr=0, gnorm=1.316, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6280
2023-04-25 11:46:16 | INFO | train_inner | epoch 024:     64 / 196 loss=2.488, nll_loss=0.52, ppl=1.43, wps=3802.5, ups=0.96, wpb=3952.5, bsz=201.2, num_updates=4560, lr=0, gnorm=1.271, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6301
2023-04-25 11:46:37 | INFO | train_inner | epoch 024:     84 / 196 loss=2.461, nll_loss=0.491, ppl=1.41, wps=3958.4, ups=0.98, wpb=4057.6, bsz=183.1, num_updates=4580, lr=0, gnorm=1.218, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6321
2023-04-25 11:46:58 | INFO | train_inner | epoch 024:    104 / 196 loss=2.463, nll_loss=0.493, ppl=1.41, wps=3869.1, ups=0.97, wpb=4003.2, bsz=179.3, num_updates=4600, lr=0, gnorm=1.229, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6342
2023-04-25 11:47:18 | INFO | train_inner | epoch 024:    124 / 196 loss=2.457, nll_loss=0.486, ppl=1.4, wps=3943.8, ups=0.97, wpb=4064.8, bsz=180.5, num_updates=4620, lr=0, gnorm=1.23, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6363
2023-04-25 11:47:39 | INFO | train_inner | epoch 024:    144 / 196 loss=2.458, nll_loss=0.488, ppl=1.4, wps=3896.8, ups=0.98, wpb=3982.6, bsz=172.7, num_updates=4640, lr=0, gnorm=1.226, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6383
2023-04-25 11:47:59 | INFO | train_inner | epoch 024:    164 / 196 loss=2.487, nll_loss=0.52, ppl=1.43, wps=3961, ups=0.96, wpb=4112.5, bsz=197.5, num_updates=4660, lr=0, gnorm=1.267, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6404
2023-04-25 11:48:20 | INFO | train_inner | epoch 024:    184 / 196 loss=2.463, nll_loss=0.491, ppl=1.41, wps=3916.6, ups=0.96, wpb=4076.3, bsz=200.9, num_updates=4680, lr=0, gnorm=1.218, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6425
2023-04-25 11:48:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:48:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:49:02 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1199.2 | wpb 905.1 | bsz 34.2 | num_updates 4692 | best_loss 3.179
2023-04-25 11:49:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4692 updates
2023-04-25 11:49:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint24.pt
2023-04-25 11:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint24.pt
2023-04-25 11:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint24.pt (epoch 24 @ 4692 updates, score 3.349) (writing took 24.99878365173936 seconds)
2023-04-25 11:49:36 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-04-25 11:49:36 | INFO | train | epoch 024 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 2879.4 | ups 0.72 | wpb 4016.8 | bsz 186.3 | num_updates 4692 | lr 0 | gnorm 1.251 | clip 100 | loss_scale 0.25 | train_wall 200 | gb_free 10.4 | wall 6501
2023-04-25 11:49:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:49:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:49:36 | INFO | fairseq.trainer | begin training epoch 25
2023-04-25 11:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:49:45 | INFO | train_inner | epoch 025:      8 / 196 loss=2.508, nll_loss=0.544, ppl=1.46, wps=952, ups=0.24, wpb=4044.2, bsz=216, num_updates=4700, lr=0, gnorm=1.341, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6510
2023-04-25 11:50:06 | INFO | train_inner | epoch 025:     28 / 196 loss=2.457, nll_loss=0.487, ppl=1.4, wps=3972.1, ups=0.98, wpb=4070.8, bsz=181.2, num_updates=4720, lr=0, gnorm=1.225, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6530
2023-04-25 11:50:26 | INFO | train_inner | epoch 025:     48 / 196 loss=2.468, nll_loss=0.498, ppl=1.41, wps=3949.6, ups=0.98, wpb=4049.8, bsz=183, num_updates=4740, lr=0, gnorm=1.235, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6551
2023-04-25 11:50:47 | INFO | train_inner | epoch 025:     68 / 196 loss=2.458, nll_loss=0.488, ppl=1.4, wps=3906.5, ups=0.97, wpb=4029.2, bsz=179.8, num_updates=4760, lr=0, gnorm=1.227, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6571
2023-04-25 11:51:07 | INFO | train_inner | epoch 025:     88 / 196 loss=2.457, nll_loss=0.486, ppl=1.4, wps=3991.1, ups=0.99, wpb=4019.1, bsz=174.8, num_updates=4780, lr=0, gnorm=1.202, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6591
2023-04-25 11:51:27 | INFO | train_inner | epoch 025:    108 / 196 loss=2.472, nll_loss=0.503, ppl=1.42, wps=3876.4, ups=0.97, wpb=3986.2, bsz=184.8, num_updates=4800, lr=0, gnorm=1.284, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6612
2023-04-25 11:51:48 | INFO | train_inner | epoch 025:    128 / 196 loss=2.462, nll_loss=0.492, ppl=1.41, wps=3796.4, ups=0.97, wpb=3917.7, bsz=179.5, num_updates=4820, lr=0, gnorm=1.245, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6633
2023-04-25 11:52:09 | INFO | train_inner | epoch 025:    148 / 196 loss=2.454, nll_loss=0.483, ppl=1.4, wps=3844.8, ups=0.98, wpb=3925, bsz=170.7, num_updates=4840, lr=0, gnorm=1.37, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6653
2023-04-25 11:52:29 | INFO | train_inner | epoch 025:    168 / 196 loss=2.464, nll_loss=0.495, ppl=1.41, wps=3945.7, ups=0.97, wpb=4057.4, bsz=198.2, num_updates=4860, lr=0, gnorm=1.244, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6673
2023-04-25 11:52:50 | INFO | train_inner | epoch 025:    188 / 196 loss=2.489, nll_loss=0.52, ppl=1.43, wps=3971.5, ups=0.95, wpb=4173.1, bsz=216, num_updates=4880, lr=0, gnorm=1.279, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6695
2023-04-25 11:52:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:52:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:53:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1196.3 | wpb 905.1 | bsz 34.2 | num_updates 4888 | best_loss 3.179
2023-04-25 11:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4888 updates
2023-04-25 11:53:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint25.pt
2023-04-25 11:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint25.pt
2023-04-25 11:53:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint25.pt (epoch 25 @ 4888 updates, score 3.349) (writing took 25.214651506394148 seconds)
2023-04-25 11:54:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-04-25 11:54:03 | INFO | train | epoch 025 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 2952 | ups 0.73 | wpb 4016.8 | bsz 186.3 | num_updates 4888 | lr 0 | gnorm 1.267 | clip 100 | loss_scale 0.25 | train_wall 200 | gb_free 10.4 | wall 6768
2023-04-25 11:54:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:54:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:54:03 | INFO | fairseq.trainer | begin training epoch 26
2023-04-25 11:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:54:16 | INFO | train_inner | epoch 026:     12 / 196 loss=2.457, nll_loss=0.487, ppl=1.4, wps=895, ups=0.23, wpb=3822.5, bsz=160.2, num_updates=4900, lr=0, gnorm=1.315, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6780
2023-04-25 11:54:36 | INFO | train_inner | epoch 026:     32 / 196 loss=2.464, nll_loss=0.494, ppl=1.41, wps=3978.9, ups=0.99, wpb=4018.1, bsz=180.9, num_updates=4920, lr=0, gnorm=1.264, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6800
2023-04-25 11:54:56 | INFO | train_inner | epoch 026:     52 / 196 loss=2.471, nll_loss=0.5, ppl=1.41, wps=3906.4, ups=0.97, wpb=4011.7, bsz=191.8, num_updates=4940, lr=0, gnorm=1.252, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6821
2023-04-25 11:55:17 | INFO | train_inner | epoch 026:     72 / 196 loss=2.476, nll_loss=0.506, ppl=1.42, wps=3954.8, ups=0.95, wpb=4154.9, bsz=203.5, num_updates=4960, lr=0, gnorm=1.215, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6842
2023-04-25 11:55:38 | INFO | train_inner | epoch 026:     92 / 196 loss=2.498, nll_loss=0.531, ppl=1.44, wps=4055.1, ups=0.97, wpb=4201.7, bsz=207.7, num_updates=4980, lr=0, gnorm=1.274, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6862
2023-04-25 11:55:59 | INFO | train_inner | epoch 026:    112 / 196 loss=2.467, nll_loss=0.497, ppl=1.41, wps=3806.9, ups=0.97, wpb=3912, bsz=184.9, num_updates=5000, lr=0, gnorm=1.268, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6883
2023-04-25 11:56:19 | INFO | train_inner | epoch 026:    132 / 196 loss=2.459, nll_loss=0.49, ppl=1.4, wps=3823.9, ups=0.96, wpb=3962.8, bsz=186.6, num_updates=5020, lr=0, gnorm=1.232, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=6904
2023-04-25 11:56:40 | INFO | train_inner | epoch 026:    152 / 196 loss=2.464, nll_loss=0.493, ppl=1.41, wps=3860.4, ups=0.97, wpb=3966.2, bsz=180.2, num_updates=5040, lr=0, gnorm=1.254, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6924
2023-04-25 11:57:00 | INFO | train_inner | epoch 026:    172 / 196 loss=2.444, nll_loss=0.474, ppl=1.39, wps=3873.2, ups=0.98, wpb=3962.8, bsz=171.8, num_updates=5060, lr=0, gnorm=1.211, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6945
2023-04-25 11:57:21 | INFO | train_inner | epoch 026:    192 / 196 loss=2.478, nll_loss=0.508, ppl=1.42, wps=4031.1, ups=0.98, wpb=4133.6, bsz=192.8, num_updates=5080, lr=0, gnorm=1.24, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=6965
2023-04-25 11:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 11:57:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:57:54 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1184.6 | wpb 905.1 | bsz 34.2 | num_updates 5084 | best_loss 3.179
2023-04-25 11:57:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5084 updates
2023-04-25 11:57:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint26.pt
2023-04-25 11:58:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint26.pt
2023-04-25 11:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint26.pt (epoch 26 @ 5084 updates, score 3.349) (writing took 22.424434080719948 seconds)
2023-04-25 11:58:17 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-04-25 11:58:17 | INFO | train | epoch 026 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 3102.1 | ups 0.77 | wpb 4016.8 | bsz 186.3 | num_updates 5084 | lr 0 | gnorm 1.251 | clip 100 | loss_scale 0.5 | train_wall 200 | gb_free 10.4 | wall 7021
2023-04-25 11:58:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 11:58:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 11:58:17 | INFO | fairseq.trainer | begin training epoch 27
2023-04-25 11:58:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 11:58:34 | INFO | train_inner | epoch 027:     16 / 196 loss=2.461, nll_loss=0.49, ppl=1.4, wps=1027.8, ups=0.27, wpb=3741.2, bsz=165.7, num_updates=5100, lr=0, gnorm=1.306, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7038
2023-04-25 11:58:54 | INFO | train_inner | epoch 027:     36 / 196 loss=2.466, nll_loss=0.497, ppl=1.41, wps=3961.5, ups=0.97, wpb=4102.8, bsz=198.2, num_updates=5120, lr=0, gnorm=1.293, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7059
2023-04-25 11:59:15 | INFO | train_inner | epoch 027:     56 / 196 loss=2.458, nll_loss=0.487, ppl=1.4, wps=3870, ups=0.97, wpb=3989.6, bsz=175.3, num_updates=5140, lr=0, gnorm=1.239, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7079
2023-04-25 11:59:35 | INFO | train_inner | epoch 027:     76 / 196 loss=2.47, nll_loss=0.501, ppl=1.42, wps=3993.1, ups=0.98, wpb=4071.7, bsz=179.6, num_updates=5160, lr=0, gnorm=1.233, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7100
2023-04-25 11:59:56 | INFO | train_inner | epoch 027:     96 / 196 loss=2.465, nll_loss=0.495, ppl=1.41, wps=3908.9, ups=0.95, wpb=4100.6, bsz=195.8, num_updates=5180, lr=0, gnorm=1.239, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7121
2023-04-25 12:00:17 | INFO | train_inner | epoch 027:    116 / 196 loss=2.487, nll_loss=0.518, ppl=1.43, wps=4051.2, ups=0.98, wpb=4123.7, bsz=211.6, num_updates=5200, lr=0, gnorm=1.282, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7141
2023-04-25 12:00:37 | INFO | train_inner | epoch 027:    136 / 196 loss=2.456, nll_loss=0.483, ppl=1.4, wps=3875.7, ups=0.97, wpb=3986.6, bsz=175.2, num_updates=5220, lr=0, gnorm=1.228, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7162
2023-04-25 12:00:58 | INFO | train_inner | epoch 027:    156 / 196 loss=2.46, nll_loss=0.489, ppl=1.4, wps=3873.9, ups=0.96, wpb=4027.5, bsz=181.1, num_updates=5240, lr=0, gnorm=1.286, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.1, wall=7182
2023-04-25 12:01:19 | INFO | train_inner | epoch 027:    176 / 196 loss=2.458, nll_loss=0.487, ppl=1.4, wps=3867.7, ups=0.97, wpb=3972.8, bsz=184.5, num_updates=5260, lr=0, gnorm=1.23, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7203
2023-04-25 12:01:39 | INFO | train_inner | epoch 027:    196 / 196 loss=2.496, nll_loss=0.53, ppl=1.44, wps=3851.8, ups=1, wpb=3868.4, bsz=182.8, num_updates=5280, lr=0, gnorm=1.464, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7223
2023-04-25 12:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 12:01:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 12:02:09 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1177.7 | wpb 905.1 | bsz 34.2 | num_updates 5280 | best_loss 3.179
2023-04-25 12:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5280 updates
2023-04-25 12:02:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint27.pt
2023-04-25 12:02:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint27.pt
2023-04-25 12:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint27.pt (epoch 27 @ 5280 updates, score 3.349) (writing took 24.0711870752275 seconds)
2023-04-25 12:02:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-04-25 12:02:33 | INFO | train | epoch 027 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 3072.2 | ups 0.76 | wpb 4016.8 | bsz 186.3 | num_updates 5280 | lr 0 | gnorm 1.274 | clip 100 | loss_scale 0.5 | train_wall 201 | gb_free 10.4 | wall 7278
2023-04-25 12:02:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 12:02:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 12:02:33 | INFO | fairseq.trainer | begin training epoch 28
2023-04-25 12:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 12:02:54 | INFO | train_inner | epoch 028:     20 / 196 loss=2.476, nll_loss=0.507, ppl=1.42, wps=1098.5, ups=0.26, wpb=4146.4, bsz=200.4, num_updates=5300, lr=0, gnorm=1.23, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7299
2023-04-25 12:03:15 | INFO | train_inner | epoch 028:     40 / 196 loss=2.47, nll_loss=0.5, ppl=1.41, wps=4013, ups=0.97, wpb=4121.3, bsz=196.1, num_updates=5320, lr=0, gnorm=1.226, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7319
2023-04-25 12:03:35 | INFO | train_inner | epoch 028:     60 / 196 loss=2.458, nll_loss=0.488, ppl=1.4, wps=3904.6, ups=0.97, wpb=4024.9, bsz=182, num_updates=5340, lr=0, gnorm=1.234, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7340
2023-04-25 12:03:56 | INFO | train_inner | epoch 028:     80 / 196 loss=2.454, nll_loss=0.482, ppl=1.4, wps=3910.4, ups=0.98, wpb=3987.1, bsz=183.6, num_updates=5360, lr=0, gnorm=1.231, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7360
2023-04-25 12:04:16 | INFO | train_inner | epoch 028:    100 / 196 loss=2.495, nll_loss=0.529, ppl=1.44, wps=3859.4, ups=0.97, wpb=3963.8, bsz=197.6, num_updates=5380, lr=0, gnorm=1.283, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7381
2023-04-25 12:04:37 | INFO | train_inner | epoch 028:    120 / 196 loss=2.464, nll_loss=0.493, ppl=1.41, wps=3947.4, ups=0.97, wpb=4059.2, bsz=175.7, num_updates=5400, lr=0, gnorm=1.225, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7401
2023-04-25 12:04:57 | INFO | train_inner | epoch 028:    140 / 196 loss=2.465, nll_loss=0.496, ppl=1.41, wps=3968.8, ups=0.97, wpb=4072.2, bsz=194.9, num_updates=5420, lr=0, gnorm=1.262, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7422
2023-04-25 12:05:18 | INFO | train_inner | epoch 028:    160 / 196 loss=2.454, nll_loss=0.484, ppl=1.4, wps=3824.9, ups=0.97, wpb=3933.6, bsz=170.1, num_updates=5440, lr=0, gnorm=1.239, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7442
2023-04-25 12:05:39 | INFO | train_inner | epoch 028:    180 / 196 loss=2.464, nll_loss=0.493, ppl=1.41, wps=3884.9, ups=0.97, wpb=4012.4, bsz=182.2, num_updates=5460, lr=0, gnorm=1.24, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=7463
2023-04-25 12:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-04-25 12:05:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 12:06:24 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.349 | nll_loss 1.457 | ppl 2.75 | bleu 47.94 | wps 1189.6 | wpb 905.1 | bsz 34.2 | num_updates 5476 | best_loss 3.179
2023-04-25 12:06:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5476 updates
2023-04-25 12:06:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint28.pt
2023-04-25 12:06:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/checkpoints/checkpoint28.pt
2023-04-25 12:06:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint28.pt (epoch 28 @ 5476 updates, score 3.349) (writing took 27.566037841141224 seconds)
2023-04-25 12:07:03 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-04-25 12:07:03 | INFO | train | epoch 028 | loss 2.468 | nll_loss 0.498 | ppl 1.41 | wps 2921.8 | ups 0.73 | wpb 4016.8 | bsz 186.3 | num_updates 5476 | lr 0 | gnorm 1.249 | clip 100 | loss_scale 0.5 | train_wall 200 | gb_free 10.4 | wall 7547
2023-04-25 12:07:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-04-25 12:07:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 196
2023-04-25 12:07:03 | INFO | fairseq.trainer | begin training epoch 29
2023-04-25 12:07:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-04-25 12:07:07 | INFO | train_inner | epoch 029:      4 / 196 loss=2.472, nll_loss=0.502, ppl=1.42, wps=872.8, ups=0.23, wpb=3857.6, bsz=177.1, num_updates=5480, lr=0, gnorm=1.31, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7551
2023-04-25 12:07:27 | INFO | train_inner | epoch 029:     24 / 196 loss=2.461, nll_loss=0.489, ppl=1.4, wps=3926.9, ups=0.99, wpb=3978.3, bsz=172, num_updates=5500, lr=0, gnorm=1.234, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=7572
slurmstepd: error: *** JOB 115447 ON 99server CANCELLED AT 2023-04-25T12:07:39 ***
2023-04-25 12:07:50 | INFO | train_inner | epoch 029:     44 / 196 loss=2.449, nll_loss=0.477, ppl=1.39, wps=3522.9, ups=0.89, wpb=3943.9, bsz=167, num_updates=5520, lr=0, gnorm=1.216, clip=100, loss_scale=0.5, train_wall=22, gb_free=10.4, wall=7594
