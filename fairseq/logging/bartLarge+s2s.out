2023-05-06 07:16:23 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:57809
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:57809
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:57809
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:57809
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | initialized host 100server as rank 0
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | initialized host 100server as rank 3
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | initialized host 100server as rank 1
2023-05-06 07:16:25 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-06 07:16:25 | INFO | fairseq.distributed.utils | initialized host 100server as rank 2
2023-05-06 07:16:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:57809', 'distributed_port': 57809, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'training/bartLarge+s2s1', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='training/bartLarge+s2s1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='5000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='training/bartLarge+s2s1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='5000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 100, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 5000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-06 07:16:30 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-05-06 07:16:40 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-05-06 07:16:41 | INFO | fairseq_cli.train | task: GraphToSeq
2023-05-06 07:16:41 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-05-06 07:16:41 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-06 07:16:41 | INFO | fairseq_cli.train | num. shared model params: 557,445,120 (num. trained: 557,445,120)
2023-05-06 07:16:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-05-06 07:16:41 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-05-06 07:16:41 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-05-06 07:16:48 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-05-06 07:16:48 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2023-05-06 07:16:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-05-06 07:16:48 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-06 07:16:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-05-06 07:16:48 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-06 07:16:48 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-06 07:16:48 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-06 07:16:48 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-06 07:16:48 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-05-06 07:16:48 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2023-05-06 07:16:48 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-05-06 07:16:48 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt
2023-05-06 07:17:16 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt (epoch 41 @ 0 updates)
2023-05-06 07:17:16 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-05-06 07:17:16 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-05-06 07:17:16 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-05-06 07:17:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:17:16 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-06 07:17:16 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-06 07:17:16 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-06 07:17:17 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-06 07:17:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:17:17 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-06 07:17:17 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-06 07:17:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-06 07:17:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:17:27 | INFO | fairseq.trainer | begin training epoch 1
2023-05-06 07:17:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:17:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-05-06 07:17:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-05-06 07:17:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-05-06 07:17:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-05-06 07:17:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-05-06 07:17:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-06 07:17:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-06 07:17:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-06 07:17:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-06 07:17:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-06 07:17:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-05-06 07:18:05 | INFO | train_inner | epoch 001:     31 / 189 loss=10.21, nll_loss=8.688, ppl=412.4, wps=3742.6, ups=0.89, wpb=4206.4, bsz=197.6, num_updates=20, lr=6e-06, gnorm=125.002, clip=100, loss_scale=0.0625, train_wall=35, gb_free=10.4, wall=77
2023-05-06 07:18:25 | INFO | train_inner | epoch 001:     51 / 189 loss=6.477, nll_loss=4.709, ppl=26.15, wps=4157.3, ups=1.01, wpb=4130.4, bsz=184.5, num_updates=40, lr=1.2e-05, gnorm=16.97, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=96
2023-05-06 07:18:45 | INFO | train_inner | epoch 001:     71 / 189 loss=5.234, nll_loss=3.414, ppl=10.66, wps=4250.8, ups=1.01, wpb=4225.2, bsz=201.2, num_updates=60, lr=1.8e-05, gnorm=4.073, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=116
2023-05-06 07:19:05 | INFO | train_inner | epoch 001:     91 / 189 loss=4.69, nll_loss=2.845, ppl=7.19, wps=4080.4, ups=0.99, wpb=4128.4, bsz=187.3, num_updates=80, lr=2.4e-05, gnorm=3.111, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=137
2023-05-06 07:19:25 | INFO | train_inner | epoch 001:    111 / 189 loss=4.378, nll_loss=2.516, ppl=5.72, wps=4057.4, ups=0.99, wpb=4104.1, bsz=182.4, num_updates=100, lr=3e-05, gnorm=2.705, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=157
2023-05-06 07:19:45 | INFO | train_inner | epoch 001:    131 / 189 loss=4.249, nll_loss=2.398, ppl=5.27, wps=4109.5, ups=0.98, wpb=4201.2, bsz=195.9, num_updates=120, lr=2.98776e-05, gnorm=2.491, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=177
2023-05-06 07:20:06 | INFO | train_inner | epoch 001:    151 / 189 loss=4.173, nll_loss=2.322, ppl=5, wps=4146.2, ups=1, wpb=4156, bsz=205, num_updates=140, lr=2.97551e-05, gnorm=5.891, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=197
2023-05-06 07:20:26 | INFO | train_inner | epoch 001:    171 / 189 loss=4.031, nll_loss=2.168, ppl=4.49, wps=4153, ups=0.98, wpb=4233.6, bsz=211.6, num_updates=160, lr=2.96327e-05, gnorm=6.171, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=218
2023-05-06 07:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:20:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:21:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.533 | nll_loss 1.548 | ppl 2.92 | bleu 37.02 | wps 1246 | wpb 928.3 | bsz 35.1 | num_updates 178
2023-05-06 07:21:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 178 updates
2023-05-06 07:21:12 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint1.pt
2023-05-06 07:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint1.pt
2023-05-06 07:22:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint1.pt (epoch 1 @ 178 updates, score 3.533) (writing took 68.25682254694402 seconds)
2023-05-06 07:22:21 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-06 07:22:21 | INFO | train | epoch 001 | loss 5.287 | nll_loss 3.483 | ppl 11.18 | wps 2660.7 | ups 0.64 | wpb 4167.4 | bsz 194 | num_updates 178 | lr 2.95224e-05 | gnorm 18.925 | clip 100 | loss_scale 0.0625 | train_wall 193 | gb_free 10.4 | wall 333
2023-05-06 07:22:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:22:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:22:21 | INFO | fairseq.trainer | begin training epoch 2
2023-05-06 07:22:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:22:23 | INFO | train_inner | epoch 002:      2 / 189 loss=3.958, nll_loss=2.095, ppl=4.27, wps=702.2, ups=0.17, wpb=4111.6, bsz=181.9, num_updates=180, lr=2.95102e-05, gnorm=2.353, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=335
2023-05-06 07:22:43 | INFO | train_inner | epoch 002:     22 / 189 loss=3.912, nll_loss=2.046, ppl=4.13, wps=4115.6, ups=0.98, wpb=4204.1, bsz=219.2, num_updates=200, lr=2.93878e-05, gnorm=3.915, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=355
2023-05-06 07:23:03 | INFO | train_inner | epoch 002:     42 / 189 loss=3.779, nll_loss=1.902, ppl=3.74, wps=4109.5, ups=1, wpb=4092.4, bsz=188.5, num_updates=220, lr=2.92653e-05, gnorm=2.488, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.7, wall=375
2023-05-06 07:23:23 | INFO | train_inner | epoch 002:     62 / 189 loss=3.737, nll_loss=1.859, ppl=3.63, wps=4216.8, ups=1, wpb=4211.5, bsz=186.2, num_updates=240, lr=2.91429e-05, gnorm=2.12, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=395
2023-05-06 07:23:44 | INFO | train_inner | epoch 002:     82 / 189 loss=3.722, nll_loss=1.847, ppl=3.6, wps=4122, ups=0.98, wpb=4199.1, bsz=190.4, num_updates=260, lr=2.90204e-05, gnorm=2.035, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=416
2023-05-06 07:24:04 | INFO | train_inner | epoch 002:    102 / 189 loss=3.688, nll_loss=1.811, ppl=3.51, wps=4134, ups=0.98, wpb=4237.2, bsz=189, num_updates=280, lr=2.8898e-05, gnorm=1.993, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=436
2023-05-06 07:24:25 | INFO | train_inner | epoch 002:    122 / 189 loss=3.684, nll_loss=1.811, ppl=3.51, wps=4101.6, ups=0.98, wpb=4180.2, bsz=194.7, num_updates=300, lr=2.87755e-05, gnorm=2.005, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=456
2023-05-06 07:24:45 | INFO | train_inner | epoch 002:    142 / 189 loss=3.656, nll_loss=1.783, ppl=3.44, wps=4004.8, ups=0.98, wpb=4095.1, bsz=192.1, num_updates=320, lr=2.86531e-05, gnorm=1.945, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=477
2023-05-06 07:25:06 | INFO | train_inner | epoch 002:    162 / 189 loss=3.61, nll_loss=1.734, ppl=3.33, wps=4073.6, ups=0.98, wpb=4151.9, bsz=186.2, num_updates=340, lr=2.85306e-05, gnorm=1.834, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=497
2023-05-06 07:25:26 | INFO | train_inner | epoch 002:    182 / 189 loss=3.632, nll_loss=1.764, ppl=3.4, wps=4093.2, ups=0.98, wpb=4193.2, bsz=204.6, num_updates=360, lr=2.84082e-05, gnorm=1.897, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=518
2023-05-06 07:25:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:25:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:26:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.326 | nll_loss 1.381 | ppl 2.61 | bleu 42.84 | wps 1251.4 | wpb 928.3 | bsz 35.1 | num_updates 367 | best_loss 3.326
2023-05-06 07:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 367 updates
2023-05-06 07:26:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint2.pt
2023-05-06 07:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint2.pt
2023-05-06 07:27:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint2.pt (epoch 2 @ 367 updates, score 3.326) (writing took 108.70094712078571 seconds)
2023-05-06 07:27:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-06 07:27:50 | INFO | train | epoch 002 | loss 3.711 | nll_loss 1.837 | ppl 3.57 | wps 2390.1 | ups 0.57 | wpb 4165.6 | bsz 193.2 | num_updates 367 | lr 2.83653e-05 | gnorm 2.246 | clip 100 | loss_scale 0.0625 | train_wall 191 | gb_free 10.4 | wall 662
2023-05-06 07:27:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:27:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:27:51 | INFO | fairseq.trainer | begin training epoch 3
2023-05-06 07:27:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:28:04 | INFO | train_inner | epoch 003:     13 / 189 loss=3.481, nll_loss=1.59, ppl=3.01, wps=515.8, ups=0.13, wpb=4070.6, bsz=166.6, num_updates=380, lr=2.82857e-05, gnorm=2.557, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=676
2023-05-06 07:28:24 | INFO | train_inner | epoch 003:     33 / 189 loss=3.447, nll_loss=1.553, ppl=2.93, wps=4246.7, ups=1, wpb=4242.2, bsz=210.3, num_updates=400, lr=2.81633e-05, gnorm=1.774, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=696
2023-05-06 07:28:44 | INFO | train_inner | epoch 003:     53 / 189 loss=3.449, nll_loss=1.558, ppl=2.94, wps=4170.5, ups=1, wpb=4174.5, bsz=190.5, num_updates=420, lr=2.80408e-05, gnorm=1.856, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=716
2023-05-06 07:29:04 | INFO | train_inner | epoch 003:     73 / 189 loss=3.429, nll_loss=1.538, ppl=2.9, wps=4183.8, ups=1.01, wpb=4145.3, bsz=192.5, num_updates=440, lr=2.79184e-05, gnorm=1.765, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=735
2023-05-06 07:29:24 | INFO | train_inner | epoch 003:     93 / 189 loss=3.416, nll_loss=1.524, ppl=2.87, wps=4027.2, ups=1, wpb=4037.2, bsz=177.4, num_updates=460, lr=2.77959e-05, gnorm=2.832, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=756
2023-05-06 07:29:44 | INFO | train_inner | epoch 003:    113 / 189 loss=3.424, nll_loss=1.535, ppl=2.9, wps=4042.9, ups=0.99, wpb=4086.6, bsz=183.6, num_updates=480, lr=2.76735e-05, gnorm=1.832, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=776
2023-05-06 07:30:04 | INFO | train_inner | epoch 003:    133 / 189 loss=3.407, nll_loss=1.518, ppl=2.86, wps=4079.8, ups=0.99, wpb=4105, bsz=188.1, num_updates=500, lr=2.7551e-05, gnorm=1.741, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=796
2023-05-06 07:30:25 | INFO | train_inner | epoch 003:    153 / 189 loss=3.412, nll_loss=1.527, ppl=2.88, wps=4071.8, ups=0.97, wpb=4195.8, bsz=197.2, num_updates=520, lr=2.74286e-05, gnorm=1.676, clip=100, loss_scale=0.0625, train_wall=21, gb_free=10.4, wall=816
2023-05-06 07:30:45 | INFO | train_inner | epoch 003:    173 / 189 loss=3.494, nll_loss=1.623, ppl=3.08, wps=4230.9, ups=0.98, wpb=4325.7, bsz=231.3, num_updates=540, lr=2.73061e-05, gnorm=6.699, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=837
2023-05-06 07:31:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:31:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:31:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.236 | nll_loss 1.282 | ppl 2.43 | bleu 45.54 | wps 1273.1 | wpb 928.3 | bsz 35.1 | num_updates 556 | best_loss 3.236
2023-05-06 07:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 556 updates
2023-05-06 07:31:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint3.pt
2023-05-06 07:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint3.pt
2023-05-06 07:32:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint3.pt (epoch 3 @ 556 updates, score 3.236) (writing took 57.86657506227493 seconds)
2023-05-06 07:32:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-06 07:32:27 | INFO | train | epoch 003 | loss 3.432 | nll_loss 1.544 | ppl 2.92 | wps 2843 | ups 0.68 | wpb 4165.6 | bsz 193.2 | num_updates 556 | lr 2.72082e-05 | gnorm 2.476 | clip 100 | loss_scale 0.0625 | train_wall 189 | gb_free 10.4 | wall 939
2023-05-06 07:32:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:32:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:32:27 | INFO | fairseq.trainer | begin training epoch 4
2023-05-06 07:32:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:32:32 | INFO | train_inner | epoch 004:      4 / 189 loss=3.422, nll_loss=1.542, ppl=2.91, wps=801.2, ups=0.19, wpb=4261.3, bsz=195.2, num_updates=560, lr=2.71837e-05, gnorm=2.113, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=943
2023-05-06 07:32:52 | INFO | train_inner | epoch 004:     24 / 189 loss=3.269, nll_loss=1.365, ppl=2.58, wps=4127.6, ups=0.98, wpb=4202.2, bsz=196.6, num_updates=580, lr=2.70612e-05, gnorm=1.656, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=964
2023-05-06 07:33:12 | INFO | train_inner | epoch 004:     44 / 189 loss=3.275, nll_loss=1.371, ppl=2.59, wps=4118.1, ups=1, wpb=4101.2, bsz=187.2, num_updates=600, lr=2.69388e-05, gnorm=1.675, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=984
2023-05-06 07:33:32 | INFO | train_inner | epoch 004:     64 / 189 loss=3.251, nll_loss=1.346, ppl=2.54, wps=4069.3, ups=0.99, wpb=4112.9, bsz=187.1, num_updates=620, lr=2.68163e-05, gnorm=1.739, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1004
2023-05-06 07:33:53 | INFO | train_inner | epoch 004:     84 / 189 loss=3.275, nll_loss=1.376, ppl=2.59, wps=4083.9, ups=0.98, wpb=4174.4, bsz=197.2, num_updates=640, lr=2.66939e-05, gnorm=1.753, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1025
2023-05-06 07:34:13 | INFO | train_inner | epoch 004:    104 / 189 loss=3.266, nll_loss=1.366, ppl=2.58, wps=4142.3, ups=0.99, wpb=4178.4, bsz=179.7, num_updates=660, lr=2.65714e-05, gnorm=1.655, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1045
2023-05-06 07:34:34 | INFO | train_inner | epoch 004:    124 / 189 loss=3.257, nll_loss=1.357, ppl=2.56, wps=4039.4, ups=0.98, wpb=4130.2, bsz=206.4, num_updates=680, lr=2.6449e-05, gnorm=1.72, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1065
2023-05-06 07:34:54 | INFO | train_inner | epoch 004:    144 / 189 loss=3.237, nll_loss=1.339, ppl=2.53, wps=4078.7, ups=0.98, wpb=4167.2, bsz=184.8, num_updates=700, lr=2.63265e-05, gnorm=1.654, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1086
2023-05-06 07:35:14 | INFO | train_inner | epoch 004:    164 / 189 loss=3.219, nll_loss=1.317, ppl=2.49, wps=4074.3, ups=0.98, wpb=4170.4, bsz=182.1, num_updates=720, lr=2.62041e-05, gnorm=1.702, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1106
2023-05-06 07:35:35 | INFO | train_inner | epoch 004:    184 / 189 loss=3.238, nll_loss=1.34, ppl=2.53, wps=4022.8, ups=0.97, wpb=4160.6, bsz=198.4, num_updates=740, lr=2.60816e-05, gnorm=1.624, clip=100, loss_scale=0.0625, train_wall=21, gb_free=9.8, wall=1127
2023-05-06 07:35:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:35:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:36:09 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.192 | nll_loss 1.246 | ppl 2.37 | bleu 47.31 | wps 1256.4 | wpb 928.3 | bsz 35.1 | num_updates 745 | best_loss 3.192
2023-05-06 07:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 745 updates
2023-05-06 07:36:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint4.pt
2023-05-06 07:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint4.pt
2023-05-06 07:37:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint4.pt (epoch 4 @ 745 updates, score 3.192) (writing took 82.09905146248639 seconds)
2023-05-06 07:37:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-06 07:37:31 | INFO | train | epoch 004 | loss 3.26 | nll_loss 1.359 | ppl 2.57 | wps 2591 | ups 0.62 | wpb 4165.6 | bsz 193.2 | num_updates 745 | lr 2.6051e-05 | gnorm 1.732 | clip 100 | loss_scale 0.0625 | train_wall 191 | gb_free 10.4 | wall 1243
2023-05-06 07:37:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:37:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:37:31 | INFO | fairseq.trainer | begin training epoch 5
2023-05-06 07:37:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:37:46 | INFO | train_inner | epoch 005:     15 / 189 loss=3.132, nll_loss=1.217, ppl=2.32, wps=642.6, ups=0.15, wpb=4215.1, bsz=195.3, num_updates=760, lr=2.59592e-05, gnorm=2.274, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1258
2023-05-06 07:38:07 | INFO | train_inner | epoch 005:     35 / 189 loss=3.132, nll_loss=1.217, ppl=2.32, wps=4142.9, ups=1, wpb=4139.4, bsz=200.5, num_updates=780, lr=2.58367e-05, gnorm=1.587, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1279
2023-05-06 07:38:27 | INFO | train_inner | epoch 005:     55 / 189 loss=3.133, nll_loss=1.221, ppl=2.33, wps=4101.8, ups=0.98, wpb=4183.9, bsz=193.1, num_updates=800, lr=2.57143e-05, gnorm=1.566, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1299
2023-05-06 07:38:47 | INFO | train_inner | epoch 005:     75 / 189 loss=3.1, nll_loss=1.184, ppl=2.27, wps=3951.2, ups=0.99, wpb=4005.4, bsz=170.1, num_updates=820, lr=2.55918e-05, gnorm=1.569, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1319
2023-05-06 07:39:08 | INFO | train_inner | epoch 005:     95 / 189 loss=3.171, nll_loss=1.267, ppl=2.41, wps=4134.2, ups=0.98, wpb=4202.1, bsz=206.5, num_updates=840, lr=2.54694e-05, gnorm=2.111, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1340
2023-05-06 07:39:28 | INFO | train_inner | epoch 005:    115 / 189 loss=3.118, nll_loss=1.205, ppl=2.31, wps=4122.7, ups=0.98, wpb=4209.2, bsz=187.4, num_updates=860, lr=2.53469e-05, gnorm=1.605, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1360
2023-05-06 07:39:49 | INFO | train_inner | epoch 005:    135 / 189 loss=3.114, nll_loss=1.204, ppl=2.3, wps=4066.2, ups=0.97, wpb=4171.5, bsz=190.7, num_updates=880, lr=2.52245e-05, gnorm=1.577, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1381
2023-05-06 07:40:09 | INFO | train_inner | epoch 005:    155 / 189 loss=3.153, nll_loss=1.247, ppl=2.37, wps=4086.4, ups=0.98, wpb=4174.7, bsz=194.2, num_updates=900, lr=2.5102e-05, gnorm=2.33, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1401
2023-05-06 07:40:30 | INFO | train_inner | epoch 005:    175 / 189 loss=3.143, nll_loss=1.237, ppl=2.36, wps=4153.5, ups=0.98, wpb=4253.9, bsz=210.4, num_updates=920, lr=2.49796e-05, gnorm=1.614, clip=100, loss_scale=0.0625, train_wall=20, gb_free=9.9, wall=1421
2023-05-06 07:40:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:40:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:41:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.194 | nll_loss 1.25 | ppl 2.38 | bleu 47.4 | wps 1264.1 | wpb 928.3 | bsz 35.1 | num_updates 934 | best_loss 3.192
2023-05-06 07:41:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 934 updates
2023-05-06 07:41:12 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint5.pt
2023-05-06 07:41:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint5.pt
2023-05-06 07:41:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint5.pt (epoch 5 @ 934 updates, score 3.194) (writing took 24.15476823411882 seconds)
2023-05-06 07:41:36 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-06 07:41:36 | INFO | train | epoch 005 | loss 3.129 | nll_loss 1.219 | ppl 2.33 | wps 3209.9 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 934 | lr 2.48939e-05 | gnorm 1.795 | clip 100 | loss_scale 0.0625 | train_wall 191 | gb_free 10.4 | wall 1488
2023-05-06 07:41:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:41:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:41:36 | INFO | fairseq.trainer | begin training epoch 6
2023-05-06 07:41:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:41:42 | INFO | train_inner | epoch 006:      6 / 189 loss=3.09, nll_loss=1.18, ppl=2.27, wps=1112.8, ups=0.27, wpb=4047.7, bsz=175.1, num_updates=940, lr=2.48571e-05, gnorm=1.605, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1494
2023-05-06 07:42:03 | INFO | train_inner | epoch 006:     26 / 189 loss=3.038, nll_loss=1.116, ppl=2.17, wps=4086.6, ups=0.98, wpb=4156.4, bsz=202, num_updates=960, lr=2.47347e-05, gnorm=1.798, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1515
2023-05-06 07:42:23 | INFO | train_inner | epoch 006:     46 / 189 loss=3.006, nll_loss=1.082, ppl=2.12, wps=4083.8, ups=0.99, wpb=4131.7, bsz=187.7, num_updates=980, lr=2.46122e-05, gnorm=1.604, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1535
2023-05-06 07:42:43 | INFO | train_inner | epoch 006:     66 / 189 loss=3.012, nll_loss=1.088, ppl=2.13, wps=4075.1, ups=1, wpb=4087.7, bsz=195, num_updates=1000, lr=2.44898e-05, gnorm=1.577, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1555
2023-05-06 07:43:04 | INFO | train_inner | epoch 006:     86 / 189 loss=3.009, nll_loss=1.086, ppl=2.12, wps=4046.6, ups=0.97, wpb=4159.4, bsz=190.2, num_updates=1020, lr=2.43673e-05, gnorm=1.538, clip=100, loss_scale=0.0625, train_wall=20, gb_free=10.4, wall=1575
2023-05-06 07:43:24 | INFO | train_inner | epoch 006:    106 / 189 loss=3.052, nll_loss=1.135, ppl=2.2, wps=4086.9, ups=0.97, wpb=4208.4, bsz=213.2, num_updates=1040, lr=2.42449e-05, gnorm=1.677, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1596
2023-05-06 07:43:44 | INFO | train_inner | epoch 006:    126 / 189 loss=3.045, nll_loss=1.128, ppl=2.19, wps=4169.8, ups=0.99, wpb=4207, bsz=186.5, num_updates=1060, lr=2.41224e-05, gnorm=5.227, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1616
2023-05-06 07:44:05 | INFO | train_inner | epoch 006:    146 / 189 loss=3.024, nll_loss=1.107, ppl=2.15, wps=4126.3, ups=0.98, wpb=4216.2, bsz=193.9, num_updates=1080, lr=2.4e-05, gnorm=1.837, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1637
2023-05-06 07:44:25 | INFO | train_inner | epoch 006:    166 / 189 loss=3.003, nll_loss=1.083, ppl=2.12, wps=4131.6, ups=0.99, wpb=4187.6, bsz=192, num_updates=1100, lr=2.38776e-05, gnorm=1.487, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1657
2023-05-06 07:44:45 | INFO | train_inner | epoch 006:    186 / 189 loss=3.031, nll_loss=1.117, ppl=2.17, wps=4205.7, ups=0.99, wpb=4227.2, bsz=197.7, num_updates=1120, lr=2.37551e-05, gnorm=1.524, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1677
2023-05-06 07:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:44:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:45:17 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.179 | nll_loss 1.245 | ppl 2.37 | bleu 48.17 | wps 1258.3 | wpb 928.3 | bsz 35.1 | num_updates 1123 | best_loss 3.179
2023-05-06 07:45:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1123 updates
2023-05-06 07:45:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint6.pt
2023-05-06 07:45:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint6.pt
2023-05-06 07:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint6.pt (epoch 6 @ 1123 updates, score 3.179) (writing took 72.59342640638351 seconds)
2023-05-06 07:46:29 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-06 07:46:29 | INFO | train | epoch 006 | loss 3.024 | nll_loss 1.105 | ppl 2.15 | wps 2685.1 | ups 0.64 | wpb 4165.6 | bsz 193.2 | num_updates 1123 | lr 2.37367e-05 | gnorm 2.007 | clip 100 | loss_scale 0.125 | train_wall 191 | gb_free 10.4 | wall 1781
2023-05-06 07:46:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:46:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:46:30 | INFO | fairseq.trainer | begin training epoch 7
2023-05-06 07:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:46:47 | INFO | train_inner | epoch 007:     17 / 189 loss=2.923, nll_loss=0.992, ppl=1.99, wps=676.9, ups=0.16, wpb=4138.3, bsz=182, num_updates=1140, lr=2.36327e-05, gnorm=1.474, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1799
2023-05-06 07:47:07 | INFO | train_inner | epoch 007:     37 / 189 loss=2.916, nll_loss=0.983, ppl=1.98, wps=4228.1, ups=1, wpb=4223.2, bsz=194.5, num_updates=1160, lr=2.35102e-05, gnorm=1.509, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1819
2023-05-06 07:47:27 | INFO | train_inner | epoch 007:     57 / 189 loss=2.889, nll_loss=0.955, ppl=1.94, wps=4101.5, ups=1.01, wpb=4065.4, bsz=176.2, num_updates=1180, lr=2.33878e-05, gnorm=1.464, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1839
2023-05-06 07:47:48 | INFO | train_inner | epoch 007:     77 / 189 loss=2.942, nll_loss=1.013, ppl=2.02, wps=4121.9, ups=0.99, wpb=4171.6, bsz=197.3, num_updates=1200, lr=2.32653e-05, gnorm=1.493, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1859
2023-05-06 07:48:08 | INFO | train_inner | epoch 007:     97 / 189 loss=2.948, nll_loss=1.022, ppl=2.03, wps=3997, ups=0.97, wpb=4133, bsz=209.8, num_updates=1220, lr=2.31429e-05, gnorm=1.527, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=1880
2023-05-06 07:48:29 | INFO | train_inner | epoch 007:    117 / 189 loss=2.979, nll_loss=1.054, ppl=2.08, wps=4221.5, ups=0.98, wpb=4304.7, bsz=213, num_updates=1240, lr=2.30204e-05, gnorm=2.036, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1900
2023-05-06 07:48:49 | INFO | train_inner | epoch 007:    137 / 189 loss=2.937, nll_loss=1.009, ppl=2.01, wps=4138.1, ups=0.98, wpb=4222.5, bsz=185.7, num_updates=1260, lr=2.2898e-05, gnorm=1.533, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1921
2023-05-06 07:49:09 | INFO | train_inner | epoch 007:    157 / 189 loss=2.949, nll_loss=1.025, ppl=2.03, wps=4046.7, ups=0.99, wpb=4106.6, bsz=198.8, num_updates=1280, lr=2.27755e-05, gnorm=2.4, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1941
2023-05-06 07:49:30 | INFO | train_inner | epoch 007:    177 / 189 loss=2.936, nll_loss=1.01, ppl=2.01, wps=4133.4, ups=0.99, wpb=4177.8, bsz=190.8, num_updates=1300, lr=2.26531e-05, gnorm=1.924, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=1961
2023-05-06 07:49:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:49:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:50:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.19 | nll_loss 1.254 | ppl 2.39 | bleu 48.55 | wps 1248.7 | wpb 928.3 | bsz 35.1 | num_updates 1312 | best_loss 3.179
2023-05-06 07:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1312 updates
2023-05-06 07:50:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint7.pt
2023-05-06 07:50:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint7.pt
2023-05-06 07:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint7.pt (epoch 7 @ 1312 updates, score 3.19) (writing took 30.460286773741245 seconds)
2023-05-06 07:50:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-06 07:50:41 | INFO | train | epoch 007 | loss 2.931 | nll_loss 1.002 | ppl 2 | wps 3131.6 | ups 0.75 | wpb 4165.6 | bsz 193.2 | num_updates 1312 | lr 2.25796e-05 | gnorm 1.694 | clip 100 | loss_scale 0.125 | train_wall 191 | gb_free 10.4 | wall 2033
2023-05-06 07:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:50:41 | INFO | fairseq.trainer | begin training epoch 8
2023-05-06 07:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:50:49 | INFO | train_inner | epoch 008:      8 / 189 loss=2.878, nll_loss=0.947, ppl=1.93, wps=1032.9, ups=0.25, wpb=4112.1, bsz=181.5, num_updates=1320, lr=2.25306e-05, gnorm=1.449, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2041
2023-05-06 07:51:09 | INFO | train_inner | epoch 008:     28 / 189 loss=2.842, nll_loss=0.903, ppl=1.87, wps=4098.5, ups=1, wpb=4091.5, bsz=191.6, num_updates=1340, lr=2.24082e-05, gnorm=1.468, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2061
2023-05-06 07:51:29 | INFO | train_inner | epoch 008:     48 / 189 loss=2.847, nll_loss=0.908, ppl=1.88, wps=4138.8, ups=0.99, wpb=4183.2, bsz=193.6, num_updates=1360, lr=2.22857e-05, gnorm=1.474, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2081
2023-05-06 07:51:49 | INFO | train_inner | epoch 008:     68 / 189 loss=2.848, nll_loss=0.91, ppl=1.88, wps=4104.6, ups=1, wpb=4115.4, bsz=179.1, num_updates=1380, lr=2.21633e-05, gnorm=1.485, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2101
2023-05-06 07:52:10 | INFO | train_inner | epoch 008:     88 / 189 loss=2.854, nll_loss=0.919, ppl=1.89, wps=4100.9, ups=0.98, wpb=4197.9, bsz=197.4, num_updates=1400, lr=2.20408e-05, gnorm=1.486, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2122
2023-05-06 07:52:30 | INFO | train_inner | epoch 008:    108 / 189 loss=2.829, nll_loss=0.89, ppl=1.85, wps=4071, ups=0.98, wpb=4149.4, bsz=183.2, num_updates=1420, lr=2.19184e-05, gnorm=1.452, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2142
2023-05-06 07:52:50 | INFO | train_inner | epoch 008:    128 / 189 loss=2.838, nll_loss=0.901, ppl=1.87, wps=4167.9, ups=1, wpb=4166.4, bsz=186.5, num_updates=1440, lr=2.17959e-05, gnorm=1.453, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2162
2023-05-06 07:53:11 | INFO | train_inner | epoch 008:    148 / 189 loss=2.868, nll_loss=0.939, ppl=1.92, wps=4100.5, ups=0.96, wpb=4257.1, bsz=213.2, num_updates=1460, lr=2.16735e-05, gnorm=1.497, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2183
2023-05-06 07:53:31 | INFO | train_inner | epoch 008:    168 / 189 loss=2.842, nll_loss=0.906, ppl=1.87, wps=4047.2, ups=0.99, wpb=4092.2, bsz=190.8, num_updates=1480, lr=2.1551e-05, gnorm=1.534, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2203
2023-05-06 07:53:51 | INFO | train_inner | epoch 008:    188 / 189 loss=2.902, nll_loss=0.974, ppl=1.96, wps=4184.2, ups=0.99, wpb=4230.1, bsz=204.9, num_updates=1500, lr=2.14286e-05, gnorm=1.816, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2223
2023-05-06 07:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:53:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:54:21 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.201 | nll_loss 1.271 | ppl 2.41 | bleu 47.95 | wps 1244.7 | wpb 928.3 | bsz 35.1 | num_updates 1501 | best_loss 3.179
2023-05-06 07:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1501 updates
2023-05-06 07:54:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint8.pt
2023-05-06 07:54:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint8.pt
2023-05-06 07:54:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint8.pt (epoch 8 @ 1501 updates, score 3.201) (writing took 25.257801737636328 seconds)
2023-05-06 07:54:47 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-06 07:54:47 | INFO | train | epoch 008 | loss 2.853 | nll_loss 0.918 | ppl 1.89 | wps 3203.2 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 1501 | lr 2.14224e-05 | gnorm 1.515 | clip 100 | loss_scale 0.125 | train_wall 190 | gb_free 10.4 | wall 2278
2023-05-06 07:54:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:54:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:54:47 | INFO | fairseq.trainer | begin training epoch 9
2023-05-06 07:54:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:55:07 | INFO | train_inner | epoch 009:     19 / 189 loss=2.755, nll_loss=0.809, ppl=1.75, wps=1104.2, ups=0.26, wpb=4170.6, bsz=187.2, num_updates=1520, lr=2.13061e-05, gnorm=1.407, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2299
2023-05-06 07:55:28 | INFO | train_inner | epoch 009:     39 / 189 loss=2.78, nll_loss=0.834, ppl=1.78, wps=4042.6, ups=0.97, wpb=4182.6, bsz=206.2, num_updates=1540, lr=2.11837e-05, gnorm=1.45, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2319
2023-05-06 07:55:48 | INFO | train_inner | epoch 009:     59 / 189 loss=2.796, nll_loss=0.855, ppl=1.81, wps=4077.9, ups=0.96, wpb=4230.7, bsz=201.6, num_updates=1560, lr=2.10612e-05, gnorm=1.827, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2340
2023-05-06 07:56:09 | INFO | train_inner | epoch 009:     79 / 189 loss=2.796, nll_loss=0.854, ppl=1.81, wps=4085.4, ups=0.98, wpb=4177.1, bsz=198.3, num_updates=1580, lr=2.09388e-05, gnorm=1.539, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2361
2023-05-06 07:56:29 | INFO | train_inner | epoch 009:     99 / 189 loss=2.778, nll_loss=0.835, ppl=1.78, wps=4110.4, ups=0.99, wpb=4170.1, bsz=186.3, num_updates=1600, lr=2.08163e-05, gnorm=1.426, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2381
2023-05-06 07:56:49 | INFO | train_inner | epoch 009:    119 / 189 loss=2.774, nll_loss=0.83, ppl=1.78, wps=4068.4, ups=0.99, wpb=4102.7, bsz=172.9, num_updates=1620, lr=2.06939e-05, gnorm=1.463, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2401
2023-05-06 07:57:10 | INFO | train_inner | epoch 009:    139 / 189 loss=2.796, nll_loss=0.856, ppl=1.81, wps=4154.3, ups=0.99, wpb=4200.1, bsz=203.8, num_updates=1640, lr=2.05714e-05, gnorm=1.465, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2421
2023-05-06 07:57:30 | INFO | train_inner | epoch 009:    159 / 189 loss=2.809, nll_loss=0.87, ppl=1.83, wps=4037.5, ups=0.99, wpb=4087.3, bsz=185.9, num_updates=1660, lr=2.0449e-05, gnorm=1.854, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2442
2023-05-06 07:57:50 | INFO | train_inner | epoch 009:    179 / 189 loss=2.796, nll_loss=0.857, ppl=1.81, wps=4127.3, ups=0.98, wpb=4220.3, bsz=201.6, num_updates=1680, lr=2.03265e-05, gnorm=1.479, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2462
2023-05-06 07:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 07:58:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:58:29 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.223 | nll_loss 1.295 | ppl 2.45 | bleu 48.17 | wps 1251.2 | wpb 928.3 | bsz 35.1 | num_updates 1690 | best_loss 3.179
2023-05-06 07:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1690 updates
2023-05-06 07:58:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint9.pt
2023-05-06 07:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint9.pt
2023-05-06 07:58:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint9.pt (epoch 9 @ 1690 updates, score 3.223) (writing took 26.32541816495359 seconds)
2023-05-06 07:58:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-06 07:58:55 | INFO | train | epoch 009 | loss 2.785 | nll_loss 0.843 | ppl 1.79 | wps 3166.9 | ups 0.76 | wpb 4165.6 | bsz 193.2 | num_updates 1690 | lr 2.02653e-05 | gnorm 1.54 | clip 100 | loss_scale 0.125 | train_wall 191 | gb_free 10.4 | wall 2527
2023-05-06 07:58:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 07:58:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 07:58:55 | INFO | fairseq.trainer | begin training epoch 10
2023-05-06 07:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 07:59:05 | INFO | train_inner | epoch 010:     10 / 189 loss=2.727, nll_loss=0.779, ppl=1.72, wps=1074.6, ups=0.27, wpb=4040.6, bsz=177.6, num_updates=1700, lr=2.02041e-05, gnorm=1.396, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2537
2023-05-06 07:59:26 | INFO | train_inner | epoch 010:     30 / 189 loss=2.733, nll_loss=0.781, ppl=1.72, wps=4064.8, ups=0.98, wpb=4158.9, bsz=192.3, num_updates=1720, lr=2.00816e-05, gnorm=1.527, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2558
2023-05-06 07:59:46 | INFO | train_inner | epoch 010:     50 / 189 loss=2.707, nll_loss=0.754, ppl=1.69, wps=4175.3, ups=0.99, wpb=4228.7, bsz=199.4, num_updates=1740, lr=1.99592e-05, gnorm=1.419, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2578
2023-05-06 08:00:07 | INFO | train_inner | epoch 010:     70 / 189 loss=2.717, nll_loss=0.767, ppl=1.7, wps=4089.6, ups=0.98, wpb=4153.7, bsz=189, num_updates=1760, lr=1.98367e-05, gnorm=1.419, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2598
2023-05-06 08:00:26 | INFO | train_inner | epoch 010:     90 / 189 loss=2.722, nll_loss=0.774, ppl=1.71, wps=4135.2, ups=1, wpb=4134.1, bsz=196, num_updates=1780, lr=1.97143e-05, gnorm=1.413, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2618
2023-05-06 08:00:47 | INFO | train_inner | epoch 010:    110 / 189 loss=2.713, nll_loss=0.764, ppl=1.7, wps=4110.7, ups=1, wpb=4122.1, bsz=184.9, num_updates=1800, lr=1.95918e-05, gnorm=1.434, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2638
2023-05-06 08:01:07 | INFO | train_inner | epoch 010:    130 / 189 loss=2.736, nll_loss=0.79, ppl=1.73, wps=4201.4, ups=1, wpb=4216.4, bsz=184.8, num_updates=1820, lr=1.94694e-05, gnorm=1.456, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2658
2023-05-06 08:01:27 | INFO | train_inner | epoch 010:    150 / 189 loss=2.713, nll_loss=0.765, ppl=1.7, wps=4128, ups=0.99, wpb=4151, bsz=188.7, num_updates=1840, lr=1.93469e-05, gnorm=1.456, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2679
2023-05-06 08:01:47 | INFO | train_inner | epoch 010:    170 / 189 loss=2.747, nll_loss=0.803, ppl=1.74, wps=4203, ups=0.99, wpb=4254.6, bsz=207, num_updates=1860, lr=1.92245e-05, gnorm=1.426, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2699
2023-05-06 08:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:02:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:02:34 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.23 | nll_loss 1.309 | ppl 2.48 | bleu 47.79 | wps 1269 | wpb 928.3 | bsz 35.1 | num_updates 1879 | best_loss 3.179
2023-05-06 08:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1879 updates
2023-05-06 08:02:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint10.pt
2023-05-06 08:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint10.pt
2023-05-06 08:03:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint10.pt (epoch 10 @ 1879 updates, score 3.23) (writing took 27.834861550480127 seconds)
2023-05-06 08:03:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-06 08:03:02 | INFO | train | epoch 010 | loss 2.725 | nll_loss 0.777 | ppl 1.71 | wps 3190 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 1879 | lr 1.91082e-05 | gnorm 1.471 | clip 100 | loss_scale 0.125 | train_wall 190 | gb_free 10.4 | wall 2774
2023-05-06 08:03:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:03:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:03:02 | INFO | fairseq.trainer | begin training epoch 11
2023-05-06 08:03:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:03:03 | INFO | train_inner | epoch 011:      1 / 189 loss=2.761, nll_loss=0.819, ppl=1.76, wps=1086.2, ups=0.26, wpb=4149.1, bsz=205.2, num_updates=1880, lr=1.9102e-05, gnorm=1.741, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2775
2023-05-06 08:03:23 | INFO | train_inner | epoch 011:     21 / 189 loss=2.655, nll_loss=0.694, ppl=1.62, wps=4177.4, ups=1.01, wpb=4126.8, bsz=183.2, num_updates=1900, lr=1.89796e-05, gnorm=1.362, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2795
2023-05-06 08:03:44 | INFO | train_inner | epoch 011:     41 / 189 loss=2.643, nll_loss=0.686, ppl=1.61, wps=4005.7, ups=0.97, wpb=4137.2, bsz=190.7, num_updates=1920, lr=1.88571e-05, gnorm=1.409, clip=100, loss_scale=0.125, train_wall=21, gb_free=10.4, wall=2816
2023-05-06 08:04:04 | INFO | train_inner | epoch 011:     61 / 189 loss=2.693, nll_loss=0.743, ppl=1.67, wps=4099, ups=0.98, wpb=4188, bsz=196.1, num_updates=1940, lr=1.87347e-05, gnorm=1.593, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2836
2023-05-06 08:04:24 | INFO | train_inner | epoch 011:     81 / 189 loss=2.665, nll_loss=0.711, ppl=1.64, wps=4036, ups=0.99, wpb=4063.2, bsz=195.2, num_updates=1960, lr=1.86122e-05, gnorm=1.438, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2856
2023-05-06 08:04:45 | INFO | train_inner | epoch 011:    101 / 189 loss=2.68, nll_loss=0.729, ppl=1.66, wps=4243.3, ups=0.99, wpb=4304.8, bsz=214.5, num_updates=1980, lr=1.84898e-05, gnorm=1.398, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2876
2023-05-06 08:05:05 | INFO | train_inner | epoch 011:    121 / 189 loss=2.676, nll_loss=0.724, ppl=1.65, wps=4210.8, ups=0.99, wpb=4249.1, bsz=195, num_updates=2000, lr=1.83673e-05, gnorm=1.421, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2897
2023-05-06 08:05:25 | INFO | train_inner | epoch 011:    141 / 189 loss=2.697, nll_loss=0.748, ppl=1.68, wps=4201.9, ups=1, wpb=4217.2, bsz=198.4, num_updates=2020, lr=1.82449e-05, gnorm=1.432, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2917
2023-05-06 08:05:45 | INFO | train_inner | epoch 011:    161 / 189 loss=2.667, nll_loss=0.714, ppl=1.64, wps=3999.8, ups=0.98, wpb=4061.1, bsz=181.5, num_updates=2040, lr=1.81224e-05, gnorm=1.428, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.4, wall=2937
2023-05-06 08:06:05 | INFO | train_inner | epoch 011:    181 / 189 loss=2.669, nll_loss=0.718, ppl=1.64, wps=4223.1, ups=1, wpb=4226.9, bsz=185.8, num_updates=2060, lr=1.8e-05, gnorm=1.425, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=2957
2023-05-06 08:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:06:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:06:41 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.26 | nll_loss 1.342 | ppl 2.54 | bleu 47.74 | wps 1264.2 | wpb 928.3 | bsz 35.1 | num_updates 2068 | best_loss 3.179
2023-05-06 08:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2068 updates
2023-05-06 08:06:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint11.pt
2023-05-06 08:07:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint11.pt
2023-05-06 08:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint11.pt (epoch 11 @ 2068 updates, score 3.26) (writing took 26.67755912989378 seconds)
2023-05-06 08:07:08 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-06 08:07:08 | INFO | train | epoch 011 | loss 2.672 | nll_loss 0.719 | ppl 1.65 | wps 3198.4 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 2068 | lr 1.7951e-05 | gnorm 1.435 | clip 100 | loss_scale 0.25 | train_wall 190 | gb_free 10.4 | wall 3020
2023-05-06 08:07:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:07:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:07:08 | INFO | fairseq.trainer | begin training epoch 12
2023-05-06 08:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:07:21 | INFO | train_inner | epoch 012:     12 / 189 loss=2.624, nll_loss=0.667, ppl=1.59, wps=1078.5, ups=0.27, wpb=4067.8, bsz=196, num_updates=2080, lr=1.78776e-05, gnorm=1.365, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3032
2023-05-06 08:07:41 | INFO | train_inner | epoch 012:     32 / 189 loss=2.64, nll_loss=0.684, ppl=1.61, wps=4123.9, ups=0.99, wpb=4169.3, bsz=203.1, num_updates=2100, lr=1.77551e-05, gnorm=1.557, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3053
2023-05-06 08:08:01 | INFO | train_inner | epoch 012:     52 / 189 loss=2.632, nll_loss=0.673, ppl=1.59, wps=4162.8, ups=0.99, wpb=4215.4, bsz=197.5, num_updates=2120, lr=1.76327e-05, gnorm=1.381, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3073
2023-05-06 08:08:22 | INFO | train_inner | epoch 012:     72 / 189 loss=2.643, nll_loss=0.687, ppl=1.61, wps=4045.9, ups=0.97, wpb=4154.7, bsz=199.2, num_updates=2140, lr=1.75102e-05, gnorm=1.397, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3093
2023-05-06 08:08:42 | INFO | train_inner | epoch 012:     92 / 189 loss=2.608, nll_loss=0.65, ppl=1.57, wps=4091, ups=1, wpb=4101.9, bsz=189.3, num_updates=2160, lr=1.73878e-05, gnorm=1.372, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3114
2023-05-06 08:09:02 | INFO | train_inner | epoch 012:    112 / 189 loss=2.603, nll_loss=0.644, ppl=1.56, wps=4058.5, ups=0.99, wpb=4105.2, bsz=182.2, num_updates=2180, lr=1.72653e-05, gnorm=1.395, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3134
2023-05-06 08:09:22 | INFO | train_inner | epoch 012:    132 / 189 loss=2.622, nll_loss=0.663, ppl=1.58, wps=4137.3, ups=0.99, wpb=4196.1, bsz=190.6, num_updates=2200, lr=1.71429e-05, gnorm=1.407, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3154
2023-05-06 08:09:43 | INFO | train_inner | epoch 012:    152 / 189 loss=2.635, nll_loss=0.679, ppl=1.6, wps=4232.2, ups=0.99, wpb=4293.6, bsz=196.7, num_updates=2220, lr=1.70204e-05, gnorm=1.402, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3174
2023-05-06 08:10:03 | INFO | train_inner | epoch 012:    172 / 189 loss=2.628, nll_loss=0.672, ppl=1.59, wps=4075.8, ups=0.98, wpb=4137.9, bsz=189.1, num_updates=2240, lr=1.6898e-05, gnorm=1.384, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3195
2023-05-06 08:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:10:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:10:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.278 | nll_loss 1.365 | ppl 2.58 | bleu 47.98 | wps 1264.9 | wpb 928.3 | bsz 35.1 | num_updates 2257 | best_loss 3.179
2023-05-06 08:10:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2257 updates
2023-05-06 08:10:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint12.pt
2023-05-06 08:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint12.pt
2023-05-06 08:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint12.pt (epoch 12 @ 2257 updates, score 3.278) (writing took 26.725893642753363 seconds)
2023-05-06 08:11:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-06 08:11:15 | INFO | train | epoch 012 | loss 2.625 | nll_loss 0.668 | ppl 1.59 | wps 3193.5 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 2257 | lr 1.67939e-05 | gnorm 1.408 | clip 100 | loss_scale 0.25 | train_wall 190 | gb_free 10.4 | wall 3266
2023-05-06 08:11:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:11:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:11:15 | INFO | fairseq.trainer | begin training epoch 13
2023-05-06 08:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:11:18 | INFO | train_inner | epoch 013:      3 / 189 loss=2.628, nll_loss=0.672, ppl=1.59, wps=1106, ups=0.27, wpb=4157.1, bsz=188.7, num_updates=2260, lr=1.67755e-05, gnorm=1.428, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3270
2023-05-06 08:11:38 | INFO | train_inner | epoch 013:     23 / 189 loss=2.571, nll_loss=0.609, ppl=1.53, wps=4100.2, ups=0.98, wpb=4180.6, bsz=199.3, num_updates=2280, lr=1.66531e-05, gnorm=1.36, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3290
2023-05-06 08:11:59 | INFO | train_inner | epoch 013:     43 / 189 loss=2.586, nll_loss=0.625, ppl=1.54, wps=4084.5, ups=0.97, wpb=4190.1, bsz=201.9, num_updates=2300, lr=1.65306e-05, gnorm=1.358, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3311
2023-05-06 08:12:19 | INFO | train_inner | epoch 013:     63 / 189 loss=2.572, nll_loss=0.61, ppl=1.53, wps=4145.3, ups=0.99, wpb=4178.7, bsz=194.4, num_updates=2320, lr=1.64082e-05, gnorm=1.368, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3331
2023-05-06 08:12:39 | INFO | train_inner | epoch 013:     83 / 189 loss=2.575, nll_loss=0.613, ppl=1.53, wps=4092.2, ups=1, wpb=4108, bsz=188, num_updates=2340, lr=1.62857e-05, gnorm=1.384, clip=100, loss_scale=0.25, train_wall=20, gb_free=9.9, wall=3351
2023-05-06 08:12:59 | INFO | train_inner | epoch 013:    103 / 189 loss=2.6, nll_loss=0.641, ppl=1.56, wps=4180.3, ups=1, wpb=4198.3, bsz=192.4, num_updates=2360, lr=1.61633e-05, gnorm=1.501, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3371
2023-05-06 08:13:19 | INFO | train_inner | epoch 013:    123 / 189 loss=2.586, nll_loss=0.626, ppl=1.54, wps=4161.8, ups=0.99, wpb=4182.7, bsz=190.2, num_updates=2380, lr=1.60408e-05, gnorm=1.366, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3391
2023-05-06 08:13:40 | INFO | train_inner | epoch 013:    143 / 189 loss=2.58, nll_loss=0.618, ppl=1.53, wps=3956.4, ups=0.97, wpb=4078.6, bsz=188.8, num_updates=2400, lr=1.59184e-05, gnorm=1.38, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=3412
2023-05-06 08:14:00 | INFO | train_inner | epoch 013:    163 / 189 loss=2.608, nll_loss=0.653, ppl=1.57, wps=4138.5, ups=0.99, wpb=4171.1, bsz=201.6, num_updates=2420, lr=1.57959e-05, gnorm=1.438, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3432
2023-05-06 08:14:21 | INFO | train_inner | epoch 013:    183 / 189 loss=2.585, nll_loss=0.625, ppl=1.54, wps=4051, ups=0.97, wpb=4183.3, bsz=185.5, num_updates=2440, lr=1.56735e-05, gnorm=1.385, clip=100, loss_scale=0.25, train_wall=21, gb_free=10.4, wall=3453
2023-05-06 08:14:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:14:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:14:55 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.292 | nll_loss 1.378 | ppl 2.6 | bleu 48.12 | wps 1272 | wpb 928.3 | bsz 35.1 | num_updates 2446 | best_loss 3.179
2023-05-06 08:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2446 updates
2023-05-06 08:14:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint13.pt
2023-05-06 08:15:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint13.pt
2023-05-06 08:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint13.pt (epoch 13 @ 2446 updates, score 3.292) (writing took 19.844947008416057 seconds)
2023-05-06 08:15:14 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-06 08:15:15 | INFO | train | epoch 013 | loss 2.585 | nll_loss 0.625 | ppl 1.54 | wps 3283 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 2446 | lr 1.56367e-05 | gnorm 1.392 | clip 100 | loss_scale 0.25 | train_wall 191 | gb_free 10.4 | wall 3506
2023-05-06 08:15:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:15:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:15:15 | INFO | fairseq.trainer | begin training epoch 14
2023-05-06 08:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:15:29 | INFO | train_inner | epoch 014:     14 / 189 loss=2.559, nll_loss=0.595, ppl=1.51, wps=1193.9, ups=0.29, wpb=4074.4, bsz=181.2, num_updates=2460, lr=1.5551e-05, gnorm=1.363, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3521
2023-05-06 08:15:49 | INFO | train_inner | epoch 014:     34 / 189 loss=2.547, nll_loss=0.583, ppl=1.5, wps=4165, ups=1, wpb=4184.9, bsz=194.1, num_updates=2480, lr=1.54286e-05, gnorm=1.343, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3541
2023-05-06 08:16:10 | INFO | train_inner | epoch 014:     54 / 189 loss=2.533, nll_loss=0.568, ppl=1.48, wps=4135.6, ups=0.98, wpb=4214.4, bsz=198.9, num_updates=2500, lr=1.53061e-05, gnorm=1.335, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3561
2023-05-06 08:16:30 | INFO | train_inner | epoch 014:     74 / 189 loss=2.588, nll_loss=0.628, ppl=1.55, wps=4120.5, ups=0.99, wpb=4182.9, bsz=211.9, num_updates=2520, lr=1.51837e-05, gnorm=1.49, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3582
2023-05-06 08:16:50 | INFO | train_inner | epoch 014:     94 / 189 loss=2.545, nll_loss=0.58, ppl=1.49, wps=4236.3, ups=1.01, wpb=4214.9, bsz=188.8, num_updates=2540, lr=1.50612e-05, gnorm=1.36, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3602
2023-05-06 08:17:10 | INFO | train_inner | epoch 014:    114 / 189 loss=2.541, nll_loss=0.578, ppl=1.49, wps=4143.9, ups=0.99, wpb=4187.2, bsz=191.2, num_updates=2560, lr=1.49388e-05, gnorm=1.373, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3622
2023-05-06 08:17:30 | INFO | train_inner | epoch 014:    134 / 189 loss=2.545, nll_loss=0.582, ppl=1.5, wps=4161.8, ups=0.99, wpb=4220.9, bsz=188.2, num_updates=2580, lr=1.48163e-05, gnorm=1.303, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3642
2023-05-06 08:17:51 | INFO | train_inner | epoch 014:    154 / 189 loss=2.551, nll_loss=0.587, ppl=1.5, wps=4065.1, ups=0.98, wpb=4146.8, bsz=196.4, num_updates=2600, lr=1.46939e-05, gnorm=1.389, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3662
2023-05-06 08:18:11 | INFO | train_inner | epoch 014:    174 / 189 loss=2.533, nll_loss=0.569, ppl=1.48, wps=4049, ups=1, wpb=4052.6, bsz=179.4, num_updates=2620, lr=1.45714e-05, gnorm=1.409, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3682
2023-05-06 08:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:18:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:18:54 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.32 | nll_loss 1.414 | ppl 2.66 | bleu 47.91 | wps 1243.5 | wpb 928.3 | bsz 35.1 | num_updates 2635 | best_loss 3.179
2023-05-06 08:18:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2635 updates
2023-05-06 08:18:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint14.pt
2023-05-06 08:19:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint14.pt
2023-05-06 08:19:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint14.pt (epoch 14 @ 2635 updates, score 3.32) (writing took 20.113594790920615 seconds)
2023-05-06 08:19:15 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-06 08:19:15 | INFO | train | epoch 014 | loss 2.549 | nll_loss 0.586 | ppl 1.5 | wps 3280.3 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 2635 | lr 1.44796e-05 | gnorm 1.375 | clip 100 | loss_scale 0.25 | train_wall 190 | gb_free 10.4 | wall 3746
2023-05-06 08:19:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:19:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:19:15 | INFO | fairseq.trainer | begin training epoch 15
2023-05-06 08:19:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:19:20 | INFO | train_inner | epoch 015:      5 / 189 loss=2.556, nll_loss=0.594, ppl=1.51, wps=1227.6, ups=0.29, wpb=4242.4, bsz=206.9, num_updates=2640, lr=1.4449e-05, gnorm=1.363, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3752
2023-05-06 08:19:40 | INFO | train_inner | epoch 015:     25 / 189 loss=2.52, nll_loss=0.556, ppl=1.47, wps=4153.9, ups=0.98, wpb=4217.9, bsz=207.2, num_updates=2660, lr=1.43265e-05, gnorm=1.318, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3772
2023-05-06 08:20:00 | INFO | train_inner | epoch 015:     45 / 189 loss=2.51, nll_loss=0.542, ppl=1.46, wps=4093.5, ups=1, wpb=4109.8, bsz=193, num_updates=2680, lr=1.42041e-05, gnorm=1.344, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3792
2023-05-06 08:20:20 | INFO | train_inner | epoch 015:     65 / 189 loss=2.507, nll_loss=0.541, ppl=1.46, wps=4111.2, ups=1, wpb=4109.7, bsz=187.4, num_updates=2700, lr=1.40816e-05, gnorm=1.359, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3812
2023-05-06 08:20:40 | INFO | train_inner | epoch 015:     85 / 189 loss=2.494, nll_loss=0.526, ppl=1.44, wps=4036.5, ups=1, wpb=4026, bsz=180.1, num_updates=2720, lr=1.39592e-05, gnorm=1.306, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3832
2023-05-06 08:21:00 | INFO | train_inner | epoch 015:    105 / 189 loss=2.511, nll_loss=0.545, ppl=1.46, wps=4223.3, ups=1, wpb=4217.1, bsz=183.2, num_updates=2740, lr=1.38367e-05, gnorm=1.298, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3852
2023-05-06 08:21:20 | INFO | train_inner | epoch 015:    125 / 189 loss=2.556, nll_loss=0.595, ppl=1.51, wps=4110, ups=0.98, wpb=4201.1, bsz=210.7, num_updates=2760, lr=1.37143e-05, gnorm=2.761, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3872
2023-05-06 08:21:41 | INFO | train_inner | epoch 015:    145 / 189 loss=2.515, nll_loss=0.549, ppl=1.46, wps=4123.9, ups=1, wpb=4129, bsz=176, num_updates=2780, lr=1.35918e-05, gnorm=1.325, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3892
2023-05-06 08:22:01 | INFO | train_inner | epoch 015:    165 / 189 loss=2.52, nll_loss=0.556, ppl=1.47, wps=4082.3, ups=1, wpb=4101.3, bsz=188.1, num_updates=2800, lr=1.34694e-05, gnorm=1.379, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3912
2023-05-06 08:22:21 | INFO | train_inner | epoch 015:    185 / 189 loss=2.522, nll_loss=0.559, ppl=1.47, wps=4220.9, ups=0.98, wpb=4297.2, bsz=198.6, num_updates=2820, lr=1.33469e-05, gnorm=1.37, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=3933
2023-05-06 08:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:22:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:22:53 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.334 | nll_loss 1.435 | ppl 2.7 | bleu 47.86 | wps 1265.6 | wpb 928.3 | bsz 35.1 | num_updates 2824 | best_loss 3.179
2023-05-06 08:22:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2824 updates
2023-05-06 08:22:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint15.pt
2023-05-06 08:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint15.pt
2023-05-06 08:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint15.pt (epoch 15 @ 2824 updates, score 3.334) (writing took 20.241363847628236 seconds)
2023-05-06 08:23:13 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-06 08:23:13 | INFO | train | epoch 015 | loss 2.519 | nll_loss 0.554 | ppl 1.47 | wps 3297.7 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 2824 | lr 1.33224e-05 | gnorm 1.487 | clip 100 | loss_scale 0.25 | train_wall 189 | gb_free 10.4 | wall 3985
2023-05-06 08:23:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:23:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:23:13 | INFO | fairseq.trainer | begin training epoch 16
2023-05-06 08:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:23:29 | INFO | train_inner | epoch 016:     16 / 189 loss=2.487, nll_loss=0.52, ppl=1.43, wps=1197.3, ups=0.29, wpb=4100.2, bsz=187.5, num_updates=2840, lr=1.32245e-05, gnorm=1.294, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4001
2023-05-06 08:23:50 | INFO | train_inner | epoch 016:     36 / 189 loss=2.513, nll_loss=0.548, ppl=1.46, wps=4120.2, ups=0.98, wpb=4185.6, bsz=212.2, num_updates=2860, lr=1.3102e-05, gnorm=1.446, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4022
2023-05-06 08:24:10 | INFO | train_inner | epoch 016:     56 / 189 loss=2.478, nll_loss=0.509, ppl=1.42, wps=4171.5, ups=1, wpb=4164.2, bsz=187.5, num_updates=2880, lr=1.29796e-05, gnorm=1.282, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4042
2023-05-06 08:24:30 | INFO | train_inner | epoch 016:     76 / 189 loss=2.491, nll_loss=0.524, ppl=1.44, wps=4145.7, ups=0.98, wpb=4220.2, bsz=194.9, num_updates=2900, lr=1.28571e-05, gnorm=1.305, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4062
2023-05-06 08:24:50 | INFO | train_inner | epoch 016:     96 / 189 loss=2.492, nll_loss=0.524, ppl=1.44, wps=4208.6, ups=1, wpb=4227.2, bsz=193.9, num_updates=2920, lr=1.27347e-05, gnorm=1.304, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4082
2023-05-06 08:25:11 | INFO | train_inner | epoch 016:    116 / 189 loss=2.488, nll_loss=0.522, ppl=1.44, wps=4073.8, ups=0.98, wpb=4157.3, bsz=188.8, num_updates=2940, lr=1.26122e-05, gnorm=1.32, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4102
2023-05-06 08:25:31 | INFO | train_inner | epoch 016:    136 / 189 loss=2.484, nll_loss=0.516, ppl=1.43, wps=4003.1, ups=0.98, wpb=4104.8, bsz=196.5, num_updates=2960, lr=1.24898e-05, gnorm=1.304, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4123
2023-05-06 08:25:51 | INFO | train_inner | epoch 016:    156 / 189 loss=2.5, nll_loss=0.534, ppl=1.45, wps=4165.1, ups=0.99, wpb=4211.9, bsz=198.5, num_updates=2980, lr=1.23673e-05, gnorm=1.34, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4143
2023-05-06 08:26:11 | INFO | train_inner | epoch 016:    176 / 189 loss=2.49, nll_loss=0.522, ppl=1.44, wps=4135.4, ups=1, wpb=4146.2, bsz=192.1, num_updates=3000, lr=1.22449e-05, gnorm=1.317, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4163
2023-05-06 08:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:26:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:26:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.352 | nll_loss 1.457 | ppl 2.74 | bleu 47.81 | wps 1246.7 | wpb 928.3 | bsz 35.1 | num_updates 3013 | best_loss 3.179
2023-05-06 08:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3013 updates
2023-05-06 08:26:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint16.pt
2023-05-06 08:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint16.pt
2023-05-06 08:27:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint16.pt (epoch 16 @ 3013 updates, score 3.352) (writing took 31.36646061204374 seconds)
2023-05-06 08:27:24 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-06 08:27:24 | INFO | train | epoch 016 | loss 2.489 | nll_loss 0.522 | ppl 1.44 | wps 3135.5 | ups 0.75 | wpb 4165.6 | bsz 193.2 | num_updates 3013 | lr 1.21653e-05 | gnorm 1.321 | clip 100 | loss_scale 0.25 | train_wall 190 | gb_free 10.4 | wall 4236
2023-05-06 08:27:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:27:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:27:24 | INFO | fairseq.trainer | begin training epoch 17
2023-05-06 08:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:27:31 | INFO | train_inner | epoch 017:      7 / 189 loss=2.465, nll_loss=0.496, ppl=1.41, wps=1024.6, ups=0.25, wpb=4097.8, bsz=180.8, num_updates=3020, lr=1.21224e-05, gnorm=1.276, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4243
2023-05-06 08:27:52 | INFO | train_inner | epoch 017:     27 / 189 loss=2.451, nll_loss=0.481, ppl=1.4, wps=4131.8, ups=0.98, wpb=4200.3, bsz=188, num_updates=3040, lr=1.2e-05, gnorm=1.256, clip=100, loss_scale=0.25, train_wall=20, gb_free=9.4, wall=4264
2023-05-06 08:28:12 | INFO | train_inner | epoch 017:     47 / 189 loss=2.475, nll_loss=0.508, ppl=1.42, wps=3987.3, ups=0.99, wpb=4020.6, bsz=188, num_updates=3060, lr=1.18776e-05, gnorm=1.381, clip=100, loss_scale=0.25, train_wall=20, gb_free=10.4, wall=4284
2023-05-06 08:28:33 | INFO | train_inner | epoch 017:     67 / 189 loss=2.465, nll_loss=0.496, ppl=1.41, wps=4080, ups=0.97, wpb=4208.9, bsz=209, num_updates=3080, lr=1.17551e-05, gnorm=1.277, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=4304
2023-05-06 08:28:53 | INFO | train_inner | epoch 017:     87 / 189 loss=2.476, nll_loss=0.507, ppl=1.42, wps=4128.9, ups=0.98, wpb=4202.9, bsz=196.7, num_updates=3100, lr=1.16327e-05, gnorm=1.316, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4325
2023-05-06 08:29:13 | INFO | train_inner | epoch 017:    107 / 189 loss=2.474, nll_loss=0.508, ppl=1.42, wps=4110.7, ups=0.99, wpb=4168.7, bsz=207.1, num_updates=3120, lr=1.15102e-05, gnorm=1.353, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4345
2023-05-06 08:29:34 | INFO | train_inner | epoch 017:    127 / 189 loss=2.464, nll_loss=0.496, ppl=1.41, wps=4129.3, ups=0.97, wpb=4261.1, bsz=202.4, num_updates=3140, lr=1.13878e-05, gnorm=1.298, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=4366
2023-05-06 08:29:54 | INFO | train_inner | epoch 017:    147 / 189 loss=2.462, nll_loss=0.495, ppl=1.41, wps=4120.5, ups=0.99, wpb=4173.8, bsz=195.4, num_updates=3160, lr=1.12653e-05, gnorm=1.294, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4386
2023-05-06 08:30:14 | INFO | train_inner | epoch 017:    167 / 189 loss=2.453, nll_loss=0.484, ppl=1.4, wps=4032.1, ups=1, wpb=4040.2, bsz=172.7, num_updates=3180, lr=1.11429e-05, gnorm=1.305, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4406
2023-05-06 08:30:35 | INFO | train_inner | epoch 017:    187 / 189 loss=2.478, nll_loss=0.51, ppl=1.42, wps=4196.9, ups=0.98, wpb=4289.4, bsz=192.1, num_updates=3200, lr=1.10204e-05, gnorm=1.298, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4426
2023-05-06 08:30:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:30:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:31:05 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.358 | nll_loss 1.471 | ppl 2.77 | bleu 47.64 | wps 1269.1 | wpb 928.3 | bsz 35.1 | num_updates 3202 | best_loss 3.179
2023-05-06 08:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3202 updates
2023-05-06 08:31:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint17.pt
2023-05-06 08:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint17.pt
2023-05-06 08:31:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint17.pt (epoch 17 @ 3202 updates, score 3.358) (writing took 24.958488730713725 seconds)
2023-05-06 08:31:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-06 08:31:30 | INFO | train | epoch 017 | loss 2.465 | nll_loss 0.497 | ppl 1.41 | wps 3210.7 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 3202 | lr 1.10082e-05 | gnorm 1.306 | clip 100 | loss_scale 0.5 | train_wall 191 | gb_free 10.4 | wall 4481
2023-05-06 08:31:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:31:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:31:30 | INFO | fairseq.trainer | begin training epoch 18
2023-05-06 08:31:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:31:48 | INFO | train_inner | epoch 018:     18 / 189 loss=2.435, nll_loss=0.464, ppl=1.38, wps=1137, ups=0.27, wpb=4168.9, bsz=181.9, num_updates=3220, lr=1.0898e-05, gnorm=1.215, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4500
2023-05-06 08:32:08 | INFO | train_inner | epoch 018:     38 / 189 loss=2.431, nll_loss=0.458, ppl=1.37, wps=4076.7, ups=1, wpb=4066.9, bsz=168.6, num_updates=3240, lr=1.07755e-05, gnorm=1.237, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4520
2023-05-06 08:32:28 | INFO | train_inner | epoch 018:     58 / 189 loss=2.44, nll_loss=0.469, ppl=1.38, wps=4117, ups=0.98, wpb=4213.1, bsz=192.7, num_updates=3260, lr=1.06531e-05, gnorm=1.25, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4540
2023-05-06 08:32:49 | INFO | train_inner | epoch 018:     78 / 189 loss=2.446, nll_loss=0.478, ppl=1.39, wps=4129.7, ups=0.98, wpb=4231.9, bsz=210.6, num_updates=3280, lr=1.05306e-05, gnorm=1.272, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4561
2023-05-06 08:33:09 | INFO | train_inner | epoch 018:     98 / 189 loss=2.442, nll_loss=0.474, ppl=1.39, wps=4168.7, ups=1, wpb=4176.4, bsz=186.3, num_updates=3300, lr=1.04082e-05, gnorm=1.259, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4581
2023-05-06 08:33:29 | INFO | train_inner | epoch 018:    118 / 189 loss=2.464, nll_loss=0.499, ppl=1.41, wps=4030, ups=0.98, wpb=4101.9, bsz=203.7, num_updates=3320, lr=1.02857e-05, gnorm=1.391, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4601
2023-05-06 08:33:50 | INFO | train_inner | epoch 018:    138 / 189 loss=2.438, nll_loss=0.468, ppl=1.38, wps=4027.1, ups=0.98, wpb=4117.4, bsz=181.9, num_updates=3340, lr=1.01633e-05, gnorm=1.282, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4621
2023-05-06 08:34:10 | INFO | train_inner | epoch 018:    158 / 189 loss=2.444, nll_loss=0.475, ppl=1.39, wps=4180.7, ups=0.99, wpb=4219.9, bsz=201.4, num_updates=3360, lr=1.00408e-05, gnorm=1.273, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4642
2023-05-06 08:34:30 | INFO | train_inner | epoch 018:    178 / 189 loss=2.456, nll_loss=0.488, ppl=1.4, wps=4096.2, ups=0.98, wpb=4182.9, bsz=209.1, num_updates=3380, lr=9.91837e-06, gnorm=1.315, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4662
2023-05-06 08:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:34:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:35:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.377 | nll_loss 1.493 | ppl 2.81 | bleu 48.03 | wps 1264.2 | wpb 928.3 | bsz 35.1 | num_updates 3391 | best_loss 3.179
2023-05-06 08:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3391 updates
2023-05-06 08:35:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint18.pt
2023-05-06 08:35:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint18.pt
2023-05-06 08:35:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint18.pt (epoch 18 @ 3391 updates, score 3.377) (writing took 28.599084125831723 seconds)
2023-05-06 08:35:38 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-06 08:35:38 | INFO | train | epoch 018 | loss 2.445 | nll_loss 0.475 | ppl 1.39 | wps 3165.5 | ups 0.76 | wpb 4165.6 | bsz 193.2 | num_updates 3391 | lr 9.85102e-06 | gnorm 1.279 | clip 100 | loss_scale 0.5 | train_wall 190 | gb_free 10.4 | wall 4730
2023-05-06 08:35:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:35:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:35:38 | INFO | fairseq.trainer | begin training epoch 19
2023-05-06 08:35:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:35:48 | INFO | train_inner | epoch 019:      9 / 189 loss=2.443, nll_loss=0.475, ppl=1.39, wps=1080.4, ups=0.26, wpb=4181.5, bsz=199.5, num_updates=3400, lr=9.79592e-06, gnorm=1.291, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4739
2023-05-06 08:36:08 | INFO | train_inner | epoch 019:     29 / 189 loss=2.416, nll_loss=0.444, ppl=1.36, wps=3991, ups=0.98, wpb=4065.6, bsz=189.5, num_updates=3420, lr=9.67347e-06, gnorm=1.236, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4760
2023-05-06 08:36:28 | INFO | train_inner | epoch 019:     49 / 189 loss=2.406, nll_loss=0.433, ppl=1.35, wps=4118.7, ups=1, wpb=4124.7, bsz=177.6, num_updates=3440, lr=9.55102e-06, gnorm=1.206, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4780
2023-05-06 08:36:48 | INFO | train_inner | epoch 019:     69 / 189 loss=2.432, nll_loss=0.462, ppl=1.38, wps=4115.3, ups=0.99, wpb=4164.9, bsz=199, num_updates=3460, lr=9.42857e-06, gnorm=1.264, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4800
2023-05-06 08:37:09 | INFO | train_inner | epoch 019:     89 / 189 loss=2.422, nll_loss=0.453, ppl=1.37, wps=4057.6, ups=0.98, wpb=4123.8, bsz=191.1, num_updates=3480, lr=9.30612e-06, gnorm=1.235, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4820
2023-05-06 08:37:29 | INFO | train_inner | epoch 019:    109 / 189 loss=2.42, nll_loss=0.45, ppl=1.37, wps=4115.1, ups=0.99, wpb=4173.5, bsz=192.9, num_updates=3500, lr=9.18367e-06, gnorm=1.212, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4841
2023-05-06 08:37:49 | INFO | train_inner | epoch 019:    129 / 189 loss=2.452, nll_loss=0.485, ppl=1.4, wps=4180.4, ups=0.98, wpb=4250.4, bsz=207.1, num_updates=3520, lr=9.06122e-06, gnorm=1.381, clip=100, loss_scale=0.5, train_wall=20, gb_free=9.9, wall=4861
2023-05-06 08:38:10 | INFO | train_inner | epoch 019:    149 / 189 loss=2.416, nll_loss=0.445, ppl=1.36, wps=4097, ups=0.98, wpb=4172.6, bsz=199, num_updates=3540, lr=8.93878e-06, gnorm=1.249, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4881
2023-05-06 08:38:30 | INFO | train_inner | epoch 019:    169 / 189 loss=2.431, nll_loss=0.462, ppl=1.38, wps=4141.2, ups=0.99, wpb=4196.2, bsz=187.7, num_updates=3560, lr=8.81633e-06, gnorm=1.274, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4902
2023-05-06 08:38:50 | INFO | train_inner | epoch 019:    189 / 189 loss=2.425, nll_loss=0.454, ppl=1.37, wps=4193.1, ups=1, wpb=4197.2, bsz=188.3, num_updates=3580, lr=8.69388e-06, gnorm=1.251, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=4922
2023-05-06 08:38:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:38:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:39:18 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.389 | nll_loss 1.512 | ppl 2.85 | bleu 47.63 | wps 1283.2 | wpb 928.3 | bsz 35.1 | num_updates 3580 | best_loss 3.179
2023-05-06 08:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3580 updates
2023-05-06 08:39:18 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint19.pt
2023-05-06 08:39:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint19.pt
2023-05-06 08:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint19.pt (epoch 19 @ 3580 updates, score 3.389) (writing took 52.41954234428704 seconds)
2023-05-06 08:40:10 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-06 08:40:10 | INFO | train | epoch 019 | loss 2.425 | nll_loss 0.455 | ppl 1.37 | wps 2892.4 | ups 0.69 | wpb 4165.6 | bsz 193.2 | num_updates 3580 | lr 8.69388e-06 | gnorm 1.257 | clip 100 | loss_scale 0.5 | train_wall 190 | gb_free 10.4 | wall 5002
2023-05-06 08:40:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:40:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:40:11 | INFO | fairseq.trainer | begin training epoch 20
2023-05-06 08:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:40:31 | INFO | train_inner | epoch 020:     20 / 189 loss=2.392, nll_loss=0.42, ppl=1.34, wps=829.3, ups=0.2, wpb=4197.1, bsz=192.5, num_updates=3600, lr=8.57143e-06, gnorm=1.215, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5023
2023-05-06 08:40:51 | INFO | train_inner | epoch 020:     40 / 189 loss=2.413, nll_loss=0.441, ppl=1.36, wps=4087.5, ups=0.99, wpb=4126.2, bsz=198.7, num_updates=3620, lr=8.44898e-06, gnorm=1.261, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5043
2023-05-06 08:41:12 | INFO | train_inner | epoch 020:     60 / 189 loss=2.393, nll_loss=0.421, ppl=1.34, wps=3952.9, ups=0.97, wpb=4057.9, bsz=171.8, num_updates=3640, lr=8.32653e-06, gnorm=1.207, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5064
2023-05-06 08:41:32 | INFO | train_inner | epoch 020:     80 / 189 loss=2.417, nll_loss=0.447, ppl=1.36, wps=4091.1, ups=0.98, wpb=4164.5, bsz=202.7, num_updates=3660, lr=8.20408e-06, gnorm=1.262, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5084
2023-05-06 08:41:52 | INFO | train_inner | epoch 020:    100 / 189 loss=2.397, nll_loss=0.426, ppl=1.34, wps=4102.4, ups=0.99, wpb=4141.8, bsz=182.4, num_updates=3680, lr=8.08163e-06, gnorm=1.231, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5104
2023-05-06 08:42:13 | INFO | train_inner | epoch 020:    120 / 189 loss=2.419, nll_loss=0.449, ppl=1.37, wps=3994.9, ups=0.96, wpb=4160.6, bsz=210.8, num_updates=3700, lr=7.95918e-06, gnorm=1.236, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=5125
2023-05-06 08:42:33 | INFO | train_inner | epoch 020:    140 / 189 loss=2.395, nll_loss=0.424, ppl=1.34, wps=4089, ups=1.01, wpb=4064.6, bsz=174.1, num_updates=3720, lr=7.83673e-06, gnorm=1.205, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5145
2023-05-06 08:42:53 | INFO | train_inner | epoch 020:    160 / 189 loss=2.4, nll_loss=0.43, ppl=1.35, wps=4222.7, ups=1, wpb=4230.1, bsz=189.1, num_updates=3740, lr=7.71429e-06, gnorm=1.196, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5165
2023-05-06 08:43:14 | INFO | train_inner | epoch 020:    180 / 189 loss=2.447, nll_loss=0.48, ppl=1.39, wps=4224.9, ups=0.98, wpb=4304.5, bsz=212.7, num_updates=3760, lr=7.59184e-06, gnorm=1.443, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5185
2023-05-06 08:43:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:43:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:43:51 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.399 | nll_loss 1.524 | ppl 2.88 | bleu 47.61 | wps 1279.3 | wpb 928.3 | bsz 35.1 | num_updates 3769 | best_loss 3.179
2023-05-06 08:43:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3769 updates
2023-05-06 08:43:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint20.pt
2023-05-06 08:44:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint20.pt
2023-05-06 08:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint20.pt (epoch 20 @ 3769 updates, score 3.399) (writing took 46.64815810881555 seconds)
2023-05-06 08:44:37 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-06 08:44:37 | INFO | train | epoch 020 | loss 2.41 | nll_loss 0.439 | ppl 1.36 | wps 2949.3 | ups 0.71 | wpb 4165.6 | bsz 193.2 | num_updates 3769 | lr 7.53673e-06 | gnorm 1.251 | clip 100 | loss_scale 0.5 | train_wall 191 | gb_free 10.4 | wall 5269
2023-05-06 08:44:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:44:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:44:38 | INFO | fairseq.trainer | begin training epoch 21
2023-05-06 08:44:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:44:49 | INFO | train_inner | epoch 021:     11 / 189 loss=2.44, nll_loss=0.474, ppl=1.39, wps=881.6, ups=0.21, wpb=4200.7, bsz=215.4, num_updates=3780, lr=7.46939e-06, gnorm=1.279, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5281
2023-05-06 08:45:09 | INFO | train_inner | epoch 021:     31 / 189 loss=2.39, nll_loss=0.418, ppl=1.34, wps=4148.8, ups=1.01, wpb=4104.3, bsz=184.6, num_updates=3800, lr=7.34694e-06, gnorm=1.198, clip=100, loss_scale=0.5, train_wall=20, gb_free=9.3, wall=5300
2023-05-06 08:45:29 | INFO | train_inner | epoch 021:     51 / 189 loss=2.383, nll_loss=0.41, ppl=1.33, wps=4036.3, ups=0.98, wpb=4123.2, bsz=193, num_updates=3820, lr=7.22449e-06, gnorm=1.19, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5321
2023-05-06 08:45:49 | INFO | train_inner | epoch 021:     71 / 189 loss=2.4, nll_loss=0.431, ppl=1.35, wps=4136.2, ups=0.98, wpb=4213.6, bsz=207, num_updates=3840, lr=7.10204e-06, gnorm=1.197, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5341
2023-05-06 08:46:10 | INFO | train_inner | epoch 021:     91 / 189 loss=2.399, nll_loss=0.426, ppl=1.34, wps=4010.4, ups=0.97, wpb=4135, bsz=189.3, num_updates=3860, lr=6.97959e-06, gnorm=1.202, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=5362
2023-05-06 08:46:30 | INFO | train_inner | epoch 021:    111 / 189 loss=2.401, nll_loss=0.431, ppl=1.35, wps=4172.4, ups=0.99, wpb=4231.3, bsz=201.7, num_updates=3880, lr=6.85714e-06, gnorm=1.187, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5382
2023-05-06 08:46:50 | INFO | train_inner | epoch 021:    131 / 189 loss=2.379, nll_loss=0.407, ppl=1.33, wps=4153.7, ups=1, wpb=4164.6, bsz=176.6, num_updates=3900, lr=6.73469e-06, gnorm=1.203, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5402
2023-05-06 08:47:10 | INFO | train_inner | epoch 021:    151 / 189 loss=2.382, nll_loss=0.411, ppl=1.33, wps=4131.3, ups=0.99, wpb=4156.6, bsz=177.9, num_updates=3920, lr=6.61224e-06, gnorm=1.179, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5422
2023-05-06 08:47:31 | INFO | train_inner | epoch 021:    171 / 189 loss=2.394, nll_loss=0.423, ppl=1.34, wps=4076.4, ups=0.97, wpb=4193.1, bsz=197.9, num_updates=3940, lr=6.4898e-06, gnorm=1.205, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5443
2023-05-06 08:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:47:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:48:17 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.408 | nll_loss 1.539 | ppl 2.91 | bleu 47.63 | wps 1270.8 | wpb 928.3 | bsz 35.1 | num_updates 3958 | best_loss 3.179
2023-05-06 08:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3958 updates
2023-05-06 08:48:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint21.pt
2023-05-06 08:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint21.pt
2023-05-06 08:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint21.pt (epoch 21 @ 3958 updates, score 3.408) (writing took 25.83200871013105 seconds)
2023-05-06 08:48:43 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-05-06 08:48:43 | INFO | train | epoch 021 | loss 2.395 | nll_loss 0.424 | ppl 1.34 | wps 3202 | ups 0.77 | wpb 4165.6 | bsz 193.2 | num_updates 3958 | lr 6.37959e-06 | gnorm 1.202 | clip 100 | loss_scale 0.5 | train_wall 190 | gb_free 10.4 | wall 5515
2023-05-06 08:48:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:48:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:48:43 | INFO | fairseq.trainer | begin training epoch 22
2023-05-06 08:48:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:48:45 | INFO | train_inner | epoch 022:      2 / 189 loss=2.396, nll_loss=0.424, ppl=1.34, wps=1119.4, ups=0.27, wpb=4163.5, bsz=190.8, num_updates=3960, lr=6.36735e-06, gnorm=1.203, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5517
2023-05-06 08:49:05 | INFO | train_inner | epoch 022:     22 / 189 loss=2.374, nll_loss=0.401, ppl=1.32, wps=4105.7, ups=1, wpb=4105.9, bsz=189.5, num_updates=3980, lr=6.2449e-06, gnorm=1.133, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5537
2023-05-06 08:49:26 | INFO | train_inner | epoch 022:     42 / 189 loss=2.389, nll_loss=0.419, ppl=1.34, wps=4069.1, ups=0.97, wpb=4192.4, bsz=212.1, num_updates=4000, lr=6.12245e-06, gnorm=1.199, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5558
2023-05-06 08:49:46 | INFO | train_inner | epoch 022:     62 / 189 loss=2.383, nll_loss=0.41, ppl=1.33, wps=4191.3, ups=0.99, wpb=4216.2, bsz=183.1, num_updates=4020, lr=6e-06, gnorm=1.179, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5578
2023-05-06 08:50:06 | INFO | train_inner | epoch 022:     82 / 189 loss=2.372, nll_loss=0.398, ppl=1.32, wps=4057.2, ups=0.99, wpb=4114.5, bsz=179.7, num_updates=4040, lr=5.87755e-06, gnorm=1.188, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5598
2023-05-06 08:50:27 | INFO | train_inner | epoch 022:    102 / 189 loss=2.38, nll_loss=0.41, ppl=1.33, wps=4078.1, ups=0.99, wpb=4101.8, bsz=190.8, num_updates=4060, lr=5.7551e-06, gnorm=1.163, clip=100, loss_scale=0.5, train_wall=20, gb_free=10.4, wall=5618
2023-05-06 08:50:47 | INFO | train_inner | epoch 022:    122 / 189 loss=2.412, nll_loss=0.446, ppl=1.36, wps=4147.7, ups=0.97, wpb=4294.1, bsz=217.3, num_updates=4080, lr=5.63265e-06, gnorm=1.288, clip=100, loss_scale=0.5, train_wall=21, gb_free=10.4, wall=5639
2023-05-06 08:51:08 | INFO | train_inner | epoch 022:    142 / 189 loss=2.376, nll_loss=0.404, ppl=1.32, wps=4134.5, ups=0.99, wpb=4189.9, bsz=189.1, num_updates=4100, lr=5.5102e-06, gnorm=1.142, clip=100, loss_scale=1, train_wall=20, gb_free=9.3, wall=5659
2023-05-06 08:51:28 | INFO | train_inner | epoch 022:    162 / 189 loss=2.382, nll_loss=0.411, ppl=1.33, wps=4110.7, ups=0.99, wpb=4169.1, bsz=188.4, num_updates=4120, lr=5.38776e-06, gnorm=1.187, clip=100, loss_scale=1, train_wall=20, gb_free=9.4, wall=5680
2023-05-06 08:51:48 | INFO | train_inner | epoch 022:    182 / 189 loss=2.378, nll_loss=0.406, ppl=1.33, wps=4022.1, ups=0.98, wpb=4117.3, bsz=187.6, num_updates=4140, lr=5.26531e-06, gnorm=1.194, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5700
2023-05-06 08:51:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:51:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:52:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.42 | nll_loss 1.55 | ppl 2.93 | bleu 47.83 | wps 1264.2 | wpb 928.3 | bsz 35.1 | num_updates 4147 | best_loss 3.179
2023-05-06 08:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4147 updates
2023-05-06 08:52:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint22.pt
2023-05-06 08:52:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint22.pt
2023-05-06 08:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint22.pt (epoch 22 @ 4147 updates, score 3.42) (writing took 17.30303356051445 seconds)
2023-05-06 08:52:41 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-05-06 08:52:41 | INFO | train | epoch 022 | loss 2.383 | nll_loss 0.412 | ppl 1.33 | wps 3312.2 | ups 0.8 | wpb 4165.6 | bsz 193.2 | num_updates 4147 | lr 5.22245e-06 | gnorm 1.187 | clip 100 | loss_scale 1 | train_wall 191 | gb_free 10.4 | wall 5753
2023-05-06 08:52:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:52:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:52:41 | INFO | fairseq.trainer | begin training epoch 23
2023-05-06 08:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:52:54 | INFO | train_inner | epoch 023:     13 / 189 loss=2.376, nll_loss=0.404, ppl=1.32, wps=1256.2, ups=0.3, wpb=4145.6, bsz=195.8, num_updates=4160, lr=5.14286e-06, gnorm=1.166, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5766
2023-05-06 08:53:14 | INFO | train_inner | epoch 023:     33 / 189 loss=2.372, nll_loss=0.4, ppl=1.32, wps=4112.8, ups=0.99, wpb=4138.4, bsz=194.5, num_updates=4180, lr=5.02041e-06, gnorm=1.144, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5786
2023-05-06 08:53:35 | INFO | train_inner | epoch 023:     53 / 189 loss=2.359, nll_loss=0.386, ppl=1.31, wps=4067.1, ups=0.97, wpb=4196.2, bsz=186.9, num_updates=4200, lr=4.89796e-06, gnorm=1.141, clip=100, loss_scale=1, train_wall=21, gb_free=10.4, wall=5807
2023-05-06 08:53:55 | INFO | train_inner | epoch 023:     73 / 189 loss=2.362, nll_loss=0.391, ppl=1.31, wps=4166.9, ups=1, wpb=4183.7, bsz=183.8, num_updates=4220, lr=4.77551e-06, gnorm=1.121, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5827
2023-05-06 08:54:15 | INFO | train_inner | epoch 023:     93 / 189 loss=2.376, nll_loss=0.404, ppl=1.32, wps=4150.7, ups=0.99, wpb=4175.2, bsz=192.5, num_updates=4240, lr=4.65306e-06, gnorm=1.144, clip=100, loss_scale=1, train_wall=20, gb_free=9.6, wall=5847
2023-05-06 08:54:36 | INFO | train_inner | epoch 023:    113 / 189 loss=2.374, nll_loss=0.402, ppl=1.32, wps=4040, ups=0.98, wpb=4107.2, bsz=191.2, num_updates=4260, lr=4.53061e-06, gnorm=1.163, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5867
2023-05-06 08:54:56 | INFO | train_inner | epoch 023:    133 / 189 loss=2.362, nll_loss=0.39, ppl=1.31, wps=4100, ups=0.98, wpb=4164.2, bsz=187.2, num_updates=4280, lr=4.40816e-06, gnorm=1.136, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5888
2023-05-06 08:55:16 | INFO | train_inner | epoch 023:    153 / 189 loss=2.401, nll_loss=0.434, ppl=1.35, wps=4156.6, ups=0.99, wpb=4184.8, bsz=215.1, num_updates=4300, lr=4.28571e-06, gnorm=1.246, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5908
2023-05-06 08:55:36 | INFO | train_inner | epoch 023:    173 / 189 loss=2.376, nll_loss=0.405, ppl=1.32, wps=4137.5, ups=0.99, wpb=4160.9, bsz=187, num_updates=4320, lr=4.16327e-06, gnorm=1.179, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5928
2023-05-06 08:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:55:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:56:20 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.433 | nll_loss 1.564 | ppl 2.96 | bleu 47.91 | wps 1275 | wpb 928.3 | bsz 35.1 | num_updates 4336 | best_loss 3.179
2023-05-06 08:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4336 updates
2023-05-06 08:56:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint23.pt
2023-05-06 08:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint23.pt
2023-05-06 08:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint23.pt (epoch 23 @ 4336 updates, score 3.433) (writing took 19.567882031202316 seconds)
2023-05-06 08:56:40 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-05-06 08:56:40 | INFO | train | epoch 023 | loss 2.372 | nll_loss 0.4 | ppl 1.32 | wps 3294.6 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 4336 | lr 4.06531e-06 | gnorm 1.157 | clip 100 | loss_scale 1 | train_wall 190 | gb_free 10.4 | wall 5992
2023-05-06 08:56:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 08:56:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 08:56:40 | INFO | fairseq.trainer | begin training epoch 24
2023-05-06 08:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 08:56:44 | INFO | train_inner | epoch 024:      4 / 189 loss=2.361, nll_loss=0.389, ppl=1.31, wps=1224.3, ups=0.29, wpb=4165.2, bsz=200.4, num_updates=4340, lr=4.04082e-06, gnorm=1.134, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=5996
2023-05-06 08:57:05 | INFO | train_inner | epoch 024:     24 / 189 loss=2.347, nll_loss=0.374, ppl=1.3, wps=3989.3, ups=0.99, wpb=4048.9, bsz=172.4, num_updates=4360, lr=3.91837e-06, gnorm=1.136, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6016
2023-05-06 08:57:25 | INFO | train_inner | epoch 024:     44 / 189 loss=2.377, nll_loss=0.406, ppl=1.32, wps=4151.9, ups=0.97, wpb=4274.5, bsz=220, num_updates=4380, lr=3.79592e-06, gnorm=1.151, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6037
2023-05-06 08:57:45 | INFO | train_inner | epoch 024:     64 / 189 loss=2.364, nll_loss=0.391, ppl=1.31, wps=4195.9, ups=0.99, wpb=4224, bsz=199.1, num_updates=4400, lr=3.67347e-06, gnorm=1.137, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6057
2023-05-06 08:58:06 | INFO | train_inner | epoch 024:     84 / 189 loss=2.361, nll_loss=0.39, ppl=1.31, wps=4112.8, ups=0.98, wpb=4189.8, bsz=187.3, num_updates=4420, lr=3.55102e-06, gnorm=1.145, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6077
2023-05-06 08:58:26 | INFO | train_inner | epoch 024:    104 / 189 loss=2.354, nll_loss=0.382, ppl=1.3, wps=4015.5, ups=0.99, wpb=4050.7, bsz=185, num_updates=4440, lr=3.42857e-06, gnorm=1.166, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6098
2023-05-06 08:58:46 | INFO | train_inner | epoch 024:    124 / 189 loss=2.352, nll_loss=0.379, ppl=1.3, wps=4070.7, ups=1.01, wpb=4031.5, bsz=176.4, num_updates=4460, lr=3.30612e-06, gnorm=1.147, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6117
2023-05-06 08:59:06 | INFO | train_inner | epoch 024:    144 / 189 loss=2.356, nll_loss=0.385, ppl=1.31, wps=4014.9, ups=0.99, wpb=4037.2, bsz=176.1, num_updates=4480, lr=3.18367e-06, gnorm=1.145, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6138
2023-05-06 08:59:26 | INFO | train_inner | epoch 024:    164 / 189 loss=2.389, nll_loss=0.42, ppl=1.34, wps=4221.2, ups=0.99, wpb=4266.4, bsz=201.8, num_updates=4500, lr=3.06122e-06, gnorm=1.242, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6158
2023-05-06 08:59:47 | INFO | train_inner | epoch 024:    184 / 189 loss=2.366, nll_loss=0.394, ppl=1.31, wps=4204.2, ups=0.97, wpb=4355.4, bsz=218.6, num_updates=4520, lr=2.93878e-06, gnorm=1.144, clip=100, loss_scale=1, train_wall=21, gb_free=10.4, wall=6178
2023-05-06 08:59:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 08:59:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:00:20 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.438 | nll_loss 1.575 | ppl 2.98 | bleu 47.87 | wps 1239 | wpb 928.3 | bsz 35.1 | num_updates 4525 | best_loss 3.179
2023-05-06 09:00:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4525 updates
2023-05-06 09:00:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint24.pt
2023-05-06 09:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint24.pt
2023-05-06 09:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint24.pt (epoch 24 @ 4525 updates, score 3.438) (writing took 19.7084541041404 seconds)
2023-05-06 09:00:40 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-05-06 09:00:40 | INFO | train | epoch 024 | loss 2.363 | nll_loss 0.392 | ppl 1.31 | wps 3279.5 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 4525 | lr 2.90816e-06 | gnorm 1.155 | clip 100 | loss_scale 1 | train_wall 190 | gb_free 10.4 | wall 6232
2023-05-06 09:00:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:00:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:00:40 | INFO | fairseq.trainer | begin training epoch 25
2023-05-06 09:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:00:56 | INFO | train_inner | epoch 025:     15 / 189 loss=2.388, nll_loss=0.42, ppl=1.34, wps=1228.1, ups=0.29, wpb=4247.1, bsz=206.2, num_updates=4540, lr=2.81633e-06, gnorm=1.222, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6248
2023-05-06 09:01:16 | INFO | train_inner | epoch 025:     35 / 189 loss=2.348, nll_loss=0.375, ppl=1.3, wps=4112.2, ups=0.99, wpb=4144.4, bsz=199.4, num_updates=4560, lr=2.69388e-06, gnorm=1.142, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6268
2023-05-06 09:01:36 | INFO | train_inner | epoch 025:     55 / 189 loss=2.347, nll_loss=0.375, ppl=1.3, wps=4025.7, ups=0.99, wpb=4080, bsz=185.7, num_updates=4580, lr=2.57143e-06, gnorm=1.14, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6288
2023-05-06 09:01:57 | INFO | train_inner | epoch 025:     75 / 189 loss=2.36, nll_loss=0.389, ppl=1.31, wps=4093.9, ups=0.98, wpb=4164.5, bsz=191.4, num_updates=4600, lr=2.44898e-06, gnorm=1.173, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6308
2023-05-06 09:02:17 | INFO | train_inner | epoch 025:     95 / 189 loss=2.355, nll_loss=0.383, ppl=1.3, wps=4019.9, ups=0.98, wpb=4081.2, bsz=191.4, num_updates=4620, lr=2.32653e-06, gnorm=1.149, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6329
2023-05-06 09:02:37 | INFO | train_inner | epoch 025:    115 / 189 loss=2.346, nll_loss=0.374, ppl=1.3, wps=4143.6, ups=1, wpb=4159.8, bsz=180.5, num_updates=4640, lr=2.20408e-06, gnorm=1.093, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6349
2023-05-06 09:02:57 | INFO | train_inner | epoch 025:    135 / 189 loss=2.346, nll_loss=0.373, ppl=1.29, wps=4094.8, ups=0.99, wpb=4126.9, bsz=174.9, num_updates=4660, lr=2.08163e-06, gnorm=1.109, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6369
2023-05-06 09:03:18 | INFO | train_inner | epoch 025:    155 / 189 loss=2.353, nll_loss=0.38, ppl=1.3, wps=4185.5, ups=0.98, wpb=4285.4, bsz=203.2, num_updates=4680, lr=1.95918e-06, gnorm=1.113, clip=100, loss_scale=1, train_wall=20, gb_free=10.2, wall=6389
2023-05-06 09:03:38 | INFO | train_inner | epoch 025:    175 / 189 loss=2.368, nll_loss=0.396, ppl=1.32, wps=4164.3, ups=0.98, wpb=4236, bsz=206.7, num_updates=4700, lr=1.83673e-06, gnorm=1.143, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6410
2023-05-06 09:03:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:03:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:04:21 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.444 | nll_loss 1.58 | ppl 2.99 | bleu 47.78 | wps 1261.2 | wpb 928.3 | bsz 35.1 | num_updates 4714 | best_loss 3.179
2023-05-06 09:04:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4714 updates
2023-05-06 09:04:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint25.pt
2023-05-06 09:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint25.pt
2023-05-06 09:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint25.pt (epoch 25 @ 4714 updates, score 3.444) (writing took 19.666774043813348 seconds)
2023-05-06 09:04:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-05-06 09:04:40 | INFO | train | epoch 025 | loss 2.357 | nll_loss 0.386 | ppl 1.31 | wps 3274.3 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 4714 | lr 1.75102e-06 | gnorm 1.143 | clip 100 | loss_scale 1 | train_wall 191 | gb_free 10.4 | wall 6472
2023-05-06 09:04:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:04:41 | INFO | fairseq.trainer | begin training epoch 26
2023-05-06 09:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:04:47 | INFO | train_inner | epoch 026:      6 / 189 loss=2.363, nll_loss=0.392, ppl=1.31, wps=1216.6, ups=0.29, wpb=4184.8, bsz=200, num_updates=4720, lr=1.71429e-06, gnorm=1.124, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6479
2023-05-06 09:05:07 | INFO | train_inner | epoch 026:     26 / 189 loss=2.355, nll_loss=0.384, ppl=1.3, wps=4070.7, ups=0.98, wpb=4160.1, bsz=191.8, num_updates=4740, lr=1.59184e-06, gnorm=1.104, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6499
2023-05-06 09:05:27 | INFO | train_inner | epoch 026:     46 / 189 loss=2.338, nll_loss=0.365, ppl=1.29, wps=4082.5, ups=0.99, wpb=4117.2, bsz=194.1, num_updates=4760, lr=1.46939e-06, gnorm=1.084, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6519
2023-05-06 09:05:48 | INFO | train_inner | epoch 026:     66 / 189 loss=2.355, nll_loss=0.383, ppl=1.3, wps=4096.1, ups=0.98, wpb=4180.6, bsz=200.2, num_updates=4780, lr=1.34694e-06, gnorm=1.119, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6540
2023-05-06 09:06:08 | INFO | train_inner | epoch 026:     86 / 189 loss=2.387, nll_loss=0.419, ppl=1.34, wps=4144.9, ups=0.99, wpb=4204.8, bsz=213.2, num_updates=4800, lr=1.22449e-06, gnorm=1.476, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6560
2023-05-06 09:06:28 | INFO | train_inner | epoch 026:    106 / 189 loss=2.356, nll_loss=0.385, ppl=1.31, wps=4097, ups=0.98, wpb=4171.9, bsz=195.8, num_updates=4820, lr=1.10204e-06, gnorm=1.13, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6580
2023-05-06 09:06:49 | INFO | train_inner | epoch 026:    126 / 189 loss=2.344, nll_loss=0.371, ppl=1.29, wps=4135.8, ups=0.97, wpb=4244.3, bsz=191.1, num_updates=4840, lr=9.79592e-07, gnorm=1.079, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6601
2023-05-06 09:07:09 | INFO | train_inner | epoch 026:    146 / 189 loss=2.343, nll_loss=0.369, ppl=1.29, wps=4165.4, ups=1, wpb=4161.1, bsz=184.1, num_updates=4860, lr=8.57143e-07, gnorm=1.107, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6621
2023-05-06 09:07:29 | INFO | train_inner | epoch 026:    166 / 189 loss=2.34, nll_loss=0.367, ppl=1.29, wps=3992.7, ups=0.99, wpb=4044.2, bsz=176.5, num_updates=4880, lr=7.34694e-07, gnorm=1.115, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6641
2023-05-06 09:07:50 | INFO | train_inner | epoch 026:    186 / 189 loss=2.355, nll_loss=0.382, ppl=1.3, wps=4087.2, ups=0.97, wpb=4225.8, bsz=197.2, num_updates=4900, lr=6.12245e-07, gnorm=1.103, clip=100, loss_scale=1, train_wall=21, gb_free=10.4, wall=6662
2023-05-06 09:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:07:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:08:21 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.443 | nll_loss 1.581 | ppl 2.99 | bleu 47.89 | wps 1251.9 | wpb 928.3 | bsz 35.1 | num_updates 4903 | best_loss 3.179
2023-05-06 09:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4903 updates
2023-05-06 09:08:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint26.pt
2023-05-06 09:08:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint26.pt
2023-05-06 09:08:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint26.pt (epoch 26 @ 4903 updates, score 3.443) (writing took 20.68934972025454 seconds)
2023-05-06 09:08:42 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-05-06 09:08:42 | INFO | train | epoch 026 | loss 2.352 | nll_loss 0.38 | ppl 1.3 | wps 3261.1 | ups 0.78 | wpb 4165.6 | bsz 193.2 | num_updates 4903 | lr 5.93878e-07 | gnorm 1.141 | clip 100 | loss_scale 1 | train_wall 191 | gb_free 10.4 | wall 6714
2023-05-06 09:08:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:08:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:08:42 | INFO | fairseq.trainer | begin training epoch 27
2023-05-06 09:08:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:08:59 | INFO | train_inner | epoch 027:     17 / 189 loss=2.341, nll_loss=0.367, ppl=1.29, wps=1218.7, ups=0.29, wpb=4229.1, bsz=181, num_updates=4920, lr=4.89796e-07, gnorm=1.075, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6731
2023-05-06 09:09:20 | INFO | train_inner | epoch 027:     37 / 189 loss=2.359, nll_loss=0.388, ppl=1.31, wps=4079.2, ups=0.98, wpb=4183.2, bsz=211.3, num_updates=4940, lr=3.67347e-07, gnorm=1.118, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6752
2023-05-06 09:09:40 | INFO | train_inner | epoch 027:     57 / 189 loss=2.342, nll_loss=0.37, ppl=1.29, wps=4078.1, ups=1, wpb=4058.8, bsz=174.6, num_updates=4960, lr=2.44898e-07, gnorm=1.083, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6771
2023-05-06 09:10:00 | INFO | train_inner | epoch 027:     77 / 189 loss=2.335, nll_loss=0.361, ppl=1.28, wps=4035.2, ups=0.99, wpb=4074.9, bsz=175.7, num_updates=4980, lr=1.22449e-07, gnorm=1.065, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6792
2023-05-06 09:10:20 | INFO | train_inner | epoch 027:     97 / 189 loss=2.364, nll_loss=0.394, ppl=1.31, wps=4197.1, ups=0.97, wpb=4312.2, bsz=239.5, num_updates=5000, lr=0, gnorm=1.135, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6812
2023-05-06 09:10:41 | INFO | train_inner | epoch 027:    117 / 189 loss=2.355, nll_loss=0.383, ppl=1.3, wps=4177.5, ups=0.99, wpb=4238.2, bsz=191.7, num_updates=5020, lr=0, gnorm=1.088, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6833
2023-05-06 09:11:01 | INFO | train_inner | epoch 027:    137 / 189 loss=2.341, nll_loss=0.369, ppl=1.29, wps=4128.6, ups=0.99, wpb=4151.2, bsz=186.5, num_updates=5040, lr=0, gnorm=1.067, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6853
2023-05-06 09:11:21 | INFO | train_inner | epoch 027:    157 / 189 loss=2.339, nll_loss=0.366, ppl=1.29, wps=4111.7, ups=1, wpb=4129.5, bsz=184.7, num_updates=5060, lr=0, gnorm=1.062, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6873
2023-05-06 09:11:41 | INFO | train_inner | epoch 027:    177 / 189 loss=2.369, nll_loss=0.401, ppl=1.32, wps=4078.5, ups=1, wpb=4088.9, bsz=193.4, num_updates=5080, lr=0, gnorm=1.148, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6893
2023-05-06 09:11:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:11:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:12:21 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.445 | nll_loss 1.583 | ppl 3 | bleu 47.75 | wps 1269.8 | wpb 928.3 | bsz 35.1 | num_updates 5092 | best_loss 3.179
2023-05-06 09:12:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5092 updates
2023-05-06 09:12:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint27.pt
2023-05-06 09:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint27.pt
2023-05-06 09:12:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint27.pt (epoch 27 @ 5092 updates, score 3.445) (writing took 19.52036938816309 seconds)
2023-05-06 09:12:41 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-05-06 09:12:41 | INFO | train | epoch 027 | loss 2.349 | nll_loss 0.377 | ppl 1.3 | wps 3299.3 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 5092 | lr 0 | gnorm 1.093 | clip 100 | loss_scale 1 | train_wall 190 | gb_free 10.4 | wall 6952
2023-05-06 09:12:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:12:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:12:41 | INFO | fairseq.trainer | begin training epoch 28
2023-05-06 09:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:12:49 | INFO | train_inner | epoch 028:      8 / 189 loss=2.341, nll_loss=0.368, ppl=1.29, wps=1254.8, ups=0.29, wpb=4276.7, bsz=186.9, num_updates=5100, lr=0, gnorm=1.065, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6961
2023-05-06 09:13:10 | INFO | train_inner | epoch 028:     28 / 189 loss=2.359, nll_loss=0.387, ppl=1.31, wps=4009.7, ups=0.98, wpb=4102.3, bsz=203.2, num_updates=5120, lr=0, gnorm=1.11, clip=100, loss_scale=1, train_wall=20, gb_free=10.4, wall=6981
2023-05-06 09:13:30 | INFO | train_inner | epoch 028:     48 / 189 loss=2.343, nll_loss=0.37, ppl=1.29, wps=4109.7, ups=0.98, wpb=4188.9, bsz=198.2, num_updates=5140, lr=0, gnorm=1.089, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7002
2023-05-06 09:13:50 | INFO | train_inner | epoch 028:     68 / 189 loss=2.348, nll_loss=0.376, ppl=1.3, wps=4163.4, ups=0.99, wpb=4198.6, bsz=190, num_updates=5160, lr=0, gnorm=1.098, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7022
2023-05-06 09:14:11 | INFO | train_inner | epoch 028:     88 / 189 loss=2.37, nll_loss=0.402, ppl=1.32, wps=4131.2, ups=0.97, wpb=4243.6, bsz=214.9, num_updates=5180, lr=0, gnorm=1.13, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7043
2023-05-06 09:14:31 | INFO | train_inner | epoch 028:    108 / 189 loss=2.335, nll_loss=0.362, ppl=1.29, wps=4099.7, ups=1, wpb=4109.9, bsz=175.2, num_updates=5200, lr=0, gnorm=1.081, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7063
2023-05-06 09:14:51 | INFO | train_inner | epoch 028:    128 / 189 loss=2.344, nll_loss=0.371, ppl=1.29, wps=4121.1, ups=0.98, wpb=4219.2, bsz=199.4, num_updates=5220, lr=0, gnorm=1.117, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7083
2023-05-06 09:15:11 | INFO | train_inner | epoch 028:    148 / 189 loss=2.339, nll_loss=0.366, ppl=1.29, wps=4059, ups=1.01, wpb=4030.8, bsz=179.5, num_updates=5240, lr=0, gnorm=1.123, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7103
2023-05-06 09:15:31 | INFO | train_inner | epoch 028:    168 / 189 loss=2.338, nll_loss=0.364, ppl=1.29, wps=4074.1, ups=0.99, wpb=4128.9, bsz=178.7, num_updates=5260, lr=0, gnorm=1.041, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7123
2023-05-06 09:15:52 | INFO | train_inner | epoch 028:    188 / 189 loss=2.358, nll_loss=0.387, ppl=1.31, wps=4114.7, ups=0.99, wpb=4146.2, bsz=202.6, num_updates=5280, lr=0, gnorm=1.107, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7143
2023-05-06 09:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:15:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:16:21 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.445 | nll_loss 1.583 | ppl 3 | bleu 47.75 | wps 1269 | wpb 928.3 | bsz 35.1 | num_updates 5281 | best_loss 3.179
2023-05-06 09:16:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5281 updates
2023-05-06 09:16:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint28.pt
2023-05-06 09:16:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint28.pt
2023-05-06 09:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint28.pt (epoch 28 @ 5281 updates, score 3.445) (writing took 20.204440912231803 seconds)
2023-05-06 09:16:41 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-05-06 09:16:42 | INFO | train | epoch 028 | loss 2.348 | nll_loss 0.376 | ppl 1.3 | wps 3264.9 | ups 0.78 | wpb 4165.6 | bsz 193.2 | num_updates 5281 | lr 0 | gnorm 1.097 | clip 100 | loss_scale 2 | train_wall 191 | gb_free 10.4 | wall 7193
2023-05-06 09:16:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:16:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:16:42 | INFO | fairseq.trainer | begin training epoch 29
2023-05-06 09:16:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:17:01 | INFO | train_inner | epoch 029:     19 / 189 loss=2.336, nll_loss=0.364, ppl=1.29, wps=1170.9, ups=0.29, wpb=4069.4, bsz=174.8, num_updates=5300, lr=0, gnorm=1.063, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7213
2023-05-06 09:17:21 | INFO | train_inner | epoch 029:     39 / 189 loss=2.335, nll_loss=0.363, ppl=1.29, wps=4161.1, ups=1, wpb=4157.6, bsz=174.5, num_updates=5320, lr=0, gnorm=1.063, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7233
2023-05-06 09:17:41 | INFO | train_inner | epoch 029:     59 / 189 loss=2.35, nll_loss=0.378, ppl=1.3, wps=4148.9, ups=0.99, wpb=4193.7, bsz=190.6, num_updates=5340, lr=0, gnorm=1.102, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7253
2023-05-06 09:18:02 | INFO | train_inner | epoch 029:     79 / 189 loss=2.344, nll_loss=0.371, ppl=1.29, wps=4117.3, ups=0.98, wpb=4193.8, bsz=187.6, num_updates=5360, lr=0, gnorm=1.072, clip=100, loss_scale=2, train_wall=20, gb_free=9.9, wall=7273
2023-05-06 09:18:22 | INFO | train_inner | epoch 029:     99 / 189 loss=2.343, nll_loss=0.371, ppl=1.29, wps=4111.4, ups=0.99, wpb=4165.9, bsz=200.7, num_updates=5380, lr=0, gnorm=1.078, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7294
2023-05-06 09:18:42 | INFO | train_inner | epoch 029:    119 / 189 loss=2.379, nll_loss=0.411, ppl=1.33, wps=4244.1, ups=1, wpb=4264.1, bsz=219.3, num_updates=5400, lr=0, gnorm=1.173, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7314
2023-05-06 09:19:02 | INFO | train_inner | epoch 029:    139 / 189 loss=2.332, nll_loss=0.359, ppl=1.28, wps=4049, ups=0.99, wpb=4084.8, bsz=172.4, num_updates=5420, lr=0, gnorm=1.062, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7334
2023-05-06 09:19:23 | INFO | train_inner | epoch 029:    159 / 189 loss=2.353, nll_loss=0.38, ppl=1.3, wps=4123.2, ups=0.98, wpb=4223.8, bsz=204.1, num_updates=5440, lr=0, gnorm=1.101, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7354
2023-05-06 09:19:43 | INFO | train_inner | epoch 029:    179 / 189 loss=2.363, nll_loss=0.39, ppl=1.31, wps=4014.2, ups=0.97, wpb=4117.6, bsz=207, num_updates=5460, lr=0, gnorm=1.153, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7375
2023-05-06 09:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:19:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:20:21 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.445 | nll_loss 1.583 | ppl 3 | bleu 47.75 | wps 1267.2 | wpb 928.3 | bsz 35.1 | num_updates 5470 | best_loss 3.179
2023-05-06 09:20:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5470 updates
2023-05-06 09:20:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint29.pt
2023-05-06 09:20:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint29.pt
2023-05-06 09:20:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint29.pt (epoch 29 @ 5470 updates, score 3.445) (writing took 19.607454707846045 seconds)
2023-05-06 09:20:41 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-05-06 09:20:41 | INFO | train | epoch 029 | loss 2.349 | nll_loss 0.377 | ppl 1.3 | wps 3289.4 | ups 0.79 | wpb 4165.6 | bsz 193.2 | num_updates 5470 | lr 0 | gnorm 1.098 | clip 100 | loss_scale 2 | train_wall 190 | gb_free 10.4 | wall 7433
2023-05-06 09:20:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:20:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:20:41 | INFO | fairseq.trainer | begin training epoch 30
2023-05-06 09:20:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:20:51 | INFO | train_inner | epoch 030:     10 / 189 loss=2.348, nll_loss=0.375, ppl=1.3, wps=1236.5, ups=0.29, wpb=4221.2, bsz=198.4, num_updates=5480, lr=0, gnorm=1.094, clip=100, loss_scale=2, train_wall=20, gb_free=9.6, wall=7443
2023-05-06 09:21:12 | INFO | train_inner | epoch 030:     30 / 189 loss=2.341, nll_loss=0.368, ppl=1.29, wps=4042.8, ups=0.99, wpb=4084.2, bsz=182, num_updates=5500, lr=0, gnorm=1.108, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7463
2023-05-06 09:21:32 | INFO | train_inner | epoch 030:     50 / 189 loss=2.354, nll_loss=0.382, ppl=1.3, wps=4071.8, ups=0.98, wpb=4169.2, bsz=201, num_updates=5520, lr=0, gnorm=1.124, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7484
2023-05-06 09:21:53 | INFO | train_inner | epoch 030:     70 / 189 loss=2.347, nll_loss=0.374, ppl=1.3, wps=4102.3, ups=0.98, wpb=4201.2, bsz=192.7, num_updates=5540, lr=0, gnorm=1.083, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7504
2023-05-06 09:22:13 | INFO | train_inner | epoch 030:     90 / 189 loss=2.356, nll_loss=0.383, ppl=1.3, wps=4151.6, ups=0.99, wpb=4199.4, bsz=196, num_updates=5560, lr=0, gnorm=1.105, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7525
2023-05-06 09:22:33 | INFO | train_inner | epoch 030:    110 / 189 loss=2.339, nll_loss=0.366, ppl=1.29, wps=4123.3, ups=0.98, wpb=4210.3, bsz=192.9, num_updates=5580, lr=0, gnorm=1.08, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7545
2023-05-06 09:22:53 | INFO | train_inner | epoch 030:    130 / 189 loss=2.338, nll_loss=0.364, ppl=1.29, wps=4103.9, ups=0.99, wpb=4135.9, bsz=180.5, num_updates=5600, lr=0, gnorm=1.071, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7565
2023-05-06 09:23:14 | INFO | train_inner | epoch 030:    150 / 189 loss=2.349, nll_loss=0.377, ppl=1.3, wps=4079.4, ups=0.98, wpb=4181.9, bsz=201, num_updates=5620, lr=0, gnorm=1.094, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7586
2023-05-06 09:23:34 | INFO | train_inner | epoch 030:    170 / 189 loss=2.382, nll_loss=0.413, ppl=1.33, wps=4127.7, ups=0.98, wpb=4232.6, bsz=214.2, num_updates=5640, lr=0, gnorm=1.187, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7606
2023-05-06 09:23:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-06 09:23:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:24:22 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.445 | nll_loss 1.583 | ppl 3 | bleu 47.75 | wps 1260.6 | wpb 928.3 | bsz 35.1 | num_updates 5659 | best_loss 3.179
2023-05-06 09:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5659 updates
2023-05-06 09:24:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint30.pt
2023-05-06 09:24:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/bartLarge+s2s1/checkpoint30.pt
2023-05-06 09:24:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/bartLarge+s2s1/checkpoint30.pt (epoch 30 @ 5659 updates, score 3.445) (writing took 20.00178498029709 seconds)
2023-05-06 09:24:44 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-05-06 09:24:44 | INFO | train | epoch 030 | loss 2.349 | nll_loss 0.377 | ppl 1.3 | wps 3235.2 | ups 0.78 | wpb 4165.6 | bsz 193.2 | num_updates 5659 | lr 0 | gnorm 1.102 | clip 100 | loss_scale 2 | train_wall 191 | gb_free 10.4 | wall 7676
2023-05-06 09:24:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-06 09:24:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 189
2023-05-06 09:24:45 | INFO | fairseq.trainer | begin training epoch 31
2023-05-06 09:24:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-06 09:24:47 | INFO | train_inner | epoch 031:      1 / 189 loss=2.337, nll_loss=0.364, ppl=1.29, wps=1126.8, ups=0.28, wpb=4087.1, bsz=181, num_updates=5660, lr=0, gnorm=1.075, clip=100, loss_scale=2, train_wall=21, gb_free=10.4, wall=7679
slurmstepd: error: *** JOB 116010 ON 100server CANCELLED AT 2023-05-06T09:24:59 ***
2023-05-06 09:25:08 | INFO | train_inner | epoch 031:     21 / 189 loss=2.343, nll_loss=0.369, ppl=1.29, wps=4115.2, ups=0.99, wpb=4168.2, bsz=182.9, num_updates=5680, lr=0, gnorm=1.078, clip=100, loss_scale=2, train_wall=20, gb_free=10.4, wall=7700
