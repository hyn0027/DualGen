2023-05-20 05:32:10 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-05-20 05:32:13 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:33056
2023-05-20 05:32:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 0
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 1
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 5
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 2
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 4
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 6
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 7
2023-05-20 05:32:14 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-20 05:32:14 | INFO | fairseq.distributed.utils | initialized host 99server as rank 3
2023-05-20 05:32:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:33056', 'distributed_port': 33056, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [4e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'training/stage2', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=2, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[4e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', save_dir='training/stage2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='5000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=200, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=2, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[4e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', save_dir='training/stage2', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='5000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=200, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [4e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 200, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 5000.0, 'lr': [4e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-20 05:32:20 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-05-20 05:32:33 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (1): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (2): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (3): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (4): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (5): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (6): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (7): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (8): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (9): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (10): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (11): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
    )
    (graph_embeddings): Linear(in_features=1024, out_features=64, bias=True)
    (graph_embeddings_inverse): Linear(in_features=1024, out_features=64, bias=True)
    (gamma_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-05-20 05:32:33 | INFO | fairseq_cli.train | task: GraphToSeq
2023-05-20 05:32:33 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-05-20 05:32:33 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-20 05:32:33 | INFO | fairseq_cli.train | num. shared model params: 559,173,888 (num. trained: 152,883,456)
2023-05-20 05:32:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-05-20 05:32:33 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-05-20 05:32:33 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-05-20 05:32:53 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-05-20 05:32:56 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-05-20 05:32:56 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-05-20 05:32:56 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-20 05:32:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 05:32:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-20 05:32:57 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-05-20 05:32:57 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-05-20 05:32:57 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt
2023-05-20 05:33:35 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt (epoch 141 @ 0 updates)
2023-05-20 05:33:35 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-05-20 05:33:35 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-05-20 05:33:35 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 05:33:35 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 05:33:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 05:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:33:39 | INFO | fairseq.trainer | begin training epoch 1
2023-05-20 05:33:39 | INFO | fairseq_cli.train | Start iterating over samples
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-05-20 05:33:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-05-20 05:34:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-05-20 05:34:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-05-20 05:34:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-05-20 05:34:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-05-20 05:35:48 | INFO | train_inner | epoch 001:     25 / 65 loss=6.677, nll_loss=4.937, ppl=30.64, wps=3116.9, ups=0.26, wpb=12134.4, bsz=549.9, num_updates=20, lr=4e-06, gnorm=10.325, clip=100, loss_scale=4, train_wall=125, gb_free=17.8, wall=171
2023-05-20 05:36:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-20 05:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 05:37:13 | INFO | train_inner | epoch 001:     47 / 65 loss=6.703, nll_loss=4.971, ppl=31.37, wps=2845.4, ups=0.23, wpb=12177.9, bsz=581, num_updates=40, lr=8e-06, gnorm=11.925, clip=100, loss_scale=1, train_wall=85, gb_free=18.1, wall=256
2023-05-20 05:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:38:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:39:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.959 | nll_loss 4.064 | ppl 16.73 | bleu 5.58 | wps 815.4 | wpb 2785 | bsz 105.2 | num_updates 58
2023-05-20 05:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 58 updates
2023-05-20 05:39:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint1.pt
2023-05-20 05:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint1.pt
2023-05-20 05:39:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint1.pt (epoch 1 @ 58 updates, score 5.959) (writing took 15.370329402387142 seconds)
2023-05-20 05:39:19 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-20 05:39:19 | INFO | train | epoch 001 | loss 6.652 | nll_loss 4.916 | ppl 30.2 | wps 2401 | ups 0.2 | wpb 12029.1 | bsz 561.7 | num_updates 58 | lr 1.16e-05 | gnorm 11.217 | clip 100 | loss_scale 1 | train_wall 278 | gb_free 18.5 | wall 382
2023-05-20 05:39:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:39:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:39:19 | INFO | fairseq.trainer | begin training epoch 2
2023-05-20 05:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:39:26 | INFO | train_inner | epoch 002:      2 / 65 loss=6.567, nll_loss=4.831, ppl=28.45, wps=1788, ups=0.15, wpb=11869, bsz=565, num_updates=60, lr=1.2e-05, gnorm=11.099, clip=100, loss_scale=1, train_wall=74, gb_free=18.4, wall=389
2023-05-20 05:40:46 | INFO | train_inner | epoch 002:     22 / 65 loss=6.493, nll_loss=4.748, ppl=26.87, wps=3061.2, ups=0.25, wpb=12273.4, bsz=568.2, num_updates=80, lr=1.6e-05, gnorm=10.39, clip=100, loss_scale=1, train_wall=80, gb_free=18.5, wall=469
2023-05-20 05:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 05:42:07 | INFO | train_inner | epoch 002:     43 / 65 loss=6.445, nll_loss=4.706, ppl=26.09, wps=2957.2, ups=0.25, wpb=12024.9, bsz=551.1, num_updates=100, lr=2e-05, gnorm=9.001, clip=100, loss_scale=0.5, train_wall=81, gb_free=18.4, wall=551
2023-05-20 05:42:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 05:43:24 | INFO | train_inner | epoch 002:     64 / 65 loss=6.359, nll_loss=4.609, ppl=24.41, wps=3223.1, ups=0.26, wpb=12269.8, bsz=576.3, num_updates=120, lr=2.4e-05, gnorm=7.095, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.5, wall=627
2023-05-20 05:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:43:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:44:08 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.693 | nll_loss 3.795 | ppl 13.88 | bleu 6.67 | wps 854.8 | wpb 2785 | bsz 105.2 | num_updates 121 | best_loss 5.693
2023-05-20 05:44:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 121 updates
2023-05-20 05:44:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint2.pt
2023-05-20 05:44:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint2.pt
2023-05-20 05:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint2.pt (epoch 2 @ 121 updates, score 5.693) (writing took 12.321452643722296 seconds)
2023-05-20 05:44:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-20 05:44:20 | INFO | train | epoch 002 | loss 6.436 | nll_loss 4.692 | ppl 25.85 | wps 2524.7 | ups 0.21 | wpb 12067.9 | bsz 560.8 | num_updates 121 | lr 2.42e-05 | gnorm 8.817 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.4 | wall 683
2023-05-20 05:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:44:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:44:20 | INFO | fairseq.trainer | begin training epoch 3
2023-05-20 05:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:45:36 | INFO | train_inner | epoch 003:     19 / 65 loss=6.288, nll_loss=4.546, ppl=23.37, wps=1788.1, ups=0.15, wpb=11805.3, bsz=546.1, num_updates=140, lr=2.8e-05, gnorm=20.603, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.2, wall=759
2023-05-20 05:46:53 | INFO | train_inner | epoch 003:     39 / 65 loss=6.346, nll_loss=4.608, ppl=24.38, wps=3111.4, ups=0.26, wpb=11999.8, bsz=542.5, num_updates=160, lr=3.2e-05, gnorm=7.061, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.3, wall=836
2023-05-20 05:48:10 | INFO | train_inner | epoch 003:     59 / 65 loss=6.207, nll_loss=4.457, ppl=21.96, wps=3192.3, ups=0.26, wpb=12286.5, bsz=573.9, num_updates=180, lr=3.6e-05, gnorm=7.753, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.1, wall=913
2023-05-20 05:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:48:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:49:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.597 | nll_loss 3.679 | ppl 12.81 | bleu 8.4 | wps 1059.5 | wpb 2785 | bsz 105.2 | num_updates 186 | best_loss 5.597
2023-05-20 05:49:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 186 updates
2023-05-20 05:49:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint3.pt
2023-05-20 05:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint3.pt
2023-05-20 05:49:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint3.pt (epoch 3 @ 186 updates, score 5.597) (writing took 14.641397673636675 seconds)
2023-05-20 05:49:19 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-20 05:49:19 | INFO | train | epoch 003 | loss 6.271 | nll_loss 4.525 | ppl 23.02 | wps 2621.6 | ups 0.22 | wpb 12057.2 | bsz 559 | num_updates 186 | lr 3.72e-05 | gnorm 13.18 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 17.9 | wall 982
2023-05-20 05:49:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:49:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:49:19 | INFO | fairseq.trainer | begin training epoch 4
2023-05-20 05:49:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:50:13 | INFO | train_inner | epoch 004:     14 / 65 loss=6.096, nll_loss=4.322, ppl=20.01, wps=1908.5, ups=0.16, wpb=11723.2, bsz=541, num_updates=200, lr=4e-05, gnorm=14.169, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.4, wall=1036
2023-05-20 05:51:33 | INFO | train_inner | epoch 004:     34 / 65 loss=6.029, nll_loss=4.25, ppl=19.03, wps=3039.2, ups=0.25, wpb=12229.8, bsz=561.5, num_updates=220, lr=3.98333e-05, gnorm=7.647, clip=100, loss_scale=0.25, train_wall=80, gb_free=18.1, wall=1116
2023-05-20 05:52:52 | INFO | train_inner | epoch 004:     54 / 65 loss=6.058, nll_loss=4.288, ppl=19.54, wps=3083.5, ups=0.25, wpb=12190, bsz=570.5, num_updates=240, lr=3.96667e-05, gnorm=8.462, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.5, wall=1195
2023-05-20 05:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:53:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:54:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.423 | nll_loss 3.49 | ppl 11.24 | bleu 9.99 | wps 877.4 | wpb 2785 | bsz 105.2 | num_updates 251 | best_loss 5.423
2023-05-20 05:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 251 updates
2023-05-20 05:54:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint4.pt
2023-05-20 05:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint4.pt
2023-05-20 05:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint4.pt (epoch 4 @ 251 updates, score 5.423) (writing took 14.46285793185234 seconds)
2023-05-20 05:54:21 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-20 05:54:21 | INFO | train | epoch 004 | loss 6.038 | nll_loss 4.262 | ppl 19.18 | wps 2593.8 | ups 0.22 | wpb 12048.9 | bsz 558.9 | num_updates 251 | lr 3.9575e-05 | gnorm 8.63 | clip 100 | loss_scale 0.25 | train_wall 249 | gb_free 18.5 | wall 1284
2023-05-20 05:54:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:54:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:54:21 | INFO | fairseq.trainer | begin training epoch 5
2023-05-20 05:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:54:55 | INFO | train_inner | epoch 005:      9 / 65 loss=5.957, nll_loss=4.173, ppl=18.03, wps=1923.5, ups=0.16, wpb=11768.5, bsz=544.2, num_updates=260, lr=3.95e-05, gnorm=8.008, clip=100, loss_scale=0.25, train_wall=69, gb_free=18.1, wall=1318
2023-05-20 05:56:14 | INFO | train_inner | epoch 005:     29 / 65 loss=5.855, nll_loss=4.061, ppl=16.69, wps=2998.8, ups=0.25, wpb=11965.7, bsz=549.7, num_updates=280, lr=3.93333e-05, gnorm=6.021, clip=100, loss_scale=0.25, train_wall=80, gb_free=18.7, wall=1397
2023-05-20 05:57:30 | INFO | train_inner | epoch 005:     49 / 65 loss=5.904, nll_loss=4.117, ppl=17.35, wps=3271.5, ups=0.26, wpb=12447.5, bsz=598.9, num_updates=300, lr=3.91667e-05, gnorm=15.007, clip=100, loss_scale=0.25, train_wall=76, gb_free=17.8, wall=1474
2023-05-20 05:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:58:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:59:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.487 | nll_loss 3.575 | ppl 11.91 | bleu 10.01 | wps 882.3 | wpb 2785 | bsz 105.2 | num_updates 316 | best_loss 5.423
2023-05-20 05:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 316 updates
2023-05-20 05:59:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint5.pt
2023-05-20 05:59:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint5.pt
2023-05-20 05:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint5.pt (epoch 5 @ 316 updates, score 5.487) (writing took 13.277930527925491 seconds)
2023-05-20 05:59:23 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-20 05:59:23 | INFO | train | epoch 005 | loss 5.896 | nll_loss 4.108 | ppl 17.24 | wps 2596.8 | ups 0.22 | wpb 12050.9 | bsz 558.7 | num_updates 316 | lr 3.90333e-05 | gnorm 9.338 | clip 100 | loss_scale 0.25 | train_wall 249 | gb_free 18.6 | wall 1586
2023-05-20 05:59:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:59:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:59:23 | INFO | fairseq.trainer | begin training epoch 6
2023-05-20 05:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:59:37 | INFO | train_inner | epoch 006:      4 / 65 loss=5.926, nll_loss=4.141, ppl=17.65, wps=1838.2, ups=0.16, wpb=11651.5, bsz=535.6, num_updates=320, lr=3.9e-05, gnorm=10.869, clip=100, loss_scale=0.25, train_wall=75, gb_free=18, wall=1600
2023-05-20 06:00:56 | INFO | train_inner | epoch 006:     24 / 65 loss=5.913, nll_loss=4.132, ppl=17.54, wps=3085.3, ups=0.25, wpb=12214.2, bsz=578.8, num_updates=340, lr=3.88333e-05, gnorm=8.915, clip=100, loss_scale=0.25, train_wall=79, gb_free=17.2, wall=1679
2023-05-20 06:02:12 | INFO | train_inner | epoch 006:     44 / 65 loss=5.781, nll_loss=3.98, ppl=15.78, wps=3199.1, ups=0.26, wpb=12162.5, bsz=541.9, num_updates=360, lr=3.86667e-05, gnorm=7.041, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.3, wall=1756
2023-05-20 06:03:31 | INFO | train_inner | epoch 006:     64 / 65 loss=5.744, nll_loss=3.936, ppl=15.3, wps=3097.5, ups=0.25, wpb=12189.2, bsz=579.7, num_updates=380, lr=3.85e-05, gnorm=9.525, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.3, wall=1834
2023-05-20 06:03:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:03:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:04:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.297 | nll_loss 3.372 | ppl 10.35 | bleu 9.67 | wps 888.4 | wpb 2785 | bsz 105.2 | num_updates 381 | best_loss 5.297
2023-05-20 06:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 381 updates
2023-05-20 06:04:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint6.pt
2023-05-20 06:04:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint6.pt
2023-05-20 06:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint6.pt (epoch 6 @ 381 updates, score 5.297) (writing took 41.70733551308513 seconds)
2023-05-20 06:04:56 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-20 06:04:56 | INFO | train | epoch 006 | loss 5.823 | nll_loss 4.028 | ppl 16.31 | wps 2350 | ups 0.19 | wpb 12054.7 | bsz 559.9 | num_updates 381 | lr 3.84917e-05 | gnorm 9.12 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 18.7 | wall 1919
2023-05-20 06:04:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:04:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:04:57 | INFO | fairseq.trainer | begin training epoch 7
2023-05-20 06:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:06:09 | INFO | train_inner | epoch 007:     19 / 65 loss=5.817, nll_loss=4.022, ppl=16.25, wps=1486, ups=0.13, wpb=11732.8, bsz=527.5, num_updates=400, lr=3.83333e-05, gnorm=22.518, clip=100, loss_scale=0.25, train_wall=75, gb_free=17.9, wall=1992
2023-05-20 06:07:25 | INFO | train_inner | epoch 007:     39 / 65 loss=5.746, nll_loss=3.948, ppl=15.44, wps=3274.1, ups=0.27, wpb=12348.2, bsz=579.8, num_updates=420, lr=3.81667e-05, gnorm=8.056, clip=100, loss_scale=0.25, train_wall=75, gb_free=17.8, wall=2068
2023-05-20 06:08:42 | INFO | train_inner | epoch 007:     59 / 65 loss=5.81, nll_loss=4.017, ppl=16.18, wps=3122, ups=0.26, wpb=12085.4, bsz=572.5, num_updates=440, lr=3.8e-05, gnorm=12.558, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.5, wall=2145
2023-05-20 06:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:09:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:09:43 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.339 | nll_loss 3.426 | ppl 10.75 | bleu 9.15 | wps 823.8 | wpb 2785 | bsz 105.2 | num_updates 446 | best_loss 5.297
2023-05-20 06:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 446 updates
2023-05-20 06:09:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint7.pt
2023-05-20 06:09:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint7.pt
2023-05-20 06:09:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint7.pt (epoch 7 @ 446 updates, score 5.339) (writing took 13.810367856174707 seconds)
2023-05-20 06:09:57 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-20 06:09:57 | INFO | train | epoch 007 | loss 5.789 | nll_loss 3.994 | ppl 15.93 | wps 2607.3 | ups 0.22 | wpb 12050.8 | bsz 559.8 | num_updates 446 | lr 3.795e-05 | gnorm 13.73 | clip 100 | loss_scale 0.25 | train_wall 244 | gb_free 18.2 | wall 2220
2023-05-20 06:09:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:09:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:09:57 | INFO | fairseq.trainer | begin training epoch 8
2023-05-20 06:09:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:10:50 | INFO | train_inner | epoch 008:     14 / 65 loss=5.78, nll_loss=3.986, ppl=15.84, wps=1840.9, ups=0.16, wpb=11780.2, bsz=537.2, num_updates=460, lr=3.78333e-05, gnorm=6.793, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.5, wall=2273
2023-05-20 06:12:09 | INFO | train_inner | epoch 008:     34 / 65 loss=5.838, nll_loss=4.059, ppl=16.67, wps=3071.5, ups=0.25, wpb=12188.2, bsz=560.5, num_updates=480, lr=3.76667e-05, gnorm=13.579, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.5, wall=2352
2023-05-20 06:13:25 | INFO | train_inner | epoch 008:     54 / 65 loss=5.64, nll_loss=3.828, ppl=14.2, wps=3210.9, ups=0.26, wpb=12128.5, bsz=584.2, num_updates=500, lr=3.75e-05, gnorm=25.195, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.6, wall=2428
2023-05-20 06:14:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:14:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:14:46 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.963 | nll_loss 2.972 | ppl 7.85 | bleu 13.82 | wps 838.1 | wpb 2785 | bsz 105.2 | num_updates 511 | best_loss 4.963
2023-05-20 06:14:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 511 updates
2023-05-20 06:14:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint8.pt
2023-05-20 06:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint8.pt
2023-05-20 06:15:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint8.pt (epoch 8 @ 511 updates, score 4.963) (writing took 16.979695800691843 seconds)
2023-05-20 06:15:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-20 06:15:03 | INFO | train | epoch 008 | loss 5.721 | nll_loss 3.92 | ppl 15.14 | wps 2558.7 | ups 0.21 | wpb 12057.1 | bsz 559.6 | num_updates 511 | lr 3.74083e-05 | gnorm 15.355 | clip 100 | loss_scale 0.25 | train_wall 248 | gb_free 18.2 | wall 2526
2023-05-20 06:15:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:15:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:15:03 | INFO | fairseq.trainer | begin training epoch 9
2023-05-20 06:15:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:15:37 | INFO | train_inner | epoch 009:      9 / 65 loss=5.518, nll_loss=3.686, ppl=12.87, wps=1774.7, ups=0.15, wpb=11751.3, bsz=520.8, num_updates=520, lr=3.73333e-05, gnorm=13.773, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.7, wall=2560
2023-05-20 06:16:55 | INFO | train_inner | epoch 009:     29 / 65 loss=5.507, nll_loss=3.675, ppl=12.77, wps=3120.5, ups=0.26, wpb=12139, bsz=567.8, num_updates=540, lr=3.71667e-05, gnorm=9.259, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.5, wall=2638
2023-05-20 06:18:10 | INFO | train_inner | epoch 009:     49 / 65 loss=5.453, nll_loss=3.616, ppl=12.26, wps=3207.2, ups=0.27, wpb=12026.2, bsz=549, num_updates=560, lr=3.7e-05, gnorm=11.757, clip=100, loss_scale=0.25, train_wall=75, gb_free=17.7, wall=2713
2023-05-20 06:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:19:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:19:48 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.896 | nll_loss 2.903 | ppl 7.48 | bleu 15.09 | wps 842.2 | wpb 2785 | bsz 105.2 | num_updates 576 | best_loss 4.896
2023-05-20 06:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 576 updates
2023-05-20 06:19:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint9.pt
2023-05-20 06:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint9.pt
2023-05-20 06:20:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint9.pt (epoch 9 @ 576 updates, score 4.896) (writing took 13.744065277278423 seconds)
2023-05-20 06:20:02 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-20 06:20:02 | INFO | train | epoch 009 | loss 5.466 | nll_loss 3.631 | ppl 12.39 | wps 2617.7 | ups 0.22 | wpb 12052 | bsz 558.8 | num_updates 576 | lr 3.68667e-05 | gnorm 10.715 | clip 100 | loss_scale 0.25 | train_wall 245 | gb_free 18.6 | wall 2825
2023-05-20 06:20:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:20:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:20:02 | INFO | fairseq.trainer | begin training epoch 10
2023-05-20 06:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:20:17 | INFO | train_inner | epoch 010:      4 / 65 loss=5.433, nll_loss=3.597, ppl=12.1, wps=1886.7, ups=0.16, wpb=11990.6, bsz=585, num_updates=580, lr=3.68333e-05, gnorm=7.181, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.5, wall=2840
2023-05-20 06:21:33 | INFO | train_inner | epoch 010:     24 / 65 loss=5.368, nll_loss=3.522, ppl=11.49, wps=3245.8, ups=0.27, wpb=12234.7, bsz=545.5, num_updates=600, lr=3.66667e-05, gnorm=5.704, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.5, wall=2916
2023-05-20 06:22:51 | INFO | train_inner | epoch 010:     44 / 65 loss=5.384, nll_loss=3.537, ppl=11.61, wps=3092.8, ups=0.26, wpb=12126.6, bsz=545.6, num_updates=620, lr=3.65e-05, gnorm=9.905, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.6, wall=2994
2023-05-20 06:24:09 | INFO | train_inner | epoch 010:     64 / 65 loss=5.4, nll_loss=3.558, ppl=11.77, wps=3138.2, ups=0.26, wpb=12212.8, bsz=599.6, num_updates=640, lr=3.63333e-05, gnorm=9.347, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.1, wall=3072
2023-05-20 06:24:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:24:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:24:48 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.889 | nll_loss 2.883 | ppl 7.38 | bleu 15.77 | wps 923 | wpb 2785 | bsz 105.2 | num_updates 641 | best_loss 4.889
2023-05-20 06:24:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 641 updates
2023-05-20 06:24:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint10.pt
2023-05-20 06:24:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint10.pt
2023-05-20 06:25:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint10.pt (epoch 10 @ 641 updates, score 4.889) (writing took 37.022616017609835 seconds)
2023-05-20 06:25:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-20 06:25:25 | INFO | train | epoch 010 | loss 5.386 | nll_loss 3.541 | ppl 11.64 | wps 2430.6 | ups 0.2 | wpb 12055 | bsz 559.5 | num_updates 641 | lr 3.6325e-05 | gnorm 8.204 | clip 100 | loss_scale 0.5 | train_wall 248 | gb_free 18.7 | wall 3148
2023-05-20 06:25:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:25:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:25:25 | INFO | fairseq.trainer | begin training epoch 11
2023-05-20 06:25:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:26:39 | INFO | train_inner | epoch 011:     19 / 65 loss=5.364, nll_loss=3.524, ppl=11.51, wps=1570.4, ups=0.13, wpb=11773, bsz=532.8, num_updates=660, lr=3.61667e-05, gnorm=14.446, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.3, wall=3222
2023-05-20 06:27:57 | INFO | train_inner | epoch 011:     39 / 65 loss=5.327, nll_loss=3.482, ppl=11.17, wps=3142.4, ups=0.26, wpb=12203.5, bsz=570.4, num_updates=680, lr=3.6e-05, gnorm=7.27, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.4, wall=3300
2023-05-20 06:29:13 | INFO | train_inner | epoch 011:     59 / 65 loss=5.322, nll_loss=3.473, ppl=11.1, wps=3177.5, ups=0.26, wpb=12137.3, bsz=578.6, num_updates=700, lr=3.58333e-05, gnorm=6.868, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.3, wall=3376
2023-05-20 06:29:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:29:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:30:14 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.925 | nll_loss 2.956 | ppl 7.76 | bleu 14.95 | wps 876.3 | wpb 2785 | bsz 105.2 | num_updates 706 | best_loss 4.889
2023-05-20 06:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 706 updates
2023-05-20 06:30:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint11.pt
2023-05-20 06:30:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint11.pt
2023-05-20 06:30:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint11.pt (epoch 11 @ 706 updates, score 4.925) (writing took 12.445493455976248 seconds)
2023-05-20 06:30:26 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-20 06:30:26 | INFO | train | epoch 011 | loss 5.335 | nll_loss 3.489 | ppl 11.23 | wps 2597.1 | ups 0.22 | wpb 12043.3 | bsz 557.6 | num_updates 706 | lr 3.57833e-05 | gnorm 9.535 | clip 100 | loss_scale 0.5 | train_wall 250 | gb_free 18.3 | wall 3449
2023-05-20 06:30:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:30:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:30:26 | INFO | fairseq.trainer | begin training epoch 12
2023-05-20 06:30:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:31:18 | INFO | train_inner | epoch 012:     14 / 65 loss=5.386, nll_loss=3.549, ppl=11.71, wps=1896.1, ups=0.16, wpb=11879.7, bsz=561.5, num_updates=720, lr=3.56667e-05, gnorm=5.752, clip=100, loss_scale=0.5, train_wall=74, gb_free=18, wall=3501
2023-05-20 06:32:35 | INFO | train_inner | epoch 012:     34 / 65 loss=5.504, nll_loss=3.685, ppl=12.86, wps=3164.6, ups=0.26, wpb=12085.6, bsz=549.3, num_updates=740, lr=3.55e-05, gnorm=4.393, clip=100, loss_scale=0.5, train_wall=76, gb_free=17.7, wall=3578
2023-05-20 06:33:53 | INFO | train_inner | epoch 012:     54 / 65 loss=5.314, nll_loss=3.465, ppl=11.05, wps=3123.4, ups=0.26, wpb=12209.4, bsz=570.4, num_updates=760, lr=3.53333e-05, gnorm=8.227, clip=100, loss_scale=0.5, train_wall=78, gb_free=17.8, wall=3656
2023-05-20 06:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:34:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:35:09 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.741 | nll_loss 2.706 | ppl 6.52 | bleu 17.3 | wps 877.1 | wpb 2785 | bsz 105.2 | num_updates 771 | best_loss 4.741
2023-05-20 06:35:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 771 updates
2023-05-20 06:35:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint12.pt
2023-05-20 06:35:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint12.pt
2023-05-20 06:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint12.pt (epoch 12 @ 771 updates, score 4.741) (writing took 16.886951692402363 seconds)
2023-05-20 06:35:26 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-20 06:35:26 | INFO | train | epoch 012 | loss 5.377 | nll_loss 3.538 | ppl 11.62 | wps 2610.9 | ups 0.22 | wpb 12044.3 | bsz 556.3 | num_updates 771 | lr 3.52417e-05 | gnorm 6.636 | clip 100 | loss_scale 0.5 | train_wall 244 | gb_free 18.5 | wall 3749
2023-05-20 06:35:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:35:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:35:26 | INFO | fairseq.trainer | begin training epoch 13
2023-05-20 06:35:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:36:00 | INFO | train_inner | epoch 013:      9 / 65 loss=5.215, nll_loss=3.348, ppl=10.18, wps=1843.9, ups=0.16, wpb=11744, bsz=529, num_updates=780, lr=3.51667e-05, gnorm=10.422, clip=100, loss_scale=0.5, train_wall=72, gb_free=18.4, wall=3783
2023-05-20 06:37:16 | INFO | train_inner | epoch 013:     29 / 65 loss=5.131, nll_loss=3.26, ppl=9.58, wps=3166.8, ups=0.27, wpb=11937.9, bsz=530.7, num_updates=800, lr=3.5e-05, gnorm=10.831, clip=100, loss_scale=0.5, train_wall=75, gb_free=18.6, wall=3859
2023-05-20 06:38:33 | INFO | train_inner | epoch 013:     49 / 65 loss=5.213, nll_loss=3.352, ppl=10.21, wps=3163.8, ups=0.26, wpb=12224.7, bsz=587.8, num_updates=820, lr=3.48333e-05, gnorm=9.002, clip=100, loss_scale=0.5, train_wall=77, gb_free=17.5, wall=3936
2023-05-20 06:39:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:39:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:40:06 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.674 | nll_loss 2.651 | ppl 6.28 | bleu 17.33 | wps 946.8 | wpb 2785 | bsz 105.2 | num_updates 836 | best_loss 4.674
2023-05-20 06:40:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 836 updates
2023-05-20 06:40:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint13.pt
2023-05-20 06:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint13.pt
2023-05-20 06:40:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint13.pt (epoch 13 @ 836 updates, score 4.674) (writing took 16.377937097102404 seconds)
2023-05-20 06:40:22 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-20 06:40:22 | INFO | train | epoch 013 | loss 5.18 | nll_loss 3.314 | ppl 9.94 | wps 2646 | ups 0.22 | wpb 12049.5 | bsz 559.8 | num_updates 836 | lr 3.47e-05 | gnorm 9.465 | clip 100 | loss_scale 0.5 | train_wall 243 | gb_free 18.7 | wall 4045
2023-05-20 06:40:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:40:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:40:22 | INFO | fairseq.trainer | begin training epoch 14
2023-05-20 06:40:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:40:36 | INFO | train_inner | epoch 014:      4 / 65 loss=5.15, nll_loss=3.278, ppl=9.7, wps=1922, ups=0.16, wpb=11857.8, bsz=536.4, num_updates=840, lr=3.46667e-05, gnorm=7.921, clip=100, loss_scale=0.5, train_wall=71, gb_free=18.5, wall=4059
2023-05-20 06:41:52 | INFO | train_inner | epoch 014:     24 / 65 loss=5.12, nll_loss=3.247, ppl=9.5, wps=3215.4, ups=0.26, wpb=12248.6, bsz=551.1, num_updates=860, lr=3.45e-05, gnorm=5.921, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.5, wall=4135
2023-05-20 06:43:10 | INFO | train_inner | epoch 014:     44 / 65 loss=5.143, nll_loss=3.274, ppl=9.67, wps=3159.4, ups=0.26, wpb=12212, bsz=574.5, num_updates=880, lr=3.43333e-05, gnorm=7.271, clip=100, loss_scale=0.5, train_wall=77, gb_free=18.5, wall=4213
2023-05-20 06:44:27 | INFO | train_inner | epoch 014:     64 / 65 loss=5.114, nll_loss=3.244, ppl=9.47, wps=3124, ups=0.26, wpb=12051.9, bsz=577.6, num_updates=900, lr=3.41667e-05, gnorm=6.82, clip=100, loss_scale=0.5, train_wall=77, gb_free=18.1, wall=4290
2023-05-20 06:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:44:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:45:03 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.7 | nll_loss 2.697 | ppl 6.49 | bleu 18.56 | wps 979.8 | wpb 2785 | bsz 105.2 | num_updates 901 | best_loss 4.674
2023-05-20 06:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 901 updates
2023-05-20 06:45:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint14.pt
2023-05-20 06:45:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint14.pt
2023-05-20 06:45:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint14.pt (epoch 14 @ 901 updates, score 4.7) (writing took 32.34678779542446 seconds)
2023-05-20 06:45:35 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-20 06:45:35 | INFO | train | epoch 014 | loss 5.125 | nll_loss 3.254 | ppl 9.54 | wps 2498.3 | ups 0.21 | wpb 12050.6 | bsz 558 | num_updates 901 | lr 3.41583e-05 | gnorm 6.602 | clip 100 | loss_scale 0.5 | train_wall 246 | gb_free 18.2 | wall 4358
2023-05-20 06:45:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:45:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:45:36 | INFO | fairseq.trainer | begin training epoch 15
2023-05-20 06:45:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:46:50 | INFO | train_inner | epoch 015:     19 / 65 loss=5.164, nll_loss=3.302, ppl=9.86, wps=1643.1, ups=0.14, wpb=11729, bsz=544.2, num_updates=920, lr=3.4e-05, gnorm=10.026, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.2, wall=4433
2023-05-20 06:48:07 | INFO | train_inner | epoch 015:     39 / 65 loss=5.158, nll_loss=3.294, ppl=9.81, wps=3186.9, ups=0.26, wpb=12256.7, bsz=572.4, num_updates=940, lr=3.38333e-05, gnorm=4.625, clip=100, loss_scale=0.5, train_wall=77, gb_free=17.8, wall=4510
2023-05-20 06:49:25 | INFO | train_inner | epoch 015:     59 / 65 loss=5.065, nll_loss=3.189, ppl=9.12, wps=3083.8, ups=0.25, wpb=12160, bsz=556.1, num_updates=960, lr=3.36667e-05, gnorm=5.267, clip=100, loss_scale=0.5, train_wall=79, gb_free=18.3, wall=4588
2023-05-20 06:49:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:49:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:50:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.645 | nll_loss 2.621 | ppl 6.15 | bleu 19.36 | wps 978.9 | wpb 2785 | bsz 105.2 | num_updates 966 | best_loss 4.645
2023-05-20 06:50:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 966 updates
2023-05-20 06:50:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint15.pt
2023-05-20 06:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint15.pt
2023-05-20 06:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint15.pt (epoch 15 @ 966 updates, score 4.645) (writing took 19.259798999875784 seconds)
2023-05-20 06:50:41 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-20 06:50:41 | INFO | train | epoch 015 | loss 5.124 | nll_loss 3.254 | ppl 9.54 | wps 2566.4 | ups 0.21 | wpb 12062.8 | bsz 559.4 | num_updates 966 | lr 3.36167e-05 | gnorm 6.816 | clip 100 | loss_scale 0.5 | train_wall 248 | gb_free 18.5 | wall 4664
2023-05-20 06:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:50:41 | INFO | fairseq.trainer | begin training epoch 16
2023-05-20 06:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:51:34 | INFO | train_inner | epoch 016:     14 / 65 loss=5.081, nll_loss=3.202, ppl=9.2, wps=1834.7, ups=0.16, wpb=11829.1, bsz=571.6, num_updates=980, lr=3.35e-05, gnorm=5.434, clip=100, loss_scale=0.5, train_wall=72, gb_free=17.9, wall=4717
2023-05-20 06:52:53 | INFO | train_inner | epoch 016:     34 / 65 loss=5.091, nll_loss=3.221, ppl=9.32, wps=3079.2, ups=0.25, wpb=12157.9, bsz=575.6, num_updates=1000, lr=3.33333e-05, gnorm=6.563, clip=100, loss_scale=0.5, train_wall=79, gb_free=17.2, wall=4796
2023-05-20 06:54:11 | INFO | train_inner | epoch 016:     54 / 65 loss=5.005, nll_loss=3.119, ppl=8.69, wps=3124.6, ups=0.26, wpb=12181.4, bsz=548.4, num_updates=1020, lr=3.31667e-05, gnorm=6.033, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.8, wall=4874
2023-05-20 06:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:54:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:55:24 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.609 | nll_loss 2.585 | ppl 6 | bleu 19.73 | wps 980.4 | wpb 2785 | bsz 105.2 | num_updates 1031 | best_loss 4.609
2023-05-20 06:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1031 updates
2023-05-20 06:55:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint16.pt
2023-05-20 06:55:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint16.pt
2023-05-20 06:55:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint16.pt (epoch 16 @ 1031 updates, score 4.609) (writing took 24.049624025821686 seconds)
2023-05-20 06:55:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-20 06:55:48 | INFO | train | epoch 016 | loss 5.057 | nll_loss 3.18 | ppl 9.06 | wps 2550 | ups 0.21 | wpb 12053 | bsz 559.7 | num_updates 1031 | lr 3.3075e-05 | gnorm 6.642 | clip 100 | loss_scale 0.5 | train_wall 248 | gb_free 18.2 | wall 4971
2023-05-20 06:55:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 06:55:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 06:55:50 | INFO | fairseq.trainer | begin training epoch 17
2023-05-20 06:55:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 06:56:26 | INFO | train_inner | epoch 017:      9 / 65 loss=5.014, nll_loss=3.13, ppl=8.76, wps=1741.1, ups=0.15, wpb=11702.7, bsz=529.2, num_updates=1040, lr=3.3e-05, gnorm=10.32, clip=100, loss_scale=0.5, train_wall=74, gb_free=18.3, wall=5009
2023-05-20 06:57:40 | INFO | train_inner | epoch 017:     29 / 65 loss=4.993, nll_loss=3.109, ppl=8.63, wps=3322.8, ups=0.27, wpb=12409, bsz=604, num_updates=1060, lr=3.28333e-05, gnorm=5.989, clip=100, loss_scale=0.5, train_wall=75, gb_free=18.4, wall=5084
2023-05-20 06:58:57 | INFO | train_inner | epoch 017:     49 / 65 loss=4.989, nll_loss=3.105, ppl=8.6, wps=3170.8, ups=0.26, wpb=12182.7, bsz=547.7, num_updates=1080, lr=3.26667e-05, gnorm=5.607, clip=100, loss_scale=0.5, train_wall=77, gb_free=18.5, wall=5160
2023-05-20 06:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 06:59:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:00:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.609 | nll_loss 2.587 | ppl 6.01 | bleu 20 | wps 994 | wpb 2785 | bsz 105.2 | num_updates 1096 | best_loss 4.609
2023-05-20 07:00:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1096 updates
2023-05-20 07:00:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint17.pt
2023-05-20 07:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint17.pt
2023-05-20 07:00:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint17.pt (epoch 17 @ 1096 updates, score 4.609) (writing took 12.653552412986755 seconds)
2023-05-20 07:00:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-20 07:00:46 | INFO | train | epoch 017 | loss 4.976 | nll_loss 3.088 | ppl 8.51 | wps 2628.6 | ups 0.22 | wpb 12054.2 | bsz 559.6 | num_updates 1096 | lr 3.25333e-05 | gnorm 6.435 | clip 100 | loss_scale 0.5 | train_wall 248 | gb_free 18.2 | wall 5269
2023-05-20 07:00:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:00:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:00:47 | INFO | fairseq.trainer | begin training epoch 18
2023-05-20 07:00:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:01:02 | INFO | train_inner | epoch 018:      4 / 65 loss=4.934, nll_loss=3.041, ppl=8.23, wps=1859.4, ups=0.16, wpb=11560.3, bsz=509.2, num_updates=1100, lr=3.25e-05, gnorm=6.216, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.5, wall=5285
2023-05-20 07:02:20 | INFO | train_inner | epoch 018:     24 / 65 loss=4.955, nll_loss=3.065, ppl=8.37, wps=3155.8, ups=0.26, wpb=12296.4, bsz=580, num_updates=1120, lr=3.23333e-05, gnorm=17.132, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.1, wall=5363
2023-05-20 07:03:36 | INFO | train_inner | epoch 018:     44 / 65 loss=4.918, nll_loss=3.025, ppl=8.14, wps=3190.7, ups=0.26, wpb=12121.2, bsz=554.1, num_updates=1140, lr=3.21667e-05, gnorm=4.884, clip=100, loss_scale=1, train_wall=76, gb_free=18.1, wall=5439
2023-05-20 07:04:51 | INFO | train_inner | epoch 018:     64 / 65 loss=4.955, nll_loss=3.069, ppl=8.39, wps=3228.1, ups=0.27, wpb=12150.6, bsz=579.1, num_updates=1160, lr=3.2e-05, gnorm=5.622, clip=100, loss_scale=1, train_wall=75, gb_free=18.2, wall=5514
2023-05-20 07:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:04:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:05:27 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.588 | nll_loss 2.592 | ppl 6.03 | bleu 18.97 | wps 966.9 | wpb 2785 | bsz 105.2 | num_updates 1161 | best_loss 4.588
2023-05-20 07:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1161 updates
2023-05-20 07:05:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint18.pt
2023-05-20 07:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint18.pt
2023-05-20 07:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint18.pt (epoch 18 @ 1161 updates, score 4.588) (writing took 13.12239083275199 seconds)
2023-05-20 07:05:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-20 07:05:41 | INFO | train | epoch 018 | loss 4.94 | nll_loss 3.05 | ppl 8.28 | wps 2655.8 | ups 0.22 | wpb 12049.9 | bsz 559.4 | num_updates 1161 | lr 3.19917e-05 | gnorm 9.065 | clip 100 | loss_scale 1 | train_wall 245 | gb_free 18.6 | wall 5564
2023-05-20 07:05:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:05:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:05:41 | INFO | fairseq.trainer | begin training epoch 19
2023-05-20 07:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:06:52 | INFO | train_inner | epoch 019:     19 / 65 loss=4.925, nll_loss=3.031, ppl=8.17, wps=1957.1, ups=0.16, wpb=11872.2, bsz=573.8, num_updates=1180, lr=3.18333e-05, gnorm=6.427, clip=100, loss_scale=1, train_wall=72, gb_free=18.1, wall=5635
2023-05-20 07:08:09 | INFO | train_inner | epoch 019:     39 / 65 loss=4.894, nll_loss=2.996, ppl=7.98, wps=3154.4, ups=0.26, wpb=12084.9, bsz=560.1, num_updates=1200, lr=3.16667e-05, gnorm=4.226, clip=100, loss_scale=1, train_wall=77, gb_free=18.3, wall=5712
2023-05-20 07:09:24 | INFO | train_inner | epoch 019:     59 / 65 loss=4.843, nll_loss=2.941, ppl=7.68, wps=3261.8, ups=0.27, wpb=12293.9, bsz=562.5, num_updates=1220, lr=3.15e-05, gnorm=3.636, clip=100, loss_scale=1, train_wall=75, gb_free=18.4, wall=5787
2023-05-20 07:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:09:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:10:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.6 | nll_loss 2.577 | ppl 5.97 | bleu 20.03 | wps 951.6 | wpb 2785 | bsz 105.2 | num_updates 1226 | best_loss 4.588
2023-05-20 07:10:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1226 updates
2023-05-20 07:10:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint19.pt
2023-05-20 07:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint19.pt
2023-05-20 07:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint19.pt (epoch 19 @ 1226 updates, score 4.6) (writing took 9.200152896344662 seconds)
2023-05-20 07:10:29 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-20 07:10:29 | INFO | train | epoch 019 | loss 4.883 | nll_loss 2.984 | ppl 7.91 | wps 2720.1 | ups 0.23 | wpb 12055 | bsz 559.7 | num_updates 1226 | lr 3.145e-05 | gnorm 4.531 | clip 100 | loss_scale 1 | train_wall 243 | gb_free 18.5 | wall 5853
2023-05-20 07:10:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:10:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:10:29 | INFO | fairseq.trainer | begin training epoch 20
2023-05-20 07:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:11:24 | INFO | train_inner | epoch 020:     14 / 65 loss=4.868, nll_loss=2.97, ppl=7.84, wps=1911.9, ups=0.17, wpb=11450.6, bsz=507.1, num_updates=1240, lr=3.13333e-05, gnorm=4.485, clip=100, loss_scale=1, train_wall=75, gb_free=18.6, wall=5907
2023-05-20 07:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 07:12:42 | INFO | train_inner | epoch 020:     35 / 65 loss=4.92, nll_loss=3.032, ppl=8.18, wps=3141.8, ups=0.26, wpb=12235.5, bsz=573, num_updates=1260, lr=3.11667e-05, gnorm=6.284, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.4, wall=5985
2023-05-20 07:13:58 | INFO | train_inner | epoch 020:     55 / 65 loss=4.814, nll_loss=2.906, ppl=7.49, wps=3222.4, ups=0.26, wpb=12244, bsz=554.1, num_updates=1280, lr=3.1e-05, gnorm=8.748, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.8, wall=6061
2023-05-20 07:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:14:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:15:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.538 | nll_loss 2.509 | ppl 5.69 | bleu 20.7 | wps 1039.1 | wpb 2785 | bsz 105.2 | num_updates 1290 | best_loss 4.538
2023-05-20 07:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1290 updates
2023-05-20 07:15:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint20.pt
2023-05-20 07:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint20.pt
2023-05-20 07:15:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint20.pt (epoch 20 @ 1290 updates, score 4.538) (writing took 12.509741265326738 seconds)
2023-05-20 07:15:20 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-20 07:15:20 | INFO | train | epoch 020 | loss 4.871 | nll_loss 2.973 | ppl 7.85 | wps 2653.4 | ups 0.22 | wpb 12032.5 | bsz 558.7 | num_updates 1290 | lr 3.09167e-05 | gnorm 6.71 | clip 100 | loss_scale 0.5 | train_wall 245 | gb_free 18.6 | wall 6143
2023-05-20 07:15:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:15:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:15:20 | INFO | fairseq.trainer | begin training epoch 21
2023-05-20 07:15:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:15:57 | INFO | train_inner | epoch 021:     10 / 65 loss=4.851, nll_loss=2.948, ppl=7.72, wps=1987.8, ups=0.17, wpb=11847.7, bsz=572.7, num_updates=1300, lr=3.08333e-05, gnorm=5.645, clip=100, loss_scale=0.5, train_wall=74, gb_free=18.7, wall=6180
2023-05-20 07:17:13 | INFO | train_inner | epoch 021:     30 / 65 loss=4.846, nll_loss=2.949, ppl=7.72, wps=3265.6, ups=0.26, wpb=12323.4, bsz=581, num_updates=1320, lr=3.06667e-05, gnorm=8.454, clip=100, loss_scale=0.5, train_wall=75, gb_free=16.9, wall=6256
2023-05-20 07:18:29 | INFO | train_inner | epoch 021:     50 / 65 loss=4.802, nll_loss=2.899, ppl=7.46, wps=3140.6, ups=0.26, wpb=12003.3, bsz=557.8, num_updates=1340, lr=3.05e-05, gnorm=6.155, clip=100, loss_scale=0.5, train_wall=76, gb_free=18.6, wall=6332
2023-05-20 07:19:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:19:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:19:57 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.522 | nll_loss 2.492 | ppl 5.63 | bleu 21.29 | wps 965.5 | wpb 2785 | bsz 105.2 | num_updates 1355 | best_loss 4.522
2023-05-20 07:19:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1355 updates
2023-05-20 07:19:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint21.pt
2023-05-20 07:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint21.pt
2023-05-20 07:20:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint21.pt (epoch 21 @ 1355 updates, score 4.522) (writing took 12.88252742215991 seconds)
2023-05-20 07:20:10 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-05-20 07:20:10 | INFO | train | epoch 021 | loss 4.825 | nll_loss 2.923 | ppl 7.58 | wps 2695.2 | ups 0.22 | wpb 12054.7 | bsz 559.9 | num_updates 1355 | lr 3.0375e-05 | gnorm 6.912 | clip 100 | loss_scale 0.5 | train_wall 242 | gb_free 18.5 | wall 6433
2023-05-20 07:20:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:20:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:20:10 | INFO | fairseq.trainer | begin training epoch 22
2023-05-20 07:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:20:30 | INFO | train_inner | epoch 022:      5 / 65 loss=4.806, nll_loss=2.901, ppl=7.47, wps=1950.1, ups=0.17, wpb=11766.3, bsz=542.8, num_updates=1360, lr=3.03333e-05, gnorm=7.481, clip=100, loss_scale=0.5, train_wall=73, gb_free=18.4, wall=6453
2023-05-20 07:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 07:21:49 | INFO | train_inner | epoch 022:     26 / 65 loss=4.772, nll_loss=2.86, ppl=7.26, wps=3073.6, ups=0.25, wpb=12206.6, bsz=561.1, num_updates=1380, lr=3.01667e-05, gnorm=9.564, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.1, wall=6532
2023-05-20 07:23:07 | INFO | train_inner | epoch 022:     46 / 65 loss=4.774, nll_loss=2.864, ppl=7.28, wps=3111.6, ups=0.26, wpb=12146.7, bsz=558.8, num_updates=1400, lr=3e-05, gnorm=4.117, clip=100, loss_scale=0.25, train_wall=78, gb_free=17.5, wall=6610
2023-05-20 07:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:24:53 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.547 | nll_loss 2.54 | ppl 5.82 | bleu 21.21 | wps 924.3 | wpb 2785 | bsz 105.2 | num_updates 1419 | best_loss 4.522
2023-05-20 07:24:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1419 updates
2023-05-20 07:24:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint22.pt
2023-05-20 07:24:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint22.pt
2023-05-20 07:25:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint22.pt (epoch 22 @ 1419 updates, score 4.547) (writing took 10.183384470641613 seconds)
2023-05-20 07:25:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-05-20 07:25:03 | INFO | train | epoch 022 | loss 4.786 | nll_loss 2.877 | ppl 7.34 | wps 2635.3 | ups 0.22 | wpb 12059.3 | bsz 559.2 | num_updates 1419 | lr 2.98417e-05 | gnorm 7.258 | clip 100 | loss_scale 0.25 | train_wall 246 | gb_free 18.7 | wall 6726
2023-05-20 07:25:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:25:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:25:03 | INFO | fairseq.trainer | begin training epoch 23
2023-05-20 07:25:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:25:07 | INFO | train_inner | epoch 023:      1 / 65 loss=4.814, nll_loss=2.91, ppl=7.51, wps=1969.1, ups=0.17, wpb=11767.2, bsz=557.6, num_updates=1420, lr=2.98333e-05, gnorm=7.44, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.4, wall=6730
2023-05-20 07:26:26 | INFO | train_inner | epoch 023:     21 / 65 loss=4.797, nll_loss=2.891, ppl=7.42, wps=3072.7, ups=0.25, wpb=12227, bsz=538.2, num_updates=1440, lr=2.96667e-05, gnorm=11.264, clip=100, loss_scale=0.25, train_wall=79, gb_free=17.9, wall=6809
2023-05-20 07:27:41 | INFO | train_inner | epoch 023:     41 / 65 loss=4.78, nll_loss=2.871, ppl=7.32, wps=3224.9, ups=0.27, wpb=12111.8, bsz=569.8, num_updates=1460, lr=2.95e-05, gnorm=7.64, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.4, wall=6884
2023-05-20 07:28:57 | INFO | train_inner | epoch 023:     61 / 65 loss=4.756, nll_loss=2.844, ppl=7.18, wps=3209.6, ups=0.26, wpb=12150.6, bsz=572, num_updates=1480, lr=2.93333e-05, gnorm=4.886, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.7, wall=6960
2023-05-20 07:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:29:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:29:42 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.507 | nll_loss 2.476 | ppl 5.56 | bleu 21.8 | wps 1012.3 | wpb 2785 | bsz 105.2 | num_updates 1484 | best_loss 4.507
2023-05-20 07:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1484 updates
2023-05-20 07:29:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint23.pt
2023-05-20 07:29:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint23.pt
2023-05-20 07:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint23.pt (epoch 23 @ 1484 updates, score 4.507) (writing took 13.101118132472038 seconds)
2023-05-20 07:29:55 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-05-20 07:29:55 | INFO | train | epoch 023 | loss 4.777 | nll_loss 2.868 | ppl 7.3 | wps 2682 | ups 0.22 | wpb 12054.2 | bsz 559.6 | num_updates 1484 | lr 2.93e-05 | gnorm 8.599 | clip 100 | loss_scale 0.25 | train_wall 245 | gb_free 18.8 | wall 7018
2023-05-20 07:29:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:29:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:29:55 | INFO | fairseq.trainer | begin training epoch 24
2023-05-20 07:29:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:30:59 | INFO | train_inner | epoch 024:     16 / 65 loss=4.71, nll_loss=2.785, ppl=6.89, wps=1909.6, ups=0.16, wpb=11640.4, bsz=539.6, num_updates=1500, lr=2.91667e-05, gnorm=9.049, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.5, wall=7082
2023-05-20 07:32:16 | INFO | train_inner | epoch 024:     36 / 65 loss=4.712, nll_loss=2.798, ppl=6.96, wps=3193.1, ups=0.26, wpb=12299.1, bsz=559.2, num_updates=1520, lr=2.9e-05, gnorm=4.849, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.2, wall=7159
2023-05-20 07:33:37 | INFO | train_inner | epoch 024:     56 / 65 loss=4.779, nll_loss=2.874, ppl=7.33, wps=3016.5, ups=0.25, wpb=12151.6, bsz=580.4, num_updates=1540, lr=2.88333e-05, gnorm=18.579, clip=100, loss_scale=0.25, train_wall=80, gb_free=18.4, wall=7240
2023-05-20 07:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:34:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:34:38 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.569 | nll_loss 2.559 | ppl 5.89 | bleu 21.88 | wps 1072 | wpb 2785 | bsz 105.2 | num_updates 1549 | best_loss 4.507
2023-05-20 07:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1549 updates
2023-05-20 07:34:38 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint24.pt
2023-05-20 07:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint24.pt
2023-05-20 07:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint24.pt (epoch 24 @ 1549 updates, score 4.569) (writing took 10.333426516503096 seconds)
2023-05-20 07:34:49 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-05-20 07:34:49 | INFO | train | epoch 024 | loss 4.742 | nll_loss 2.83 | ppl 7.11 | wps 2671.2 | ups 0.22 | wpb 12057.5 | bsz 560 | num_updates 1549 | lr 2.87583e-05 | gnorm 9.523 | clip 100 | loss_scale 0.25 | train_wall 251 | gb_free 18.8 | wall 7312
2023-05-20 07:34:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:34:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:34:49 | INFO | fairseq.trainer | begin training epoch 25
2023-05-20 07:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:35:31 | INFO | train_inner | epoch 025:     11 / 65 loss=4.78, nll_loss=2.876, ppl=7.34, wps=2065.4, ups=0.18, wpb=11792, bsz=559.6, num_updates=1560, lr=2.86667e-05, gnorm=4.851, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.6, wall=7354
2023-05-20 07:36:47 | INFO | train_inner | epoch 025:     31 / 65 loss=4.67, nll_loss=2.745, ppl=6.7, wps=3163, ups=0.26, wpb=11987.1, bsz=531, num_updates=1580, lr=2.85e-05, gnorm=4.067, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.2, wall=7430
2023-05-20 07:38:03 | INFO | train_inner | epoch 025:     51 / 65 loss=4.673, nll_loss=2.754, ppl=6.75, wps=3218.7, ups=0.26, wpb=12262.2, bsz=559.7, num_updates=1600, lr=2.83333e-05, gnorm=4.479, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.5, wall=7506
2023-05-20 07:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:38:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:39:34 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.476 | nll_loss 2.455 | ppl 5.48 | bleu 22.28 | wps 915.1 | wpb 2785 | bsz 105.2 | num_updates 1614 | best_loss 4.476
2023-05-20 07:39:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1614 updates
2023-05-20 07:39:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint25.pt
2023-05-20 07:39:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint25.pt
2023-05-20 07:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint25.pt (epoch 25 @ 1614 updates, score 4.476) (writing took 13.647081434726715 seconds)
2023-05-20 07:39:47 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-05-20 07:39:47 | INFO | train | epoch 025 | loss 4.706 | nll_loss 2.789 | ppl 6.91 | wps 2626.7 | ups 0.22 | wpb 12057.2 | bsz 560.3 | num_updates 1614 | lr 2.82167e-05 | gnorm 5.315 | clip 100 | loss_scale 0.25 | train_wall 248 | gb_free 18.2 | wall 7610
2023-05-20 07:39:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:39:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:39:47 | INFO | fairseq.trainer | begin training epoch 26
2023-05-20 07:39:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:40:10 | INFO | train_inner | epoch 026:      6 / 65 loss=4.756, nll_loss=2.845, ppl=7.18, wps=1909.6, ups=0.16, wpb=12146.2, bsz=624.6, num_updates=1620, lr=2.81667e-05, gnorm=7.443, clip=100, loss_scale=0.25, train_wall=77, gb_free=17, wall=7633
2023-05-20 07:41:29 | INFO | train_inner | epoch 026:     26 / 65 loss=4.618, nll_loss=2.688, ppl=6.45, wps=3021.6, ups=0.25, wpb=12010.1, bsz=551.7, num_updates=1640, lr=2.8e-05, gnorm=10.453, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.7, wall=7713
2023-05-20 07:42:47 | INFO | train_inner | epoch 026:     46 / 65 loss=4.64, nll_loss=2.714, ppl=6.56, wps=3125.8, ups=0.26, wpb=12166.5, bsz=556.2, num_updates=1660, lr=2.78333e-05, gnorm=6.658, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.2, wall=7790
2023-05-20 07:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:43:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:44:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.522 | nll_loss 2.515 | ppl 5.72 | bleu 22.21 | wps 1009.4 | wpb 2785 | bsz 105.2 | num_updates 1679 | best_loss 4.476
2023-05-20 07:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1679 updates
2023-05-20 07:44:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint26.pt
2023-05-20 07:44:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint26.pt
2023-05-20 07:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint26.pt (epoch 26 @ 1679 updates, score 4.522) (writing took 12.163723167032003 seconds)
2023-05-20 07:44:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-05-20 07:44:43 | INFO | train | epoch 026 | loss 4.654 | nll_loss 2.73 | ppl 6.63 | wps 2647.8 | ups 0.22 | wpb 12056.4 | bsz 559.7 | num_updates 1679 | lr 2.7675e-05 | gnorm 7.447 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 18.7 | wall 7906
2023-05-20 07:44:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:44:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:44:43 | INFO | fairseq.trainer | begin training epoch 27
2023-05-20 07:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:44:46 | INFO | train_inner | epoch 027:      1 / 65 loss=4.685, nll_loss=2.767, ppl=6.8, wps=1976.6, ups=0.17, wpb=11766.3, bsz=528, num_updates=1680, lr=2.76667e-05, gnorm=6.158, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.5, wall=7909
2023-05-20 07:46:02 | INFO | train_inner | epoch 027:     21 / 65 loss=4.695, nll_loss=2.778, ppl=6.86, wps=3225.4, ups=0.26, wpb=12249.9, bsz=578.4, num_updates=1700, lr=2.75e-05, gnorm=5.364, clip=100, loss_scale=0.25, train_wall=76, gb_free=17, wall=7985
2023-05-20 07:47:19 | INFO | train_inner | epoch 027:     41 / 65 loss=4.646, nll_loss=2.724, ppl=6.61, wps=3152.5, ups=0.26, wpb=12096.9, bsz=528.7, num_updates=1720, lr=2.73333e-05, gnorm=4.32, clip=100, loss_scale=0.25, train_wall=77, gb_free=18, wall=8062
2023-05-20 07:48:35 | INFO | train_inner | epoch 027:     61 / 65 loss=4.678, nll_loss=2.759, ppl=6.77, wps=3217.8, ups=0.26, wpb=12218.9, bsz=598.1, num_updates=1740, lr=2.71667e-05, gnorm=7.123, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.5, wall=8138
2023-05-20 07:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:48:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:49:20 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.509 | nll_loss 2.495 | ppl 5.64 | bleu 22.81 | wps 1109.2 | wpb 2785 | bsz 105.2 | num_updates 1744 | best_loss 4.476
2023-05-20 07:49:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1744 updates
2023-05-20 07:49:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint27.pt
2023-05-20 07:49:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint27.pt
2023-05-20 07:49:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint27.pt (epoch 27 @ 1744 updates, score 4.509) (writing took 11.768996711820364 seconds)
2023-05-20 07:49:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-05-20 07:49:32 | INFO | train | epoch 027 | loss 4.672 | nll_loss 2.753 | ppl 6.74 | wps 2713.8 | ups 0.23 | wpb 12044.6 | bsz 557.2 | num_updates 1744 | lr 2.71333e-05 | gnorm 5.726 | clip 100 | loss_scale 0.25 | train_wall 246 | gb_free 18 | wall 8195
2023-05-20 07:49:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:49:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:49:32 | INFO | fairseq.trainer | begin training epoch 28
2023-05-20 07:49:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:50:33 | INFO | train_inner | epoch 028:     16 / 65 loss=4.657, nll_loss=2.735, ppl=6.66, wps=1993, ups=0.17, wpb=11790.9, bsz=538, num_updates=1760, lr=2.7e-05, gnorm=7.998, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.3, wall=8256
2023-05-20 07:51:54 | INFO | train_inner | epoch 028:     36 / 65 loss=4.578, nll_loss=2.639, ppl=6.23, wps=2966.4, ups=0.25, wpb=12023.7, bsz=544.2, num_updates=1780, lr=2.68333e-05, gnorm=11.835, clip=100, loss_scale=0.25, train_wall=81, gb_free=18.5, wall=8337
2023-05-20 07:53:11 | INFO | train_inner | epoch 028:     56 / 65 loss=4.574, nll_loss=2.641, ppl=6.24, wps=3201.9, ups=0.26, wpb=12274.8, bsz=575.6, num_updates=1800, lr=2.66667e-05, gnorm=5.506, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.5, wall=8414
2023-05-20 07:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:53:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:54:19 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.463 | nll_loss 2.442 | ppl 5.43 | bleu 23.01 | wps 895.9 | wpb 2785 | bsz 105.2 | num_updates 1809 | best_loss 4.463
2023-05-20 07:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1809 updates
2023-05-20 07:54:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint28.pt
2023-05-20 07:54:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint28.pt
2023-05-20 07:54:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint28.pt (epoch 28 @ 1809 updates, score 4.463) (writing took 15.458646532148123 seconds)
2023-05-20 07:54:34 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-05-20 07:54:34 | INFO | train | epoch 028 | loss 4.602 | nll_loss 2.67 | ppl 6.36 | wps 2585.9 | ups 0.21 | wpb 12048.1 | bsz 557.8 | num_updates 1809 | lr 2.65917e-05 | gnorm 10.752 | clip 100 | loss_scale 0.25 | train_wall 249 | gb_free 18.7 | wall 8498
2023-05-20 07:54:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:54:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:54:34 | INFO | fairseq.trainer | begin training epoch 29
2023-05-20 07:54:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:55:17 | INFO | train_inner | epoch 029:     11 / 65 loss=4.566, nll_loss=2.63, ppl=6.19, wps=1844.9, ups=0.16, wpb=11627.2, bsz=518.6, num_updates=1820, lr=2.65e-05, gnorm=13.67, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.2, wall=8540
2023-05-20 07:56:33 | INFO | train_inner | epoch 029:     31 / 65 loss=4.578, nll_loss=2.645, ppl=6.25, wps=3221.2, ups=0.26, wpb=12244, bsz=574.4, num_updates=1840, lr=2.63333e-05, gnorm=4.299, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.3, wall=8616
2023-05-20 07:57:49 | INFO | train_inner | epoch 029:     51 / 65 loss=4.566, nll_loss=2.633, ppl=6.2, wps=3236.2, ups=0.26, wpb=12280.4, bsz=576, num_updates=1860, lr=2.61667e-05, gnorm=6.995, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.1, wall=8692
2023-05-20 07:58:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 07:58:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:59:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.482 | nll_loss 2.461 | ppl 5.5 | bleu 23.12 | wps 906.3 | wpb 2785 | bsz 105.2 | num_updates 1874 | best_loss 4.463
2023-05-20 07:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1874 updates
2023-05-20 07:59:16 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint29.pt
2023-05-20 07:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint29.pt
2023-05-20 07:59:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint29.pt (epoch 29 @ 1874 updates, score 4.482) (writing took 10.94485792517662 seconds)
2023-05-20 07:59:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-05-20 07:59:27 | INFO | train | epoch 029 | loss 4.566 | nll_loss 2.632 | ppl 6.2 | wps 2679.9 | ups 0.22 | wpb 12058.9 | bsz 559.8 | num_updates 1874 | lr 2.605e-05 | gnorm 5.483 | clip 100 | loss_scale 0.25 | train_wall 244 | gb_free 17.8 | wall 8790
2023-05-20 07:59:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 07:59:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 07:59:27 | INFO | fairseq.trainer | begin training epoch 30
2023-05-20 07:59:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 07:59:49 | INFO | train_inner | epoch 030:      6 / 65 loss=4.586, nll_loss=2.653, ppl=6.29, wps=1950.7, ups=0.17, wpb=11656.9, bsz=552.4, num_updates=1880, lr=2.6e-05, gnorm=9.319, clip=100, loss_scale=0.25, train_wall=71, gb_free=18.4, wall=8812
2023-05-20 08:01:03 | INFO | train_inner | epoch 030:     26 / 65 loss=4.582, nll_loss=2.655, ppl=6.3, wps=3258.6, ups=0.27, wpb=12155.2, bsz=559.1, num_updates=1900, lr=2.58333e-05, gnorm=10.188, clip=100, loss_scale=0.5, train_wall=75, gb_free=17.5, wall=8886
2023-05-20 08:02:21 | INFO | train_inner | epoch 030:     46 / 65 loss=4.672, nll_loss=2.754, ppl=6.75, wps=3200.2, ups=0.26, wpb=12406, bsz=580, num_updates=1920, lr=2.56667e-05, gnorm=3.697, clip=100, loss_scale=0.5, train_wall=77, gb_free=18.3, wall=8964
2023-05-20 08:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:03:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:04:03 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.475 | nll_loss 2.484 | ppl 5.59 | bleu 22.52 | wps 981.8 | wpb 2785 | bsz 105.2 | num_updates 1939 | best_loss 4.463
2023-05-20 08:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1939 updates
2023-05-20 08:04:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint30.pt
2023-05-20 08:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint30.pt
2023-05-20 08:04:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint30.pt (epoch 30 @ 1939 updates, score 4.475) (writing took 10.411245625466108 seconds)
2023-05-20 08:04:13 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-05-20 08:04:13 | INFO | train | epoch 030 | loss 4.624 | nll_loss 2.701 | ppl 6.5 | wps 2737.2 | ups 0.23 | wpb 12054.9 | bsz 558.9 | num_updates 1939 | lr 2.55083e-05 | gnorm 8.535 | clip 100 | loss_scale 0.5 | train_wall 241 | gb_free 18.5 | wall 9076
2023-05-20 08:04:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:04:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:04:13 | INFO | fairseq.trainer | begin training epoch 31
2023-05-20 08:04:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:04:16 | INFO | train_inner | epoch 031:      1 / 65 loss=4.624, nll_loss=2.702, ppl=6.51, wps=2020.5, ups=0.17, wpb=11684.9, bsz=537.1, num_updates=1940, lr=2.55e-05, gnorm=8.799, clip=100, loss_scale=0.5, train_wall=71, gb_free=18.3, wall=9079
2023-05-20 08:05:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 08:05:39 | INFO | train_inner | epoch 031:     22 / 65 loss=4.512, nll_loss=2.572, ppl=5.95, wps=2966.4, ups=0.24, wpb=12245.1, bsz=557.4, num_updates=1960, lr=2.53333e-05, gnorm=6.97, clip=100, loss_scale=0.25, train_wall=82, gb_free=18.5, wall=9162
2023-05-20 08:06:58 | INFO | train_inner | epoch 031:     42 / 65 loss=4.545, nll_loss=2.61, ppl=6.11, wps=3048.5, ups=0.25, wpb=12110.6, bsz=549.4, num_updates=1980, lr=2.51667e-05, gnorm=8.036, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.5, wall=9241
2023-05-20 08:08:11 | INFO | train_inner | epoch 031:     62 / 65 loss=4.551, nll_loss=2.617, ppl=6.13, wps=3344.2, ups=0.27, wpb=12196, bsz=587, num_updates=2000, lr=2.5e-05, gnorm=4.831, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.6, wall=9314
2023-05-20 08:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:08:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:08:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.45 | nll_loss 2.447 | ppl 5.45 | bleu 22.71 | wps 974.3 | wpb 2785 | bsz 105.2 | num_updates 2003 | best_loss 4.45
2023-05-20 08:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2003 updates
2023-05-20 08:08:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint31.pt
2023-05-20 08:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint31.pt
2023-05-20 08:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint31.pt (epoch 31 @ 2003 updates, score 4.45) (writing took 16.21623559296131 seconds)
2023-05-20 08:09:11 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-05-20 08:09:11 | INFO | train | epoch 031 | loss 4.536 | nll_loss 2.6 | ppl 6.06 | wps 2592.7 | ups 0.22 | wpb 12055.5 | bsz 558.2 | num_updates 2003 | lr 2.4975e-05 | gnorm 6.535 | clip 100 | loss_scale 0.25 | train_wall 246 | gb_free 17.5 | wall 9374
2023-05-20 08:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:09:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:09:11 | INFO | fairseq.trainer | begin training epoch 32
2023-05-20 08:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:10:15 | INFO | train_inner | epoch 032:     17 / 65 loss=4.529, nll_loss=2.594, ppl=6.04, wps=1908.9, ups=0.16, wpb=11763.3, bsz=540, num_updates=2020, lr=2.48333e-05, gnorm=5.821, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.1, wall=9438
2023-05-20 08:11:33 | INFO | train_inner | epoch 032:     37 / 65 loss=4.49, nll_loss=2.547, ppl=5.84, wps=3072.1, ups=0.25, wpb=12108.5, bsz=523.8, num_updates=2040, lr=2.46667e-05, gnorm=4.292, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.4, wall=9516
2023-05-20 08:12:54 | INFO | train_inner | epoch 032:     57 / 65 loss=4.554, nll_loss=2.616, ppl=6.13, wps=3031.2, ups=0.25, wpb=12162.2, bsz=582.6, num_updates=2060, lr=2.45e-05, gnorm=9.231, clip=100, loss_scale=0.25, train_wall=80, gb_free=17.9, wall=9597
2023-05-20 08:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:13:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:13:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.51 | nll_loss 2.485 | ppl 5.6 | bleu 23.48 | wps 1020.2 | wpb 2785 | bsz 105.2 | num_updates 2068 | best_loss 4.45
2023-05-20 08:13:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2068 updates
2023-05-20 08:13:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint32.pt
2023-05-20 08:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint32.pt
2023-05-20 08:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint32.pt (epoch 32 @ 2068 updates, score 4.51) (writing took 22.433812338858843 seconds)
2023-05-20 08:14:17 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-05-20 08:14:17 | INFO | train | epoch 032 | loss 4.527 | nll_loss 2.589 | ppl 6.02 | wps 2558 | ups 0.21 | wpb 12057.6 | bsz 560 | num_updates 2068 | lr 2.44333e-05 | gnorm 6.391 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 18.5 | wall 9680
2023-05-20 08:14:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:14:18 | INFO | fairseq.trainer | begin training epoch 33
2023-05-20 08:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:15:03 | INFO | train_inner | epoch 033:     12 / 65 loss=4.523, nll_loss=2.587, ppl=6.01, wps=1820.7, ups=0.15, wpb=11768.9, bsz=564.6, num_updates=2080, lr=2.43333e-05, gnorm=4.722, clip=100, loss_scale=0.25, train_wall=73, gb_free=18.4, wall=9726
2023-05-20 08:16:17 | INFO | train_inner | epoch 033:     32 / 65 loss=4.523, nll_loss=2.583, ppl=5.99, wps=3312.5, ups=0.27, wpb=12263.4, bsz=581.2, num_updates=2100, lr=2.41667e-05, gnorm=6.886, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.8, wall=9800
2023-05-20 08:17:34 | INFO | train_inner | epoch 033:     52 / 65 loss=4.467, nll_loss=2.527, ppl=5.76, wps=3157, ups=0.26, wpb=12147.2, bsz=553.1, num_updates=2120, lr=2.4e-05, gnorm=5.712, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.3, wall=9877
2023-05-20 08:18:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:18:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:18:58 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.436 | nll_loss 2.419 | ppl 5.35 | bleu 23.31 | wps 879.9 | wpb 2785 | bsz 105.2 | num_updates 2133 | best_loss 4.436
2023-05-20 08:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2133 updates
2023-05-20 08:18:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint33.pt
2023-05-20 08:19:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint33.pt
2023-05-20 08:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint33.pt (epoch 33 @ 2133 updates, score 4.436) (writing took 16.72184520959854 seconds)
2023-05-20 08:19:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-05-20 08:19:15 | INFO | train | epoch 033 | loss 4.502 | nll_loss 2.562 | ppl 5.91 | wps 2635 | ups 0.22 | wpb 12052.8 | bsz 559.9 | num_updates 2133 | lr 2.38917e-05 | gnorm 5.5 | clip 100 | loss_scale 0.25 | train_wall 242 | gb_free 17.8 | wall 9978
2023-05-20 08:19:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:19:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:19:15 | INFO | fairseq.trainer | begin training epoch 34
2023-05-20 08:19:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:19:41 | INFO | train_inner | epoch 034:      7 / 65 loss=4.502, nll_loss=2.56, ppl=5.9, wps=1861.1, ups=0.16, wpb=11808.2, bsz=556.5, num_updates=2140, lr=2.38333e-05, gnorm=5.129, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.2, wall=10004
2023-05-20 08:20:58 | INFO | train_inner | epoch 034:     27 / 65 loss=4.499, nll_loss=2.56, ppl=5.9, wps=3144.6, ups=0.26, wpb=12116.6, bsz=573.5, num_updates=2160, lr=2.36667e-05, gnorm=5.74, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.1, wall=10081
2023-05-20 08:22:12 | INFO | train_inner | epoch 034:     47 / 65 loss=4.491, nll_loss=2.548, ppl=5.85, wps=3355.5, ups=0.27, wpb=12368.5, bsz=588.4, num_updates=2180, lr=2.35e-05, gnorm=4.39, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.5, wall=10155
2023-05-20 08:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:23:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:23:58 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.454 | nll_loss 2.437 | ppl 5.41 | bleu 23.4 | wps 936.7 | wpb 2785 | bsz 105.2 | num_updates 2198 | best_loss 4.436
2023-05-20 08:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2198 updates
2023-05-20 08:23:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint34.pt
2023-05-20 08:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint34.pt
2023-05-20 08:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint34.pt (epoch 34 @ 2198 updates, score 4.454) (writing took 13.247464150190353 seconds)
2023-05-20 08:24:12 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-05-20 08:24:12 | INFO | train | epoch 034 | loss 4.479 | nll_loss 2.535 | ppl 5.8 | wps 2637.2 | ups 0.22 | wpb 12056.4 | bsz 559.1 | num_updates 2198 | lr 2.335e-05 | gnorm 4.772 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.7 | wall 10275
2023-05-20 08:24:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:24:12 | INFO | fairseq.trainer | begin training epoch 35
2023-05-20 08:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:24:19 | INFO | train_inner | epoch 035:      2 / 65 loss=4.44, nll_loss=2.492, ppl=5.63, wps=1819.6, ups=0.16, wpb=11603.6, bsz=507.2, num_updates=2200, lr=2.33333e-05, gnorm=3.511, clip=100, loss_scale=0.25, train_wall=78, gb_free=17.8, wall=10282
2023-05-20 08:25:34 | INFO | train_inner | epoch 035:     22 / 65 loss=4.448, nll_loss=2.501, ppl=5.66, wps=3248.3, ups=0.27, wpb=12189.1, bsz=565.6, num_updates=2220, lr=2.31667e-05, gnorm=6.513, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.1, wall=10357
2023-05-20 08:26:51 | INFO | train_inner | epoch 035:     42 / 65 loss=4.452, nll_loss=2.506, ppl=5.68, wps=3168.7, ups=0.26, wpb=12170, bsz=548.9, num_updates=2240, lr=2.3e-05, gnorm=6.841, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.6, wall=10434
2023-05-20 08:28:07 | INFO | train_inner | epoch 035:     62 / 65 loss=4.467, nll_loss=2.522, ppl=5.74, wps=3247.7, ups=0.26, wpb=12265.4, bsz=594.7, num_updates=2260, lr=2.28333e-05, gnorm=7.529, clip=100, loss_scale=0.25, train_wall=75, gb_free=17.7, wall=10510
2023-05-20 08:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:28:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:28:53 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.455 | nll_loss 2.451 | ppl 5.47 | bleu 23.75 | wps 936.5 | wpb 2785 | bsz 105.2 | num_updates 2263 | best_loss 4.436
2023-05-20 08:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2263 updates
2023-05-20 08:28:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint35.pt
2023-05-20 08:29:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint35.pt
2023-05-20 08:29:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint35.pt (epoch 35 @ 2263 updates, score 4.455) (writing took 16.06018729880452 seconds)
2023-05-20 08:29:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2023-05-20 08:29:09 | INFO | train | epoch 035 | loss 4.452 | nll_loss 2.506 | ppl 5.68 | wps 2634 | ups 0.22 | wpb 12054.5 | bsz 559.5 | num_updates 2263 | lr 2.28083e-05 | gnorm 6.665 | clip 100 | loss_scale 0.25 | train_wall 245 | gb_free 18.4 | wall 10572
2023-05-20 08:29:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:29:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:29:09 | INFO | fairseq.trainer | begin training epoch 36
2023-05-20 08:29:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:30:13 | INFO | train_inner | epoch 036:     17 / 65 loss=4.436, nll_loss=2.49, ppl=5.62, wps=1857.3, ups=0.16, wpb=11709.5, bsz=550.4, num_updates=2280, lr=2.26667e-05, gnorm=4.466, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.6, wall=10636
2023-05-20 08:31:30 | INFO | train_inner | epoch 036:     37 / 65 loss=4.402, nll_loss=2.449, ppl=5.46, wps=3138, ups=0.26, wpb=12170.5, bsz=567.6, num_updates=2300, lr=2.25e-05, gnorm=4.032, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.4, wall=10713
2023-05-20 08:32:50 | INFO | train_inner | epoch 036:     57 / 65 loss=4.417, nll_loss=2.464, ppl=5.52, wps=3063.9, ups=0.25, wpb=12275.7, bsz=548.8, num_updates=2320, lr=2.23333e-05, gnorm=5.977, clip=100, loss_scale=0.25, train_wall=80, gb_free=18.2, wall=10793
2023-05-20 08:33:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:33:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:33:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 4.428 | nll_loss 2.408 | ppl 5.31 | bleu 24.51 | wps 1050 | wpb 2785 | bsz 105.2 | num_updates 2328 | best_loss 4.428
2023-05-20 08:33:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2328 updates
2023-05-20 08:33:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint36.pt
2023-05-20 08:33:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint36.pt
2023-05-20 08:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint36.pt (epoch 36 @ 2328 updates, score 4.428) (writing took 37.37237383052707 seconds)
2023-05-20 08:34:28 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2023-05-20 08:34:28 | INFO | train | epoch 036 | loss 4.422 | nll_loss 2.472 | ppl 5.55 | wps 2456.3 | ups 0.2 | wpb 12055.9 | bsz 559.2 | num_updates 2328 | lr 2.22667e-05 | gnorm 4.999 | clip 100 | loss_scale 0.25 | train_wall 249 | gb_free 17.5 | wall 10891
2023-05-20 08:34:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:34:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:34:28 | INFO | fairseq.trainer | begin training epoch 37
2023-05-20 08:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:35:13 | INFO | train_inner | epoch 037:     12 / 65 loss=4.457, nll_loss=2.513, ppl=5.71, wps=1664.7, ups=0.14, wpb=11847.5, bsz=561.5, num_updates=2340, lr=2.21667e-05, gnorm=6.293, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.3, wall=10936
2023-05-20 08:36:32 | INFO | train_inner | epoch 037:     32 / 65 loss=4.38, nll_loss=2.428, ppl=5.38, wps=3084.5, ups=0.25, wpb=12160.1, bsz=563, num_updates=2360, lr=2.2e-05, gnorm=4.625, clip=100, loss_scale=0.25, train_wall=79, gb_free=17.7, wall=11015
2023-05-20 08:37:50 | INFO | train_inner | epoch 037:     52 / 65 loss=4.417, nll_loss=2.466, ppl=5.52, wps=3103.8, ups=0.26, wpb=12111.5, bsz=553.1, num_updates=2380, lr=2.18333e-05, gnorm=3.918, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.3, wall=11093
2023-05-20 08:38:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:38:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:39:04 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 4.448 | nll_loss 2.424 | ppl 5.37 | bleu 24.64 | wps 1210.3 | wpb 2785 | bsz 105.2 | num_updates 2393 | best_loss 4.428
2023-05-20 08:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2393 updates
2023-05-20 08:39:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint37.pt
2023-05-20 08:39:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint37.pt
2023-05-20 08:39:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint37.pt (epoch 37 @ 2393 updates, score 4.448) (writing took 10.912703324109316 seconds)
2023-05-20 08:39:15 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2023-05-20 08:39:15 | INFO | train | epoch 037 | loss 4.413 | nll_loss 2.462 | ppl 5.51 | wps 2732.6 | ups 0.23 | wpb 12057.6 | bsz 560.4 | num_updates 2393 | lr 2.1725e-05 | gnorm 4.56 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.4 | wall 11178
2023-05-20 08:39:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:39:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:39:15 | INFO | fairseq.trainer | begin training epoch 38
2023-05-20 08:39:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:39:41 | INFO | train_inner | epoch 038:      7 / 65 loss=4.402, nll_loss=2.449, ppl=5.46, wps=2115.5, ups=0.18, wpb=11754.3, bsz=558, num_updates=2400, lr=2.16667e-05, gnorm=3.383, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.7, wall=11204
2023-05-20 08:40:57 | INFO | train_inner | epoch 038:     27 / 65 loss=4.396, nll_loss=2.443, ppl=5.44, wps=3186.8, ups=0.26, wpb=12181.8, bsz=574, num_updates=2420, lr=2.15e-05, gnorm=5.019, clip=100, loss_scale=0.25, train_wall=76, gb_free=17.9, wall=11280
2023-05-20 08:42:15 | INFO | train_inner | epoch 038:     47 / 65 loss=4.392, nll_loss=2.438, ppl=5.42, wps=3145.4, ups=0.26, wpb=12176.4, bsz=559.5, num_updates=2440, lr=2.13333e-05, gnorm=6.367, clip=100, loss_scale=0.25, train_wall=77, gb_free=18, wall=11358
2023-05-20 08:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:43:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:43:58 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 4.425 | nll_loss 2.412 | ppl 5.32 | bleu 24.32 | wps 967.7 | wpb 2785 | bsz 105.2 | num_updates 2458 | best_loss 4.425
2023-05-20 08:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2458 updates
2023-05-20 08:43:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint38.pt
2023-05-20 08:44:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint38.pt
2023-05-20 08:44:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint38.pt (epoch 38 @ 2458 updates, score 4.425) (writing took 24.714675176888704 seconds)
2023-05-20 08:44:23 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2023-05-20 08:44:23 | INFO | train | epoch 038 | loss 4.385 | nll_loss 2.43 | ppl 5.39 | wps 2544.4 | ups 0.21 | wpb 12053.8 | bsz 558.5 | num_updates 2458 | lr 2.11833e-05 | gnorm 5.045 | clip 100 | loss_scale 0.25 | train_wall 248 | gb_free 18.7 | wall 11486
2023-05-20 08:44:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:44:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:44:23 | INFO | fairseq.trainer | begin training epoch 39
2023-05-20 08:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:44:30 | INFO | train_inner | epoch 039:      2 / 65 loss=4.353, nll_loss=2.395, ppl=5.26, wps=1730.2, ups=0.15, wpb=11718.6, bsz=535.4, num_updates=2460, lr=2.11667e-05, gnorm=4.101, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.6, wall=11493
2023-05-20 08:45:44 | INFO | train_inner | epoch 039:     22 / 65 loss=4.369, nll_loss=2.417, ppl=5.34, wps=3286.6, ups=0.27, wpb=12177.7, bsz=573.4, num_updates=2480, lr=2.1e-05, gnorm=6.726, clip=100, loss_scale=0.5, train_wall=74, gb_free=18.1, wall=11567
2023-05-20 08:47:02 | INFO | train_inner | epoch 039:     42 / 65 loss=4.363, nll_loss=2.406, ppl=5.3, wps=3108.6, ups=0.26, wpb=12083.6, bsz=563.2, num_updates=2500, lr=2.08333e-05, gnorm=6.805, clip=100, loss_scale=0.5, train_wall=78, gb_free=18.6, wall=11645
2023-05-20 08:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 08:48:23 | INFO | train_inner | epoch 039:     63 / 65 loss=4.347, nll_loss=2.39, ppl=5.24, wps=3040.3, ups=0.25, wpb=12333.3, bsz=564.2, num_updates=2520, lr=2.06667e-05, gnorm=6.897, clip=100, loss_scale=0.25, train_wall=81, gb_free=18.4, wall=11726
2023-05-20 08:48:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:48:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:49:05 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 4.42 | nll_loss 2.402 | ppl 5.28 | bleu 24.18 | wps 956.4 | wpb 2785 | bsz 105.2 | num_updates 2522 | best_loss 4.42
2023-05-20 08:49:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2522 updates
2023-05-20 08:49:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint39.pt
2023-05-20 08:49:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint39.pt
2023-05-20 08:49:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint39.pt (epoch 39 @ 2522 updates, score 4.42) (writing took 41.85841504856944 seconds)
2023-05-20 08:49:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2023-05-20 08:49:47 | INFO | train | epoch 039 | loss 4.358 | nll_loss 2.402 | ppl 5.29 | wps 2381.6 | ups 0.2 | wpb 12045.8 | bsz 559 | num_updates 2522 | lr 2.065e-05 | gnorm 6.56 | clip 100 | loss_scale 0.25 | train_wall 246 | gb_free 18.5 | wall 11810
2023-05-20 08:49:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:49:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:49:47 | INFO | fairseq.trainer | begin training epoch 40
2023-05-20 08:49:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:50:57 | INFO | train_inner | epoch 040:     18 / 65 loss=4.345, nll_loss=2.384, ppl=5.22, wps=1521, ups=0.13, wpb=11688.1, bsz=525.1, num_updates=2540, lr=2.05e-05, gnorm=4.344, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.4, wall=11880
2023-05-20 08:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 08:52:13 | INFO | train_inner | epoch 040:     39 / 65 loss=4.367, nll_loss=2.413, ppl=5.33, wps=3225.5, ups=0.26, wpb=12272, bsz=577.9, num_updates=2560, lr=2.03333e-05, gnorm=4.478, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.3, wall=11956
2023-05-20 08:53:27 | INFO | train_inner | epoch 040:     59 / 65 loss=4.353, nll_loss=2.396, ppl=5.26, wps=3288.6, ups=0.27, wpb=12197.5, bsz=589.1, num_updates=2580, lr=2.01667e-05, gnorm=9.202, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.3, wall=12030
2023-05-20 08:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:53:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:54:21 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 4.438 | nll_loss 2.42 | ppl 5.35 | bleu 24.99 | wps 1044.2 | wpb 2785 | bsz 105.2 | num_updates 2586 | best_loss 4.42
2023-05-20 08:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2586 updates
2023-05-20 08:54:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint40.pt
2023-05-20 08:54:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint40.pt
2023-05-20 08:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint40.pt (epoch 40 @ 2586 updates, score 4.438) (writing took 10.455826435238123 seconds)
2023-05-20 08:54:31 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2023-05-20 08:54:31 | INFO | train | epoch 040 | loss 4.35 | nll_loss 2.393 | ppl 5.25 | wps 2706.4 | ups 0.22 | wpb 12044.2 | bsz 560.1 | num_updates 2586 | lr 2.01167e-05 | gnorm 5.888 | clip 100 | loss_scale 0.125 | train_wall 241 | gb_free 18.7 | wall 12095
2023-05-20 08:54:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:54:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:54:31 | INFO | fairseq.trainer | begin training epoch 41
2023-05-20 08:54:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 08:55:26 | INFO | train_inner | epoch 041:     14 / 65 loss=4.299, nll_loss=2.334, ppl=5.04, wps=1967.9, ups=0.17, wpb=11727.9, bsz=517.1, num_updates=2600, lr=2e-05, gnorm=2.551, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.3, wall=12149
2023-05-20 08:56:45 | INFO | train_inner | epoch 041:     34 / 65 loss=4.372, nll_loss=2.417, ppl=5.34, wps=3119.7, ups=0.25, wpb=12298.7, bsz=564.2, num_updates=2620, lr=1.98333e-05, gnorm=5.882, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.5, wall=12228
2023-05-20 08:58:00 | INFO | train_inner | epoch 041:     54 / 65 loss=4.327, nll_loss=2.362, ppl=5.14, wps=3228.2, ups=0.27, wpb=12162.7, bsz=592.1, num_updates=2640, lr=1.96667e-05, gnorm=13.257, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.5, wall=12303
2023-05-20 08:58:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 08:58:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:59:14 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 4.44 | nll_loss 2.417 | ppl 5.34 | bleu 25.12 | wps 1025.6 | wpb 2785 | bsz 105.2 | num_updates 2651 | best_loss 4.42
2023-05-20 08:59:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2651 updates
2023-05-20 08:59:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint41.pt
2023-05-20 08:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint41.pt
2023-05-20 08:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint41.pt (epoch 41 @ 2651 updates, score 4.44) (writing took 10.62398125976324 seconds)
2023-05-20 08:59:25 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2023-05-20 08:59:25 | INFO | train | epoch 041 | loss 4.335 | nll_loss 2.375 | ppl 5.19 | wps 2668.5 | ups 0.22 | wpb 12053.7 | bsz 559.7 | num_updates 2651 | lr 1.9575e-05 | gnorm 7.031 | clip 100 | loss_scale 0.125 | train_wall 249 | gb_free 18.5 | wall 12388
2023-05-20 08:59:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 08:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 08:59:25 | INFO | fairseq.trainer | begin training epoch 42
2023-05-20 08:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:00:01 | INFO | train_inner | epoch 042:      9 / 65 loss=4.297, nll_loss=2.335, ppl=5.05, wps=1890.6, ups=0.17, wpb=11434.8, bsz=511.2, num_updates=2660, lr=1.95e-05, gnorm=4.525, clip=100, loss_scale=0.125, train_wall=77, gb_free=18.5, wall=12424
2023-05-20 09:01:16 | INFO | train_inner | epoch 042:     29 / 65 loss=4.332, nll_loss=2.372, ppl=5.18, wps=3249.7, ups=0.27, wpb=12186.9, bsz=569.1, num_updates=2680, lr=1.93333e-05, gnorm=12.63, clip=100, loss_scale=0.125, train_wall=75, gb_free=17.9, wall=12499
2023-05-20 09:02:29 | INFO | train_inner | epoch 042:     49 / 65 loss=4.331, nll_loss=2.371, ppl=5.17, wps=3380.7, ups=0.27, wpb=12323.7, bsz=581.8, num_updates=2700, lr=1.91667e-05, gnorm=3.875, clip=100, loss_scale=0.125, train_wall=73, gb_free=17.5, wall=12572
2023-05-20 09:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:03:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:04:03 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 4.419 | nll_loss 2.427 | ppl 5.38 | bleu 24.81 | wps 981.9 | wpb 2785 | bsz 105.2 | num_updates 2716 | best_loss 4.419
2023-05-20 09:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2716 updates
2023-05-20 09:04:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint42.pt
2023-05-20 09:04:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint42.pt
2023-05-20 09:04:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint42.pt (epoch 42 @ 2716 updates, score 4.419) (writing took 28.20744875818491 seconds)
2023-05-20 09:04:32 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2023-05-20 09:04:32 | INFO | train | epoch 042 | loss 4.32 | nll_loss 2.358 | ppl 5.13 | wps 2556.1 | ups 0.21 | wpb 12051.8 | bsz 559.6 | num_updates 2716 | lr 1.90333e-05 | gnorm 7.021 | clip 100 | loss_scale 0.125 | train_wall 244 | gb_free 18.3 | wall 12695
2023-05-20 09:04:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:04:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:04:32 | INFO | fairseq.trainer | begin training epoch 43
2023-05-20 09:04:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:04:47 | INFO | train_inner | epoch 043:      4 / 65 loss=4.317, nll_loss=2.356, ppl=5.12, wps=1717.6, ups=0.15, wpb=11799.7, bsz=537.5, num_updates=2720, lr=1.9e-05, gnorm=4.662, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.2, wall=12710
2023-05-20 09:06:02 | INFO | train_inner | epoch 043:     24 / 65 loss=4.29, nll_loss=2.321, ppl=5, wps=3220.1, ups=0.27, wpb=12066, bsz=572.4, num_updates=2740, lr=1.88333e-05, gnorm=4.743, clip=100, loss_scale=0.125, train_wall=75, gb_free=16.4, wall=12785
2023-05-20 09:07:22 | INFO | train_inner | epoch 043:     44 / 65 loss=4.28, nll_loss=2.315, ppl=4.97, wps=3079, ups=0.25, wpb=12300.6, bsz=581.3, num_updates=2760, lr=1.86667e-05, gnorm=7.489, clip=100, loss_scale=0.125, train_wall=80, gb_free=18.2, wall=12865
2023-05-20 09:08:36 | INFO | train_inner | epoch 043:     64 / 65 loss=4.277, nll_loss=2.309, ppl=4.96, wps=3283.7, ups=0.27, wpb=12179.6, bsz=552.8, num_updates=2780, lr=1.85e-05, gnorm=18.511, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.3, wall=12939
2023-05-20 09:08:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:08:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:09:13 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 4.425 | nll_loss 2.411 | ppl 5.32 | bleu 24.64 | wps 952.7 | wpb 2785 | bsz 105.2 | num_updates 2781 | best_loss 4.419
2023-05-20 09:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2781 updates
2023-05-20 09:09:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint43.pt
2023-05-20 09:09:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint43.pt
2023-05-20 09:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint43.pt (epoch 43 @ 2781 updates, score 4.425) (writing took 16.740025963634253 seconds)
2023-05-20 09:09:30 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2023-05-20 09:09:30 | INFO | train | epoch 043 | loss 4.282 | nll_loss 2.315 | ppl 4.98 | wps 2621.6 | ups 0.22 | wpb 12046.1 | bsz 558.5 | num_updates 2781 | lr 1.84917e-05 | gnorm 9.827 | clip 100 | loss_scale 0.125 | train_wall 246 | gb_free 18.7 | wall 12993
2023-05-20 09:09:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:09:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:09:30 | INFO | fairseq.trainer | begin training epoch 44
2023-05-20 09:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:10:41 | INFO | train_inner | epoch 044:     19 / 65 loss=4.296, nll_loss=2.33, ppl=5.03, wps=1886.4, ups=0.16, wpb=11850.3, bsz=578.8, num_updates=2800, lr=1.83333e-05, gnorm=4.989, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.8, wall=13064
2023-05-20 09:12:01 | INFO | train_inner | epoch 044:     39 / 65 loss=4.24, nll_loss=2.27, ppl=4.82, wps=3037.6, ups=0.25, wpb=12103.2, bsz=552.4, num_updates=2820, lr=1.81667e-05, gnorm=4.507, clip=100, loss_scale=0.125, train_wall=80, gb_free=17.4, wall=13144
2023-05-20 09:13:20 | INFO | train_inner | epoch 044:     59 / 65 loss=4.253, nll_loss=2.284, ppl=4.87, wps=3113.6, ups=0.25, wpb=12219.7, bsz=544.2, num_updates=2840, lr=1.8e-05, gnorm=3.639, clip=100, loss_scale=0.125, train_wall=78, gb_free=17.7, wall=13223
2023-05-20 09:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:13:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:14:15 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 4.443 | nll_loss 2.433 | ppl 5.4 | bleu 24.58 | wps 1036.5 | wpb 2785 | bsz 105.2 | num_updates 2846 | best_loss 4.419
2023-05-20 09:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2846 updates
2023-05-20 09:14:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint44.pt
2023-05-20 09:14:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint44.pt
2023-05-20 09:14:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint44.pt (epoch 44 @ 2846 updates, score 4.443) (writing took 11.080710615962744 seconds)
2023-05-20 09:14:28 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2023-05-20 09:14:28 | INFO | train | epoch 044 | loss 4.26 | nll_loss 2.291 | ppl 4.89 | wps 2626.9 | ups 0.22 | wpb 12040.4 | bsz 557 | num_updates 2846 | lr 1.795e-05 | gnorm 4.659 | clip 100 | loss_scale 0.125 | train_wall 251 | gb_free 18.3 | wall 13291
2023-05-20 09:14:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:14:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:14:28 | INFO | fairseq.trainer | begin training epoch 45
2023-05-20 09:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:15:21 | INFO | train_inner | epoch 045:     14 / 65 loss=4.274, nll_loss=2.306, ppl=4.94, wps=1895.2, ups=0.16, wpb=11552.5, bsz=532.9, num_updates=2860, lr=1.78333e-05, gnorm=6.166, clip=100, loss_scale=0.125, train_wall=76, gb_free=17.9, wall=13345
2023-05-20 09:16:39 | INFO | train_inner | epoch 045:     34 / 65 loss=4.264, nll_loss=2.295, ppl=4.91, wps=3162, ups=0.26, wpb=12241.9, bsz=582.4, num_updates=2880, lr=1.76667e-05, gnorm=6.634, clip=100, loss_scale=0.125, train_wall=77, gb_free=18.4, wall=13422
2023-05-20 09:17:57 | INFO | train_inner | epoch 045:     54 / 65 loss=4.227, nll_loss=2.255, ppl=4.77, wps=3089.7, ups=0.25, wpb=12137.1, bsz=543.5, num_updates=2900, lr=1.75e-05, gnorm=4.774, clip=100, loss_scale=0.125, train_wall=78, gb_free=18, wall=13500
2023-05-20 09:18:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:18:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:19:11 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 4.424 | nll_loss 2.411 | ppl 5.32 | bleu 24.97 | wps 971.4 | wpb 2785 | bsz 105.2 | num_updates 2911 | best_loss 4.419
2023-05-20 09:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2911 updates
2023-05-20 09:19:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint45.pt
2023-05-20 09:19:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint45.pt
2023-05-20 09:19:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint45.pt (epoch 45 @ 2911 updates, score 4.424) (writing took 17.64903174713254 seconds)
2023-05-20 09:19:29 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2023-05-20 09:19:29 | INFO | train | epoch 045 | loss 4.255 | nll_loss 2.286 | ppl 4.88 | wps 2608.2 | ups 0.22 | wpb 12059 | bsz 559.4 | num_updates 2911 | lr 1.74083e-05 | gnorm 5.289 | clip 100 | loss_scale 0.125 | train_wall 248 | gb_free 18.3 | wall 13592
2023-05-20 09:19:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:19:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:19:29 | INFO | fairseq.trainer | begin training epoch 46
2023-05-20 09:19:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:20:02 | INFO | train_inner | epoch 046:      9 / 65 loss=4.222, nll_loss=2.249, ppl=4.75, wps=1892.8, ups=0.16, wpb=11817, bsz=567.5, num_updates=2920, lr=1.73333e-05, gnorm=4.955, clip=100, loss_scale=0.125, train_wall=72, gb_free=18.2, wall=13625
2023-05-20 09:21:16 | INFO | train_inner | epoch 046:     29 / 65 loss=4.223, nll_loss=2.249, ppl=4.75, wps=3296.4, ups=0.27, wpb=12076.7, bsz=550.6, num_updates=2940, lr=1.71667e-05, gnorm=4.033, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.6, wall=13699
2023-05-20 09:22:34 | INFO | train_inner | epoch 046:     49 / 65 loss=4.253, nll_loss=2.282, ppl=4.86, wps=3100.2, ups=0.25, wpb=12203.6, bsz=568.8, num_updates=2960, lr=1.7e-05, gnorm=6.145, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.5, wall=13777
2023-05-20 09:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:23:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:24:11 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 4.434 | nll_loss 2.42 | ppl 5.35 | bleu 24.35 | wps 1017.1 | wpb 2785 | bsz 105.2 | num_updates 2976 | best_loss 4.419
2023-05-20 09:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2976 updates
2023-05-20 09:24:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint46.pt
2023-05-20 09:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint46.pt
2023-05-20 09:24:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint46.pt (epoch 46 @ 2976 updates, score 4.434) (writing took 10.802663657814264 seconds)
2023-05-20 09:24:22 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2023-05-20 09:24:22 | INFO | train | epoch 046 | loss 4.235 | nll_loss 2.263 | ppl 4.8 | wps 2672 | ups 0.22 | wpb 12041.7 | bsz 559 | num_updates 2976 | lr 1.68667e-05 | gnorm 5.053 | clip 100 | loss_scale 0.125 | train_wall 248 | gb_free 18.4 | wall 13885
2023-05-20 09:24:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:24:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:24:22 | INFO | fairseq.trainer | begin training epoch 47
2023-05-20 09:24:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:24:39 | INFO | train_inner | epoch 047:      4 / 65 loss=4.238, nll_loss=2.268, ppl=4.82, wps=1893.5, ups=0.16, wpb=11788, bsz=529.1, num_updates=2980, lr=1.68333e-05, gnorm=3.961, clip=100, loss_scale=0.125, train_wall=80, gb_free=18.5, wall=13902
2023-05-20 09:25:53 | INFO | train_inner | epoch 047:     24 / 65 loss=4.225, nll_loss=2.253, ppl=4.77, wps=3301.7, ups=0.27, wpb=12173.6, bsz=551.6, num_updates=3000, lr=1.66667e-05, gnorm=2.959, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.2, wall=13976
2023-05-20 09:27:10 | INFO | train_inner | epoch 047:     44 / 65 loss=4.234, nll_loss=2.263, ppl=4.8, wps=3178.9, ups=0.26, wpb=12235.4, bsz=580.3, num_updates=3020, lr=1.65e-05, gnorm=3.19, clip=100, loss_scale=0.125, train_wall=77, gb_free=17.8, wall=14053
2023-05-20 09:28:24 | INFO | train_inner | epoch 047:     64 / 65 loss=4.282, nll_loss=2.318, ppl=4.99, wps=3277.4, ups=0.27, wpb=12248.7, bsz=575.8, num_updates=3040, lr=1.63333e-05, gnorm=2.673, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.5, wall=14127
2023-05-20 09:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:28:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:29:00 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 4.446 | nll_loss 2.44 | ppl 5.43 | bleu 25.35 | wps 998.7 | wpb 2785 | bsz 105.2 | num_updates 3041 | best_loss 4.419
2023-05-20 09:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3041 updates
2023-05-20 09:29:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint47.pt
2023-05-20 09:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint47.pt
2023-05-20 09:29:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint47.pt (epoch 47 @ 3041 updates, score 4.446) (writing took 13.820891555398703 seconds)
2023-05-20 09:29:14 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2023-05-20 09:29:14 | INFO | train | epoch 047 | loss 4.246 | nll_loss 2.277 | ppl 4.85 | wps 2677.9 | ups 0.22 | wpb 12059.9 | bsz 559.5 | num_updates 3041 | lr 1.6325e-05 | gnorm 2.911 | clip 100 | loss_scale 0.125 | train_wall 245 | gb_free 18.2 | wall 14177
2023-05-20 09:29:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:29:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:29:14 | INFO | fairseq.trainer | begin training epoch 48
2023-05-20 09:29:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:30:27 | INFO | train_inner | epoch 048:     19 / 65 loss=4.248, nll_loss=2.276, ppl=4.84, wps=1918.7, ups=0.16, wpb=11776.5, bsz=547.8, num_updates=3060, lr=1.61667e-05, gnorm=3.182, clip=100, loss_scale=0.125, train_wall=75, gb_free=17.9, wall=14250
2023-05-20 09:31:46 | INFO | train_inner | epoch 048:     39 / 65 loss=4.186, nll_loss=2.209, ppl=4.62, wps=3080.3, ups=0.25, wpb=12149.7, bsz=543.6, num_updates=3080, lr=1.6e-05, gnorm=2.834, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.5, wall=14329
2023-05-20 09:33:00 | INFO | train_inner | epoch 048:     59 / 65 loss=4.219, nll_loss=2.245, ppl=4.74, wps=3260.8, ups=0.27, wpb=12139.7, bsz=586.8, num_updates=3100, lr=1.58333e-05, gnorm=4.125, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.3, wall=14403
2023-05-20 09:33:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:33:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:33:49 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 4.437 | nll_loss 2.428 | ppl 5.38 | bleu 24.73 | wps 1150.4 | wpb 2785 | bsz 105.2 | num_updates 3106 | best_loss 4.419
2023-05-20 09:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3106 updates
2023-05-20 09:33:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint48.pt
2023-05-20 09:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint48.pt
2023-05-20 09:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint48.pt (epoch 48 @ 3106 updates, score 4.437) (writing took 19.72737055271864 seconds)
2023-05-20 09:34:09 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2023-05-20 09:34:09 | INFO | train | epoch 048 | loss 4.215 | nll_loss 2.24 | ppl 4.73 | wps 2661 | ups 0.22 | wpb 12056.3 | bsz 559.5 | num_updates 3106 | lr 1.57833e-05 | gnorm 3.636 | clip 100 | loss_scale 0.25 | train_wall 245 | gb_free 18.3 | wall 14472
2023-05-20 09:34:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:34:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:34:09 | INFO | fairseq.trainer | begin training epoch 49
2023-05-20 09:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:35:01 | INFO | train_inner | epoch 049:     14 / 65 loss=4.198, nll_loss=2.222, ppl=4.67, wps=1945.8, ups=0.17, wpb=11753.5, bsz=541.1, num_updates=3120, lr=1.56667e-05, gnorm=6.47, clip=100, loss_scale=0.25, train_wall=71, gb_free=18.1, wall=14524
2023-05-20 09:36:20 | INFO | train_inner | epoch 049:     34 / 65 loss=4.199, nll_loss=2.221, ppl=4.66, wps=3068, ups=0.25, wpb=12123.2, bsz=566.3, num_updates=3140, lr=1.55e-05, gnorm=6.887, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.2, wall=14603
2023-05-20 09:36:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 09:37:39 | INFO | train_inner | epoch 049:     55 / 65 loss=4.238, nll_loss=2.268, ppl=4.82, wps=3103.8, ups=0.25, wpb=12255.7, bsz=559.5, num_updates=3160, lr=1.53333e-05, gnorm=6.017, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.3, wall=14682
2023-05-20 09:38:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:38:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:38:48 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 4.46 | nll_loss 2.456 | ppl 5.49 | bleu 24.26 | wps 1040.3 | wpb 2785 | bsz 105.2 | num_updates 3170 | best_loss 4.419
2023-05-20 09:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3170 updates
2023-05-20 09:38:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint49.pt
2023-05-20 09:38:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint49.pt
2023-05-20 09:38:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint49.pt (epoch 49 @ 3170 updates, score 4.46) (writing took 10.509361051023006 seconds)
2023-05-20 09:38:59 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2023-05-20 09:38:59 | INFO | train | epoch 049 | loss 4.209 | nll_loss 2.234 | ppl 4.7 | wps 2658 | ups 0.22 | wpb 12030.9 | bsz 557.4 | num_updates 3170 | lr 1.525e-05 | gnorm 6.059 | clip 100 | loss_scale 0.125 | train_wall 246 | gb_free 18.2 | wall 14762
2023-05-20 09:38:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:38:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:38:59 | INFO | fairseq.trainer | begin training epoch 50
2023-05-20 09:38:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:39:37 | INFO | train_inner | epoch 050:     10 / 65 loss=4.191, nll_loss=2.216, ppl=4.65, wps=2005, ups=0.17, wpb=11817.2, bsz=558.4, num_updates=3180, lr=1.51667e-05, gnorm=4.241, clip=100, loss_scale=0.125, train_wall=75, gb_free=18, wall=14800
2023-05-20 09:40:57 | INFO | train_inner | epoch 050:     30 / 65 loss=4.171, nll_loss=2.193, ppl=4.57, wps=3039.8, ups=0.25, wpb=12209.4, bsz=553.2, num_updates=3200, lr=1.5e-05, gnorm=4.168, clip=100, loss_scale=0.125, train_wall=80, gb_free=18.2, wall=14880
2023-05-20 09:42:11 | INFO | train_inner | epoch 050:     50 / 65 loss=4.222, nll_loss=2.249, ppl=4.75, wps=3315.5, ups=0.27, wpb=12221.2, bsz=578.5, num_updates=3220, lr=1.48333e-05, gnorm=4.086, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.2, wall=14954
2023-05-20 09:43:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:43:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:43:42 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 4.438 | nll_loss 2.424 | ppl 5.37 | bleu 23.98 | wps 945.2 | wpb 2785 | bsz 105.2 | num_updates 3235 | best_loss 4.419
2023-05-20 09:43:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3235 updates
2023-05-20 09:43:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint50.pt
2023-05-20 09:43:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint50.pt
2023-05-20 09:43:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint50.pt (epoch 50 @ 3235 updates, score 4.438) (writing took 16.877544589340687 seconds)
2023-05-20 09:43:59 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2023-05-20 09:43:59 | INFO | train | epoch 050 | loss 4.194 | nll_loss 2.218 | ppl 4.65 | wps 2605.7 | ups 0.22 | wpb 12056.7 | bsz 559.7 | num_updates 3235 | lr 1.47083e-05 | gnorm 4.967 | clip 100 | loss_scale 0.125 | train_wall 245 | gb_free 18.4 | wall 15062
2023-05-20 09:43:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:43:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:43:59 | INFO | fairseq.trainer | begin training epoch 51
2023-05-20 09:43:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:44:19 | INFO | train_inner | epoch 051:      5 / 65 loss=4.222, nll_loss=2.246, ppl=4.74, wps=1840.5, ups=0.16, wpb=11740.5, bsz=557.5, num_updates=3240, lr=1.46667e-05, gnorm=15.322, clip=100, loss_scale=0.125, train_wall=72, gb_free=18.5, wall=15082
2023-05-20 09:45:34 | INFO | train_inner | epoch 051:     25 / 65 loss=4.151, nll_loss=2.169, ppl=4.5, wps=3218.8, ups=0.27, wpb=12113.8, bsz=555.3, num_updates=3260, lr=1.45e-05, gnorm=5.432, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.4, wall=15157
2023-05-20 09:46:52 | INFO | train_inner | epoch 051:     45 / 65 loss=4.169, nll_loss=2.191, ppl=4.57, wps=3155.8, ups=0.26, wpb=12257.9, bsz=547.1, num_updates=3280, lr=1.43333e-05, gnorm=5.947, clip=100, loss_scale=0.125, train_wall=78, gb_free=18, wall=15235
2023-05-20 09:48:08 | INFO | train_inner | epoch 051:     65 / 65 loss=4.151, nll_loss=2.171, ppl=4.5, wps=3048.3, ups=0.26, wpb=11620.6, bsz=552.6, num_updates=3300, lr=1.41667e-05, gnorm=5.303, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.8, wall=15311
2023-05-20 09:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:48:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:48:41 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 4.419 | nll_loss 2.418 | ppl 5.35 | bleu 25.63 | wps 1031 | wpb 2785 | bsz 105.2 | num_updates 3300 | best_loss 4.419
2023-05-20 09:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3300 updates
2023-05-20 09:48:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint51.pt
2023-05-20 09:48:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint51.pt
2023-05-20 09:49:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint51.pt (epoch 51 @ 3300 updates, score 4.419) (writing took 42.850924246013165 seconds)
2023-05-20 09:49:23 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2023-05-20 09:49:24 | INFO | train | epoch 051 | loss 4.169 | nll_loss 2.189 | ppl 4.56 | wps 2410.3 | ups 0.2 | wpb 12042.7 | bsz 558.3 | num_updates 3300 | lr 1.41667e-05 | gnorm 8.143 | clip 100 | loss_scale 0.125 | train_wall 248 | gb_free 18.8 | wall 15387
2023-05-20 09:49:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:49:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:49:24 | INFO | fairseq.trainer | begin training epoch 52
2023-05-20 09:49:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:50:43 | INFO | train_inner | epoch 052:     20 / 65 loss=4.15, nll_loss=2.169, ppl=4.5, wps=1567.3, ups=0.13, wpb=12164.8, bsz=565.4, num_updates=3320, lr=1.4e-05, gnorm=3.266, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.4, wall=15466
2023-05-20 09:52:00 | INFO | train_inner | epoch 052:     40 / 65 loss=4.127, nll_loss=2.142, ppl=4.41, wps=3117.8, ups=0.26, wpb=12014.4, bsz=522, num_updates=3340, lr=1.38333e-05, gnorm=6.485, clip=100, loss_scale=0.125, train_wall=77, gb_free=18.4, wall=15543
2023-05-20 09:53:17 | INFO | train_inner | epoch 052:     60 / 65 loss=4.191, nll_loss=2.214, ppl=4.64, wps=3195.6, ups=0.26, wpb=12323.9, bsz=592.2, num_updates=3360, lr=1.36667e-05, gnorm=5.288, clip=100, loss_scale=0.125, train_wall=77, gb_free=18, wall=15620
2023-05-20 09:53:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:53:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:54:12 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 4.41 | nll_loss 2.402 | ppl 5.29 | bleu 24.89 | wps 855 | wpb 2785 | bsz 105.2 | num_updates 3365 | best_loss 4.41
2023-05-20 09:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3365 updates
2023-05-20 09:54:12 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint52.pt
2023-05-20 09:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint52.pt
2023-05-20 09:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint52.pt (epoch 52 @ 3365 updates, score 4.41) (writing took 36.91660463064909 seconds)
2023-05-20 09:54:49 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2023-05-20 09:54:49 | INFO | train | epoch 052 | loss 4.161 | nll_loss 2.18 | ppl 4.53 | wps 2412.2 | ups 0.2 | wpb 12054.4 | bsz 559 | num_updates 3365 | lr 1.3625e-05 | gnorm 5.567 | clip 100 | loss_scale 0.125 | train_wall 248 | gb_free 18.6 | wall 15712
2023-05-20 09:54:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:54:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:54:49 | INFO | fairseq.trainer | begin training epoch 53
2023-05-20 09:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 09:55:48 | INFO | train_inner | epoch 053:     15 / 65 loss=4.178, nll_loss=2.197, ppl=4.59, wps=1575.1, ups=0.13, wpb=11895.2, bsz=571.7, num_updates=3380, lr=1.35e-05, gnorm=4.951, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.5, wall=15771
2023-05-20 09:57:06 | INFO | train_inner | epoch 053:     35 / 65 loss=4.111, nll_loss=2.123, ppl=4.36, wps=3112.6, ups=0.26, wpb=12111.4, bsz=534.1, num_updates=3400, lr=1.33333e-05, gnorm=4.796, clip=100, loss_scale=0.125, train_wall=78, gb_free=18, wall=15849
2023-05-20 09:58:23 | INFO | train_inner | epoch 053:     55 / 65 loss=4.153, nll_loss=2.17, ppl=4.5, wps=3197.4, ups=0.26, wpb=12196.1, bsz=575.4, num_updates=3420, lr=1.31667e-05, gnorm=5.925, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.4, wall=15926
2023-05-20 09:58:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 09:58:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:59:28 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 4.438 | nll_loss 2.428 | ppl 5.38 | bleu 25.66 | wps 1035.4 | wpb 2785 | bsz 105.2 | num_updates 3430 | best_loss 4.41
2023-05-20 09:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 3430 updates
2023-05-20 09:59:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint53.pt
2023-05-20 09:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint53.pt
2023-05-20 09:59:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint53.pt (epoch 53 @ 3430 updates, score 4.438) (writing took 10.936967927962542 seconds)
2023-05-20 09:59:39 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2023-05-20 09:59:39 | INFO | train | epoch 053 | loss 4.142 | nll_loss 2.158 | ppl 4.46 | wps 2696.7 | ups 0.22 | wpb 12054.2 | bsz 558.7 | num_updates 3430 | lr 1.30833e-05 | gnorm 4.947 | clip 100 | loss_scale 0.125 | train_wall 247 | gb_free 18.4 | wall 16002
2023-05-20 09:59:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 09:59:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 09:59:39 | INFO | fairseq.trainer | begin training epoch 54
2023-05-20 09:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:00:17 | INFO | train_inner | epoch 054:     10 / 65 loss=4.123, nll_loss=2.138, ppl=4.4, wps=2032.7, ups=0.17, wpb=11644, bsz=538.6, num_updates=3440, lr=1.3e-05, gnorm=5.323, clip=100, loss_scale=0.125, train_wall=71, gb_free=18.6, wall=16040
2023-05-20 10:01:36 | INFO | train_inner | epoch 054:     30 / 65 loss=4.144, nll_loss=2.161, ppl=4.47, wps=3096.7, ups=0.25, wpb=12155.6, bsz=557.5, num_updates=3460, lr=1.28333e-05, gnorm=5.985, clip=100, loss_scale=0.125, train_wall=78, gb_free=18.5, wall=16119
2023-05-20 10:02:48 | INFO | train_inner | epoch 054:     50 / 65 loss=4.161, nll_loss=2.181, ppl=4.54, wps=3344.5, ups=0.28, wpb=12156.2, bsz=587.5, num_updates=3480, lr=1.26667e-05, gnorm=4.573, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.6, wall=16191
2023-05-20 10:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:03:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:04:09 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.467 | nll_loss 2.46 | ppl 5.5 | bleu 25.46 | wps 1279.9 | wpb 2785 | bsz 105.2 | num_updates 3495 | best_loss 4.41
2023-05-20 10:04:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 3495 updates
2023-05-20 10:04:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint54.pt
2023-05-20 10:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint54.pt
2023-05-20 10:04:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint54.pt (epoch 54 @ 3495 updates, score 4.467) (writing took 11.171020984649658 seconds)
2023-05-20 10:04:20 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2023-05-20 10:04:20 | INFO | train | epoch 054 | loss 4.155 | nll_loss 2.175 | ppl 4.51 | wps 2791.7 | ups 0.23 | wpb 12057.9 | bsz 560.1 | num_updates 3495 | lr 1.25417e-05 | gnorm 4.96 | clip 100 | loss_scale 0.125 | train_wall 242 | gb_free 18.8 | wall 16283
2023-05-20 10:04:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:04:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:04:20 | INFO | fairseq.trainer | begin training epoch 55
2023-05-20 10:04:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:04:42 | INFO | train_inner | epoch 055:      5 / 65 loss=4.187, nll_loss=2.213, ppl=4.64, wps=2095.3, ups=0.18, wpb=11879.3, bsz=535.8, num_updates=3500, lr=1.25e-05, gnorm=4.811, clip=100, loss_scale=0.125, train_wall=75, gb_free=17.7, wall=16305
2023-05-20 10:05:59 | INFO | train_inner | epoch 055:     25 / 65 loss=4.149, nll_loss=2.167, ppl=4.49, wps=3166.7, ups=0.26, wpb=12315.1, bsz=574.9, num_updates=3520, lr=1.23333e-05, gnorm=4.421, clip=100, loss_scale=0.125, train_wall=78, gb_free=17.4, wall=16383
2023-05-20 10:07:16 | INFO | train_inner | epoch 055:     45 / 65 loss=4.134, nll_loss=2.152, ppl=4.44, wps=3170.3, ups=0.26, wpb=12100.1, bsz=559.5, num_updates=3540, lr=1.21667e-05, gnorm=3.959, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.5, wall=16459
2023-05-20 10:08:27 | INFO | train_inner | epoch 055:     65 / 65 loss=4.176, nll_loss=2.197, ppl=4.59, wps=3299.1, ups=0.28, wpb=11759.6, bsz=553.8, num_updates=3560, lr=1.2e-05, gnorm=3.627, clip=100, loss_scale=0.125, train_wall=71, gb_free=18.7, wall=16530
2023-05-20 10:08:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:08:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:08:59 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 4.446 | nll_loss 2.441 | ppl 5.43 | bleu 25.08 | wps 1056.3 | wpb 2785 | bsz 105.2 | num_updates 3560 | best_loss 4.41
2023-05-20 10:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 3560 updates
2023-05-20 10:08:59 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint55.pt
2023-05-20 10:09:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint55.pt
2023-05-20 10:09:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint55.pt (epoch 55 @ 3560 updates, score 4.446) (writing took 10.160246267914772 seconds)
2023-05-20 10:09:09 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2023-05-20 10:09:09 | INFO | train | epoch 055 | loss 4.153 | nll_loss 2.172 | ppl 4.51 | wps 2712 | ups 0.22 | wpb 12053.6 | bsz 558.5 | num_updates 3560 | lr 1.2e-05 | gnorm 4.037 | clip 100 | loss_scale 0.125 | train_wall 246 | gb_free 18.7 | wall 16572
2023-05-20 10:09:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:09:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:09:09 | INFO | fairseq.trainer | begin training epoch 56
2023-05-20 10:09:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:10:25 | INFO | train_inner | epoch 056:     20 / 65 loss=4.123, nll_loss=2.137, ppl=4.4, wps=2066.4, ups=0.17, wpb=12211.8, bsz=554.1, num_updates=3580, lr=1.18333e-05, gnorm=6.607, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.5, wall=16648
2023-05-20 10:11:42 | INFO | train_inner | epoch 056:     40 / 65 loss=4.182, nll_loss=2.206, ppl=4.61, wps=3187.8, ups=0.26, wpb=12270.9, bsz=583.7, num_updates=3600, lr=1.16667e-05, gnorm=7.985, clip=100, loss_scale=0.125, train_wall=77, gb_free=18, wall=16725
2023-05-20 10:12:59 | INFO | train_inner | epoch 056:     60 / 65 loss=4.194, nll_loss=2.221, ppl=4.66, wps=3171.6, ups=0.26, wpb=12174.9, bsz=555.6, num_updates=3620, lr=1.15e-05, gnorm=4.828, clip=100, loss_scale=0.125, train_wall=77, gb_free=18.6, wall=16802
2023-05-20 10:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:13:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:13:50 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.473 | nll_loss 2.478 | ppl 5.57 | bleu 25.04 | wps 984 | wpb 2785 | bsz 105.2 | num_updates 3625 | best_loss 4.41
2023-05-20 10:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 3625 updates
2023-05-20 10:13:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint56.pt
2023-05-20 10:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint56.pt
2023-05-20 10:14:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint56.pt (epoch 56 @ 3625 updates, score 4.473) (writing took 11.897232860326767 seconds)
2023-05-20 10:14:02 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2023-05-20 10:14:02 | INFO | train | epoch 056 | loss 4.166 | nll_loss 2.188 | ppl 4.56 | wps 2678.2 | ups 0.22 | wpb 12055.3 | bsz 560 | num_updates 3625 | lr 1.14583e-05 | gnorm 6.312 | clip 100 | loss_scale 0.125 | train_wall 246 | gb_free 18.4 | wall 16865
2023-05-20 10:14:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:14:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:14:02 | INFO | fairseq.trainer | begin training epoch 57
2023-05-20 10:14:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:14:59 | INFO | train_inner | epoch 057:     15 / 65 loss=4.179, nll_loss=2.202, ppl=4.6, wps=1956.2, ups=0.17, wpb=11757.7, bsz=567.5, num_updates=3640, lr=1.13333e-05, gnorm=6.36, clip=100, loss_scale=0.125, train_wall=74, gb_free=17.9, wall=16922
2023-05-20 10:16:15 | INFO | train_inner | epoch 057:     35 / 65 loss=4.116, nll_loss=2.128, ppl=4.37, wps=3242.3, ups=0.26, wpb=12268.9, bsz=572.1, num_updates=3660, lr=1.11667e-05, gnorm=3.053, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.1, wall=16998
2023-05-20 10:17:30 | INFO | train_inner | epoch 057:     55 / 65 loss=4.1, nll_loss=2.113, ppl=4.33, wps=3243.2, ups=0.27, wpb=12099.6, bsz=564.9, num_updates=3680, lr=1.1e-05, gnorm=3.182, clip=100, loss_scale=0.25, train_wall=75, gb_free=17.2, wall=17073
2023-05-20 10:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:18:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:18:43 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.431 | nll_loss 2.429 | ppl 5.39 | bleu 25.05 | wps 924 | wpb 2785 | bsz 105.2 | num_updates 3690 | best_loss 4.41
2023-05-20 10:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 3690 updates
2023-05-20 10:18:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint57.pt
2023-05-20 10:18:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint57.pt
2023-05-20 10:18:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint57.pt (epoch 57 @ 3690 updates, score 4.431) (writing took 11.470728389918804 seconds)
2023-05-20 10:18:54 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2023-05-20 10:18:54 | INFO | train | epoch 057 | loss 4.123 | nll_loss 2.138 | ppl 4.4 | wps 2677.6 | ups 0.22 | wpb 12056.3 | bsz 559.9 | num_updates 3690 | lr 1.09167e-05 | gnorm 3.929 | clip 100 | loss_scale 0.25 | train_wall 244 | gb_free 18.3 | wall 17157
2023-05-20 10:18:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:18:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:18:54 | INFO | fairseq.trainer | begin training epoch 58
2023-05-20 10:18:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:19:31 | INFO | train_inner | epoch 058:     10 / 65 loss=4.11, nll_loss=2.125, ppl=4.36, wps=1935.8, ups=0.16, wpb=11734.8, bsz=563, num_updates=3700, lr=1.08333e-05, gnorm=2.756, clip=100, loss_scale=0.25, train_wall=73, gb_free=17.9, wall=17194
2023-05-20 10:20:53 | INFO | train_inner | epoch 058:     30 / 65 loss=4.1, nll_loss=2.109, ppl=4.31, wps=2934.8, ups=0.24, wpb=12093.7, bsz=546.7, num_updates=3720, lr=1.06667e-05, gnorm=4.094, clip=100, loss_scale=0.25, train_wall=82, gb_free=18.3, wall=17276
2023-05-20 10:22:12 | INFO | train_inner | epoch 058:     50 / 65 loss=4.108, nll_loss=2.12, ppl=4.35, wps=3048.6, ups=0.25, wpb=12086.6, bsz=543.4, num_updates=3740, lr=1.05e-05, gnorm=4.605, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.2, wall=17356
2023-05-20 10:23:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:23:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:23:38 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.44 | nll_loss 2.427 | ppl 5.38 | bleu 25.35 | wps 1094.4 | wpb 2785 | bsz 105.2 | num_updates 3755 | best_loss 4.41
2023-05-20 10:23:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 3755 updates
2023-05-20 10:23:38 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint58.pt
2023-05-20 10:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint58.pt
2023-05-20 10:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint58.pt (epoch 58 @ 3755 updates, score 4.44) (writing took 16.854385904967785 seconds)
2023-05-20 10:23:55 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2023-05-20 10:23:55 | INFO | train | epoch 058 | loss 4.11 | nll_loss 2.123 | ppl 4.36 | wps 2602.3 | ups 0.22 | wpb 12050.4 | bsz 559.6 | num_updates 3755 | lr 1.0375e-05 | gnorm 5.19 | clip 100 | loss_scale 0.25 | train_wall 253 | gb_free 18.5 | wall 17458
2023-05-20 10:23:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:23:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:23:55 | INFO | fairseq.trainer | begin training epoch 59
2023-05-20 10:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:24:15 | INFO | train_inner | epoch 059:      5 / 65 loss=4.102, nll_loss=2.113, ppl=4.33, wps=1937.6, ups=0.16, wpb=11832.7, bsz=553.9, num_updates=3760, lr=1.03333e-05, gnorm=7.505, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.8, wall=17478
2023-05-20 10:25:34 | INFO | train_inner | epoch 059:     25 / 65 loss=4.104, nll_loss=2.119, ppl=4.34, wps=3058.7, ups=0.25, wpb=12109.8, bsz=545.6, num_updates=3780, lr=1.01667e-05, gnorm=4.37, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.4, wall=17557
2023-05-20 10:26:52 | INFO | train_inner | epoch 059:     45 / 65 loss=4.063, nll_loss=2.07, ppl=4.2, wps=3074.4, ups=0.26, wpb=12013.5, bsz=542.1, num_updates=3800, lr=1e-05, gnorm=5.417, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.2, wall=17635
2023-05-20 10:28:04 | INFO | train_inner | epoch 059:     65 / 65 loss=4.114, nll_loss=2.126, ppl=4.37, wps=3328.2, ups=0.28, wpb=12027.1, bsz=581.2, num_updates=3820, lr=9.83333e-06, gnorm=3.467, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.3, wall=17707
2023-05-20 10:28:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:28:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:28:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.429 | nll_loss 2.433 | ppl 5.4 | bleu 24.92 | wps 942.9 | wpb 2785 | bsz 105.2 | num_updates 3820 | best_loss 4.41
2023-05-20 10:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 3820 updates
2023-05-20 10:28:40 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint59.pt
2023-05-20 10:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint59.pt
2023-05-20 10:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint59.pt (epoch 59 @ 3820 updates, score 4.429) (writing took 10.557207100093365 seconds)
2023-05-20 10:28:50 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2023-05-20 10:28:50 | INFO | train | epoch 059 | loss 4.092 | nll_loss 2.102 | ppl 4.29 | wps 2652.7 | ups 0.22 | wpb 12047.8 | bsz 558.5 | num_updates 3820 | lr 9.83333e-06 | gnorm 4.331 | clip 100 | loss_scale 0.25 | train_wall 248 | gb_free 18.3 | wall 17754
2023-05-20 10:28:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:28:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:28:51 | INFO | fairseq.trainer | begin training epoch 60
2023-05-20 10:28:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:30:05 | INFO | train_inner | epoch 060:     20 / 65 loss=4.081, nll_loss=2.092, ppl=4.26, wps=2018.8, ups=0.16, wpb=12236.4, bsz=559.1, num_updates=3840, lr=9.66667e-06, gnorm=7.229, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.3, wall=17829
2023-05-20 10:31:24 | INFO | train_inner | epoch 060:     40 / 65 loss=4.125, nll_loss=2.141, ppl=4.41, wps=3116.4, ups=0.25, wpb=12307.5, bsz=589.6, num_updates=3860, lr=9.5e-06, gnorm=4.884, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.4, wall=17908
2023-05-20 10:32:40 | INFO | train_inner | epoch 060:     60 / 65 loss=4.065, nll_loss=2.073, ppl=4.21, wps=3185.7, ups=0.26, wpb=12057.1, bsz=552.8, num_updates=3880, lr=9.33333e-06, gnorm=7.005, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.7, wall=17983
2023-05-20 10:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:32:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:33:29 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.454 | nll_loss 2.449 | ppl 5.46 | bleu 25.04 | wps 1094.1 | wpb 2785 | bsz 105.2 | num_updates 3885 | best_loss 4.41
2023-05-20 10:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 3885 updates
2023-05-20 10:33:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint60.pt
2023-05-20 10:33:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint60.pt
2023-05-20 10:33:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint60.pt (epoch 60 @ 3885 updates, score 4.454) (writing took 9.83107614889741 seconds)
2023-05-20 10:33:39 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2023-05-20 10:33:39 | INFO | train | epoch 060 | loss 4.089 | nll_loss 2.1 | ppl 4.29 | wps 2716.6 | ups 0.23 | wpb 12056.3 | bsz 559.7 | num_updates 3885 | lr 9.29167e-06 | gnorm 6.088 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.4 | wall 18042
2023-05-20 10:33:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:33:39 | INFO | fairseq.trainer | begin training epoch 61
2023-05-20 10:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:34:32 | INFO | train_inner | epoch 061:     15 / 65 loss=4.078, nll_loss=2.088, ppl=4.25, wps=2093.3, ups=0.18, wpb=11739.6, bsz=543.2, num_updates=3900, lr=9.16667e-06, gnorm=2.471, clip=100, loss_scale=0.25, train_wall=71, gb_free=18.1, wall=18095
2023-05-20 10:35:48 | INFO | train_inner | epoch 061:     35 / 65 loss=4.084, nll_loss=2.093, ppl=4.27, wps=3226.8, ups=0.26, wpb=12256.1, bsz=561.2, num_updates=3920, lr=9e-06, gnorm=2.502, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.6, wall=18171
2023-05-20 10:37:06 | INFO | train_inner | epoch 061:     55 / 65 loss=4.056, nll_loss=2.061, ppl=4.17, wps=3077.8, ups=0.26, wpb=12021.2, bsz=554.9, num_updates=3940, lr=8.83333e-06, gnorm=3.982, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.2, wall=18249
2023-05-20 10:37:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:37:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:38:13 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.43 | nll_loss 2.43 | ppl 5.39 | bleu 25.41 | wps 1090.3 | wpb 2785 | bsz 105.2 | num_updates 3950 | best_loss 4.41
2023-05-20 10:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3950 updates
2023-05-20 10:38:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint61.pt
2023-05-20 10:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint61.pt
2023-05-20 10:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint61.pt (epoch 61 @ 3950 updates, score 4.43) (writing took 17.23360974341631 seconds)
2023-05-20 10:38:30 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2023-05-20 10:38:30 | INFO | train | epoch 061 | loss 4.074 | nll_loss 2.083 | ppl 4.24 | wps 2687.2 | ups 0.22 | wpb 12051.7 | bsz 557.1 | num_updates 3950 | lr 8.75e-06 | gnorm 3.262 | clip 100 | loss_scale 0.25 | train_wall 243 | gb_free 18.6 | wall 18334
2023-05-20 10:38:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:38:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:38:31 | INFO | fairseq.trainer | begin training epoch 62
2023-05-20 10:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:39:09 | INFO | train_inner | epoch 062:     10 / 65 loss=4.044, nll_loss=2.05, ppl=4.14, wps=1930.6, ups=0.16, wpb=11836.1, bsz=538.3, num_updates=3960, lr=8.66667e-06, gnorm=4.847, clip=100, loss_scale=0.25, train_wall=74, gb_free=17.9, wall=18372
2023-05-20 10:40:24 | INFO | train_inner | epoch 062:     30 / 65 loss=4.078, nll_loss=2.086, ppl=4.25, wps=3273.1, ups=0.27, wpb=12266.2, bsz=581, num_updates=3980, lr=8.5e-06, gnorm=3.953, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.4, wall=18447
2023-05-20 10:41:42 | INFO | train_inner | epoch 062:     50 / 65 loss=4.058, nll_loss=2.066, ppl=4.19, wps=3123.8, ups=0.26, wpb=12130, bsz=560.8, num_updates=4000, lr=8.33333e-06, gnorm=5.132, clip=100, loss_scale=0.25, train_wall=78, gb_free=17, wall=18525
2023-05-20 10:42:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:42:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:43:10 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.444 | nll_loss 2.444 | ppl 5.44 | bleu 25.24 | wps 1071.8 | wpb 2785 | bsz 105.2 | num_updates 4015 | best_loss 4.41
2023-05-20 10:43:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4015 updates
2023-05-20 10:43:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint62.pt
2023-05-20 10:43:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint62.pt
2023-05-20 10:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint62.pt (epoch 62 @ 4015 updates, score 4.444) (writing took 12.922905035316944 seconds)
2023-05-20 10:43:22 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2023-05-20 10:43:22 | INFO | train | epoch 062 | loss 4.061 | nll_loss 2.068 | ppl 4.19 | wps 2682.8 | ups 0.22 | wpb 12051.4 | bsz 559.8 | num_updates 4015 | lr 8.20833e-06 | gnorm 4.213 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.5 | wall 18626
2023-05-20 10:43:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:43:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:43:23 | INFO | fairseq.trainer | begin training epoch 63
2023-05-20 10:43:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:43:39 | INFO | train_inner | epoch 063:      5 / 65 loss=4.078, nll_loss=2.088, ppl=4.25, wps=2017.3, ups=0.17, wpb=11827.4, bsz=555.6, num_updates=4020, lr=8.16667e-06, gnorm=3.207, clip=100, loss_scale=0.25, train_wall=73, gb_free=17.8, wall=18642
2023-05-20 10:44:55 | INFO | train_inner | epoch 063:     25 / 65 loss=4.019, nll_loss=2.02, ppl=4.06, wps=3133.9, ups=0.26, wpb=11944.1, bsz=554.9, num_updates=4040, lr=8e-06, gnorm=3.718, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.2, wall=18718
2023-05-20 10:45:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 10:46:18 | INFO | train_inner | epoch 063:     46 / 65 loss=4.066, nll_loss=2.073, ppl=4.21, wps=2970.8, ups=0.24, wpb=12260.5, bsz=568.1, num_updates=4060, lr=7.83333e-06, gnorm=3.899, clip=100, loss_scale=0.125, train_wall=82, gb_free=17.5, wall=18801
2023-05-20 10:47:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:47:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:48:01 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.442 | nll_loss 2.44 | ppl 5.43 | bleu 25.25 | wps 1013.1 | wpb 2785 | bsz 105.2 | num_updates 4079 | best_loss 4.41
2023-05-20 10:48:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 4079 updates
2023-05-20 10:48:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint63.pt
2023-05-20 10:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint63.pt
2023-05-20 10:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint63.pt (epoch 63 @ 4079 updates, score 4.442) (writing took 17.720275171101093 seconds)
2023-05-20 10:48:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2023-05-20 10:48:19 | INFO | train | epoch 063 | loss 4.062 | nll_loss 2.069 | ppl 4.2 | wps 2602.5 | ups 0.22 | wpb 12054.8 | bsz 560.4 | num_updates 4079 | lr 7.675e-06 | gnorm 5.402 | clip 100 | loss_scale 0.125 | train_wall 245 | gb_free 18.3 | wall 18922
2023-05-20 10:48:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:48:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:48:19 | INFO | fairseq.trainer | begin training epoch 64
2023-05-20 10:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:48:22 | INFO | train_inner | epoch 064:      1 / 65 loss=4.091, nll_loss=2.103, ppl=4.3, wps=1892.4, ups=0.16, wpb=11793.5, bsz=550.8, num_updates=4080, lr=7.66667e-06, gnorm=8.636, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.5, wall=18925
2023-05-20 10:49:38 | INFO | train_inner | epoch 064:     21 / 65 loss=4.054, nll_loss=2.062, ppl=4.18, wps=3245, ups=0.26, wpb=12291.6, bsz=560.9, num_updates=4100, lr=7.5e-06, gnorm=3.681, clip=100, loss_scale=0.125, train_wall=76, gb_free=17.2, wall=19001
2023-05-20 10:50:53 | INFO | train_inner | epoch 064:     41 / 65 loss=4.044, nll_loss=2.049, ppl=4.14, wps=3257.6, ups=0.27, wpb=12172.8, bsz=571.9, num_updates=4120, lr=7.33333e-06, gnorm=5.387, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.4, wall=19076
2023-05-20 10:52:11 | INFO | train_inner | epoch 064:     61 / 65 loss=4.054, nll_loss=2.059, ppl=4.17, wps=3102.3, ups=0.26, wpb=12119.3, bsz=569, num_updates=4140, lr=7.16667e-06, gnorm=4.286, clip=100, loss_scale=0.125, train_wall=78, gb_free=18.5, wall=19154
2023-05-20 10:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:52:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:52:55 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.446 | nll_loss 2.448 | ppl 5.46 | bleu 25.16 | wps 1100.2 | wpb 2785 | bsz 105.2 | num_updates 4144 | best_loss 4.41
2023-05-20 10:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 4144 updates
2023-05-20 10:52:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint64.pt
2023-05-20 10:53:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint64.pt
2023-05-20 10:53:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint64.pt (epoch 64 @ 4144 updates, score 4.446) (writing took 11.37478206306696 seconds)
2023-05-20 10:53:06 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2023-05-20 10:53:06 | INFO | train | epoch 064 | loss 4.05 | nll_loss 2.056 | ppl 4.16 | wps 2725.3 | ups 0.23 | wpb 12051.9 | bsz 559.4 | num_updates 4144 | lr 7.13333e-06 | gnorm 4.337 | clip 100 | loss_scale 0.125 | train_wall 245 | gb_free 18.6 | wall 19209
2023-05-20 10:53:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:53:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:53:06 | INFO | fairseq.trainer | begin training epoch 65
2023-05-20 10:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:54:06 | INFO | train_inner | epoch 065:     16 / 65 loss=4.065, nll_loss=2.075, ppl=4.21, wps=2059.8, ups=0.17, wpb=11855.8, bsz=540.8, num_updates=4160, lr=7e-06, gnorm=4.055, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.9, wall=19269
2023-05-20 10:55:28 | INFO | train_inner | epoch 065:     36 / 65 loss=4.061, nll_loss=2.066, ppl=4.19, wps=3013.4, ups=0.24, wpb=12310, bsz=582.2, num_updates=4180, lr=6.83333e-06, gnorm=5.09, clip=100, loss_scale=0.125, train_wall=82, gb_free=17.7, wall=19351
2023-05-20 10:56:42 | INFO | train_inner | epoch 065:     56 / 65 loss=4.043, nll_loss=2.049, ppl=4.14, wps=3251.1, ups=0.27, wpb=12116.9, bsz=569, num_updates=4200, lr=6.66667e-06, gnorm=11.927, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.2, wall=19425
2023-05-20 10:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 10:57:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:57:48 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.461 | nll_loss 2.467 | ppl 5.53 | bleu 25.53 | wps 1075.8 | wpb 2785 | bsz 105.2 | num_updates 4209 | best_loss 4.41
2023-05-20 10:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 4209 updates
2023-05-20 10:57:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint65.pt
2023-05-20 10:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint65.pt
2023-05-20 10:58:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint65.pt (epoch 65 @ 4209 updates, score 4.461) (writing took 17.26233908161521 seconds)
2023-05-20 10:58:05 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2023-05-20 10:58:05 | INFO | train | epoch 065 | loss 4.047 | nll_loss 2.053 | ppl 4.15 | wps 2620.8 | ups 0.22 | wpb 12044.6 | bsz 558 | num_updates 4209 | lr 6.59167e-06 | gnorm 7.593 | clip 100 | loss_scale 0.125 | train_wall 250 | gb_free 18.7 | wall 19508
2023-05-20 10:58:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 10:58:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 10:58:05 | INFO | fairseq.trainer | begin training epoch 66
2023-05-20 10:58:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 10:58:50 | INFO | train_inner | epoch 066:     11 / 65 loss=4.008, nll_loss=2.01, ppl=4.03, wps=1801, ups=0.16, wpb=11478.6, bsz=514.6, num_updates=4220, lr=6.5e-06, gnorm=5.875, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.4, wall=19553
2023-05-20 11:00:06 | INFO | train_inner | epoch 066:     31 / 65 loss=4.063, nll_loss=2.069, ppl=4.2, wps=3193.7, ups=0.26, wpb=12217.4, bsz=600.9, num_updates=4240, lr=6.33333e-06, gnorm=3.257, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.3, wall=19629
2023-05-20 11:01:21 | INFO | train_inner | epoch 066:     51 / 65 loss=4.03, nll_loss=2.035, ppl=4.1, wps=3217.7, ups=0.27, wpb=12091.7, bsz=537.4, num_updates=4260, lr=6.16667e-06, gnorm=7.947, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.6, wall=19704
2023-05-20 11:02:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:02:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:02:47 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.429 | nll_loss 2.425 | ppl 5.37 | bleu 25.71 | wps 967.6 | wpb 2785 | bsz 105.2 | num_updates 4274 | best_loss 4.41
2023-05-20 11:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 4274 updates
2023-05-20 11:02:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint66.pt
2023-05-20 11:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint66.pt
2023-05-20 11:02:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint66.pt (epoch 66 @ 4274 updates, score 4.429) (writing took 11.216869436204433 seconds)
2023-05-20 11:02:58 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2023-05-20 11:02:58 | INFO | train | epoch 066 | loss 4.038 | nll_loss 2.044 | ppl 4.12 | wps 2671.9 | ups 0.22 | wpb 12057.1 | bsz 559.9 | num_updates 4274 | lr 6.05e-06 | gnorm 4.735 | clip 100 | loss_scale 0.125 | train_wall 247 | gb_free 17.2 | wall 19801
2023-05-20 11:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:02:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:02:58 | INFO | fairseq.trainer | begin training epoch 67
2023-05-20 11:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:03:22 | INFO | train_inner | epoch 067:      6 / 65 loss=4.029, nll_loss=2.033, ppl=4.09, wps=1967.1, ups=0.17, wpb=11863.1, bsz=558.9, num_updates=4280, lr=6e-06, gnorm=3.769, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.7, wall=19825
2023-05-20 11:04:37 | INFO | train_inner | epoch 067:     26 / 65 loss=4.031, nll_loss=2.034, ppl=4.1, wps=3251.3, ups=0.27, wpb=12228.2, bsz=579.9, num_updates=4300, lr=5.83333e-06, gnorm=2.829, clip=100, loss_scale=0.125, train_wall=75, gb_free=18.4, wall=19900
2023-05-20 11:05:54 | INFO | train_inner | epoch 067:     46 / 65 loss=4.019, nll_loss=2.022, ppl=4.06, wps=3158.7, ups=0.26, wpb=12102.7, bsz=541, num_updates=4320, lr=5.66667e-06, gnorm=2.908, clip=100, loss_scale=0.125, train_wall=77, gb_free=18.6, wall=19977
2023-05-20 11:07:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:07:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:07:34 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.447 | nll_loss 2.448 | ppl 5.46 | bleu 25.48 | wps 1111.4 | wpb 2785 | bsz 105.2 | num_updates 4339 | best_loss 4.41
2023-05-20 11:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 4339 updates
2023-05-20 11:07:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint67.pt
2023-05-20 11:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint67.pt
2023-05-20 11:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint67.pt (epoch 67 @ 4339 updates, score 4.447) (writing took 11.96432563662529 seconds)
2023-05-20 11:07:46 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2023-05-20 11:07:46 | INFO | train | epoch 067 | loss 4.029 | nll_loss 2.032 | ppl 4.09 | wps 2727.9 | ups 0.23 | wpb 12058.4 | bsz 559.6 | num_updates 4339 | lr 5.50833e-06 | gnorm 3.396 | clip 100 | loss_scale 0.125 | train_wall 244 | gb_free 18.4 | wall 20089
2023-05-20 11:07:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:07:46 | INFO | fairseq.trainer | begin training epoch 68
2023-05-20 11:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:07:49 | INFO | train_inner | epoch 068:      1 / 65 loss=4.035, nll_loss=2.038, ppl=4.11, wps=2058.8, ups=0.17, wpb=11859.5, bsz=542.7, num_updates=4340, lr=5.5e-06, gnorm=4.206, clip=100, loss_scale=0.125, train_wall=73, gb_free=18.2, wall=20092
2023-05-20 11:09:10 | INFO | train_inner | epoch 068:     21 / 65 loss=4.048, nll_loss=2.054, ppl=4.15, wps=3090.8, ups=0.25, wpb=12453, bsz=620.5, num_updates=4360, lr=5.33333e-06, gnorm=2.973, clip=100, loss_scale=0.125, train_wall=80, gb_free=18.5, wall=20173
2023-05-20 11:10:25 | INFO | train_inner | epoch 068:     41 / 65 loss=4.004, nll_loss=2.006, ppl=4.02, wps=3183.6, ups=0.26, wpb=12062.6, bsz=538.6, num_updates=4380, lr=5.16667e-06, gnorm=6.557, clip=100, loss_scale=0.125, train_wall=76, gb_free=18.1, wall=20249
2023-05-20 11:11:42 | INFO | train_inner | epoch 068:     61 / 65 loss=4.009, nll_loss=2.009, ppl=4.02, wps=3176.6, ups=0.26, wpb=12079.4, bsz=537.6, num_updates=4400, lr=5e-06, gnorm=5.219, clip=100, loss_scale=0.125, train_wall=76, gb_free=18, wall=20325
2023-05-20 11:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:11:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:12:28 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.44 | nll_loss 2.438 | ppl 5.42 | bleu 25.56 | wps 1000 | wpb 2785 | bsz 105.2 | num_updates 4404 | best_loss 4.41
2023-05-20 11:12:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 4404 updates
2023-05-20 11:12:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint68.pt
2023-05-20 11:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint68.pt
2023-05-20 11:12:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint68.pt (epoch 68 @ 4404 updates, score 4.44) (writing took 10.796576149761677 seconds)
2023-05-20 11:12:38 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2023-05-20 11:12:38 | INFO | train | epoch 068 | loss 4.021 | nll_loss 2.024 | ppl 4.07 | wps 2676.1 | ups 0.22 | wpb 12048.3 | bsz 559.7 | num_updates 4404 | lr 4.96667e-06 | gnorm 4.874 | clip 100 | loss_scale 0.125 | train_wall 248 | gb_free 18.5 | wall 20381
2023-05-20 11:12:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:12:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:12:38 | INFO | fairseq.trainer | begin training epoch 69
2023-05-20 11:12:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:13:39 | INFO | train_inner | epoch 069:     16 / 65 loss=4.007, nll_loss=2.008, ppl=4.02, wps=1982.7, ups=0.17, wpb=11686.6, bsz=530.8, num_updates=4420, lr=4.83333e-06, gnorm=5.609, clip=100, loss_scale=0.125, train_wall=73, gb_free=18, wall=20442
2023-05-20 11:14:53 | INFO | train_inner | epoch 069:     36 / 65 loss=4.03, nll_loss=2.033, ppl=4.09, wps=3317.4, ups=0.27, wpb=12248.5, bsz=590.7, num_updates=4440, lr=4.66667e-06, gnorm=4.471, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.1, wall=20516
2023-05-20 11:16:09 | INFO | train_inner | epoch 069:     56 / 65 loss=4.02, nll_loss=2.025, ppl=4.07, wps=3219.9, ups=0.26, wpb=12194.7, bsz=567.4, num_updates=4460, lr=4.5e-06, gnorm=2.095, clip=100, loss_scale=0.125, train_wall=76, gb_free=17.5, wall=20592
2023-05-20 11:16:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:16:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:17:17 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.442 | nll_loss 2.443 | ppl 5.44 | bleu 25.84 | wps 1075.7 | wpb 2785 | bsz 105.2 | num_updates 4469 | best_loss 4.41
2023-05-20 11:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 4469 updates
2023-05-20 11:17:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint69.pt
2023-05-20 11:17:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint69.pt
2023-05-20 11:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint69.pt (epoch 69 @ 4469 updates, score 4.442) (writing took 16.994555316865444 seconds)
2023-05-20 11:17:34 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2023-05-20 11:17:34 | INFO | train | epoch 069 | loss 4.02 | nll_loss 2.022 | ppl 4.06 | wps 2651.6 | ups 0.22 | wpb 12052.1 | bsz 559.4 | num_updates 4469 | lr 4.425e-06 | gnorm 4.205 | clip 100 | loss_scale 0.125 | train_wall 247 | gb_free 17.4 | wall 20677
2023-05-20 11:17:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:17:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:17:34 | INFO | fairseq.trainer | begin training epoch 70
2023-05-20 11:17:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:18:16 | INFO | train_inner | epoch 070:     11 / 65 loss=4.011, nll_loss=2.011, ppl=4.03, wps=1825.9, ups=0.16, wpb=11583, bsz=527, num_updates=4480, lr=4.33333e-06, gnorm=3.74, clip=100, loss_scale=0.125, train_wall=78, gb_free=18.5, wall=20719
2023-05-20 11:19:30 | INFO | train_inner | epoch 070:     31 / 65 loss=4.045, nll_loss=2.052, ppl=4.15, wps=3302.3, ups=0.27, wpb=12221, bsz=588.8, num_updates=4500, lr=4.16667e-06, gnorm=4.982, clip=100, loss_scale=0.125, train_wall=74, gb_free=18.3, wall=20793
2023-05-20 11:20:49 | INFO | train_inner | epoch 070:     51 / 65 loss=4.008, nll_loss=2.007, ppl=4.02, wps=3072.7, ups=0.25, wpb=12220.7, bsz=555.5, num_updates=4520, lr=4e-06, gnorm=5.856, clip=100, loss_scale=0.125, train_wall=79, gb_free=18.7, wall=20872
2023-05-20 11:21:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:21:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:22:14 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.443 | nll_loss 2.442 | ppl 5.43 | bleu 25.44 | wps 984.6 | wpb 2785 | bsz 105.2 | num_updates 4534 | best_loss 4.41
2023-05-20 11:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 4534 updates
2023-05-20 11:22:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint70.pt
2023-05-20 11:22:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint70.pt
2023-05-20 11:22:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint70.pt (epoch 70 @ 4534 updates, score 4.443) (writing took 13.523332357406616 seconds)
2023-05-20 11:22:27 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2023-05-20 11:22:27 | INFO | train | epoch 070 | loss 4.02 | nll_loss 2.022 | ppl 4.06 | wps 2670.3 | ups 0.22 | wpb 12060.8 | bsz 558.8 | num_updates 4534 | lr 3.88333e-06 | gnorm 4.477 | clip 100 | loss_scale 0.125 | train_wall 245 | gb_free 18.4 | wall 20970
2023-05-20 11:22:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:22:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:22:27 | INFO | fairseq.trainer | begin training epoch 71
2023-05-20 11:22:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:22:50 | INFO | train_inner | epoch 071:      6 / 65 loss=4.019, nll_loss=2.022, ppl=4.06, wps=1971.8, ups=0.17, wpb=11849.9, bsz=538.8, num_updates=4540, lr=3.83333e-06, gnorm=3.263, clip=100, loss_scale=0.125, train_wall=72, gb_free=18.5, wall=20993
2023-05-20 11:24:02 | INFO | train_inner | epoch 071:     26 / 65 loss=4.007, nll_loss=2.006, ppl=4.02, wps=3361.7, ups=0.28, wpb=12187.4, bsz=552.2, num_updates=4560, lr=3.66667e-06, gnorm=9.498, clip=100, loss_scale=0.25, train_wall=72, gb_free=18.4, wall=21065
2023-05-20 11:25:25 | INFO | train_inner | epoch 071:     46 / 65 loss=3.989, nll_loss=1.989, ppl=3.97, wps=2930, ups=0.24, wpb=12116, bsz=557.1, num_updates=4580, lr=3.5e-06, gnorm=3.373, clip=100, loss_scale=0.25, train_wall=83, gb_free=17.5, wall=21148
2023-05-20 11:26:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:26:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:27:07 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.443 | nll_loss 2.442 | ppl 5.43 | bleu 25.38 | wps 1014.8 | wpb 2785 | bsz 105.2 | num_updates 4599 | best_loss 4.41
2023-05-20 11:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 4599 updates
2023-05-20 11:27:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint71.pt
2023-05-20 11:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint71.pt
2023-05-20 11:27:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint71.pt (epoch 71 @ 4599 updates, score 4.443) (writing took 17.63473481312394 seconds)
2023-05-20 11:27:25 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2023-05-20 11:27:25 | INFO | train | epoch 071 | loss 4.012 | nll_loss 2.013 | ppl 4.04 | wps 2636.8 | ups 0.22 | wpb 12056.8 | bsz 559.6 | num_updates 4599 | lr 3.34167e-06 | gnorm 5.29 | clip 100 | loss_scale 0.25 | train_wall 246 | gb_free 18.7 | wall 21268
2023-05-20 11:27:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:27:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:27:25 | INFO | fairseq.trainer | begin training epoch 72
2023-05-20 11:27:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:27:27 | INFO | train_inner | epoch 072:      1 / 65 loss=4.029, nll_loss=2.033, ppl=4.09, wps=1928.2, ups=0.16, wpb=11803.6, bsz=566.3, num_updates=4600, lr=3.33333e-06, gnorm=3.569, clip=100, loss_scale=0.25, train_wall=71, gb_free=18.4, wall=21270
2023-05-20 11:28:48 | INFO | train_inner | epoch 072:     21 / 65 loss=4.018, nll_loss=2.02, ppl=4.06, wps=3006.6, ups=0.25, wpb=12201.4, bsz=566.1, num_updates=4620, lr=3.16667e-06, gnorm=5.186, clip=100, loss_scale=0.25, train_wall=81, gb_free=18, wall=21351
2023-05-20 11:30:06 | INFO | train_inner | epoch 072:     41 / 65 loss=3.98, nll_loss=1.978, ppl=3.94, wps=3145, ups=0.26, wpb=12166.9, bsz=551.1, num_updates=4640, lr=3e-06, gnorm=3.771, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.4, wall=21429
2023-05-20 11:31:23 | INFO | train_inner | epoch 072:     61 / 65 loss=4.031, nll_loss=2.034, ppl=4.1, wps=3207.2, ups=0.26, wpb=12307, bsz=590.5, num_updates=4660, lr=2.83333e-06, gnorm=2.86, clip=100, loss_scale=0.25, train_wall=77, gb_free=17.8, wall=21506
2023-05-20 11:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:31:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:32:09 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.435 | nll_loss 2.432 | ppl 5.4 | bleu 25.71 | wps 1002.3 | wpb 2785 | bsz 105.2 | num_updates 4664 | best_loss 4.41
2023-05-20 11:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 4664 updates
2023-05-20 11:32:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint72.pt
2023-05-20 11:32:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint72.pt
2023-05-20 11:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint72.pt (epoch 72 @ 4664 updates, score 4.435) (writing took 10.444622684270144 seconds)
2023-05-20 11:32:19 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2023-05-20 11:32:19 | INFO | train | epoch 072 | loss 4.004 | nll_loss 2.004 | ppl 4.01 | wps 2658.9 | ups 0.22 | wpb 12051.9 | bsz 559.4 | num_updates 4664 | lr 2.8e-06 | gnorm 4.079 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 18.9 | wall 21562
2023-05-20 11:32:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:32:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:32:19 | INFO | fairseq.trainer | begin training epoch 73
2023-05-20 11:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:33:21 | INFO | train_inner | epoch 073:     16 / 65 loss=3.94, nll_loss=1.931, ppl=3.81, wps=1950.3, ups=0.17, wpb=11506.7, bsz=505.3, num_updates=4680, lr=2.66667e-06, gnorm=3.879, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.5, wall=21624
2023-05-20 11:34:39 | INFO | train_inner | epoch 073:     36 / 65 loss=4.03, nll_loss=2.037, ppl=4.1, wps=3118.5, ups=0.25, wpb=12243.6, bsz=577.6, num_updates=4700, lr=2.5e-06, gnorm=8.777, clip=100, loss_scale=0.25, train_wall=78, gb_free=17.9, wall=21702
2023-05-20 11:35:57 | INFO | train_inner | epoch 073:     56 / 65 loss=4.038, nll_loss=2.043, ppl=4.12, wps=3161.4, ups=0.26, wpb=12326.9, bsz=600.6, num_updates=4720, lr=2.33333e-06, gnorm=8.758, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.3, wall=21780
2023-05-20 11:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:36:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:37:02 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.436 | nll_loss 2.433 | ppl 5.4 | bleu 25.88 | wps 1064 | wpb 2785 | bsz 105.2 | num_updates 4729 | best_loss 4.41
2023-05-20 11:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 4729 updates
2023-05-20 11:37:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint73.pt
2023-05-20 11:37:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint73.pt
2023-05-20 11:37:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint73.pt (epoch 73 @ 4729 updates, score 4.436) (writing took 13.850970033556223 seconds)
2023-05-20 11:37:16 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2023-05-20 11:37:16 | INFO | train | epoch 073 | loss 4.001 | nll_loss 2.001 | ppl 4 | wps 2643.7 | ups 0.22 | wpb 12056.5 | bsz 559.8 | num_updates 4729 | lr 2.25833e-06 | gnorm 6.61 | clip 100 | loss_scale 0.25 | train_wall 250 | gb_free 18.7 | wall 21859
2023-05-20 11:37:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:37:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:37:16 | INFO | fairseq.trainer | begin training epoch 74
2023-05-20 11:37:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:37:59 | INFO | train_inner | epoch 074:     11 / 65 loss=3.972, nll_loss=1.967, ppl=3.91, wps=1933.7, ups=0.16, wpb=11767.3, bsz=540.9, num_updates=4740, lr=2.16667e-06, gnorm=5.718, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.6, wall=21902
2023-05-20 11:39:17 | INFO | train_inner | epoch 074:     31 / 65 loss=3.992, nll_loss=1.991, ppl=3.97, wps=3124.5, ups=0.25, wpb=12299.6, bsz=575.4, num_updates=4760, lr=2e-06, gnorm=2.924, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.4, wall=21981
2023-05-20 11:40:32 | INFO | train_inner | epoch 074:     51 / 65 loss=4.015, nll_loss=2.016, ppl=4.05, wps=3237.8, ups=0.27, wpb=12135.7, bsz=545.9, num_updates=4780, lr=1.83333e-06, gnorm=6.915, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.5, wall=22056
2023-05-20 11:41:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:41:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:41:55 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.436 | nll_loss 2.435 | ppl 5.41 | bleu 25.97 | wps 1069 | wpb 2785 | bsz 105.2 | num_updates 4794 | best_loss 4.41
2023-05-20 11:41:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 4794 updates
2023-05-20 11:41:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint74.pt
2023-05-20 11:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint74.pt
2023-05-20 11:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint74.pt (epoch 74 @ 4794 updates, score 4.436) (writing took 10.549153376370668 seconds)
2023-05-20 11:42:05 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2023-05-20 11:42:05 | INFO | train | epoch 074 | loss 4.001 | nll_loss 2.001 | ppl 4 | wps 2707.2 | ups 0.22 | wpb 12060.5 | bsz 559.6 | num_updates 4794 | lr 1.71667e-06 | gnorm 5.145 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.3 | wall 22148
2023-05-20 11:42:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:42:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:42:05 | INFO | fairseq.trainer | begin training epoch 75
2023-05-20 11:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:42:30 | INFO | train_inner | epoch 075:      6 / 65 loss=4.007, nll_loss=2.007, ppl=4.02, wps=1995.9, ups=0.17, wpb=11693, bsz=551.1, num_updates=4800, lr=1.66667e-06, gnorm=3.63, clip=100, loss_scale=0.25, train_wall=75, gb_free=18.4, wall=22173
2023-05-20 11:43:43 | INFO | train_inner | epoch 075:     26 / 65 loss=3.99, nll_loss=1.988, ppl=3.97, wps=3306, ups=0.27, wpb=12183.2, bsz=553.8, num_updates=4820, lr=1.5e-06, gnorm=3.173, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.5, wall=22246
2023-05-20 11:45:01 | INFO | train_inner | epoch 075:     46 / 65 loss=3.992, nll_loss=1.991, ppl=3.98, wps=3132.9, ups=0.26, wpb=12191.4, bsz=553.6, num_updates=4840, lr=1.33333e-06, gnorm=6.469, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.5, wall=22324
2023-05-20 11:46:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:46:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:46:39 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.439 | nll_loss 2.438 | ppl 5.42 | bleu 25.75 | wps 1099.6 | wpb 2785 | bsz 105.2 | num_updates 4859 | best_loss 4.41
2023-05-20 11:46:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 4859 updates
2023-05-20 11:46:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint75.pt
2023-05-20 11:46:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint75.pt
2023-05-20 11:47:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint75.pt (epoch 75 @ 4859 updates, score 4.439) (writing took 25.43731055036187 seconds)
2023-05-20 11:47:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2023-05-20 11:47:04 | INFO | train | epoch 075 | loss 3.997 | nll_loss 1.996 | ppl 3.99 | wps 2620.7 | ups 0.22 | wpb 12047.9 | bsz 558.6 | num_updates 4859 | lr 1.175e-06 | gnorm 4.378 | clip 100 | loss_scale 0.25 | train_wall 242 | gb_free 18.3 | wall 22447
2023-05-20 11:47:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:47:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:47:04 | INFO | fairseq.trainer | begin training epoch 76
2023-05-20 11:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:47:07 | INFO | train_inner | epoch 076:      1 / 65 loss=3.999, nll_loss=1.999, ppl=4, wps=1847.6, ups=0.16, wpb=11643.6, bsz=562.2, num_updates=4860, lr=1.16667e-06, gnorm=3.715, clip=100, loss_scale=0.25, train_wall=70, gb_free=18.1, wall=22450
2023-05-20 11:48:23 | INFO | train_inner | epoch 076:     21 / 65 loss=3.999, nll_loss=1.999, ppl=4, wps=3232.4, ups=0.26, wpb=12324.2, bsz=584.1, num_updates=4880, lr=1e-06, gnorm=4.646, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.4, wall=22527
2023-05-20 11:49:41 | INFO | train_inner | epoch 076:     41 / 65 loss=4.03, nll_loss=2.035, ppl=4.1, wps=3161.2, ups=0.26, wpb=12285.5, bsz=564.2, num_updates=4900, lr=8.33333e-07, gnorm=5.072, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.7, wall=22604
2023-05-20 11:51:00 | INFO | train_inner | epoch 076:     61 / 65 loss=3.972, nll_loss=1.968, ppl=3.91, wps=3059.1, ups=0.25, wpb=12094.6, bsz=553, num_updates=4920, lr=6.66667e-07, gnorm=5.208, clip=100, loss_scale=0.25, train_wall=79, gb_free=18.6, wall=22683
2023-05-20 11:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:51:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:51:44 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.436 | nll_loss 2.434 | ppl 5.4 | bleu 26.07 | wps 1098.3 | wpb 2785 | bsz 105.2 | num_updates 4924 | best_loss 4.41
2023-05-20 11:51:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 4924 updates
2023-05-20 11:51:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint76.pt
2023-05-20 11:51:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint76.pt
2023-05-20 11:51:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint76.pt (epoch 76 @ 4924 updates, score 4.436) (writing took 12.170630186796188 seconds)
2023-05-20 11:51:56 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2023-05-20 11:51:56 | INFO | train | epoch 076 | loss 3.996 | nll_loss 1.996 | ppl 3.99 | wps 2683.6 | ups 0.22 | wpb 12057.4 | bsz 559.4 | num_updates 4924 | lr 6.33333e-07 | gnorm 4.853 | clip 100 | loss_scale 0.25 | train_wall 249 | gb_free 18.6 | wall 22739
2023-05-20 11:51:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:51:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:51:56 | INFO | fairseq.trainer | begin training epoch 77
2023-05-20 11:51:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 11:52:58 | INFO | train_inner | epoch 077:     16 / 65 loss=3.956, nll_loss=1.95, ppl=3.86, wps=1976, ups=0.17, wpb=11604.3, bsz=523.3, num_updates=4940, lr=5e-07, gnorm=3.869, clip=100, loss_scale=0.25, train_wall=74, gb_free=18.5, wall=22801
2023-05-20 11:54:15 | INFO | train_inner | epoch 077:     36 / 65 loss=3.998, nll_loss=1.998, ppl=4, wps=3146.9, ups=0.26, wpb=12230.6, bsz=564.1, num_updates=4960, lr=3.33333e-07, gnorm=3.513, clip=100, loss_scale=0.25, train_wall=78, gb_free=18.2, wall=22879
2023-05-20 11:55:32 | INFO | train_inner | epoch 077:     56 / 65 loss=4.01, nll_loss=2.012, ppl=4.03, wps=3213.5, ups=0.26, wpb=12228.4, bsz=592.6, num_updates=4980, lr=1.66667e-07, gnorm=3.16, clip=100, loss_scale=0.25, train_wall=76, gb_free=18.7, wall=22955
2023-05-20 11:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 11:56:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:56:36 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.437 | nll_loss 2.436 | ppl 5.41 | bleu 25.9 | wps 1025.1 | wpb 2785 | bsz 105.2 | num_updates 4989 | best_loss 4.41
2023-05-20 11:56:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 4989 updates
2023-05-20 11:56:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint77.pt
2023-05-20 11:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage2/checkpoint77.pt
2023-05-20 11:56:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage2/checkpoint77.pt (epoch 77 @ 4989 updates, score 4.437) (writing took 18.514457385987043 seconds)
2023-05-20 11:56:55 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2023-05-20 11:56:55 | INFO | train | epoch 077 | loss 3.992 | nll_loss 1.991 | ppl 3.98 | wps 2623 | ups 0.22 | wpb 12051.7 | bsz 558.9 | num_updates 4989 | lr 9.16667e-08 | gnorm 3.461 | clip 100 | loss_scale 0.25 | train_wall 247 | gb_free 18.4 | wall 23038
2023-05-20 11:56:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 11:56:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 11:56:55 | INFO | fairseq.trainer | begin training epoch 78
2023-05-20 11:56:55 | INFO | fairseq_cli.train | Start iterating over samples
slurmstepd: error: *** JOB 116920 ON 99server CANCELLED AT 2023-05-20T11:57:33 ***
2023-05-20 11:57:40 | INFO | train_inner | epoch 078:     11 / 65 loss=3.983, nll_loss=1.98, ppl=3.94, wps=1814.7, ups=0.16, wpb=11670.6, bsz=527.6, num_updates=5000, lr=0, gnorm=2.932, clip=100, loss_scale=0.25, train_wall=77, gb_free=18.5, wall=23083
