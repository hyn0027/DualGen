2023-05-20 14:05:14 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-05-20 14:05:15 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11047
2023-05-20 14:05:15 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11047
2023-05-20 14:05:15 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11047
2023-05-20 14:05:15 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11047
2023-05-20 14:05:15 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-20 14:05:16 | INFO | fairseq.distributed.utils | initialized host 100server as rank 0
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-20 14:05:16 | INFO | fairseq.distributed.utils | initialized host 100server as rank 2
2023-05-20 14:05:16 | INFO | fairseq.distributed.utils | initialized host 100server as rank 3
2023-05-20 14:05:16 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
2023-05-20 14:05:16 | INFO | fairseq.distributed.utils | initialized host 100server as rank 1
2023-05-20 14:05:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 20, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11047', 'distributed_port': 11047, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [3e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'training/stage3_new', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=0, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', save_dir='training/stage3_new', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=0, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=20, lr=[3e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=4, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt', save_dir='training/stage3_new', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='2000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=100, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [3e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 100, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 2000.0, 'lr': [3e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-20 14:05:22 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-05-20 14:05:34 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (1): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (2): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (3): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (4): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (5): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (6): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (7): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (8): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (9): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (10): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (11): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
    )
    (graph_embeddings): Linear(in_features=1024, out_features=64, bias=True)
    (graph_embeddings_inverse): Linear(in_features=1024, out_features=64, bias=True)
    (gamma_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-05-20 14:05:34 | INFO | fairseq_cli.train | task: GraphToSeq
2023-05-20 14:05:34 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-05-20 14:05:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-20 14:05:34 | INFO | fairseq_cli.train | num. shared model params: 559,173,888 (num. trained: 559,173,888)
2023-05-20 14:05:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-20 14:05:34 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-05-20 14:05:34 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-05-20 14:05:35 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-05-20 14:05:35 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-05-20 14:05:35 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-05-20 14:05:36 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-05-20 14:05:36 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-05-20 14:05:36 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-05-20 14:05:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-05-20 14:05:46 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
2023-05-20 14:05:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-05-20 14:05:46 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-20 14:05:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-05-20 14:05:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 14:05:46 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 14:05:46 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 14:05:46 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-20 14:05:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-05-20 14:05:46 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2023-05-20 14:05:46 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-05-20 14:05:46 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt
2023-05-20 14:06:09 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/fairseq/training/stage1/checkpoint_best.pt (epoch 141 @ 0 updates)
2023-05-20 14:06:09 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-05-20 14:06:09 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-05-20 14:06:10 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-05-20 14:06:10 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 14:06:10 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-20 14:06:10 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-20 14:06:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 14:06:14 | INFO | fairseq.trainer | begin training epoch 1
2023-05-20 14:06:14 | INFO | fairseq_cli.train | Start iterating over samples
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-05-20 14:06:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-05-20 14:06:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-05-20 14:06:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-05-20 14:06:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-05-20 14:06:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-05-20 14:06:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-20 14:07:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 14:07:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-20 14:07:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 14:08:41 | INFO | train_inner | epoch 001:     29 / 130 loss=7.087, nll_loss=5.311, ppl=39.69, wps=1335.6, ups=0.22, wpb=6020.8, bsz=273.4, num_updates=20, lr=6e-06, gnorm=41.323, clip=100, loss_scale=0.25, train_wall=144, gb_free=10.1, wall=175
2023-05-20 14:10:06 | INFO | train_inner | epoch 001:     49 / 130 loss=5.7, nll_loss=3.866, ppl=14.58, wps=1426.7, ups=0.23, wpb=6076.2, bsz=275.4, num_updates=40, lr=1.2e-05, gnorm=12.073, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.2, wall=260
2023-05-20 14:11:33 | INFO | train_inner | epoch 001:     69 / 130 loss=5.067, nll_loss=3.234, ppl=9.41, wps=1392.8, ups=0.23, wpb=6067.7, bsz=281.4, num_updates=60, lr=1.8e-05, gnorm=12.729, clip=100, loss_scale=0.25, train_wall=87, gb_free=9.9, wall=347
2023-05-20 14:13:00 | INFO | train_inner | epoch 001:     89 / 130 loss=4.657, nll_loss=2.803, ppl=6.98, wps=1433.2, ups=0.23, wpb=6199.1, bsz=302.8, num_updates=80, lr=2.4e-05, gnorm=6.082, clip=100, loss_scale=0.25, train_wall=86, gb_free=9.8, wall=434
2023-05-20 14:14:22 | INFO | train_inner | epoch 001:    109 / 130 loss=4.298, nll_loss=2.423, ppl=5.36, wps=1492.9, ups=0.24, wpb=6148.8, bsz=299.6, num_updates=100, lr=3e-05, gnorm=3.895, clip=100, loss_scale=0.25, train_wall=82, gb_free=9.8, wall=516
2023-05-20 14:15:48 | INFO | train_inner | epoch 001:    129 / 130 loss=4.137, nll_loss=2.266, ppl=4.81, wps=1413.2, ups=0.23, wpb=6064.3, bsz=274.1, num_updates=120, lr=2.96842e-05, gnorm=3.404, clip=100, loss_scale=0.25, train_wall=86, gb_free=10, wall=602
2023-05-20 14:15:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:15:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:16:23 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 3.641 | nll_loss 1.659 | ppl 3.16 | bleu 33.64 | wps 1034.3 | wpb 1392.5 | bsz 52.6 | num_updates 121
2023-05-20 14:16:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 121 updates
2023-05-20 14:16:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint1.pt
2023-05-20 14:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint1.pt
2023-05-20 14:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint1.pt (epoch 1 @ 121 updates, score 3.641) (writing took 109.79094791412354 seconds)
2023-05-20 14:18:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-20 14:18:14 | INFO | train | epoch 001 | loss 5.15 | nll_loss 3.309 | ppl 9.91 | wps 1103.7 | ups 0.18 | wpb 6056.9 | bsz 282.4 | num_updates 121 | lr 2.96684e-05 | gnorm 13.171 | clip 100 | loss_scale 0.25 | train_wall 571 | gb_free 10.1 | wall 748
2023-05-20 14:18:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:18:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 14:18:14 | INFO | fairseq.trainer | begin training epoch 2
2023-05-20 14:18:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:19:38 | INFO | train_inner | epoch 002:     19 / 130 loss=3.959, nll_loss=2.076, ppl=4.21, wps=515.9, ups=0.09, wpb=5940.1, bsz=279.7, num_updates=140, lr=2.93684e-05, gnorm=5.744, clip=100, loss_scale=0.25, train_wall=84, gb_free=9.1, wall=832
2023-05-20 14:21:01 | INFO | train_inner | epoch 002:     39 / 130 loss=3.847, nll_loss=1.963, ppl=3.9, wps=1494.9, ups=0.24, wpb=6182, bsz=287.2, num_updates=160, lr=2.90526e-05, gnorm=2.13, clip=100, loss_scale=0.25, train_wall=83, gb_free=9.3, wall=915
2023-05-20 14:22:24 | INFO | train_inner | epoch 002:     59 / 130 loss=3.789, nll_loss=1.905, ppl=3.75, wps=1427.5, ups=0.24, wpb=5933.6, bsz=272.8, num_updates=180, lr=2.87368e-05, gnorm=3.58, clip=100, loss_scale=0.25, train_wall=83, gb_free=9.4, wall=998
2023-05-20 14:23:49 | INFO | train_inner | epoch 002:     79 / 130 loss=3.784, nll_loss=1.91, ppl=3.76, wps=1460.1, ups=0.24, wpb=6175.9, bsz=292.1, num_updates=200, lr=2.84211e-05, gnorm=4.29, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.7, wall=1083
2023-05-20 14:25:13 | INFO | train_inner | epoch 002:     99 / 130 loss=3.794, nll_loss=1.932, ppl=3.82, wps=1410.4, ups=0.24, wpb=5944.1, bsz=276.5, num_updates=220, lr=2.81053e-05, gnorm=30.213, clip=100, loss_scale=0.25, train_wall=84, gb_free=9.6, wall=1167
2023-05-20 14:26:34 | INFO | train_inner | epoch 002:    119 / 130 loss=3.696, nll_loss=1.814, ppl=3.52, wps=1521.4, ups=0.25, wpb=6200.9, bsz=286.9, num_updates=240, lr=2.77895e-05, gnorm=2.232, clip=100, loss_scale=0.25, train_wall=81, gb_free=9, wall=1248
2023-05-20 14:27:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:27:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:27:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 3.344 | nll_loss 1.383 | ppl 2.61 | bleu 42.02 | wps 1005.8 | wpb 1392.5 | bsz 52.6 | num_updates 251 | best_loss 3.344
2023-05-20 14:27:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 251 updates
2023-05-20 14:27:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint2.pt
2023-05-20 14:28:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint2.pt
2023-05-20 14:30:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint2.pt (epoch 2 @ 251 updates, score 3.344) (writing took 138.03224362432957 seconds)
2023-05-20 14:30:12 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-20 14:30:12 | INFO | train | epoch 002 | loss 3.798 | nll_loss 1.92 | ppl 3.78 | wps 1096 | ups 0.18 | wpb 6056.2 | bsz 280.9 | num_updates 251 | lr 2.76158e-05 | gnorm 7.569 | clip 100 | loss_scale 0.25 | train_wall 543 | gb_free 10.1 | wall 1466
2023-05-20 14:30:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:30:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 14:30:12 | INFO | fairseq.trainer | begin training epoch 3
2023-05-20 14:30:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:30:50 | INFO | train_inner | epoch 003:      9 / 130 loss=3.597, nll_loss=1.705, ppl=3.26, wps=455.8, ups=0.08, wpb=5814.5, bsz=253.5, num_updates=260, lr=2.74737e-05, gnorm=2.017, clip=100, loss_scale=0.25, train_wall=81, gb_free=9.1, wall=1504
2023-05-20 14:32:19 | INFO | train_inner | epoch 003:     29 / 130 loss=3.532, nll_loss=1.637, ppl=3.11, wps=1363, ups=0.22, wpb=6113.8, bsz=285.2, num_updates=280, lr=2.71579e-05, gnorm=2.276, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.5, wall=1593
2023-05-20 14:33:44 | INFO | train_inner | epoch 003:     49 / 130 loss=3.502, nll_loss=1.608, ppl=3.05, wps=1411.6, ups=0.24, wpb=5985.4, bsz=280.2, num_updates=300, lr=2.68421e-05, gnorm=1.877, clip=100, loss_scale=0.25, train_wall=85, gb_free=8.8, wall=1678
2023-05-20 14:35:08 | INFO | train_inner | epoch 003:     69 / 130 loss=3.475, nll_loss=1.581, ppl=2.99, wps=1433.8, ups=0.24, wpb=6054.4, bsz=283.4, num_updates=320, lr=2.65263e-05, gnorm=2.302, clip=100, loss_scale=0.25, train_wall=84, gb_free=8.5, wall=1763
2023-05-20 14:36:35 | INFO | train_inner | epoch 003:     89 / 130 loss=3.464, nll_loss=1.571, ppl=2.97, wps=1420, ups=0.23, wpb=6136.2, bsz=284.5, num_updates=340, lr=2.62105e-05, gnorm=1.917, clip=100, loss_scale=0.25, train_wall=86, gb_free=9.8, wall=1849
2023-05-20 14:37:57 | INFO | train_inner | epoch 003:    109 / 130 loss=3.49, nll_loss=1.606, ppl=3.05, wps=1504.4, ups=0.24, wpb=6185.2, bsz=295.9, num_updates=360, lr=2.58947e-05, gnorm=5.451, clip=100, loss_scale=0.25, train_wall=82, gb_free=9.6, wall=1931
2023-05-20 14:39:23 | INFO | train_inner | epoch 003:    129 / 130 loss=3.459, nll_loss=1.571, ppl=2.97, wps=1410.7, ups=0.23, wpb=6074.6, bsz=272.8, num_updates=380, lr=2.55789e-05, gnorm=2.393, clip=100, loss_scale=0.25, train_wall=86, gb_free=9.9, wall=2017
2023-05-20 14:39:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:39:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:40:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 3.218 | nll_loss 1.239 | ppl 2.36 | bleu 46.13 | wps 972.8 | wpb 1392.5 | bsz 52.6 | num_updates 381 | best_loss 3.218
2023-05-20 14:40:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 381 updates
2023-05-20 14:40:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint3.pt
2023-05-20 14:40:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint3.pt
2023-05-20 14:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint3.pt (epoch 3 @ 381 updates, score 3.218) (writing took 86.75950962677598 seconds)
2023-05-20 14:41:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-20 14:41:28 | INFO | train | epoch 003 | loss 3.49 | nll_loss 1.598 | ppl 3.03 | wps 1164.1 | ups 0.19 | wpb 6056.2 | bsz 280.9 | num_updates 381 | lr 2.55632e-05 | gnorm 2.646 | clip 100 | loss_scale 0.25 | train_wall 551 | gb_free 10.1 | wall 2143
2023-05-20 14:41:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:41:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 14:41:29 | INFO | fairseq.trainer | begin training epoch 4
2023-05-20 14:41:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:42:50 | INFO | train_inner | epoch 004:     19 / 130 loss=3.323, nll_loss=1.412, ppl=2.66, wps=562.6, ups=0.1, wpb=5806.5, bsz=271.2, num_updates=400, lr=2.52632e-05, gnorm=2.488, clip=100, loss_scale=0.25, train_wall=81, gb_free=10, wall=2224
2023-05-20 14:44:19 | INFO | train_inner | epoch 004:     39 / 130 loss=3.3, nll_loss=1.391, ppl=2.62, wps=1338.5, ups=0.22, wpb=5984.9, bsz=250.2, num_updates=420, lr=2.49474e-05, gnorm=1.695, clip=100, loss_scale=0.25, train_wall=89, gb_free=8.2, wall=2313
2023-05-20 14:45:46 | INFO | train_inner | epoch 004:     59 / 130 loss=3.336, nll_loss=1.436, ppl=2.71, wps=1432.1, ups=0.23, wpb=6241.2, bsz=311, num_updates=440, lr=2.46316e-05, gnorm=2.449, clip=100, loss_scale=0.25, train_wall=87, gb_free=9.1, wall=2400
2023-05-20 14:47:13 | INFO | train_inner | epoch 004:     79 / 130 loss=3.294, nll_loss=1.39, ppl=2.62, wps=1389, ups=0.23, wpb=6023.5, bsz=247.1, num_updates=460, lr=2.43158e-05, gnorm=2.287, clip=100, loss_scale=0.25, train_wall=87, gb_free=9.6, wall=2487
2023-05-20 14:48:38 | INFO | train_inner | epoch 004:     99 / 130 loss=3.297, nll_loss=1.394, ppl=2.63, wps=1436.4, ups=0.24, wpb=6089.4, bsz=282.2, num_updates=480, lr=2.4e-05, gnorm=2.056, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.9, wall=2572
2023-05-20 14:50:02 | INFO | train_inner | epoch 004:    119 / 130 loss=3.322, nll_loss=1.428, ppl=2.69, wps=1464.9, ups=0.24, wpb=6189.6, bsz=318.9, num_updates=500, lr=2.36842e-05, gnorm=1.978, clip=100, loss_scale=0.25, train_wall=84, gb_free=9.7, wall=2656
2023-05-20 14:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 14:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:51:17 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.174 | nll_loss 1.218 | ppl 2.33 | bleu 47.67 | wps 1002.7 | wpb 1392.5 | bsz 52.6 | num_updates 511 | best_loss 3.174
2023-05-20 14:51:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 511 updates
2023-05-20 14:51:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint4.pt
2023-05-20 14:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint4.pt
2023-05-20 14:52:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint4.pt (epoch 4 @ 511 updates, score 3.174) (writing took 76.200498662889 seconds)
2023-05-20 14:52:33 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-20 14:52:33 | INFO | train | epoch 004 | loss 3.31 | nll_loss 1.407 | ppl 2.65 | wps 1184.8 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 511 | lr 2.35105e-05 | gnorm 2.16 | clip 100 | loss_scale 0.25 | train_wall 551 | gb_free 10.1 | wall 2807
2023-05-20 14:52:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 14:52:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 14:52:33 | INFO | fairseq.trainer | begin training epoch 5
2023-05-20 14:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 14:53:07 | INFO | train_inner | epoch 005:      9 / 130 loss=3.236, nll_loss=1.33, ppl=2.51, wps=632.4, ups=0.11, wpb=5851.2, bsz=272.6, num_updates=520, lr=2.33684e-05, gnorm=1.969, clip=100, loss_scale=0.25, train_wall=73, gb_free=8.8, wall=2842
2023-05-20 14:54:36 | INFO | train_inner | epoch 005:     29 / 130 loss=3.202, nll_loss=1.289, ppl=2.44, wps=1355.7, ups=0.23, wpb=6012.9, bsz=273.4, num_updates=540, lr=2.30526e-05, gnorm=2.843, clip=100, loss_scale=0.25, train_wall=89, gb_free=9.5, wall=2930
2023-05-20 14:56:02 | INFO | train_inner | epoch 005:     49 / 130 loss=3.198, nll_loss=1.289, ppl=2.44, wps=1429, ups=0.23, wpb=6101.3, bsz=287.1, num_updates=560, lr=2.27368e-05, gnorm=3.134, clip=100, loss_scale=0.25, train_wall=85, gb_free=10, wall=3016
2023-05-20 14:57:25 | INFO | train_inner | epoch 005:     69 / 130 loss=3.185, nll_loss=1.275, ppl=2.42, wps=1456.9, ups=0.24, wpb=6078.7, bsz=285, num_updates=580, lr=2.24211e-05, gnorm=2.088, clip=100, loss_scale=0.25, train_wall=83, gb_free=9, wall=3099
2023-05-20 14:58:50 | INFO | train_inner | epoch 005:     89 / 130 loss=3.206, nll_loss=1.296, ppl=2.46, wps=1463.3, ups=0.24, wpb=6211.6, bsz=277, num_updates=600, lr=2.21053e-05, gnorm=1.774, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.6, wall=3184
2023-05-20 15:00:13 | INFO | train_inner | epoch 005:    109 / 130 loss=3.201, nll_loss=1.296, ppl=2.46, wps=1464.8, ups=0.24, wpb=6111.4, bsz=302.9, num_updates=620, lr=2.17895e-05, gnorm=5.149, clip=100, loss_scale=0.25, train_wall=83, gb_free=9.8, wall=3267
2023-05-20 15:01:37 | INFO | train_inner | epoch 005:    129 / 130 loss=3.181, nll_loss=1.272, ppl=2.42, wps=1443, ups=0.24, wpb=6040.9, bsz=274.4, num_updates=640, lr=2.14737e-05, gnorm=1.99, clip=100, loss_scale=0.25, train_wall=84, gb_free=9.7, wall=3351
2023-05-20 15:01:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:01:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:02:15 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.163 | nll_loss 1.197 | ppl 2.29 | bleu 47.55 | wps 985.4 | wpb 1392.5 | bsz 52.6 | num_updates 641 | best_loss 3.163
2023-05-20 15:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 641 updates
2023-05-20 15:02:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint5.pt
2023-05-20 15:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint5.pt
2023-05-20 15:03:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint5.pt (epoch 5 @ 641 updates, score 3.163) (writing took 70.763329166919 seconds)
2023-05-20 15:03:25 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-20 15:03:25 | INFO | train | epoch 005 | loss 3.195 | nll_loss 1.285 | ppl 2.44 | wps 1206.6 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 641 | lr 2.14579e-05 | gnorm 2.749 | clip 100 | loss_scale 0.25 | train_wall 545 | gb_free 10.1 | wall 3460
2023-05-20 15:03:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:03:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:03:25 | INFO | fairseq.trainer | begin training epoch 6
2023-05-20 15:03:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:04:46 | INFO | train_inner | epoch 006:     19 / 130 loss=3.101, nll_loss=1.18, ppl=2.27, wps=605.6, ups=0.11, wpb=5735, bsz=253.7, num_updates=660, lr=2.11579e-05, gnorm=1.933, clip=100, loss_scale=0.25, train_wall=82, gb_free=8.9, wall=3541
2023-05-20 15:06:08 | INFO | train_inner | epoch 006:     39 / 130 loss=3.104, nll_loss=1.187, ppl=2.28, wps=1542.3, ups=0.25, wpb=6281.2, bsz=325.4, num_updates=680, lr=2.08421e-05, gnorm=1.779, clip=100, loss_scale=0.25, train_wall=81, gb_free=10.1, wall=3622
2023-05-20 15:07:34 | INFO | train_inner | epoch 006:     59 / 130 loss=3.103, nll_loss=1.183, ppl=2.27, wps=1405.9, ups=0.23, wpb=6023, bsz=272.3, num_updates=700, lr=2.05263e-05, gnorm=2.14, clip=100, loss_scale=0.25, train_wall=86, gb_free=10, wall=3708
2023-05-20 15:08:58 | INFO | train_inner | epoch 006:     79 / 130 loss=3.097, nll_loss=1.18, ppl=2.27, wps=1439.4, ups=0.24, wpb=6076.8, bsz=268.7, num_updates=720, lr=2.02105e-05, gnorm=1.728, clip=100, loss_scale=0.25, train_wall=84, gb_free=9.3, wall=3792
2023-05-20 15:10:24 | INFO | train_inner | epoch 006:     99 / 130 loss=3.117, nll_loss=1.205, ppl=2.31, wps=1430.9, ups=0.23, wpb=6157.5, bsz=277.1, num_updates=740, lr=1.98947e-05, gnorm=2.082, clip=100, loss_scale=0.25, train_wall=86, gb_free=9.7, wall=3878
2023-05-20 15:11:47 | INFO | train_inner | epoch 006:    119 / 130 loss=3.107, nll_loss=1.195, ppl=2.29, wps=1434.9, ups=0.24, wpb=5972.2, bsz=280.8, num_updates=760, lr=1.95789e-05, gnorm=2.32, clip=100, loss_scale=0.25, train_wall=83, gb_free=9.6, wall=3961
2023-05-20 15:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:12:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:13:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.143 | nll_loss 1.192 | ppl 2.28 | bleu 48.63 | wps 992.7 | wpb 1392.5 | bsz 52.6 | num_updates 771 | best_loss 3.143
2023-05-20 15:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 771 updates
2023-05-20 15:13:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint6.pt
2023-05-20 15:13:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint6.pt
2023-05-20 15:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint6.pt (epoch 6 @ 771 updates, score 3.143) (writing took 77.51353244110942 seconds)
2023-05-20 15:14:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-20 15:14:23 | INFO | train | epoch 006 | loss 3.104 | nll_loss 1.187 | ppl 2.28 | wps 1197 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 771 | lr 1.94053e-05 | gnorm 1.984 | clip 100 | loss_scale 0.25 | train_wall 543 | gb_free 10.1 | wall 4117
2023-05-20 15:14:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:14:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:14:23 | INFO | fairseq.trainer | begin training epoch 7
2023-05-20 15:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:15:03 | INFO | train_inner | epoch 007:      9 / 130 loss=3.059, nll_loss=1.139, ppl=2.2, wps=605.8, ups=0.1, wpb=5922.9, bsz=276.2, num_updates=780, lr=1.92632e-05, gnorm=1.91, clip=100, loss_scale=0.25, train_wall=82, gb_free=9.4, wall=4157
2023-05-20 15:16:31 | INFO | train_inner | epoch 007:     29 / 130 loss=3.005, nll_loss=1.077, ppl=2.11, wps=1370.2, ups=0.23, wpb=6022.5, bsz=264.9, num_updates=800, lr=1.89474e-05, gnorm=1.981, clip=100, loss_scale=0.25, train_wall=88, gb_free=10, wall=4245
2023-05-20 15:17:52 | INFO | train_inner | epoch 007:     49 / 130 loss=3.028, nll_loss=1.103, ppl=2.15, wps=1505.8, ups=0.25, wpb=6097.5, bsz=269.8, num_updates=820, lr=1.86316e-05, gnorm=2.197, clip=100, loss_scale=0.25, train_wall=81, gb_free=9.4, wall=4326
2023-05-20 15:19:16 | INFO | train_inner | epoch 007:     69 / 130 loss=3.07, nll_loss=1.152, ppl=2.22, wps=1489.5, ups=0.24, wpb=6262.4, bsz=320.1, num_updates=840, lr=1.83158e-05, gnorm=2.134, clip=100, loss_scale=0.25, train_wall=84, gb_free=8.6, wall=4410
2023-05-20 15:20:42 | INFO | train_inner | epoch 007:     89 / 130 loss=3.02, nll_loss=1.097, ppl=2.14, wps=1394, ups=0.23, wpb=6040.7, bsz=287.1, num_updates=860, lr=1.8e-05, gnorm=1.571, clip=100, loss_scale=0.25, train_wall=87, gb_free=9.5, wall=4497
2023-05-20 15:22:10 | INFO | train_inner | epoch 007:    109 / 130 loss=3.036, nll_loss=1.117, ppl=2.17, wps=1369.3, ups=0.23, wpb=6010.2, bsz=280.9, num_updates=880, lr=1.76842e-05, gnorm=1.75, clip=100, loss_scale=0.25, train_wall=88, gb_free=8.9, wall=4584
2023-05-20 15:23:31 | INFO | train_inner | epoch 007:    129 / 130 loss=3.028, nll_loss=1.107, ppl=2.15, wps=1509.5, ups=0.25, wpb=6128.2, bsz=276.1, num_updates=900, lr=1.73684e-05, gnorm=1.568, clip=100, loss_scale=0.25, train_wall=81, gb_free=9.8, wall=4666
2023-05-20 15:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:23:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:24:09 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.138 | nll_loss 1.202 | ppl 2.3 | bleu 48.78 | wps 990.6 | wpb 1392.5 | bsz 52.6 | num_updates 901 | best_loss 3.138
2023-05-20 15:24:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 901 updates
2023-05-20 15:24:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint7.pt
2023-05-20 15:24:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint7.pt
2023-05-20 15:25:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint7.pt (epoch 7 @ 901 updates, score 3.138) (writing took 68.2839903794229 seconds)
2023-05-20 15:25:17 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-20 15:25:17 | INFO | train | epoch 007 | loss 3.031 | nll_loss 1.108 | ppl 2.16 | wps 1203.6 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 901 | lr 1.73526e-05 | gnorm 1.876 | clip 100 | loss_scale 0.25 | train_wall 549 | gb_free 10.1 | wall 4771
2023-05-20 15:25:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:25:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:25:17 | INFO | fairseq.trainer | begin training epoch 8
2023-05-20 15:25:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:26:46 | INFO | train_inner | epoch 008:     19 / 130 loss=2.961, nll_loss=1.03, ppl=2.04, wps=620.5, ups=0.1, wpb=6029.8, bsz=277.2, num_updates=920, lr=1.70526e-05, gnorm=1.602, clip=100, loss_scale=0.25, train_wall=90, gb_free=9.1, wall=4860
2023-05-20 15:28:11 | INFO | train_inner | epoch 008:     39 / 130 loss=2.953, nll_loss=1.024, ppl=2.03, wps=1395.6, ups=0.23, wpb=5963.9, bsz=272.1, num_updates=940, lr=1.67368e-05, gnorm=1.69, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.4, wall=4945
2023-05-20 15:29:38 | INFO | train_inner | epoch 008:     59 / 130 loss=2.973, nll_loss=1.045, ppl=2.06, wps=1394.9, ups=0.23, wpb=6043.2, bsz=279.4, num_updates=960, lr=1.64211e-05, gnorm=1.643, clip=100, loss_scale=0.25, train_wall=87, gb_free=10, wall=5032
2023-05-20 15:31:03 | INFO | train_inner | epoch 008:     79 / 130 loss=2.982, nll_loss=1.057, ppl=2.08, wps=1426.4, ups=0.24, wpb=6057.4, bsz=273.5, num_updates=980, lr=1.61053e-05, gnorm=1.727, clip=100, loss_scale=0.25, train_wall=85, gb_free=9.8, wall=5117
2023-05-20 15:32:25 | INFO | train_inner | epoch 008:     99 / 130 loss=2.967, nll_loss=1.04, ppl=2.06, wps=1493.3, ups=0.24, wpb=6131.9, bsz=304.4, num_updates=1000, lr=1.57895e-05, gnorm=1.878, clip=100, loss_scale=0.25, train_wall=82, gb_free=9.4, wall=5199
2023-05-20 15:33:46 | INFO | train_inner | epoch 008:    119 / 130 loss=2.96, nll_loss=1.033, ppl=2.05, wps=1478.2, ups=0.25, wpb=5966.4, bsz=266.6, num_updates=1020, lr=1.54737e-05, gnorm=1.815, clip=100, loss_scale=0.25, train_wall=81, gb_free=10.1, wall=5280
2023-05-20 15:34:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:34:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:35:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.147 | nll_loss 1.204 | ppl 2.3 | bleu 48.66 | wps 999.1 | wpb 1392.5 | bsz 52.6 | num_updates 1031 | best_loss 3.138
2023-05-20 15:35:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1031 updates
2023-05-20 15:35:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint8.pt
2023-05-20 15:35:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint8.pt
2023-05-20 15:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint8.pt (epoch 8 @ 1031 updates, score 3.147) (writing took 64.35045949369669 seconds)
2023-05-20 15:36:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-20 15:36:09 | INFO | train | epoch 008 | loss 2.97 | nll_loss 1.043 | ppl 2.06 | wps 1207.6 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 1031 | lr 1.53e-05 | gnorm 1.746 | clip 100 | loss_scale 0.5 | train_wall 551 | gb_free 10 | wall 5423
2023-05-20 15:36:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:36:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:36:09 | INFO | fairseq.trainer | begin training epoch 9
2023-05-20 15:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:36:47 | INFO | train_inner | epoch 009:      9 / 130 loss=2.954, nll_loss=1.028, ppl=2.04, wps=662.7, ups=0.11, wpb=5994.4, bsz=281.9, num_updates=1040, lr=1.51579e-05, gnorm=2.004, clip=100, loss_scale=0.5, train_wall=80, gb_free=10, wall=5461
2023-05-20 15:38:13 | INFO | train_inner | epoch 009:     29 / 130 loss=2.918, nll_loss=0.983, ppl=1.98, wps=1408.9, ups=0.23, wpb=6097.5, bsz=285, num_updates=1060, lr=1.48421e-05, gnorm=2.072, clip=100, loss_scale=0.5, train_wall=86, gb_free=8.5, wall=5547
2023-05-20 15:39:37 | INFO | train_inner | epoch 009:     49 / 130 loss=2.906, nll_loss=0.972, ppl=1.96, wps=1452.8, ups=0.24, wpb=6086.2, bsz=270.3, num_updates=1080, lr=1.45263e-05, gnorm=1.694, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.3, wall=5631
2023-05-20 15:40:59 | INFO | train_inner | epoch 009:     69 / 130 loss=2.909, nll_loss=0.977, ppl=1.97, wps=1458.1, ups=0.25, wpb=5945, bsz=266.3, num_updates=1100, lr=1.42105e-05, gnorm=2.31, clip=100, loss_scale=0.5, train_wall=81, gb_free=9.6, wall=5713
2023-05-20 15:42:22 | INFO | train_inner | epoch 009:     89 / 130 loss=2.913, nll_loss=0.98, ppl=1.97, wps=1439.6, ups=0.24, wpb=6007.2, bsz=280.2, num_updates=1120, lr=1.38947e-05, gnorm=1.509, clip=100, loss_scale=0.5, train_wall=83, gb_free=9.9, wall=5796
2023-05-20 15:43:44 | INFO | train_inner | epoch 009:    109 / 130 loss=2.905, nll_loss=0.974, ppl=1.96, wps=1497.9, ups=0.24, wpb=6178.9, bsz=295.8, num_updates=1140, lr=1.35789e-05, gnorm=2.457, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.8, wall=5879
2023-05-20 15:45:06 | INFO | train_inner | epoch 009:    129 / 130 loss=2.94, nll_loss=1.014, ppl=2.02, wps=1517.3, ups=0.24, wpb=6218.3, bsz=299.2, num_updates=1160, lr=1.32632e-05, gnorm=1.864, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.8, wall=5961
2023-05-20 15:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:45:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:45:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.146 | nll_loss 1.201 | ppl 2.3 | bleu 49.08 | wps 983.5 | wpb 1392.5 | bsz 52.6 | num_updates 1161 | best_loss 3.138
2023-05-20 15:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1161 updates
2023-05-20 15:45:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint9.pt
2023-05-20 15:46:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint9.pt
2023-05-20 15:46:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint9.pt (epoch 9 @ 1161 updates, score 3.146) (writing took 26.542793594300747 seconds)
2023-05-20 15:46:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-20 15:46:10 | INFO | train | epoch 009 | loss 2.913 | nll_loss 0.981 | ppl 1.97 | wps 1309.9 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 1161 | lr 1.32474e-05 | gnorm 1.983 | clip 100 | loss_scale 0.5 | train_wall 537 | gb_free 10.1 | wall 6024
2023-05-20 15:46:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:46:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:46:10 | INFO | fairseq.trainer | begin training epoch 10
2023-05-20 15:46:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:47:30 | INFO | train_inner | epoch 010:     19 / 130 loss=2.85, nll_loss=0.91, ppl=1.88, wps=819.9, ups=0.14, wpb=5878.9, bsz=282.4, num_updates=1180, lr=1.29474e-05, gnorm=2.641, clip=100, loss_scale=0.5, train_wall=80, gb_free=9.8, wall=6104
2023-05-20 15:48:53 | INFO | train_inner | epoch 010:     39 / 130 loss=2.854, nll_loss=0.916, ppl=1.89, wps=1463, ups=0.24, wpb=6076.4, bsz=264.1, num_updates=1200, lr=1.26316e-05, gnorm=1.434, clip=100, loss_scale=0.5, train_wall=83, gb_free=9.5, wall=6187
2023-05-20 15:50:22 | INFO | train_inner | epoch 010:     59 / 130 loss=2.854, nll_loss=0.917, ppl=1.89, wps=1392.9, ups=0.22, wpb=6194.8, bsz=276.4, num_updates=1220, lr=1.23158e-05, gnorm=1.716, clip=100, loss_scale=0.5, train_wall=89, gb_free=8.7, wall=6276
2023-05-20 15:51:46 | INFO | train_inner | epoch 010:     79 / 130 loss=2.865, nll_loss=0.928, ppl=1.9, wps=1421.4, ups=0.24, wpb=6009.3, bsz=268.6, num_updates=1240, lr=1.2e-05, gnorm=1.804, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.4, wall=6361
2023-05-20 15:53:08 | INFO | train_inner | epoch 010:     99 / 130 loss=2.879, nll_loss=0.944, ppl=1.92, wps=1479.2, ups=0.24, wpb=6061.8, bsz=285.8, num_updates=1260, lr=1.16842e-05, gnorm=1.647, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.6, wall=6442
2023-05-20 15:54:35 | INFO | train_inner | epoch 010:    119 / 130 loss=2.895, nll_loss=0.964, ppl=1.95, wps=1409.4, ups=0.23, wpb=6123.9, bsz=301.3, num_updates=1280, lr=1.13684e-05, gnorm=1.542, clip=100, loss_scale=0.5, train_wall=87, gb_free=9.2, wall=6529
2023-05-20 15:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 15:55:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:55:54 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.155 | nll_loss 1.216 | ppl 2.32 | bleu 48.72 | wps 1016.9 | wpb 1392.5 | bsz 52.6 | num_updates 1291 | best_loss 3.138
2023-05-20 15:55:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1291 updates
2023-05-20 15:55:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint10.pt
2023-05-20 15:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint10.pt
2023-05-20 15:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint10.pt (epoch 10 @ 1291 updates, score 3.155) (writing took 20.4202238060534 seconds)
2023-05-20 15:56:14 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-20 15:56:14 | INFO | train | epoch 010 | loss 2.868 | nll_loss 0.931 | ppl 1.91 | wps 1303.7 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 1291 | lr 1.11947e-05 | gnorm 1.813 | clip 100 | loss_scale 0.5 | train_wall 548 | gb_free 10.1 | wall 6628
2023-05-20 15:56:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 15:56:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 15:56:14 | INFO | fairseq.trainer | begin training epoch 11
2023-05-20 15:56:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 15:56:54 | INFO | train_inner | epoch 011:      9 / 130 loss=2.86, nll_loss=0.924, ppl=1.9, wps=848.3, ups=0.14, wpb=5864, bsz=280.5, num_updates=1300, lr=1.10526e-05, gnorm=1.793, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.1, wall=6668
2023-05-20 15:58:18 | INFO | train_inner | epoch 011:     29 / 130 loss=2.839, nll_loss=0.898, ppl=1.86, wps=1465.1, ups=0.24, wpb=6187.2, bsz=289.2, num_updates=1320, lr=1.07368e-05, gnorm=1.699, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.2, wall=6752
2023-05-20 15:59:43 | INFO | train_inner | epoch 011:     49 / 130 loss=2.814, nll_loss=0.872, ppl=1.83, wps=1405.7, ups=0.23, wpb=6003.2, bsz=266.8, num_updates=1340, lr=1.04211e-05, gnorm=1.533, clip=100, loss_scale=0.5, train_wall=85, gb_free=9.4, wall=6838
2023-05-20 16:01:07 | INFO | train_inner | epoch 011:     69 / 130 loss=2.826, nll_loss=0.885, ppl=1.85, wps=1451.8, ups=0.24, wpb=6085, bsz=299.4, num_updates=1360, lr=1.01053e-05, gnorm=2.252, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.5, wall=6921
2023-05-20 16:02:36 | INFO | train_inner | epoch 011:     89 / 130 loss=2.836, nll_loss=0.898, ppl=1.86, wps=1382.6, ups=0.23, wpb=6124.2, bsz=276.9, num_updates=1380, lr=9.78947e-06, gnorm=1.731, clip=100, loss_scale=0.5, train_wall=88, gb_free=9.8, wall=7010
2023-05-20 16:03:58 | INFO | train_inner | epoch 011:    109 / 130 loss=2.828, nll_loss=0.89, ppl=1.85, wps=1464.3, ups=0.24, wpb=5996.3, bsz=287.4, num_updates=1400, lr=9.47368e-06, gnorm=1.871, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.2, wall=7092
2023-05-20 16:05:22 | INFO | train_inner | epoch 011:    129 / 130 loss=2.825, nll_loss=0.886, ppl=1.85, wps=1453.2, ups=0.24, wpb=6122.2, bsz=274.2, num_updates=1420, lr=9.15789e-06, gnorm=1.663, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.7, wall=7176
2023-05-20 16:05:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:05:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:05:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.158 | nll_loss 1.217 | ppl 2.32 | bleu 49.02 | wps 995.7 | wpb 1392.5 | bsz 52.6 | num_updates 1421 | best_loss 3.138
2023-05-20 16:05:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1421 updates
2023-05-20 16:05:59 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint11.pt
2023-05-20 16:06:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint11.pt
2023-05-20 16:06:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint11.pt (epoch 11 @ 1421 updates, score 3.158) (writing took 20.729544423520565 seconds)
2023-05-20 16:06:20 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-20 16:06:20 | INFO | train | epoch 011 | loss 2.828 | nll_loss 0.889 | ppl 1.85 | wps 1300.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 1421 | lr 9.14211e-06 | gnorm 1.776 | clip 100 | loss_scale 0.5 | train_wall 548 | gb_free 10.1 | wall 7234
2023-05-20 16:06:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:06:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:06:20 | INFO | fairseq.trainer | begin training epoch 12
2023-05-20 16:06:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:07:39 | INFO | train_inner | epoch 012:     19 / 130 loss=2.793, nll_loss=0.849, ppl=1.8, wps=878.2, ups=0.15, wpb=6030.9, bsz=290.3, num_updates=1440, lr=8.84211e-06, gnorm=1.528, clip=100, loss_scale=0.5, train_wall=80, gb_free=9.2, wall=7313
2023-05-20 16:09:03 | INFO | train_inner | epoch 012:     39 / 130 loss=2.788, nll_loss=0.844, ppl=1.8, wps=1426.6, ups=0.24, wpb=5982, bsz=264.2, num_updates=1460, lr=8.52632e-06, gnorm=1.544, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.7, wall=7397
2023-05-20 16:10:26 | INFO | train_inner | epoch 012:     59 / 130 loss=2.805, nll_loss=0.863, ppl=1.82, wps=1473.5, ups=0.24, wpb=6076.8, bsz=295.5, num_updates=1480, lr=8.21053e-06, gnorm=1.35, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.8, wall=7480
2023-05-20 16:11:48 | INFO | train_inner | epoch 012:     79 / 130 loss=2.792, nll_loss=0.849, ppl=1.8, wps=1487.8, ups=0.24, wpb=6157.8, bsz=285.3, num_updates=1500, lr=7.89474e-06, gnorm=1.369, clip=100, loss_scale=0.5, train_wall=83, gb_free=8.9, wall=7563
2023-05-20 16:13:15 | INFO | train_inner | epoch 012:     99 / 130 loss=2.813, nll_loss=0.871, ppl=1.83, wps=1406.4, ups=0.23, wpb=6071.1, bsz=277.8, num_updates=1520, lr=7.57895e-06, gnorm=1.37, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.1, wall=7649
2023-05-20 16:14:37 | INFO | train_inner | epoch 012:    119 / 130 loss=2.789, nll_loss=0.846, ppl=1.8, wps=1466.7, ups=0.24, wpb=6051.6, bsz=274.5, num_updates=1540, lr=7.26316e-06, gnorm=1.582, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.7, wall=7731
2023-05-20 16:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:15:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:15:57 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.168 | nll_loss 1.23 | ppl 2.35 | bleu 49.31 | wps 977.2 | wpb 1392.5 | bsz 52.6 | num_updates 1551 | best_loss 3.138
2023-05-20 16:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1551 updates
2023-05-20 16:15:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint12.pt
2023-05-20 16:16:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint12.pt
2023-05-20 16:16:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint12.pt (epoch 12 @ 1551 updates, score 3.168) (writing took 34.91169549524784 seconds)
2023-05-20 16:16:32 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-20 16:16:32 | INFO | train | epoch 012 | loss 2.796 | nll_loss 0.853 | ppl 1.81 | wps 1285.5 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 1551 | lr 7.08947e-06 | gnorm 1.443 | clip 100 | loss_scale 0.5 | train_wall 540 | gb_free 10.1 | wall 7846
2023-05-20 16:16:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:16:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:16:32 | INFO | fairseq.trainer | begin training epoch 13
2023-05-20 16:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:17:09 | INFO | train_inner | epoch 013:      9 / 130 loss=2.795, nll_loss=0.851, ppl=1.8, wps=774.7, ups=0.13, wpb=5890, bsz=281.4, num_updates=1560, lr=6.94737e-06, gnorm=1.36, clip=100, loss_scale=0.5, train_wall=80, gb_free=9.9, wall=7883
2023-05-20 16:18:36 | INFO | train_inner | epoch 013:     29 / 130 loss=2.784, nll_loss=0.839, ppl=1.79, wps=1413.8, ups=0.23, wpb=6141.6, bsz=283.7, num_updates=1580, lr=6.63158e-06, gnorm=1.365, clip=100, loss_scale=0.5, train_wall=87, gb_free=10, wall=7970
2023-05-20 16:20:02 | INFO | train_inner | epoch 013:     49 / 130 loss=2.734, nll_loss=0.784, ppl=1.72, wps=1355.9, ups=0.23, wpb=5837.5, bsz=238.9, num_updates=1600, lr=6.31579e-06, gnorm=1.397, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.8, wall=8056
2023-05-20 16:21:25 | INFO | train_inner | epoch 013:     69 / 130 loss=2.765, nll_loss=0.819, ppl=1.76, wps=1464.5, ups=0.24, wpb=6022.7, bsz=297.8, num_updates=1620, lr=6e-06, gnorm=1.342, clip=100, loss_scale=0.5, train_wall=82, gb_free=8.9, wall=8139
2023-05-20 16:22:52 | INFO | train_inner | epoch 013:     89 / 130 loss=2.784, nll_loss=0.839, ppl=1.79, wps=1431.2, ups=0.23, wpb=6218.3, bsz=299.2, num_updates=1640, lr=5.68421e-06, gnorm=1.416, clip=100, loss_scale=0.5, train_wall=87, gb_free=7.8, wall=8226
2023-05-20 16:24:14 | INFO | train_inner | epoch 013:    109 / 130 loss=2.769, nll_loss=0.824, ppl=1.77, wps=1475.8, ups=0.24, wpb=6071.7, bsz=280.2, num_updates=1660, lr=5.36842e-06, gnorm=1.43, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.5, wall=8308
2023-05-20 16:25:38 | INFO | train_inner | epoch 013:    129 / 130 loss=2.762, nll_loss=0.816, ppl=1.76, wps=1472.5, ups=0.24, wpb=6178.6, bsz=282.8, num_updates=1680, lr=5.05263e-06, gnorm=1.496, clip=100, loss_scale=0.5, train_wall=84, gb_free=10, wall=8392
2023-05-20 16:25:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:25:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:26:15 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.171 | nll_loss 1.239 | ppl 2.36 | bleu 49.24 | wps 979.8 | wpb 1392.5 | bsz 52.6 | num_updates 1681 | best_loss 3.138
2023-05-20 16:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1681 updates
2023-05-20 16:26:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint13.pt
2023-05-20 16:26:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint13.pt
2023-05-20 16:26:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint13.pt (epoch 13 @ 1681 updates, score 3.171) (writing took 33.4615597948432 seconds)
2023-05-20 16:26:49 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-20 16:26:49 | INFO | train | epoch 013 | loss 2.769 | nll_loss 0.823 | ppl 1.77 | wps 1277.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 1681 | lr 5.03684e-06 | gnorm 1.411 | clip 100 | loss_scale 0.5 | train_wall 546 | gb_free 10.1 | wall 8463
2023-05-20 16:26:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:26:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:26:49 | INFO | fairseq.trainer | begin training epoch 14
2023-05-20 16:26:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:28:09 | INFO | train_inner | epoch 014:     19 / 130 loss=2.737, nll_loss=0.788, ppl=1.73, wps=766.5, ups=0.13, wpb=5798.4, bsz=243, num_updates=1700, lr=4.73684e-06, gnorm=1.322, clip=100, loss_scale=0.5, train_wall=81, gb_free=9.9, wall=8543
2023-05-20 16:29:30 | INFO | train_inner | epoch 014:     39 / 130 loss=2.737, nll_loss=0.788, ppl=1.73, wps=1523.6, ups=0.25, wpb=6170.6, bsz=291.3, num_updates=1720, lr=4.42105e-06, gnorm=1.218, clip=100, loss_scale=0.5, train_wall=81, gb_free=9.2, wall=8624
2023-05-20 16:30:56 | INFO | train_inner | epoch 014:     59 / 130 loss=2.765, nll_loss=0.82, ppl=1.77, wps=1450.3, ups=0.23, wpb=6219.6, bsz=296.8, num_updates=1740, lr=4.10526e-06, gnorm=1.442, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.6, wall=8710
2023-05-20 16:32:20 | INFO | train_inner | epoch 014:     79 / 130 loss=2.749, nll_loss=0.8, ppl=1.74, wps=1417.4, ups=0.24, wpb=5966.2, bsz=270.4, num_updates=1760, lr=3.78947e-06, gnorm=1.627, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.5, wall=8794
2023-05-20 16:33:49 | INFO | train_inner | epoch 014:     99 / 130 loss=2.734, nll_loss=0.785, ppl=1.72, wps=1362.1, ups=0.23, wpb=6035.7, bsz=272.4, num_updates=1780, lr=3.47368e-06, gnorm=1.376, clip=100, loss_scale=0.5, train_wall=89, gb_free=9.9, wall=8883
2023-05-20 16:35:14 | INFO | train_inner | epoch 014:    119 / 130 loss=2.751, nll_loss=0.804, ppl=1.75, wps=1418.5, ups=0.23, wpb=6053, bsz=299.1, num_updates=1800, lr=3.15789e-06, gnorm=1.323, clip=100, loss_scale=0.5, train_wall=85, gb_free=9.1, wall=8968
2023-05-20 16:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:35:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:36:32 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.175 | nll_loss 1.244 | ppl 2.37 | bleu 49.15 | wps 991.6 | wpb 1392.5 | bsz 52.6 | num_updates 1811 | best_loss 3.138
2023-05-20 16:36:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1811 updates
2023-05-20 16:36:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint14.pt
2023-05-20 16:36:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint14.pt
2023-05-20 16:37:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint14.pt (epoch 14 @ 1811 updates, score 3.175) (writing took 64.64320467412472 seconds)
2023-05-20 16:37:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-20 16:37:37 | INFO | train | epoch 014 | loss 2.747 | nll_loss 0.799 | ppl 1.74 | wps 1214.1 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 1811 | lr 2.98421e-06 | gnorm 1.396 | clip 100 | loss_scale 0.5 | train_wall 547 | gb_free 10.1 | wall 9111
2023-05-20 16:37:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:37:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:37:37 | INFO | fairseq.trainer | begin training epoch 15
2023-05-20 16:37:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:38:19 | INFO | train_inner | epoch 015:      9 / 130 loss=2.738, nll_loss=0.79, ppl=1.73, wps=634.8, ups=0.11, wpb=5881.9, bsz=278.8, num_updates=1820, lr=2.84211e-06, gnorm=1.634, clip=100, loss_scale=0.5, train_wall=84, gb_free=9.2, wall=9153
2023-05-20 16:39:47 | INFO | train_inner | epoch 015:     29 / 130 loss=2.732, nll_loss=0.783, ppl=1.72, wps=1382, ups=0.23, wpb=6033.1, bsz=286.6, num_updates=1840, lr=2.52632e-06, gnorm=2.067, clip=100, loss_scale=0.5, train_wall=87, gb_free=9.5, wall=9241
2023-05-20 16:41:13 | INFO | train_inner | epoch 015:     49 / 130 loss=2.75, nll_loss=0.803, ppl=1.74, wps=1415.3, ups=0.23, wpb=6150.7, bsz=288.1, num_updates=1860, lr=2.21053e-06, gnorm=1.561, clip=100, loss_scale=0.5, train_wall=87, gb_free=9.8, wall=9328
2023-05-20 16:42:32 | INFO | train_inner | epoch 015:     69 / 130 loss=2.738, nll_loss=0.79, ppl=1.73, wps=1582.6, ups=0.26, wpb=6192.5, bsz=290.7, num_updates=1880, lr=1.89474e-06, gnorm=1.285, clip=100, loss_scale=0.5, train_wall=78, gb_free=9.6, wall=9406
2023-05-20 16:43:58 | INFO | train_inner | epoch 015:     89 / 130 loss=2.712, nll_loss=0.76, ppl=1.69, wps=1405.7, ups=0.23, wpb=6083, bsz=265.6, num_updates=1900, lr=1.57895e-06, gnorm=1.635, clip=100, loss_scale=0.5, train_wall=86, gb_free=10.1, wall=9492
2023-05-20 16:45:20 | INFO | train_inner | epoch 015:    109 / 130 loss=2.729, nll_loss=0.781, ppl=1.72, wps=1472.8, ups=0.24, wpb=6027.1, bsz=280.1, num_updates=1920, lr=1.26316e-06, gnorm=1.479, clip=100, loss_scale=0.5, train_wall=82, gb_free=10, wall=9574
2023-05-20 16:46:42 | INFO | train_inner | epoch 015:    129 / 130 loss=2.731, nll_loss=0.78, ppl=1.72, wps=1489.9, ups=0.24, wpb=6126.2, bsz=288.9, num_updates=1940, lr=9.47368e-07, gnorm=5.58, clip=100, loss_scale=0.5, train_wall=82, gb_free=9.7, wall=9656
2023-05-20 16:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:46:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:47:19 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.18 | nll_loss 1.246 | ppl 2.37 | bleu 49.31 | wps 1001.9 | wpb 1392.5 | bsz 52.6 | num_updates 1941 | best_loss 3.138
2023-05-20 16:47:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1941 updates
2023-05-20 16:47:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint15.pt
2023-05-20 16:47:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint15.pt
2023-05-20 16:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint15.pt (epoch 15 @ 1941 updates, score 3.18) (writing took 32.04594994336367 seconds)
2023-05-20 16:47:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-20 16:47:51 | INFO | train | epoch 015 | loss 2.731 | nll_loss 0.781 | ppl 1.72 | wps 1282.2 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 1941 | lr 9.31579e-07 | gnorm 2.226 | clip 100 | loss_scale 0.5 | train_wall 545 | gb_free 10.1 | wall 9725
2023-05-20 16:47:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:47:51 | INFO | fairseq.trainer | begin training epoch 16
2023-05-20 16:47:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:49:15 | INFO | train_inner | epoch 016:     19 / 130 loss=2.727, nll_loss=0.778, ppl=1.71, wps=771.4, ups=0.13, wpb=5897.6, bsz=271.9, num_updates=1960, lr=6.31579e-07, gnorm=1.277, clip=100, loss_scale=0.5, train_wall=84, gb_free=10.1, wall=9809
2023-05-20 16:50:37 | INFO | train_inner | epoch 016:     39 / 130 loss=2.731, nll_loss=0.783, ppl=1.72, wps=1454.4, ups=0.24, wpb=5978.1, bsz=298.5, num_updates=1980, lr=3.15789e-07, gnorm=1.235, clip=100, loss_scale=0.5, train_wall=82, gb_free=10.1, wall=9892
2023-05-20 16:52:04 | INFO | train_inner | epoch 016:     59 / 130 loss=2.727, nll_loss=0.776, ppl=1.71, wps=1406, ups=0.23, wpb=6088.1, bsz=288.6, num_updates=2000, lr=0, gnorm=1.295, clip=100, loss_scale=0.5, train_wall=86, gb_free=8.9, wall=9978
2023-05-20 16:53:30 | INFO | train_inner | epoch 016:     79 / 130 loss=2.714, nll_loss=0.763, ppl=1.7, wps=1421.7, ups=0.23, wpb=6088.5, bsz=292.4, num_updates=2020, lr=0, gnorm=1.338, clip=100, loss_scale=0.5, train_wall=86, gb_free=9.5, wall=10064
2023-05-20 16:54:53 | INFO | train_inner | epoch 016:     99 / 130 loss=2.718, nll_loss=0.765, ppl=1.7, wps=1478.8, ups=0.24, wpb=6178.8, bsz=278.2, num_updates=2040, lr=0, gnorm=1.3, clip=100, loss_scale=0.5, train_wall=83, gb_free=8.8, wall=10147
2023-05-20 16:56:17 | INFO | train_inner | epoch 016:    119 / 130 loss=2.727, nll_loss=0.774, ppl=1.71, wps=1472.8, ups=0.24, wpb=6129.2, bsz=269.4, num_updates=2060, lr=0, gnorm=1.753, clip=100, loss_scale=1, train_wall=83, gb_free=9.7, wall=10231
2023-05-20 16:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 16:56:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:57:34 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 992.9 | wpb 1392.5 | bsz 52.6 | num_updates 2071 | best_loss 3.138
2023-05-20 16:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2071 updates
2023-05-20 16:57:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint16.pt
2023-05-20 16:57:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint16.pt
2023-05-20 16:58:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint16.pt (epoch 16 @ 2071 updates, score 3.184) (writing took 29.137060817331076 seconds)
2023-05-20 16:58:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-20 16:58:03 | INFO | train | epoch 016 | loss 2.723 | nll_loss 0.772 | ppl 1.71 | wps 1286.6 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2071 | lr 0 | gnorm 1.376 | clip 100 | loss_scale 1 | train_wall 545 | gb_free 10.1 | wall 10337
2023-05-20 16:58:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 16:58:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 16:58:03 | INFO | fairseq.trainer | begin training epoch 17
2023-05-20 16:58:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 16:58:44 | INFO | train_inner | epoch 017:      9 / 130 loss=2.704, nll_loss=0.751, ppl=1.68, wps=773.3, ups=0.14, wpb=5708.6, bsz=241.7, num_updates=2080, lr=0, gnorm=3.919, clip=100, loss_scale=1, train_wall=82, gb_free=9.7, wall=10378
2023-05-20 17:00:09 | INFO | train_inner | epoch 017:     29 / 130 loss=2.732, nll_loss=0.782, ppl=1.72, wps=1449.6, ups=0.23, wpb=6172.1, bsz=302.8, num_updates=2100, lr=0, gnorm=1.43, clip=100, loss_scale=1, train_wall=85, gb_free=9.6, wall=10463
2023-05-20 17:01:31 | INFO | train_inner | epoch 017:     49 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1519.9, ups=0.24, wpb=6207.7, bsz=291.8, num_updates=2120, lr=0, gnorm=1.367, clip=100, loss_scale=1, train_wall=82, gb_free=9.7, wall=10545
2023-05-20 17:02:55 | INFO | train_inner | epoch 017:     69 / 130 loss=2.739, nll_loss=0.789, ppl=1.73, wps=1452.9, ups=0.24, wpb=6112.5, bsz=287.9, num_updates=2140, lr=0, gnorm=1.325, clip=100, loss_scale=1, train_wall=84, gb_free=9, wall=10629
2023-05-20 17:04:20 | INFO | train_inner | epoch 017:     89 / 130 loss=2.73, nll_loss=0.778, ppl=1.72, wps=1441.7, ups=0.24, wpb=6118.9, bsz=272.2, num_updates=2160, lr=0, gnorm=2.323, clip=100, loss_scale=1, train_wall=85, gb_free=9.1, wall=10714
2023-05-20 17:05:44 | INFO | train_inner | epoch 017:    109 / 130 loss=2.713, nll_loss=0.761, ppl=1.69, wps=1447.3, ups=0.24, wpb=6063.8, bsz=279.4, num_updates=2180, lr=0, gnorm=1.966, clip=100, loss_scale=1, train_wall=84, gb_free=9.4, wall=10798
2023-05-20 17:07:08 | INFO | train_inner | epoch 017:    129 / 130 loss=2.717, nll_loss=0.764, ppl=1.7, wps=1429.2, ups=0.24, wpb=5983.4, bsz=277.2, num_updates=2200, lr=0, gnorm=1.281, clip=100, loss_scale=1, train_wall=84, gb_free=9.9, wall=10882
2023-05-20 17:07:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:07:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:07:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 964.1 | wpb 1392.5 | bsz 52.6 | num_updates 2201 | best_loss 3.138
2023-05-20 17:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2201 updates
2023-05-20 17:07:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint17.pt
2023-05-20 17:08:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint17.pt
2023-05-20 17:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint17.pt (epoch 17 @ 2201 updates, score 3.184) (writing took 34.44714907184243 seconds)
2023-05-20 17:08:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-20 17:08:20 | INFO | train | epoch 017 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1276.2 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2201 | lr 0 | gnorm 1.981 | clip 100 | loss_scale 1 | train_wall 545 | gb_free 10.1 | wall 10954
2023-05-20 17:08:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:08:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 17:08:20 | INFO | fairseq.trainer | begin training epoch 18
2023-05-20 17:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:09:45 | INFO | train_inner | epoch 018:     19 / 130 loss=2.726, nll_loss=0.775, ppl=1.71, wps=746.7, ups=0.13, wpb=5872.1, bsz=261.9, num_updates=2220, lr=0, gnorm=1.621, clip=100, loss_scale=1, train_wall=86, gb_free=9.8, wall=11039
2023-05-20 17:11:15 | INFO | train_inner | epoch 018:     39 / 130 loss=2.724, nll_loss=0.774, ppl=1.71, wps=1354, ups=0.22, wpb=6118.1, bsz=285.6, num_updates=2240, lr=0, gnorm=1.964, clip=100, loss_scale=1, train_wall=90, gb_free=9.4, wall=11129
2023-05-20 17:12:43 | INFO | train_inner | epoch 018:     59 / 130 loss=2.707, nll_loss=0.753, ppl=1.69, wps=1357.4, ups=0.23, wpb=5966.2, bsz=269.6, num_updates=2260, lr=0, gnorm=1.397, clip=100, loss_scale=1, train_wall=88, gb_free=8.1, wall=11217
2023-05-20 17:14:08 | INFO | train_inner | epoch 018:     79 / 130 loss=2.702, nll_loss=0.748, ppl=1.68, wps=1429.1, ups=0.24, wpb=6058, bsz=264.9, num_updates=2280, lr=0, gnorm=1.274, clip=100, loss_scale=1, train_wall=85, gb_free=9.1, wall=11302
2023-05-20 17:15:29 | INFO | train_inner | epoch 018:     99 / 130 loss=2.731, nll_loss=0.781, ppl=1.72, wps=1522.6, ups=0.25, wpb=6169.6, bsz=305.7, num_updates=2300, lr=0, gnorm=1.433, clip=100, loss_scale=1, train_wall=81, gb_free=9.8, wall=11383
2023-05-20 17:16:52 | INFO | train_inner | epoch 018:    119 / 130 loss=2.737, nll_loss=0.788, ppl=1.73, wps=1471.7, ups=0.24, wpb=6112.2, bsz=289.9, num_updates=2320, lr=0, gnorm=1.503, clip=100, loss_scale=1, train_wall=83, gb_free=8.5, wall=11466
2023-05-20 17:17:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:17:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:18:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 994.8 | wpb 1392.5 | bsz 52.6 | num_updates 2331 | best_loss 3.138
2023-05-20 17:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2331 updates
2023-05-20 17:18:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint18.pt
2023-05-20 17:18:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint18.pt
2023-05-20 17:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint18.pt (epoch 18 @ 2331 updates, score 3.184) (writing took 46.60277735069394 seconds)
2023-05-20 17:18:55 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-20 17:18:55 | INFO | train | epoch 018 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1239.4 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 2331 | lr 0 | gnorm 1.509 | clip 100 | loss_scale 1 | train_wall 552 | gb_free 10.1 | wall 11589
2023-05-20 17:18:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:18:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 17:18:55 | INFO | fairseq.trainer | begin training epoch 19
2023-05-20 17:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:19:36 | INFO | train_inner | epoch 019:      9 / 130 loss=2.734, nll_loss=0.785, ppl=1.72, wps=718.7, ups=0.12, wpb=5897.4, bsz=297.9, num_updates=2340, lr=0, gnorm=1.348, clip=100, loss_scale=1, train_wall=81, gb_free=9.9, wall=11630
2023-05-20 17:21:01 | INFO | train_inner | epoch 019:     29 / 130 loss=2.729, nll_loss=0.777, ppl=1.71, wps=1446.2, ups=0.24, wpb=6115.2, bsz=278.4, num_updates=2360, lr=0, gnorm=1.694, clip=100, loss_scale=1, train_wall=84, gb_free=9.3, wall=11715
2023-05-20 17:22:25 | INFO | train_inner | epoch 019:     49 / 130 loss=2.728, nll_loss=0.777, ppl=1.71, wps=1469.1, ups=0.24, wpb=6202.6, bsz=294, num_updates=2380, lr=0, gnorm=1.792, clip=100, loss_scale=1, train_wall=84, gb_free=9.6, wall=11799
2023-05-20 17:23:51 | INFO | train_inner | epoch 019:     69 / 130 loss=2.746, nll_loss=0.797, ppl=1.74, wps=1417.4, ups=0.23, wpb=6065.8, bsz=290.1, num_updates=2400, lr=0, gnorm=1.619, clip=100, loss_scale=1, train_wall=85, gb_free=9.5, wall=11885
2023-05-20 17:25:14 | INFO | train_inner | epoch 019:     89 / 130 loss=2.714, nll_loss=0.763, ppl=1.7, wps=1453, ups=0.24, wpb=6023.7, bsz=292.2, num_updates=2420, lr=0, gnorm=1.238, clip=100, loss_scale=1, train_wall=83, gb_free=10, wall=11968
2023-05-20 17:26:41 | INFO | train_inner | epoch 019:    109 / 130 loss=2.699, nll_loss=0.744, ppl=1.68, wps=1392.6, ups=0.23, wpb=6088.7, bsz=248.9, num_updates=2440, lr=0, gnorm=2.87, clip=100, loss_scale=1, train_wall=87, gb_free=9.1, wall=12055
2023-05-20 17:28:03 | INFO | train_inner | epoch 019:    129 / 130 loss=2.703, nll_loss=0.75, ppl=1.68, wps=1472.4, ups=0.24, wpb=6054.9, bsz=273.6, num_updates=2460, lr=0, gnorm=1.308, clip=100, loss_scale=1, train_wall=82, gb_free=10, wall=12137
2023-05-20 17:28:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:28:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:28:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 976.3 | wpb 1392.5 | bsz 52.6 | num_updates 2461 | best_loss 3.138
2023-05-20 17:28:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2461 updates
2023-05-20 17:28:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint19.pt
2023-05-20 17:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint19.pt
2023-05-20 17:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint19.pt (epoch 19 @ 2461 updates, score 3.184) (writing took 21.67805241420865 seconds)
2023-05-20 17:29:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-20 17:29:03 | INFO | train | epoch 019 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1296.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2461 | lr 0 | gnorm 1.732 | clip 100 | loss_scale 1 | train_wall 548 | gb_free 10.1 | wall 12197
2023-05-20 17:29:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 17:29:03 | INFO | fairseq.trainer | begin training epoch 20
2023-05-20 17:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:30:27 | INFO | train_inner | epoch 020:     19 / 130 loss=2.704, nll_loss=0.751, ppl=1.68, wps=799.1, ups=0.14, wpb=5757, bsz=259.4, num_updates=2480, lr=0, gnorm=1.514, clip=100, loss_scale=1, train_wall=86, gb_free=8.8, wall=12282
2023-05-20 17:31:56 | INFO | train_inner | epoch 020:     39 / 130 loss=2.726, nll_loss=0.775, ppl=1.71, wps=1364.7, ups=0.23, wpb=6051.9, bsz=285.1, num_updates=2500, lr=0, gnorm=1.647, clip=100, loss_scale=1, train_wall=89, gb_free=9.6, wall=12370
2023-05-20 17:33:20 | INFO | train_inner | epoch 020:     59 / 130 loss=2.734, nll_loss=0.783, ppl=1.72, wps=1459, ups=0.24, wpb=6098.8, bsz=275.7, num_updates=2520, lr=0, gnorm=1.272, clip=100, loss_scale=1, train_wall=84, gb_free=9.4, wall=12454
2023-05-20 17:34:43 | INFO | train_inner | epoch 020:     79 / 130 loss=2.715, nll_loss=0.764, ppl=1.7, wps=1467.9, ups=0.24, wpb=6098.7, bsz=276.7, num_updates=2540, lr=0, gnorm=1.304, clip=100, loss_scale=1, train_wall=83, gb_free=9.4, wall=12537
2023-05-20 17:36:09 | INFO | train_inner | epoch 020:     99 / 130 loss=2.706, nll_loss=0.752, ppl=1.68, wps=1439, ups=0.23, wpb=6237, bsz=285.8, num_updates=2560, lr=0, gnorm=1.27, clip=100, loss_scale=1, train_wall=87, gb_free=9, wall=12624
2023-05-20 17:37:35 | INFO | train_inner | epoch 020:    119 / 130 loss=2.721, nll_loss=0.771, ppl=1.71, wps=1406.7, ups=0.23, wpb=6022.6, bsz=283.1, num_updates=2580, lr=0, gnorm=1.263, clip=100, loss_scale=1, train_wall=86, gb_free=10.1, wall=12709
2023-05-20 17:38:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:38:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:38:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 984.3 | wpb 1392.5 | bsz 52.6 | num_updates 2591 | best_loss 3.138
2023-05-20 17:38:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2591 updates
2023-05-20 17:38:52 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint20.pt
2023-05-20 17:39:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint20.pt
2023-05-20 17:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint20.pt (epoch 20 @ 2591 updates, score 3.184) (writing took 37.45059225335717 seconds)
2023-05-20 17:39:30 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-20 17:39:30 | INFO | train | epoch 020 | loss 2.721 | nll_loss 0.769 | ppl 1.7 | wps 1255.3 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2591 | lr 0 | gnorm 1.378 | clip 100 | loss_scale 1 | train_wall 553 | gb_free 10.1 | wall 12824
2023-05-20 17:39:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:39:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 17:39:30 | INFO | fairseq.trainer | begin training epoch 21
2023-05-20 17:39:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:40:10 | INFO | train_inner | epoch 021:      9 / 130 loss=2.752, nll_loss=0.804, ppl=1.75, wps=762, ups=0.13, wpb=5888.1, bsz=293.9, num_updates=2600, lr=0, gnorm=1.405, clip=100, loss_scale=1, train_wall=80, gb_free=9.1, wall=12864
2023-05-20 17:41:32 | INFO | train_inner | epoch 021:     29 / 130 loss=2.722, nll_loss=0.771, ppl=1.71, wps=1497.3, ups=0.24, wpb=6139.8, bsz=291.4, num_updates=2620, lr=0, gnorm=1.536, clip=100, loss_scale=1, train_wall=82, gb_free=9.7, wall=12946
2023-05-20 17:42:57 | INFO | train_inner | epoch 021:     49 / 130 loss=2.728, nll_loss=0.778, ppl=1.71, wps=1461.9, ups=0.24, wpb=6209.8, bsz=291.9, num_updates=2640, lr=0, gnorm=1.226, clip=100, loss_scale=1, train_wall=85, gb_free=10, wall=13031
2023-05-20 17:44:25 | INFO | train_inner | epoch 021:     69 / 130 loss=2.711, nll_loss=0.758, ppl=1.69, wps=1382.1, ups=0.23, wpb=6108.6, bsz=279.9, num_updates=2660, lr=0, gnorm=1.254, clip=100, loss_scale=1, train_wall=88, gb_free=8.1, wall=13119
2023-05-20 17:45:46 | INFO | train_inner | epoch 021:     89 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=1482.2, ups=0.25, wpb=6021.1, bsz=268.4, num_updates=2680, lr=0, gnorm=1.395, clip=100, loss_scale=1, train_wall=81, gb_free=8.9, wall=13200
2023-05-20 17:47:09 | INFO | train_inner | epoch 021:    109 / 130 loss=2.715, nll_loss=0.764, ppl=1.7, wps=1443.8, ups=0.24, wpb=5944.7, bsz=292.7, num_updates=2700, lr=0, gnorm=1.29, clip=100, loss_scale=1, train_wall=82, gb_free=9.7, wall=13283
2023-05-20 17:48:32 | INFO | train_inner | epoch 021:    129 / 130 loss=2.72, nll_loss=0.769, ppl=1.7, wps=1479, ups=0.24, wpb=6135.6, bsz=267.9, num_updates=2720, lr=0, gnorm=1.307, clip=100, loss_scale=1, train_wall=83, gb_free=9.8, wall=13366
2023-05-20 17:48:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:48:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:49:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 996.2 | wpb 1392.5 | bsz 52.6 | num_updates 2721 | best_loss 3.138
2023-05-20 17:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2721 updates
2023-05-20 17:49:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint21.pt
2023-05-20 17:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint21.pt
2023-05-20 17:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint21.pt (epoch 21 @ 2721 updates, score 3.184) (writing took 35.40970465540886 seconds)
2023-05-20 17:49:44 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-05-20 17:49:44 | INFO | train | epoch 021 | loss 2.721 | nll_loss 0.77 | ppl 1.71 | wps 1282.2 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2721 | lr 0 | gnorm 1.338 | clip 100 | loss_scale 1 | train_wall 542 | gb_free 10.1 | wall 13438
2023-05-20 17:49:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:49:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 17:49:44 | INFO | fairseq.trainer | begin training epoch 22
2023-05-20 17:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 17:51:04 | INFO | train_inner | epoch 022:     19 / 130 loss=2.715, nll_loss=0.764, ppl=1.7, wps=781.4, ups=0.13, wpb=5950.2, bsz=286.7, num_updates=2740, lr=0, gnorm=1.369, clip=100, loss_scale=1, train_wall=81, gb_free=9.6, wall=13518
2023-05-20 17:52:28 | INFO | train_inner | epoch 022:     39 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1436, ups=0.24, wpb=6014.8, bsz=250.5, num_updates=2760, lr=0, gnorm=1.323, clip=100, loss_scale=1, train_wall=84, gb_free=9.1, wall=13602
2023-05-20 17:53:54 | INFO | train_inner | epoch 022:     59 / 130 loss=2.742, nll_loss=0.794, ppl=1.73, wps=1433, ups=0.23, wpb=6181.9, bsz=307.7, num_updates=2780, lr=0, gnorm=1.271, clip=100, loss_scale=1, train_wall=86, gb_free=10, wall=13688
2023-05-20 17:55:18 | INFO | train_inner | epoch 022:     79 / 130 loss=2.714, nll_loss=0.761, ppl=1.69, wps=1429.9, ups=0.24, wpb=6019.6, bsz=277, num_updates=2800, lr=0, gnorm=1.532, clip=100, loss_scale=1, train_wall=84, gb_free=9.1, wall=13772
2023-05-20 17:56:46 | INFO | train_inner | epoch 022:     99 / 130 loss=2.709, nll_loss=0.756, ppl=1.69, wps=1371.6, ups=0.23, wpb=6008.8, bsz=269.7, num_updates=2820, lr=0, gnorm=1.396, clip=100, loss_scale=1, train_wall=88, gb_free=9.9, wall=13860
2023-05-20 17:58:09 | INFO | train_inner | epoch 022:    119 / 130 loss=2.73, nll_loss=0.779, ppl=1.72, wps=1472.3, ups=0.24, wpb=6121.7, bsz=282.4, num_updates=2840, lr=0, gnorm=1.545, clip=100, loss_scale=1, train_wall=83, gb_free=10, wall=13943
2023-05-20 17:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 17:58:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 17:59:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 994.8 | wpb 1392.5 | bsz 52.6 | num_updates 2851 | best_loss 3.138
2023-05-20 17:59:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2851 updates
2023-05-20 17:59:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint22.pt
2023-05-20 17:59:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint22.pt
2023-05-20 18:00:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint22.pt (epoch 22 @ 2851 updates, score 3.184) (writing took 33.5382194891572 seconds)
2023-05-20 18:00:00 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-05-20 18:00:00 | INFO | train | epoch 022 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1278.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2851 | lr 0 | gnorm 1.4 | clip 100 | loss_scale 1 | train_wall 546 | gb_free 10.1 | wall 14054
2023-05-20 18:00:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:00:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:00:00 | INFO | fairseq.trainer | begin training epoch 23
2023-05-20 18:00:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:00:36 | INFO | train_inner | epoch 023:      9 / 130 loss=2.721, nll_loss=0.77, ppl=1.71, wps=809.4, ups=0.14, wpb=5950.2, bsz=286.9, num_updates=2860, lr=0, gnorm=1.397, clip=100, loss_scale=1, train_wall=77, gb_free=9.8, wall=14090
2023-05-20 18:02:01 | INFO | train_inner | epoch 023:     29 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1416.9, ups=0.23, wpb=6032, bsz=267.5, num_updates=2880, lr=0, gnorm=1.255, clip=100, loss_scale=1, train_wall=85, gb_free=9, wall=14175
2023-05-20 18:03:23 | INFO | train_inner | epoch 023:     49 / 130 loss=2.723, nll_loss=0.771, ppl=1.71, wps=1496.5, ups=0.24, wpb=6112.9, bsz=261.7, num_updates=2900, lr=0, gnorm=1.421, clip=100, loss_scale=1, train_wall=82, gb_free=9.8, wall=14257
2023-05-20 18:04:48 | INFO | train_inner | epoch 023:     69 / 130 loss=2.729, nll_loss=0.779, ppl=1.72, wps=1432.5, ups=0.24, wpb=6086.2, bsz=292.6, num_updates=2920, lr=0, gnorm=1.407, clip=100, loss_scale=1, train_wall=85, gb_free=9.1, wall=14342
2023-05-20 18:06:10 | INFO | train_inner | epoch 023:     89 / 130 loss=2.742, nll_loss=0.794, ppl=1.73, wps=1476.2, ups=0.24, wpb=6090.2, bsz=297.7, num_updates=2940, lr=0, gnorm=1.351, clip=100, loss_scale=1, train_wall=82, gb_free=8.8, wall=14424
2023-05-20 18:07:33 | INFO | train_inner | epoch 023:    109 / 130 loss=2.706, nll_loss=0.752, ppl=1.68, wps=1447.6, ups=0.24, wpb=5983.1, bsz=267.7, num_updates=2960, lr=0, gnorm=1.527, clip=100, loss_scale=1, train_wall=83, gb_free=9.1, wall=14507
2023-05-20 18:09:01 | INFO | train_inner | epoch 023:    129 / 130 loss=2.731, nll_loss=0.78, ppl=1.72, wps=1396.9, ups=0.23, wpb=6176, bsz=303.4, num_updates=2980, lr=0, gnorm=1.335, clip=100, loss_scale=1, train_wall=88, gb_free=9.6, wall=14595
2023-05-20 18:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:09:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:09:39 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 978.5 | wpb 1392.5 | bsz 52.6 | num_updates 2981 | best_loss 3.138
2023-05-20 18:09:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2981 updates
2023-05-20 18:09:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint23.pt
2023-05-20 18:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint23.pt
2023-05-20 18:10:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint23.pt (epoch 23 @ 2981 updates, score 3.184) (writing took 27.014005694538355 seconds)
2023-05-20 18:10:06 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-05-20 18:10:06 | INFO | train | epoch 023 | loss 2.723 | nll_loss 0.772 | ppl 1.71 | wps 1299.2 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 2981 | lr 0 | gnorm 1.39 | clip 100 | loss_scale 1 | train_wall 542 | gb_free 10.1 | wall 14660
2023-05-20 18:10:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:10:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:10:06 | INFO | fairseq.trainer | begin training epoch 24
2023-05-20 18:10:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:11:29 | INFO | train_inner | epoch 024:     19 / 130 loss=2.703, nll_loss=0.748, ppl=1.68, wps=775.3, ups=0.14, wpb=5711.5, bsz=236, num_updates=3000, lr=0, gnorm=1.352, clip=100, loss_scale=1, train_wall=83, gb_free=7.5, wall=14743
2023-05-20 18:12:50 | INFO | train_inner | epoch 024:     39 / 130 loss=2.729, nll_loss=0.78, ppl=1.72, wps=1501.6, ups=0.24, wpb=6135.5, bsz=302.2, num_updates=3020, lr=0, gnorm=1.28, clip=100, loss_scale=1, train_wall=82, gb_free=8.8, wall=14825
2023-05-20 18:14:17 | INFO | train_inner | epoch 024:     59 / 130 loss=2.728, nll_loss=0.778, ppl=1.71, wps=1417.1, ups=0.23, wpb=6114.6, bsz=283.9, num_updates=3040, lr=0, gnorm=1.316, clip=100, loss_scale=1, train_wall=86, gb_free=10.1, wall=14911
2023-05-20 18:15:41 | INFO | train_inner | epoch 024:     79 / 130 loss=2.703, nll_loss=0.75, ppl=1.68, wps=1436.6, ups=0.24, wpb=6076, bsz=280.9, num_updates=3060, lr=0, gnorm=1.444, clip=100, loss_scale=1, train_wall=84, gb_free=9.8, wall=14995
2023-05-20 18:17:03 | INFO | train_inner | epoch 024:     99 / 130 loss=2.731, nll_loss=0.782, ppl=1.72, wps=1492.1, ups=0.24, wpb=6117, bsz=285.1, num_updates=3080, lr=0, gnorm=1.263, clip=100, loss_scale=2, train_wall=82, gb_free=9.3, wall=15077
2023-05-20 18:18:30 | INFO | train_inner | epoch 024:    119 / 130 loss=2.728, nll_loss=0.778, ppl=1.72, wps=1416.4, ups=0.23, wpb=6147.6, bsz=296.1, num_updates=3100, lr=0, gnorm=1.321, clip=100, loss_scale=2, train_wall=87, gb_free=9.8, wall=15164
2023-05-20 18:19:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:19:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:19:48 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 982.1 | wpb 1392.5 | bsz 52.6 | num_updates 3111 | best_loss 3.138
2023-05-20 18:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3111 updates
2023-05-20 18:19:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint24.pt
2023-05-20 18:20:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint24.pt
2023-05-20 18:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint24.pt (epoch 24 @ 3111 updates, score 3.184) (writing took 60.17207085341215 seconds)
2023-05-20 18:20:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-05-20 18:20:48 | INFO | train | epoch 024 | loss 2.721 | nll_loss 0.77 | ppl 1.71 | wps 1226 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 3111 | lr 0 | gnorm 1.321 | clip 100 | loss_scale 2 | train_wall 545 | gb_free 10.1 | wall 15302
2023-05-20 18:20:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:20:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:20:48 | INFO | fairseq.trainer | begin training epoch 25
2023-05-20 18:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:21:30 | INFO | train_inner | epoch 025:      9 / 130 loss=2.72, nll_loss=0.767, ppl=1.7, wps=650.2, ups=0.11, wpb=5843.4, bsz=261.6, num_updates=3120, lr=0, gnorm=1.297, clip=100, loss_scale=2, train_wall=83, gb_free=10.1, wall=15344
2023-05-20 18:22:57 | INFO | train_inner | epoch 025:     29 / 130 loss=2.72, nll_loss=0.768, ppl=1.7, wps=1381.8, ups=0.23, wpb=5994.1, bsz=295.3, num_updates=3140, lr=0, gnorm=1.441, clip=100, loss_scale=2, train_wall=87, gb_free=9.6, wall=15431
2023-05-20 18:24:21 | INFO | train_inner | epoch 025:     49 / 130 loss=2.708, nll_loss=0.755, ppl=1.69, wps=1419.3, ups=0.24, wpb=6014.2, bsz=269.1, num_updates=3160, lr=0, gnorm=1.247, clip=100, loss_scale=2, train_wall=85, gb_free=10, wall=15515
2023-05-20 18:25:44 | INFO | train_inner | epoch 025:     69 / 130 loss=2.707, nll_loss=0.753, ppl=1.69, wps=1446.8, ups=0.24, wpb=5993, bsz=254.7, num_updates=3180, lr=0, gnorm=1.405, clip=100, loss_scale=2, train_wall=83, gb_free=9.3, wall=15598
2023-05-20 18:27:07 | INFO | train_inner | epoch 025:     89 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1497.7, ups=0.24, wpb=6215.9, bsz=291.6, num_updates=3200, lr=0, gnorm=1.778, clip=100, loss_scale=2, train_wall=83, gb_free=9.7, wall=15681
2023-05-20 18:28:33 | INFO | train_inner | epoch 025:    109 / 130 loss=2.731, nll_loss=0.781, ppl=1.72, wps=1426.2, ups=0.23, wpb=6101.2, bsz=285.7, num_updates=3220, lr=0, gnorm=1.724, clip=100, loss_scale=2, train_wall=85, gb_free=9.7, wall=15767
2023-05-20 18:29:57 | INFO | train_inner | epoch 025:    129 / 130 loss=2.752, nll_loss=0.805, ppl=1.75, wps=1486.3, ups=0.24, wpb=6266.8, bsz=311.5, num_updates=3240, lr=0, gnorm=1.435, clip=100, loss_scale=2, train_wall=84, gb_free=9.1, wall=15851
2023-05-20 18:29:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:29:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:30:34 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 983.7 | wpb 1392.5 | bsz 52.6 | num_updates 3241 | best_loss 3.138
2023-05-20 18:30:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3241 updates
2023-05-20 18:30:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint25.pt
2023-05-20 18:30:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint25.pt
2023-05-20 18:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint25.pt (epoch 25 @ 3241 updates, score 3.184) (writing took 33.01370056346059 seconds)
2023-05-20 18:31:07 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-05-20 18:31:07 | INFO | train | epoch 025 | loss 2.721 | nll_loss 0.77 | ppl 1.71 | wps 1270.9 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 3241 | lr 0 | gnorm 1.505 | clip 100 | loss_scale 2 | train_wall 549 | gb_free 10.1 | wall 15922
2023-05-20 18:31:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:31:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:31:07 | INFO | fairseq.trainer | begin training epoch 26
2023-05-20 18:31:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:32:28 | INFO | train_inner | epoch 026:     19 / 130 loss=2.738, nll_loss=0.79, ppl=1.73, wps=802.8, ups=0.13, wpb=6047.8, bsz=312.1, num_updates=3260, lr=0, gnorm=1.403, clip=100, loss_scale=2, train_wall=81, gb_free=10.1, wall=16002
2023-05-20 18:33:53 | INFO | train_inner | epoch 026:     39 / 130 loss=2.711, nll_loss=0.757, ppl=1.69, wps=1410.5, ups=0.24, wpb=5981.9, bsz=272.4, num_updates=3280, lr=0, gnorm=1.406, clip=100, loss_scale=2, train_wall=85, gb_free=9.2, wall=16087
2023-05-20 18:35:18 | INFO | train_inner | epoch 026:     59 / 130 loss=2.717, nll_loss=0.765, ppl=1.7, wps=1408.6, ups=0.24, wpb=5990.8, bsz=282.9, num_updates=3300, lr=0, gnorm=1.404, clip=100, loss_scale=2, train_wall=85, gb_free=9.5, wall=16172
2023-05-20 18:36:40 | INFO | train_inner | epoch 026:     79 / 130 loss=2.721, nll_loss=0.77, ppl=1.71, wps=1503.2, ups=0.24, wpb=6162.2, bsz=289.8, num_updates=3320, lr=0, gnorm=1.339, clip=100, loss_scale=2, train_wall=82, gb_free=9.7, wall=16254
2023-05-20 18:38:06 | INFO | train_inner | epoch 026:     99 / 130 loss=2.72, nll_loss=0.767, ppl=1.7, wps=1398.2, ups=0.23, wpb=6018.1, bsz=256.2, num_updates=3340, lr=0, gnorm=1.274, clip=100, loss_scale=2, train_wall=86, gb_free=8.9, wall=16340
2023-05-20 18:39:30 | INFO | train_inner | epoch 026:    119 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=1454.5, ups=0.24, wpb=6132.9, bsz=286.3, num_updates=3360, lr=0, gnorm=1.257, clip=100, loss_scale=2, train_wall=84, gb_free=9.8, wall=16424
2023-05-20 18:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:40:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:40:47 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 982.2 | wpb 1392.5 | bsz 52.6 | num_updates 3371 | best_loss 3.138
2023-05-20 18:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 3371 updates
2023-05-20 18:40:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint26.pt
2023-05-20 18:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint26.pt
2023-05-20 18:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint26.pt (epoch 26 @ 3371 updates, score 3.184) (writing took 37.97826765850186 seconds)
2023-05-20 18:41:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-05-20 18:41:25 | INFO | train | epoch 026 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1274.7 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 3371 | lr 0 | gnorm 1.329 | clip 100 | loss_scale 2 | train_wall 543 | gb_free 10.1 | wall 16539
2023-05-20 18:41:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:41:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:41:25 | INFO | fairseq.trainer | begin training epoch 27
2023-05-20 18:41:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:42:06 | INFO | train_inner | epoch 027:      9 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=749.7, ups=0.13, wpb=5831.2, bsz=261.6, num_updates=3380, lr=0, gnorm=1.334, clip=100, loss_scale=2, train_wall=81, gb_free=10, wall=16580
2023-05-20 18:43:29 | INFO | train_inner | epoch 027:     29 / 130 loss=2.726, nll_loss=0.776, ppl=1.71, wps=1491.3, ups=0.24, wpb=6228.6, bsz=298.1, num_updates=3400, lr=0, gnorm=1.346, clip=100, loss_scale=2, train_wall=83, gb_free=9.9, wall=16663
2023-05-20 18:44:56 | INFO | train_inner | epoch 027:     49 / 130 loss=2.711, nll_loss=0.757, ppl=1.69, wps=1368.3, ups=0.23, wpb=5956, bsz=260.7, num_updates=3420, lr=0, gnorm=1.258, clip=100, loss_scale=2, train_wall=87, gb_free=9.4, wall=16750
2023-05-20 18:46:22 | INFO | train_inner | epoch 027:     69 / 130 loss=2.724, nll_loss=0.772, ppl=1.71, wps=1430.1, ups=0.23, wpb=6148, bsz=269.9, num_updates=3440, lr=0, gnorm=1.426, clip=100, loss_scale=2, train_wall=86, gb_free=9, wall=16836
2023-05-20 18:47:45 | INFO | train_inner | epoch 027:     89 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1471.9, ups=0.24, wpb=6107.7, bsz=283.9, num_updates=3460, lr=0, gnorm=2.537, clip=100, loss_scale=2, train_wall=83, gb_free=8.8, wall=16919
2023-05-20 18:49:10 | INFO | train_inner | epoch 027:    109 / 130 loss=2.737, nll_loss=0.787, ppl=1.73, wps=1426.6, ups=0.24, wpb=6044.1, bsz=298.6, num_updates=3480, lr=0, gnorm=1.632, clip=100, loss_scale=2, train_wall=85, gb_free=9.5, wall=17004
2023-05-20 18:50:34 | INFO | train_inner | epoch 027:    129 / 130 loss=2.716, nll_loss=0.764, ppl=1.7, wps=1441.6, ups=0.24, wpb=6081.4, bsz=279.8, num_updates=3500, lr=0, gnorm=1.238, clip=100, loss_scale=2, train_wall=84, gb_free=9, wall=17088
2023-05-20 18:50:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 18:50:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:51:11 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 987.3 | wpb 1392.5 | bsz 52.6 | num_updates 3501 | best_loss 3.138
2023-05-20 18:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 3501 updates
2023-05-20 18:51:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint27.pt
2023-05-20 18:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint27.pt
2023-05-20 18:51:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint27.pt (epoch 27 @ 3501 updates, score 3.184) (writing took 27.785764276981354 seconds)
2023-05-20 18:51:39 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-05-20 18:51:39 | INFO | train | epoch 027 | loss 2.721 | nll_loss 0.77 | ppl 1.7 | wps 1282.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 3501 | lr 0 | gnorm 1.565 | clip 100 | loss_scale 2 | train_wall 549 | gb_free 10.1 | wall 17153
2023-05-20 18:51:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 18:51:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 18:51:39 | INFO | fairseq.trainer | begin training epoch 28
2023-05-20 18:51:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 18:53:00 | INFO | train_inner | epoch 028:     19 / 130 loss=2.734, nll_loss=0.785, ppl=1.72, wps=819.3, ups=0.14, wpb=5959.9, bsz=273.9, num_updates=3520, lr=0, gnorm=1.33, clip=100, loss_scale=2, train_wall=81, gb_free=9.4, wall=17234
2023-05-20 18:54:25 | INFO | train_inner | epoch 028:     39 / 130 loss=2.736, nll_loss=0.786, ppl=1.72, wps=1400.1, ups=0.23, wpb=5999, bsz=276.5, num_updates=3540, lr=0, gnorm=1.557, clip=100, loss_scale=2, train_wall=86, gb_free=9.1, wall=17320
2023-05-20 18:55:53 | INFO | train_inner | epoch 028:     59 / 130 loss=2.717, nll_loss=0.766, ppl=1.7, wps=1382.7, ups=0.23, wpb=6073.8, bsz=269.9, num_updates=3560, lr=0, gnorm=1.403, clip=100, loss_scale=2, train_wall=88, gb_free=9.8, wall=17407
2023-05-20 18:57:19 | INFO | train_inner | epoch 028:     79 / 130 loss=2.705, nll_loss=0.752, ppl=1.68, wps=1401.3, ups=0.23, wpb=6011.9, bsz=281, num_updates=3580, lr=0, gnorm=1.307, clip=100, loss_scale=2, train_wall=86, gb_free=10, wall=17493
2023-05-20 18:58:43 | INFO | train_inner | epoch 028:     99 / 130 loss=2.717, nll_loss=0.765, ppl=1.7, wps=1462.8, ups=0.24, wpb=6158, bsz=290.2, num_updates=3600, lr=0, gnorm=1.714, clip=100, loss_scale=2, train_wall=84, gb_free=8.9, wall=17577
2023-05-20 19:00:08 | INFO | train_inner | epoch 028:    119 / 130 loss=2.716, nll_loss=0.763, ppl=1.7, wps=1445, ups=0.24, wpb=6122.9, bsz=285.6, num_updates=3620, lr=0, gnorm=1.433, clip=100, loss_scale=2, train_wall=85, gb_free=9.7, wall=17662
2023-05-20 19:00:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:00:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:01:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 983.6 | wpb 1392.5 | bsz 52.6 | num_updates 3631 | best_loss 3.138
2023-05-20 19:01:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 3631 updates
2023-05-20 19:01:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint28.pt
2023-05-20 19:01:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint28.pt
2023-05-20 19:01:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint28.pt (epoch 28 @ 3631 updates, score 3.184) (writing took 27.194654788821936 seconds)
2023-05-20 19:01:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-05-20 19:01:53 | INFO | train | epoch 028 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1283.4 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 3631 | lr 0 | gnorm 1.478 | clip 100 | loss_scale 2 | train_wall 549 | gb_free 10.1 | wall 17767
2023-05-20 19:01:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:01:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:01:53 | INFO | fairseq.trainer | begin training epoch 29
2023-05-20 19:01:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:02:33 | INFO | train_inner | epoch 029:      9 / 130 loss=2.725, nll_loss=0.774, ppl=1.71, wps=799.6, ups=0.14, wpb=5802.2, bsz=273.2, num_updates=3640, lr=0, gnorm=1.557, clip=100, loss_scale=2, train_wall=81, gb_free=9.9, wall=17807
2023-05-20 19:04:01 | INFO | train_inner | epoch 029:     29 / 130 loss=2.71, nll_loss=0.757, ppl=1.69, wps=1363.6, ups=0.23, wpb=6021, bsz=258.6, num_updates=3660, lr=0, gnorm=1.253, clip=100, loss_scale=2, train_wall=88, gb_free=9.1, wall=17896
2023-05-20 19:05:28 | INFO | train_inner | epoch 029:     49 / 130 loss=2.73, nll_loss=0.779, ppl=1.72, wps=1404.5, ups=0.23, wpb=6068.2, bsz=277.1, num_updates=3680, lr=0, gnorm=1.772, clip=100, loss_scale=2, train_wall=86, gb_free=8.3, wall=17982
2023-05-20 19:06:49 | INFO | train_inner | epoch 029:     69 / 130 loss=2.726, nll_loss=0.776, ppl=1.71, wps=1495.8, ups=0.25, wpb=6098.8, bsz=284.2, num_updates=3700, lr=0, gnorm=1.826, clip=100, loss_scale=2, train_wall=81, gb_free=9.9, wall=18064
2023-05-20 19:08:10 | INFO | train_inner | epoch 029:     89 / 130 loss=2.721, nll_loss=0.77, ppl=1.71, wps=1522.1, ups=0.25, wpb=6136.5, bsz=295.6, num_updates=3720, lr=0, gnorm=1.303, clip=100, loss_scale=2, train_wall=81, gb_free=9.1, wall=18144
2023-05-20 19:09:36 | INFO | train_inner | epoch 029:    109 / 130 loss=2.73, nll_loss=0.78, ppl=1.72, wps=1449.5, ups=0.23, wpb=6208, bsz=296.2, num_updates=3740, lr=0, gnorm=1.628, clip=100, loss_scale=2, train_wall=86, gb_free=9.3, wall=18230
2023-05-20 19:10:59 | INFO | train_inner | epoch 029:    129 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=1465, ups=0.24, wpb=6071.4, bsz=289.4, num_updates=3760, lr=0, gnorm=1.308, clip=100, loss_scale=2, train_wall=83, gb_free=9.5, wall=18313
2023-05-20 19:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:11:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:11:35 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1000.6 | wpb 1392.5 | bsz 52.6 | num_updates 3761 | best_loss 3.138
2023-05-20 19:11:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 3761 updates
2023-05-20 19:11:35 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint29.pt
2023-05-20 19:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint29.pt
2023-05-20 19:12:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint29.pt (epoch 29 @ 3761 updates, score 3.184) (writing took 59.74617297947407 seconds)
2023-05-20 19:12:35 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-05-20 19:12:35 | INFO | train | epoch 029 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1225.2 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 3761 | lr 0 | gnorm 1.504 | clip 100 | loss_scale 2 | train_wall 546 | gb_free 10 | wall 18409
2023-05-20 19:12:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:12:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:12:35 | INFO | fairseq.trainer | begin training epoch 30
2023-05-20 19:12:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:13:57 | INFO | train_inner | epoch 030:     19 / 130 loss=2.748, nll_loss=0.8, ppl=1.74, wps=657.2, ups=0.11, wpb=5871.8, bsz=292.7, num_updates=3780, lr=0, gnorm=1.522, clip=100, loss_scale=2, train_wall=83, gb_free=9.8, wall=18491
2023-05-20 19:15:23 | INFO | train_inner | epoch 030:     39 / 130 loss=2.699, nll_loss=0.744, ppl=1.67, wps=1391.9, ups=0.23, wpb=5996.2, bsz=259.9, num_updates=3800, lr=0, gnorm=1.456, clip=100, loss_scale=2, train_wall=86, gb_free=9.7, wall=18578
2023-05-20 19:16:47 | INFO | train_inner | epoch 030:     59 / 130 loss=2.712, nll_loss=0.76, ppl=1.69, wps=1451.6, ups=0.24, wpb=6086.3, bsz=272.8, num_updates=3820, lr=0, gnorm=1.32, clip=100, loss_scale=2, train_wall=84, gb_free=9.2, wall=18661
2023-05-20 19:18:13 | INFO | train_inner | epoch 030:     79 / 130 loss=2.743, nll_loss=0.793, ppl=1.73, wps=1453.4, ups=0.23, wpb=6240.6, bsz=297.8, num_updates=3840, lr=0, gnorm=2.227, clip=100, loss_scale=2, train_wall=86, gb_free=9.1, wall=18747
2023-05-20 19:19:39 | INFO | train_inner | epoch 030:     99 / 130 loss=2.732, nll_loss=0.783, ppl=1.72, wps=1422.3, ups=0.23, wpb=6109.4, bsz=288.3, num_updates=3860, lr=0, gnorm=1.435, clip=100, loss_scale=2, train_wall=86, gb_free=9.8, wall=18833
2023-05-20 19:21:00 | INFO | train_inner | epoch 030:    119 / 130 loss=2.721, nll_loss=0.769, ppl=1.7, wps=1486.9, ups=0.25, wpb=5984.4, bsz=280.4, num_updates=3880, lr=0, gnorm=1.477, clip=100, loss_scale=2, train_wall=80, gb_free=9.5, wall=18914
2023-05-20 19:21:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:21:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:22:20 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 985.8 | wpb 1392.5 | bsz 52.6 | num_updates 3891 | best_loss 3.138
2023-05-20 19:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3891 updates
2023-05-20 19:22:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint30.pt
2023-05-20 19:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint30.pt
2023-05-20 19:22:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint30.pt (epoch 30 @ 3891 updates, score 3.184) (writing took 29.810650777071714 seconds)
2023-05-20 19:22:50 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-05-20 19:22:50 | INFO | train | epoch 030 | loss 2.723 | nll_loss 0.771 | ppl 1.71 | wps 1281.5 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 3891 | lr 0 | gnorm 1.545 | clip 100 | loss_scale 2 | train_wall 547 | gb_free 10.1 | wall 19024
2023-05-20 19:22:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:22:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:22:50 | INFO | fairseq.trainer | begin training epoch 31
2023-05-20 19:22:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:23:28 | INFO | train_inner | epoch 031:      9 / 130 loss=2.706, nll_loss=0.755, ppl=1.69, wps=794.3, ups=0.13, wpb=5907.2, bsz=253.3, num_updates=3900, lr=0, gnorm=1.388, clip=100, loss_scale=2, train_wall=82, gb_free=9.9, wall=19062
2023-05-20 19:24:53 | INFO | train_inner | epoch 031:     29 / 130 loss=2.725, nll_loss=0.774, ppl=1.71, wps=1446.8, ups=0.24, wpb=6144.7, bsz=291.8, num_updates=3920, lr=0, gnorm=1.273, clip=100, loss_scale=2, train_wall=85, gb_free=8.4, wall=19147
2023-05-20 19:26:20 | INFO | train_inner | epoch 031:     49 / 130 loss=2.694, nll_loss=0.74, ppl=1.67, wps=1396.7, ups=0.23, wpb=6058.8, bsz=266.4, num_updates=3940, lr=0, gnorm=1.178, clip=100, loss_scale=2, train_wall=87, gb_free=9.2, wall=19234
2023-05-20 19:27:42 | INFO | train_inner | epoch 031:     69 / 130 loss=2.736, nll_loss=0.786, ppl=1.72, wps=1495.3, ups=0.24, wpb=6156.4, bsz=298.8, num_updates=3960, lr=0, gnorm=1.348, clip=100, loss_scale=2, train_wall=82, gb_free=9.6, wall=19316
2023-05-20 19:29:09 | INFO | train_inner | epoch 031:     89 / 130 loss=2.738, nll_loss=0.788, ppl=1.73, wps=1387.3, ups=0.23, wpb=6020.1, bsz=279.2, num_updates=3980, lr=0, gnorm=1.315, clip=100, loss_scale=2, train_wall=87, gb_free=9.6, wall=19403
2023-05-20 19:30:36 | INFO | train_inner | epoch 031:    109 / 130 loss=2.719, nll_loss=0.768, ppl=1.7, wps=1411.7, ups=0.23, wpb=6136.6, bsz=291.3, num_updates=4000, lr=0, gnorm=1.353, clip=100, loss_scale=2, train_wall=87, gb_free=8.8, wall=19490
2023-05-20 19:32:00 | INFO | train_inner | epoch 031:    129 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1440.5, ups=0.24, wpb=6007.6, bsz=279.9, num_updates=4020, lr=0, gnorm=1.761, clip=100, loss_scale=2, train_wall=83, gb_free=9, wall=19574
2023-05-20 19:32:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:32:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:32:38 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1007.5 | wpb 1392.5 | bsz 52.6 | num_updates 4021 | best_loss 3.138
2023-05-20 19:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4021 updates
2023-05-20 19:32:38 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint31.pt
2023-05-20 19:33:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint31.pt
2023-05-20 19:33:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint31.pt (epoch 31 @ 4021 updates, score 3.184) (writing took 51.16230998933315 seconds)
2023-05-20 19:33:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-05-20 19:33:30 | INFO | train | epoch 031 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1230.1 | ups 0.2 | wpb 6056.2 | bsz 280.9 | num_updates 4021 | lr 0 | gnorm 1.386 | clip 100 | loss_scale 2 | train_wall 550 | gb_free 9.9 | wall 19664
2023-05-20 19:33:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:33:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:33:30 | INFO | fairseq.trainer | begin training epoch 32
2023-05-20 19:33:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:34:54 | INFO | train_inner | epoch 032:     19 / 130 loss=2.72, nll_loss=0.768, ppl=1.7, wps=665.1, ups=0.11, wpb=5799.4, bsz=258.3, num_updates=4040, lr=0, gnorm=1.459, clip=100, loss_scale=2, train_wall=85, gb_free=9.7, wall=19748
2023-05-20 19:36:17 | INFO | train_inner | epoch 032:     39 / 130 loss=2.742, nll_loss=0.793, ppl=1.73, wps=1495.9, ups=0.24, wpb=6219.5, bsz=286.7, num_updates=4060, lr=0, gnorm=1.251, clip=100, loss_scale=2, train_wall=83, gb_free=8.6, wall=19831
2023-05-20 19:37:40 | INFO | train_inner | epoch 032:     59 / 130 loss=2.7, nll_loss=0.747, ppl=1.68, wps=1428.3, ups=0.24, wpb=5939.8, bsz=261.9, num_updates=4080, lr=0, gnorm=1.438, clip=100, loss_scale=2, train_wall=83, gb_free=9.5, wall=19914
2023-05-20 19:39:07 | INFO | train_inner | epoch 032:     79 / 130 loss=2.708, nll_loss=0.755, ppl=1.69, wps=1407.8, ups=0.23, wpb=6126.7, bsz=266, num_updates=4100, lr=0, gnorm=1.401, clip=100, loss_scale=4, train_wall=87, gb_free=9.5, wall=20001
2023-05-20 19:40:32 | INFO | train_inner | epoch 032:     99 / 130 loss=2.735, nll_loss=0.785, ppl=1.72, wps=1411.1, ups=0.24, wpb=5996.2, bsz=289.7, num_updates=4120, lr=0, gnorm=1.399, clip=100, loss_scale=4, train_wall=85, gb_free=9.7, wall=20086
2023-05-20 19:42:01 | INFO | train_inner | epoch 032:    119 / 130 loss=2.731, nll_loss=0.781, ppl=1.72, wps=1387.8, ups=0.23, wpb=6126, bsz=303.1, num_updates=4140, lr=0, gnorm=1.226, clip=100, loss_scale=4, train_wall=88, gb_free=9.4, wall=20175
2023-05-20 19:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:42:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:43:17 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 996.4 | wpb 1392.5 | bsz 52.6 | num_updates 4151 | best_loss 3.138
2023-05-20 19:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 4151 updates
2023-05-20 19:43:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint32.pt
2023-05-20 19:43:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint32.pt
2023-05-20 19:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint32.pt (epoch 32 @ 4151 updates, score 3.184) (writing took 38.082615192979574 seconds)
2023-05-20 19:43:55 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-05-20 19:43:55 | INFO | train | epoch 032 | loss 2.723 | nll_loss 0.772 | ppl 1.71 | wps 1258.4 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 4151 | lr 0 | gnorm 1.386 | clip 100 | loss_scale 4 | train_wall 551 | gb_free 10.1 | wall 20289
2023-05-20 19:43:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:43:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:43:55 | INFO | fairseq.trainer | begin training epoch 33
2023-05-20 19:43:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:44:34 | INFO | train_inner | epoch 033:      9 / 130 loss=2.722, nll_loss=0.772, ppl=1.71, wps=772.2, ups=0.13, wpb=5931.5, bsz=287.4, num_updates=4160, lr=0, gnorm=1.565, clip=100, loss_scale=4, train_wall=79, gb_free=8.4, wall=20328
2023-05-20 19:45:59 | INFO | train_inner | epoch 033:     29 / 130 loss=2.717, nll_loss=0.764, ppl=1.7, wps=1417.4, ups=0.24, wpb=6011.5, bsz=266.9, num_updates=4180, lr=0, gnorm=1.446, clip=100, loss_scale=4, train_wall=85, gb_free=9.5, wall=20413
2023-05-20 19:47:20 | INFO | train_inner | epoch 033:     49 / 130 loss=2.73, nll_loss=0.781, ppl=1.72, wps=1527.5, ups=0.25, wpb=6175.5, bsz=285.6, num_updates=4200, lr=0, gnorm=1.334, clip=100, loss_scale=4, train_wall=81, gb_free=9.5, wall=20494
2023-05-20 19:48:42 | INFO | train_inner | epoch 033:     69 / 130 loss=2.724, nll_loss=0.774, ppl=1.71, wps=1466.1, ups=0.24, wpb=6057.7, bsz=302.9, num_updates=4220, lr=0, gnorm=1.531, clip=100, loss_scale=4, train_wall=83, gb_free=9.8, wall=20577
2023-05-20 19:50:08 | INFO | train_inner | epoch 033:     89 / 130 loss=2.722, nll_loss=0.77, ppl=1.71, wps=1439.3, ups=0.23, wpb=6125.7, bsz=277.4, num_updates=4240, lr=0, gnorm=1.378, clip=100, loss_scale=4, train_wall=85, gb_free=9.3, wall=20662
2023-05-20 19:51:33 | INFO | train_inner | epoch 033:    109 / 130 loss=2.721, nll_loss=0.769, ppl=1.7, wps=1427, ups=0.23, wpb=6089.5, bsz=275.6, num_updates=4260, lr=0, gnorm=1.493, clip=100, loss_scale=4, train_wall=85, gb_free=9.2, wall=20747
2023-05-20 19:52:54 | INFO | train_inner | epoch 033:    129 / 130 loss=2.719, nll_loss=0.768, ppl=1.7, wps=1515.5, ups=0.25, wpb=6146, bsz=290.9, num_updates=4280, lr=0, gnorm=1.405, clip=100, loss_scale=4, train_wall=81, gb_free=9.6, wall=20828
2023-05-20 19:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 19:52:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:53:31 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1008.4 | wpb 1392.5 | bsz 52.6 | num_updates 4281 | best_loss 3.138
2023-05-20 19:53:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 4281 updates
2023-05-20 19:53:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint33.pt
2023-05-20 19:53:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint33.pt
2023-05-20 19:53:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint33.pt (epoch 33 @ 4281 updates, score 3.184) (writing took 19.521714497357607 seconds)
2023-05-20 19:53:51 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-05-20 19:53:51 | INFO | train | epoch 033 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1322.5 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 4281 | lr 0 | gnorm 1.435 | clip 100 | loss_scale 4 | train_wall 539 | gb_free 10.1 | wall 20885
2023-05-20 19:53:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 19:53:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 19:53:51 | INFO | fairseq.trainer | begin training epoch 34
2023-05-20 19:53:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 19:55:12 | INFO | train_inner | epoch 034:     19 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=854.8, ups=0.15, wpb=5892.4, bsz=278.2, num_updates=4300, lr=0, gnorm=1.377, clip=100, loss_scale=4, train_wall=83, gb_free=10.1, wall=20966
2023-05-20 19:56:37 | INFO | train_inner | epoch 034:     39 / 130 loss=2.734, nll_loss=0.784, ppl=1.72, wps=1423.2, ups=0.23, wpb=6066.9, bsz=290.2, num_updates=4320, lr=0, gnorm=1.422, clip=100, loss_scale=4, train_wall=85, gb_free=9.7, wall=21051
2023-05-20 19:58:05 | INFO | train_inner | epoch 034:     59 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1379.4, ups=0.23, wpb=6031.1, bsz=269.8, num_updates=4340, lr=0, gnorm=1.425, clip=100, loss_scale=4, train_wall=87, gb_free=10, wall=21139
2023-05-20 19:59:27 | INFO | train_inner | epoch 034:     79 / 130 loss=2.718, nll_loss=0.767, ppl=1.7, wps=1507.2, ups=0.24, wpb=6244.7, bsz=282.6, num_updates=4360, lr=0, gnorm=1.663, clip=100, loss_scale=4, train_wall=83, gb_free=9.9, wall=21222
2023-05-20 20:00:50 | INFO | train_inner | epoch 034:     99 / 130 loss=2.737, nll_loss=0.788, ppl=1.73, wps=1468.1, ups=0.24, wpb=6022.3, bsz=309.2, num_updates=4380, lr=0, gnorm=1.425, clip=100, loss_scale=4, train_wall=82, gb_free=9.7, wall=21304
2023-05-20 20:02:14 | INFO | train_inner | epoch 034:    119 / 130 loss=2.712, nll_loss=0.76, ppl=1.69, wps=1448.4, ups=0.24, wpb=6126.7, bsz=267.9, num_updates=4400, lr=0, gnorm=1.346, clip=100, loss_scale=4, train_wall=85, gb_free=9.8, wall=21388
2023-05-20 20:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:02:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:03:34 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 982.3 | wpb 1392.5 | bsz 52.6 | num_updates 4411 | best_loss 3.138
2023-05-20 20:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 4411 updates
2023-05-20 20:03:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint34.pt
2023-05-20 20:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint34.pt
2023-05-20 20:03:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint34.pt (epoch 34 @ 4411 updates, score 3.184) (writing took 20.10669295489788 seconds)
2023-05-20 20:03:54 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-05-20 20:03:54 | INFO | train | epoch 034 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1304.7 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 4411 | lr 0 | gnorm 1.476 | clip 100 | loss_scale 4 | train_wall 546 | gb_free 10.1 | wall 21488
2023-05-20 20:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:03:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:03:54 | INFO | fairseq.trainer | begin training epoch 35
2023-05-20 20:03:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:04:32 | INFO | train_inner | epoch 035:      9 / 130 loss=2.716, nll_loss=0.764, ppl=1.7, wps=823.5, ups=0.14, wpb=5690.1, bsz=242.6, num_updates=4420, lr=0, gnorm=1.674, clip=100, loss_scale=4, train_wall=82, gb_free=9.5, wall=21526
2023-05-20 20:05:52 | INFO | train_inner | epoch 035:     29 / 130 loss=2.725, nll_loss=0.774, ppl=1.71, wps=1556.8, ups=0.25, wpb=6168.8, bsz=290.1, num_updates=4440, lr=0, gnorm=1.278, clip=100, loss_scale=4, train_wall=79, gb_free=9.5, wall=21606
2023-05-20 20:07:15 | INFO | train_inner | epoch 035:     49 / 130 loss=2.725, nll_loss=0.775, ppl=1.71, wps=1459.2, ups=0.24, wpb=6107.4, bsz=294.1, num_updates=4460, lr=0, gnorm=1.358, clip=100, loss_scale=4, train_wall=84, gb_free=9.5, wall=21689
2023-05-20 20:08:42 | INFO | train_inner | epoch 035:     69 / 130 loss=2.719, nll_loss=0.767, ppl=1.7, wps=1388.8, ups=0.23, wpb=6020.8, bsz=266.4, num_updates=4480, lr=0, gnorm=1.392, clip=100, loss_scale=4, train_wall=87, gb_free=9.5, wall=21776
2023-05-20 20:10:10 | INFO | train_inner | epoch 035:     89 / 130 loss=2.704, nll_loss=0.75, ppl=1.68, wps=1383.6, ups=0.23, wpb=6059, bsz=262.5, num_updates=4500, lr=0, gnorm=1.406, clip=100, loss_scale=4, train_wall=87, gb_free=8.9, wall=21864
2023-05-20 20:10:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-20 20:11:35 | INFO | train_inner | epoch 035:    110 / 130 loss=2.737, nll_loss=0.789, ppl=1.73, wps=1451.6, ups=0.23, wpb=6226.1, bsz=317.9, num_updates=4520, lr=0, gnorm=1.418, clip=100, loss_scale=2, train_wall=86, gb_free=9.3, wall=21949
2023-05-20 20:12:57 | INFO | train_inner | epoch 035:    130 / 130 loss=2.722, nll_loss=0.77, ppl=1.71, wps=1421.2, ups=0.24, wpb=5817.2, bsz=267.2, num_updates=4540, lr=0, gnorm=1.759, clip=100, loss_scale=2, train_wall=82, gb_free=10.1, wall=22031
2023-05-20 20:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:12:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:13:33 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 990.8 | wpb 1392.5 | bsz 52.6 | num_updates 4540 | best_loss 3.138
2023-05-20 20:13:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 4540 updates
2023-05-20 20:13:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint35.pt
2023-05-20 20:13:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint35.pt
2023-05-20 20:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint35.pt (epoch 35 @ 4540 updates, score 3.184) (writing took 19.88969210162759 seconds)
2023-05-20 20:13:53 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2023-05-20 20:13:53 | INFO | train | epoch 035 | loss 2.723 | nll_loss 0.772 | ppl 1.71 | wps 1304.3 | ups 0.22 | wpb 6055.2 | bsz 280.7 | num_updates 4540 | lr 0 | gnorm 1.427 | clip 100 | loss_scale 2 | train_wall 542 | gb_free 10.1 | wall 22087
2023-05-20 20:13:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:13:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:13:53 | INFO | fairseq.trainer | begin training epoch 36
2023-05-20 20:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:15:16 | INFO | train_inner | epoch 036:     20 / 130 loss=2.735, nll_loss=0.785, ppl=1.72, wps=880.2, ups=0.14, wpb=6097.3, bsz=300, num_updates=4560, lr=0, gnorm=1.323, clip=100, loss_scale=2, train_wall=83, gb_free=9.8, wall=22170
2023-05-20 20:16:41 | INFO | train_inner | epoch 036:     40 / 130 loss=2.727, nll_loss=0.777, ppl=1.71, wps=1436.5, ups=0.24, wpb=6102.1, bsz=289.4, num_updates=4580, lr=0, gnorm=1.299, clip=100, loss_scale=2, train_wall=85, gb_free=9.4, wall=22255
2023-05-20 20:18:03 | INFO | train_inner | epoch 036:     60 / 130 loss=2.703, nll_loss=0.75, ppl=1.68, wps=1451.1, ups=0.24, wpb=5974.5, bsz=260.4, num_updates=4600, lr=0, gnorm=1.433, clip=100, loss_scale=2, train_wall=82, gb_free=10, wall=22337
2023-05-20 20:19:28 | INFO | train_inner | epoch 036:     80 / 130 loss=2.724, nll_loss=0.773, ppl=1.71, wps=1456, ups=0.24, wpb=6176.4, bsz=292.2, num_updates=4620, lr=0, gnorm=1.564, clip=100, loss_scale=2, train_wall=85, gb_free=9.2, wall=22422
2023-05-20 20:20:52 | INFO | train_inner | epoch 036:    100 / 130 loss=2.728, nll_loss=0.777, ppl=1.71, wps=1477.5, ups=0.24, wpb=6178.6, bsz=270.9, num_updates=4640, lr=0, gnorm=1.428, clip=100, loss_scale=2, train_wall=84, gb_free=9.2, wall=22506
2023-05-20 20:22:18 | INFO | train_inner | epoch 036:    120 / 130 loss=2.717, nll_loss=0.765, ppl=1.7, wps=1414.6, ups=0.23, wpb=6085.2, bsz=283.3, num_updates=4660, lr=0, gnorm=1.289, clip=100, loss_scale=2, train_wall=86, gb_free=9.6, wall=22592
2023-05-20 20:22:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:22:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:23:33 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1003.5 | wpb 1392.5 | bsz 52.6 | num_updates 4670 | best_loss 3.138
2023-05-20 20:23:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 4670 updates
2023-05-20 20:23:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint36.pt
2023-05-20 20:23:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint36.pt
2023-05-20 20:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint36.pt (epoch 36 @ 4670 updates, score 3.184) (writing took 27.93551905453205 seconds)
2023-05-20 20:24:02 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2023-05-20 20:24:02 | INFO | train | epoch 036 | loss 2.723 | nll_loss 0.772 | ppl 1.71 | wps 1291.8 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 4670 | lr 0 | gnorm 1.39 | clip 100 | loss_scale 2 | train_wall 544 | gb_free 10.1 | wall 22696
2023-05-20 20:24:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:24:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:24:02 | INFO | fairseq.trainer | begin training epoch 37
2023-05-20 20:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:24:45 | INFO | train_inner | epoch 037:     10 / 130 loss=2.735, nll_loss=0.783, ppl=1.72, wps=795.4, ups=0.14, wpb=5881.4, bsz=265, num_updates=4680, lr=0, gnorm=1.372, clip=100, loss_scale=2, train_wall=83, gb_free=9.6, wall=22740
2023-05-20 20:26:11 | INFO | train_inner | epoch 037:     30 / 130 loss=2.736, nll_loss=0.786, ppl=1.72, wps=1443.1, ups=0.23, wpb=6160.5, bsz=296.1, num_updates=4700, lr=0, gnorm=1.749, clip=100, loss_scale=2, train_wall=85, gb_free=9, wall=22825
2023-05-20 20:27:37 | INFO | train_inner | epoch 037:     50 / 130 loss=2.703, nll_loss=0.75, ppl=1.68, wps=1394.8, ups=0.23, wpb=6013.5, bsz=274.2, num_updates=4720, lr=0, gnorm=1.271, clip=100, loss_scale=2, train_wall=86, gb_free=9.9, wall=22911
2023-05-20 20:28:59 | INFO | train_inner | epoch 037:     70 / 130 loss=2.721, nll_loss=0.769, ppl=1.7, wps=1494.9, ups=0.24, wpb=6130.7, bsz=292.4, num_updates=4740, lr=0, gnorm=1.472, clip=100, loss_scale=2, train_wall=82, gb_free=10.1, wall=22993
2023-05-20 20:30:24 | INFO | train_inner | epoch 037:     90 / 130 loss=2.721, nll_loss=0.769, ppl=1.7, wps=1428.6, ups=0.24, wpb=6054.2, bsz=277.6, num_updates=4760, lr=0, gnorm=1.618, clip=100, loss_scale=2, train_wall=85, gb_free=10, wall=23078
2023-05-20 20:31:50 | INFO | train_inner | epoch 037:    110 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1388.6, ups=0.23, wpb=5989.4, bsz=274.4, num_updates=4780, lr=0, gnorm=1.441, clip=100, loss_scale=2, train_wall=86, gb_free=8.1, wall=23164
2023-05-20 20:33:10 | INFO | train_inner | epoch 037:    130 / 130 loss=2.727, nll_loss=0.777, ppl=1.71, wps=1465.7, ups=0.25, wpb=5886.4, bsz=276.1, num_updates=4800, lr=0, gnorm=1.467, clip=100, loss_scale=2, train_wall=80, gb_free=10, wall=23245
2023-05-20 20:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:33:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:33:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 988.9 | wpb 1392.5 | bsz 52.6 | num_updates 4800 | best_loss 3.138
2023-05-20 20:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 4800 updates
2023-05-20 20:33:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint37.pt
2023-05-20 20:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint37.pt
2023-05-20 20:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint37.pt (epoch 37 @ 4800 updates, score 3.184) (writing took 19.94822448119521 seconds)
2023-05-20 20:34:06 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2023-05-20 20:34:06 | INFO | train | epoch 037 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1303.6 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 4800 | lr 0 | gnorm 1.492 | clip 100 | loss_scale 2 | train_wall 547 | gb_free 10 | wall 23300
2023-05-20 20:34:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:34:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:34:06 | INFO | fairseq.trainer | begin training epoch 38
2023-05-20 20:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:35:32 | INFO | train_inner | epoch 038:     20 / 130 loss=2.717, nll_loss=0.765, ppl=1.7, wps=854.8, ups=0.14, wpb=6045.8, bsz=277.4, num_updates=4820, lr=0, gnorm=2.74, clip=100, loss_scale=2, train_wall=85, gb_free=9.9, wall=23386
2023-05-20 20:36:54 | INFO | train_inner | epoch 038:     40 / 130 loss=2.725, nll_loss=0.774, ppl=1.71, wps=1501.5, ups=0.24, wpb=6154.9, bsz=290.7, num_updates=4840, lr=0, gnorm=1.375, clip=100, loss_scale=2, train_wall=82, gb_free=9.9, wall=23468
2023-05-20 20:38:23 | INFO | train_inner | epoch 038:     60 / 130 loss=2.722, nll_loss=0.77, ppl=1.71, wps=1364.6, ups=0.22, wpb=6065, bsz=281.6, num_updates=4860, lr=0, gnorm=1.378, clip=100, loss_scale=2, train_wall=89, gb_free=9.7, wall=23557
2023-05-20 20:39:48 | INFO | train_inner | epoch 038:     80 / 130 loss=2.732, nll_loss=0.78, ppl=1.72, wps=1453.3, ups=0.23, wpb=6200.5, bsz=296.1, num_updates=4880, lr=0, gnorm=1.748, clip=100, loss_scale=2, train_wall=85, gb_free=9.8, wall=23642
2023-05-20 20:41:13 | INFO | train_inner | epoch 038:    100 / 130 loss=2.716, nll_loss=0.764, ppl=1.7, wps=1399.5, ups=0.24, wpb=5953.8, bsz=264.2, num_updates=4900, lr=0, gnorm=1.335, clip=100, loss_scale=2, train_wall=85, gb_free=10.1, wall=23727
2023-05-20 20:42:37 | INFO | train_inner | epoch 038:    120 / 130 loss=2.703, nll_loss=0.751, ppl=1.68, wps=1472.6, ups=0.24, wpb=6155.9, bsz=280.9, num_updates=4920, lr=0, gnorm=1.474, clip=100, loss_scale=2, train_wall=83, gb_free=9.4, wall=23811
2023-05-20 20:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:43:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:43:50 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 985.1 | wpb 1392.5 | bsz 52.6 | num_updates 4930 | best_loss 3.138
2023-05-20 20:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 4930 updates
2023-05-20 20:43:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint38.pt
2023-05-20 20:44:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint38.pt
2023-05-20 20:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint38.pt (epoch 38 @ 4930 updates, score 3.184) (writing took 20.328347764909267 seconds)
2023-05-20 20:44:10 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2023-05-20 20:44:10 | INFO | train | epoch 038 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1303.4 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 4930 | lr 0 | gnorm 1.679 | clip 100 | loss_scale 2 | train_wall 547 | gb_free 10.1 | wall 23904
2023-05-20 20:44:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:44:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:44:10 | INFO | fairseq.trainer | begin training epoch 39
2023-05-20 20:44:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:44:51 | INFO | train_inner | epoch 039:     10 / 130 loss=2.729, nll_loss=0.779, ppl=1.72, wps=848, ups=0.15, wpb=5709.7, bsz=261.6, num_updates=4940, lr=0, gnorm=2.002, clip=100, loss_scale=2, train_wall=78, gb_free=10, wall=23946
2023-05-20 20:46:14 | INFO | train_inner | epoch 039:     30 / 130 loss=2.756, nll_loss=0.809, ppl=1.75, wps=1466.3, ups=0.24, wpb=6056.1, bsz=293.3, num_updates=4960, lr=0, gnorm=1.704, clip=100, loss_scale=2, train_wall=83, gb_free=8.9, wall=24028
2023-05-20 20:47:39 | INFO | train_inner | epoch 039:     50 / 130 loss=2.711, nll_loss=0.758, ppl=1.69, wps=1457.2, ups=0.24, wpb=6166.7, bsz=296.5, num_updates=4980, lr=0, gnorm=1.358, clip=100, loss_scale=2, train_wall=85, gb_free=9.3, wall=24113
2023-05-20 20:49:04 | INFO | train_inner | epoch 039:     70 / 130 loss=2.725, nll_loss=0.773, ppl=1.71, wps=1441.1, ups=0.23, wpb=6143.3, bsz=299.2, num_updates=5000, lr=0, gnorm=1.437, clip=100, loss_scale=2, train_wall=85, gb_free=9.6, wall=24198
2023-05-20 20:50:27 | INFO | train_inner | epoch 039:     90 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=1458.5, ups=0.24, wpb=6061.2, bsz=271.9, num_updates=5020, lr=0, gnorm=1.555, clip=100, loss_scale=2, train_wall=83, gb_free=9.7, wall=24281
2023-05-20 20:51:52 | INFO | train_inner | epoch 039:    110 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=1472.3, ups=0.24, wpb=6221.2, bsz=285.1, num_updates=5040, lr=0, gnorm=1.435, clip=100, loss_scale=2, train_wall=84, gb_free=9.3, wall=24366
2023-05-20 20:53:14 | INFO | train_inner | epoch 039:    130 / 130 loss=2.708, nll_loss=0.755, ppl=1.69, wps=1405.6, ups=0.24, wpb=5796.2, bsz=253.8, num_updates=5060, lr=0, gnorm=1.549, clip=100, loss_scale=2, train_wall=82, gb_free=10.1, wall=24448
2023-05-20 20:53:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 20:53:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:53:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1001.6 | wpb 1392.5 | bsz 52.6 | num_updates 5060 | best_loss 3.138
2023-05-20 20:53:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 5060 updates
2023-05-20 20:53:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint39.pt
2023-05-20 20:54:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint39.pt
2023-05-20 20:54:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint39.pt (epoch 39 @ 5060 updates, score 3.184) (writing took 19.77536014840007 seconds)
2023-05-20 20:54:09 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2023-05-20 20:54:09 | INFO | train | epoch 039 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1314.1 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 5060 | lr 0 | gnorm 1.566 | clip 100 | loss_scale 2 | train_wall 543 | gb_free 10.1 | wall 24504
2023-05-20 20:54:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 20:54:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 20:54:10 | INFO | fairseq.trainer | begin training epoch 40
2023-05-20 20:54:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 20:54:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-20 20:55:43 | INFO | train_inner | epoch 040:     21 / 130 loss=2.731, nll_loss=0.781, ppl=1.72, wps=817, ups=0.13, wpb=6092.1, bsz=284.3, num_updates=5080, lr=0, gnorm=1.312, clip=100, loss_scale=1, train_wall=93, gb_free=9.6, wall=24597
2023-05-20 20:57:08 | INFO | train_inner | epoch 040:     41 / 130 loss=2.708, nll_loss=0.754, ppl=1.69, wps=1418.8, ups=0.24, wpb=6031.1, bsz=266.2, num_updates=5100, lr=0, gnorm=1.714, clip=100, loss_scale=1, train_wall=85, gb_free=9.7, wall=24682
2023-05-20 20:58:28 | INFO | train_inner | epoch 040:     61 / 130 loss=2.739, nll_loss=0.791, ppl=1.73, wps=1560, ups=0.25, wpb=6239.3, bsz=295.2, num_updates=5120, lr=0, gnorm=2.065, clip=100, loss_scale=1, train_wall=80, gb_free=9.3, wall=24762
2023-05-20 20:59:48 | INFO | train_inner | epoch 040:     81 / 130 loss=2.713, nll_loss=0.761, ppl=1.7, wps=1526.8, ups=0.25, wpb=6088.8, bsz=277.1, num_updates=5140, lr=0, gnorm=1.538, clip=100, loss_scale=1, train_wall=80, gb_free=9.5, wall=24842
2023-05-20 21:01:10 | INFO | train_inner | epoch 040:    101 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=1462.2, ups=0.24, wpb=6018.8, bsz=279.6, num_updates=5160, lr=0, gnorm=1.479, clip=100, loss_scale=1, train_wall=82, gb_free=9.6, wall=24924
2023-05-20 21:02:33 | INFO | train_inner | epoch 040:    121 / 130 loss=2.725, nll_loss=0.775, ppl=1.71, wps=1489.5, ups=0.24, wpb=6126.6, bsz=304.4, num_updates=5180, lr=0, gnorm=1.465, clip=100, loss_scale=1, train_wall=82, gb_free=9.5, wall=25007
2023-05-20 21:03:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:03:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:03:42 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 971 | wpb 1392.5 | bsz 52.6 | num_updates 5189 | best_loss 3.138
2023-05-20 21:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 5189 updates
2023-05-20 21:03:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint40.pt
2023-05-20 21:04:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint40.pt
2023-05-20 21:04:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint40.pt (epoch 40 @ 5189 updates, score 3.184) (writing took 36.343946412205696 seconds)
2023-05-20 21:04:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2023-05-20 21:04:19 | INFO | train | epoch 040 | loss 2.723 | nll_loss 0.771 | ppl 1.71 | wps 1280.4 | ups 0.21 | wpb 6055.1 | bsz 281.3 | num_updates 5189 | lr 0 | gnorm 1.584 | clip 100 | loss_scale 1 | train_wall 535 | gb_free 10.1 | wall 25114
2023-05-20 21:04:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:04:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:04:20 | INFO | fairseq.trainer | begin training epoch 41
2023-05-20 21:04:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:05:08 | INFO | train_inner | epoch 041:     11 / 130 loss=2.709, nll_loss=0.755, ppl=1.69, wps=746.7, ups=0.13, wpb=5795.9, bsz=254.5, num_updates=5200, lr=0, gnorm=1.342, clip=100, loss_scale=1, train_wall=81, gb_free=9, wall=25162
2023-05-20 21:06:31 | INFO | train_inner | epoch 041:     31 / 130 loss=2.71, nll_loss=0.758, ppl=1.69, wps=1499.1, ups=0.24, wpb=6217.4, bsz=279.5, num_updates=5220, lr=0, gnorm=1.246, clip=100, loss_scale=1, train_wall=83, gb_free=9.7, wall=25245
2023-05-20 21:07:59 | INFO | train_inner | epoch 041:     51 / 130 loss=2.723, nll_loss=0.773, ppl=1.71, wps=1379.6, ups=0.23, wpb=6065.2, bsz=285.7, num_updates=5240, lr=0, gnorm=1.606, clip=100, loss_scale=1, train_wall=88, gb_free=9.3, wall=25333
2023-05-20 21:09:25 | INFO | train_inner | epoch 041:     71 / 130 loss=2.732, nll_loss=0.782, ppl=1.72, wps=1435.2, ups=0.23, wpb=6195.2, bsz=294.7, num_updates=5260, lr=0, gnorm=1.443, clip=100, loss_scale=1, train_wall=86, gb_free=9.4, wall=25419
2023-05-20 21:10:48 | INFO | train_inner | epoch 041:     91 / 130 loss=2.732, nll_loss=0.783, ppl=1.72, wps=1467.4, ups=0.24, wpb=6104.7, bsz=299.6, num_updates=5280, lr=0, gnorm=1.289, clip=100, loss_scale=1, train_wall=83, gb_free=9.3, wall=25502
2023-05-20 21:12:13 | INFO | train_inner | epoch 041:    111 / 130 loss=2.728, nll_loss=0.776, ppl=1.71, wps=1424.6, ups=0.24, wpb=6034, bsz=277, num_updates=5300, lr=0, gnorm=1.634, clip=100, loss_scale=1, train_wall=85, gb_free=9.9, wall=25587
2023-05-20 21:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:13:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:14:05 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 981.6 | wpb 1392.5 | bsz 52.6 | num_updates 5319 | best_loss 3.138
2023-05-20 21:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 5319 updates
2023-05-20 21:14:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint41.pt
2023-05-20 21:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint41.pt
2023-05-20 21:14:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint41.pt (epoch 41 @ 5319 updates, score 3.184) (writing took 20.247106544673443 seconds)
2023-05-20 21:14:26 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2023-05-20 21:14:26 | INFO | train | epoch 041 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1299 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 5319 | lr 0 | gnorm 1.422 | clip 100 | loss_scale 1 | train_wall 549 | gb_free 10.1 | wall 25720
2023-05-20 21:14:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:14:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:14:26 | INFO | fairseq.trainer | begin training epoch 42
2023-05-20 21:14:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:14:30 | INFO | train_inner | epoch 042:      1 / 130 loss=2.72, nll_loss=0.768, ppl=1.7, wps=831.4, ups=0.15, wpb=5690.2, bsz=260.4, num_updates=5320, lr=0, gnorm=1.395, clip=100, loss_scale=1, train_wall=80, gb_free=9.7, wall=25724
2023-05-20 21:15:59 | INFO | train_inner | epoch 042:     21 / 130 loss=2.704, nll_loss=0.751, ppl=1.68, wps=1344.1, ups=0.22, wpb=5997.5, bsz=263.9, num_updates=5340, lr=0, gnorm=1.325, clip=100, loss_scale=1, train_wall=89, gb_free=9.9, wall=25813
2023-05-20 21:17:18 | INFO | train_inner | epoch 042:     41 / 130 loss=2.74, nll_loss=0.79, ppl=1.73, wps=1547.4, ups=0.25, wpb=6121.4, bsz=297.5, num_updates=5360, lr=0, gnorm=1.446, clip=100, loss_scale=1, train_wall=79, gb_free=9.6, wall=25892
2023-05-20 21:18:42 | INFO | train_inner | epoch 042:     61 / 130 loss=2.722, nll_loss=0.77, ppl=1.71, wps=1436.1, ups=0.24, wpb=5999.8, bsz=267.2, num_updates=5380, lr=0, gnorm=2.136, clip=100, loss_scale=1, train_wall=83, gb_free=9.5, wall=25976
2023-05-20 21:20:07 | INFO | train_inner | epoch 042:     81 / 130 loss=2.704, nll_loss=0.751, ppl=1.68, wps=1427.1, ups=0.23, wpb=6090.7, bsz=267.5, num_updates=5400, lr=0, gnorm=1.31, clip=100, loss_scale=1, train_wall=85, gb_free=9.5, wall=26061
2023-05-20 21:21:32 | INFO | train_inner | epoch 042:    101 / 130 loss=2.735, nll_loss=0.785, ppl=1.72, wps=1465.1, ups=0.24, wpb=6231.2, bsz=312.2, num_updates=5420, lr=0, gnorm=1.307, clip=100, loss_scale=1, train_wall=85, gb_free=9, wall=26146
2023-05-20 21:22:56 | INFO | train_inner | epoch 042:    121 / 130 loss=2.725, nll_loss=0.775, ppl=1.71, wps=1469, ups=0.24, wpb=6137.9, bsz=288.8, num_updates=5440, lr=0, gnorm=1.354, clip=100, loss_scale=1, train_wall=83, gb_free=9.1, wall=26230
2023-05-20 21:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:24:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1012.3 | wpb 1392.5 | bsz 52.6 | num_updates 5449 | best_loss 3.138
2023-05-20 21:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 5449 updates
2023-05-20 21:24:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint42.pt
2023-05-20 21:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint42.pt
2023-05-20 21:24:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint42.pt (epoch 42 @ 5449 updates, score 3.184) (writing took 20.76926624774933 seconds)
2023-05-20 21:24:26 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2023-05-20 21:24:26 | INFO | train | epoch 042 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1311.5 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 5449 | lr 0 | gnorm 1.478 | clip 100 | loss_scale 1 | train_wall 543 | gb_free 10.1 | wall 26320
2023-05-20 21:24:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:24:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:24:26 | INFO | fairseq.trainer | begin training epoch 43
2023-05-20 21:24:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:25:14 | INFO | train_inner | epoch 043:     11 / 130 loss=2.712, nll_loss=0.759, ppl=1.69, wps=843.7, ups=0.14, wpb=5820.5, bsz=252.1, num_updates=5460, lr=0, gnorm=1.504, clip=100, loss_scale=1, train_wall=82, gb_free=9.9, wall=26368
2023-05-20 21:26:40 | INFO | train_inner | epoch 043:     31 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1368.2, ups=0.23, wpb=5876.1, bsz=285.3, num_updates=5480, lr=0, gnorm=1.277, clip=100, loss_scale=1, train_wall=86, gb_free=10, wall=26454
2023-05-20 21:28:06 | INFO | train_inner | epoch 043:     51 / 130 loss=2.736, nll_loss=0.786, ppl=1.72, wps=1426.7, ups=0.23, wpb=6184.8, bsz=288.8, num_updates=5500, lr=0, gnorm=2.006, clip=100, loss_scale=1, train_wall=87, gb_free=9.7, wall=26540
2023-05-20 21:29:35 | INFO | train_inner | epoch 043:     71 / 130 loss=2.745, nll_loss=0.796, ppl=1.74, wps=1386.9, ups=0.22, wpb=6179.4, bsz=291.4, num_updates=5520, lr=0, gnorm=1.495, clip=100, loss_scale=1, train_wall=89, gb_free=9.2, wall=26629
2023-05-20 21:31:02 | INFO | train_inner | epoch 043:     91 / 130 loss=2.723, nll_loss=0.773, ppl=1.71, wps=1440.1, ups=0.23, wpb=6216.6, bsz=304.2, num_updates=5540, lr=0, gnorm=1.258, clip=100, loss_scale=1, train_wall=86, gb_free=10.1, wall=26716
2023-05-20 21:32:25 | INFO | train_inner | epoch 043:    111 / 130 loss=2.695, nll_loss=0.741, ppl=1.67, wps=1448.4, ups=0.24, wpb=6045.5, bsz=264.8, num_updates=5560, lr=0, gnorm=1.246, clip=100, loss_scale=1, train_wall=83, gb_free=9.5, wall=26799
2023-05-20 21:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:33:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:34:16 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 985 | wpb 1392.5 | bsz 52.6 | num_updates 5579 | best_loss 3.138
2023-05-20 21:34:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 5579 updates
2023-05-20 21:34:16 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint43.pt
2023-05-20 21:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint43.pt
2023-05-20 21:34:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint43.pt (epoch 43 @ 5579 updates, score 3.184) (writing took 19.61910853534937 seconds)
2023-05-20 21:34:36 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2023-05-20 21:34:36 | INFO | train | epoch 043 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1291.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 5579 | lr 0 | gnorm 1.482 | clip 100 | loss_scale 1 | train_wall 553 | gb_free 10 | wall 26930
2023-05-20 21:34:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:34:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:34:36 | INFO | fairseq.trainer | begin training epoch 44
2023-05-20 21:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:34:39 | INFO | train_inner | epoch 044:      1 / 130 loss=2.727, nll_loss=0.774, ppl=1.71, wps=878.3, ups=0.15, wpb=5883.6, bsz=266.6, num_updates=5580, lr=0, gnorm=1.563, clip=100, loss_scale=1, train_wall=78, gb_free=10, wall=26933
2023-05-20 21:36:04 | INFO | train_inner | epoch 044:     21 / 130 loss=2.749, nll_loss=0.8, ppl=1.74, wps=1457.2, ups=0.24, wpb=6192.3, bsz=312.4, num_updates=5600, lr=0, gnorm=1.7, clip=100, loss_scale=1, train_wall=85, gb_free=9.5, wall=27018
2023-05-20 21:37:30 | INFO | train_inner | epoch 044:     41 / 130 loss=2.725, nll_loss=0.773, ppl=1.71, wps=1410, ups=0.23, wpb=6057.7, bsz=282.7, num_updates=5620, lr=0, gnorm=1.333, clip=100, loss_scale=1, train_wall=86, gb_free=9.1, wall=27104
2023-05-20 21:38:50 | INFO | train_inner | epoch 044:     61 / 130 loss=2.713, nll_loss=0.761, ppl=1.69, wps=1513.9, ups=0.25, wpb=6072.2, bsz=270.8, num_updates=5640, lr=0, gnorm=1.61, clip=100, loss_scale=1, train_wall=80, gb_free=9.3, wall=27184
2023-05-20 21:40:15 | INFO | train_inner | epoch 044:     81 / 130 loss=2.705, nll_loss=0.752, ppl=1.68, wps=1425.5, ups=0.24, wpb=6007.2, bsz=289.6, num_updates=5660, lr=0, gnorm=1.455, clip=100, loss_scale=1, train_wall=84, gb_free=9.8, wall=27269
2023-05-20 21:41:39 | INFO | train_inner | epoch 044:    101 / 130 loss=2.722, nll_loss=0.771, ppl=1.71, wps=1457.7, ups=0.24, wpb=6126.5, bsz=284.5, num_updates=5680, lr=0, gnorm=1.338, clip=100, loss_scale=1, train_wall=84, gb_free=9.8, wall=27353
2023-05-20 21:43:05 | INFO | train_inner | epoch 044:    121 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1392.5, ups=0.23, wpb=6035.2, bsz=252.1, num_updates=5700, lr=0, gnorm=1.367, clip=100, loss_scale=1, train_wall=87, gb_free=9.5, wall=27439
2023-05-20 21:43:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:43:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:44:15 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1004.8 | wpb 1392.5 | bsz 52.6 | num_updates 5709 | best_loss 3.138
2023-05-20 21:44:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 5709 updates
2023-05-20 21:44:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint44.pt
2023-05-20 21:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint44.pt
2023-05-20 21:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint44.pt (epoch 44 @ 5709 updates, score 3.184) (writing took 25.877461288124323 seconds)
2023-05-20 21:44:41 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2023-05-20 21:44:41 | INFO | train | epoch 044 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1301 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 5709 | lr 0 | gnorm 1.46 | clip 100 | loss_scale 1 | train_wall 543 | gb_free 10.1 | wall 27535
2023-05-20 21:44:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:44:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:44:41 | INFO | fairseq.trainer | begin training epoch 45
2023-05-20 21:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:45:26 | INFO | train_inner | epoch 045:     11 / 130 loss=2.729, nll_loss=0.778, ppl=1.71, wps=833.2, ups=0.14, wpb=5843.9, bsz=278.4, num_updates=5720, lr=0, gnorm=1.553, clip=100, loss_scale=1, train_wall=79, gb_free=9.7, wall=27580
2023-05-20 21:46:51 | INFO | train_inner | epoch 045:     31 / 130 loss=2.727, nll_loss=0.777, ppl=1.71, wps=1425.1, ups=0.24, wpb=6057.1, bsz=274.9, num_updates=5740, lr=0, gnorm=1.299, clip=100, loss_scale=1, train_wall=85, gb_free=10.1, wall=27665
2023-05-20 21:48:15 | INFO | train_inner | epoch 045:     51 / 130 loss=2.726, nll_loss=0.774, ppl=1.71, wps=1451.1, ups=0.24, wpb=6122.8, bsz=298.1, num_updates=5760, lr=0, gnorm=1.275, clip=100, loss_scale=1, train_wall=84, gb_free=10.1, wall=27749
2023-05-20 21:49:39 | INFO | train_inner | epoch 045:     71 / 130 loss=2.724, nll_loss=0.773, ppl=1.71, wps=1457.9, ups=0.24, wpb=6090.1, bsz=281.6, num_updates=5780, lr=0, gnorm=1.365, clip=100, loss_scale=1, train_wall=83, gb_free=8.8, wall=27833
2023-05-20 21:51:05 | INFO | train_inner | epoch 045:     91 / 130 loss=2.725, nll_loss=0.774, ppl=1.71, wps=1398.6, ups=0.23, wpb=6035.3, bsz=270.1, num_updates=5800, lr=0, gnorm=1.508, clip=100, loss_scale=1, train_wall=86, gb_free=9.5, wall=27919
2023-05-20 21:52:30 | INFO | train_inner | epoch 045:    111 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1438, ups=0.23, wpb=6142.7, bsz=284, num_updates=5820, lr=0, gnorm=1.231, clip=100, loss_scale=1, train_wall=85, gb_free=10, wall=28004
2023-05-20 21:53:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 21:53:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:54:18 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 999.9 | wpb 1392.5 | bsz 52.6 | num_updates 5839 | best_loss 3.138
2023-05-20 21:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 5839 updates
2023-05-20 21:54:18 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint45.pt
2023-05-20 21:54:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint45.pt
2023-05-20 21:54:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint45.pt (epoch 45 @ 5839 updates, score 3.184) (writing took 20.162110913544893 seconds)
2023-05-20 21:54:38 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2023-05-20 21:54:38 | INFO | train | epoch 045 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1317.9 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 5839 | lr 0 | gnorm 1.378 | clip 100 | loss_scale 1 | train_wall 541 | gb_free 10.1 | wall 28132
2023-05-20 21:54:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 21:54:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 21:54:38 | INFO | fairseq.trainer | begin training epoch 46
2023-05-20 21:54:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 21:54:43 | INFO | train_inner | epoch 046:      1 / 130 loss=2.711, nll_loss=0.759, ppl=1.69, wps=894, ups=0.15, wpb=5923.1, bsz=270.6, num_updates=5840, lr=0, gnorm=1.42, clip=100, loss_scale=1, train_wall=76, gb_free=9.6, wall=28137
2023-05-20 21:56:06 | INFO | train_inner | epoch 046:     21 / 130 loss=2.721, nll_loss=0.771, ppl=1.71, wps=1441.8, ups=0.24, wpb=6015.4, bsz=297.7, num_updates=5860, lr=0, gnorm=1.33, clip=100, loss_scale=1, train_wall=83, gb_free=9.7, wall=28220
2023-05-20 21:57:30 | INFO | train_inner | epoch 046:     41 / 130 loss=2.72, nll_loss=0.77, ppl=1.71, wps=1432.8, ups=0.24, wpb=6028.9, bsz=286.7, num_updates=5880, lr=0, gnorm=1.819, clip=100, loss_scale=1, train_wall=84, gb_free=9.5, wall=28304
2023-05-20 21:58:54 | INFO | train_inner | epoch 046:     61 / 130 loss=2.708, nll_loss=0.754, ppl=1.69, wps=1448.9, ups=0.24, wpb=6063, bsz=262.9, num_updates=5900, lr=0, gnorm=1.486, clip=100, loss_scale=1, train_wall=84, gb_free=9.9, wall=28388
2023-05-20 22:00:18 | INFO | train_inner | epoch 046:     81 / 130 loss=2.732, nll_loss=0.782, ppl=1.72, wps=1449.1, ups=0.24, wpb=6064.5, bsz=288.9, num_updates=5920, lr=0, gnorm=2.212, clip=100, loss_scale=1, train_wall=84, gb_free=9.9, wall=28472
2023-05-20 22:01:46 | INFO | train_inner | epoch 046:    101 / 130 loss=2.728, nll_loss=0.778, ppl=1.71, wps=1398.1, ups=0.23, wpb=6146.2, bsz=276.6, num_updates=5940, lr=0, gnorm=1.305, clip=100, loss_scale=1, train_wall=88, gb_free=9.6, wall=28560
2023-05-20 22:03:12 | INFO | train_inner | epoch 046:    121 / 130 loss=2.733, nll_loss=0.782, ppl=1.72, wps=1447.4, ups=0.23, wpb=6260.7, bsz=299.6, num_updates=5960, lr=0, gnorm=1.268, clip=100, loss_scale=1, train_wall=86, gb_free=10.1, wall=28646
2023-05-20 22:03:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:03:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:04:23 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 994.9 | wpb 1392.5 | bsz 52.6 | num_updates 5969 | best_loss 3.138
2023-05-20 22:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 5969 updates
2023-05-20 22:04:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint46.pt
2023-05-20 22:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint46.pt
2023-05-20 22:04:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint46.pt (epoch 46 @ 5969 updates, score 3.184) (writing took 19.835274022072554 seconds)
2023-05-20 22:04:43 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2023-05-20 22:04:43 | INFO | train | epoch 046 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1301.7 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 5969 | lr 0 | gnorm 1.559 | clip 100 | loss_scale 1 | train_wall 548 | gb_free 10.1 | wall 28737
2023-05-20 22:04:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:04:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:04:43 | INFO | fairseq.trainer | begin training epoch 47
2023-05-20 22:04:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:05:29 | INFO | train_inner | epoch 047:     11 / 130 loss=2.701, nll_loss=0.746, ppl=1.68, wps=835.6, ups=0.15, wpb=5720.2, bsz=231.8, num_updates=5980, lr=0, gnorm=1.527, clip=100, loss_scale=1, train_wall=81, gb_free=9.4, wall=28783
2023-05-20 22:06:54 | INFO | train_inner | epoch 047:     31 / 130 loss=2.742, nll_loss=0.794, ppl=1.73, wps=1441.4, ups=0.24, wpb=6117.7, bsz=297.1, num_updates=6000, lr=0, gnorm=1.325, clip=100, loss_scale=1, train_wall=85, gb_free=9.6, wall=28868
2023-05-20 22:08:17 | INFO | train_inner | epoch 047:     51 / 130 loss=2.71, nll_loss=0.756, ppl=1.69, wps=1469, ups=0.24, wpb=6103.2, bsz=262, num_updates=6020, lr=0, gnorm=1.411, clip=100, loss_scale=1, train_wall=83, gb_free=10, wall=28951
2023-05-20 22:09:41 | INFO | train_inner | epoch 047:     71 / 130 loss=2.712, nll_loss=0.759, ppl=1.69, wps=1461.2, ups=0.24, wpb=6102.4, bsz=287.9, num_updates=6040, lr=0, gnorm=1.332, clip=100, loss_scale=1, train_wall=83, gb_free=9.8, wall=29035
2023-05-20 22:11:09 | INFO | train_inner | epoch 047:     91 / 130 loss=2.72, nll_loss=0.769, ppl=1.7, wps=1380.2, ups=0.23, wpb=6076.9, bsz=289.9, num_updates=6060, lr=0, gnorm=1.47, clip=100, loss_scale=1, train_wall=88, gb_free=9.2, wall=29123
2023-05-20 22:12:32 | INFO | train_inner | epoch 047:    111 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=1490.1, ups=0.24, wpb=6201.5, bsz=297.9, num_updates=6080, lr=0, gnorm=1.28, clip=100, loss_scale=1, train_wall=83, gb_free=9.8, wall=29206
2023-05-20 22:13:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:13:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:14:23 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 995 | wpb 1392.5 | bsz 52.6 | num_updates 6099 | best_loss 3.138
2023-05-20 22:14:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 6099 updates
2023-05-20 22:14:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint47.pt
2023-05-20 22:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint47.pt
2023-05-20 22:14:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint47.pt (epoch 47 @ 6099 updates, score 3.184) (writing took 19.845049925148487 seconds)
2023-05-20 22:14:42 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2023-05-20 22:14:42 | INFO | train | epoch 047 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1313.5 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6099 | lr 0 | gnorm 1.424 | clip 100 | loss_scale 2 | train_wall 543 | gb_free 10.1 | wall 29337
2023-05-20 22:14:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:14:43 | INFO | fairseq.trainer | begin training epoch 48
2023-05-20 22:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:14:46 | INFO | train_inner | epoch 048:      1 / 130 loss=2.738, nll_loss=0.788, ppl=1.73, wps=867.4, ups=0.15, wpb=5811.8, bsz=278.4, num_updates=6100, lr=0, gnorm=1.629, clip=100, loss_scale=2, train_wall=78, gb_free=9.8, wall=29340
2023-05-20 22:16:14 | INFO | train_inner | epoch 048:     21 / 130 loss=2.745, nll_loss=0.797, ppl=1.74, wps=1398.5, ups=0.23, wpb=6164.9, bsz=302.9, num_updates=6120, lr=0, gnorm=1.344, clip=100, loss_scale=2, train_wall=88, gb_free=10.1, wall=29428
2023-05-20 22:17:41 | INFO | train_inner | epoch 048:     41 / 130 loss=2.705, nll_loss=0.751, ppl=1.68, wps=1401.8, ups=0.23, wpb=6071.6, bsz=262.9, num_updates=6140, lr=0, gnorm=1.269, clip=100, loss_scale=2, train_wall=87, gb_free=9.2, wall=29515
2023-05-20 22:19:06 | INFO | train_inner | epoch 048:     61 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1413.6, ups=0.23, wpb=6040.6, bsz=261.2, num_updates=6160, lr=0, gnorm=1.299, clip=100, loss_scale=2, train_wall=85, gb_free=9.3, wall=29600
2023-05-20 22:20:33 | INFO | train_inner | epoch 048:     81 / 130 loss=2.719, nll_loss=0.767, ppl=1.7, wps=1404.3, ups=0.23, wpb=6098.4, bsz=277.7, num_updates=6180, lr=0, gnorm=2.244, clip=100, loss_scale=2, train_wall=87, gb_free=9.5, wall=29687
2023-05-20 22:21:56 | INFO | train_inner | epoch 048:    101 / 130 loss=2.71, nll_loss=0.758, ppl=1.69, wps=1468.7, ups=0.24, wpb=6074.4, bsz=293.2, num_updates=6200, lr=0, gnorm=1.266, clip=100, loss_scale=2, train_wall=83, gb_free=9.1, wall=29770
2023-05-20 22:23:19 | INFO | train_inner | epoch 048:    121 / 130 loss=2.741, nll_loss=0.793, ppl=1.73, wps=1447.3, ups=0.24, wpb=6045.8, bsz=289.5, num_updates=6220, lr=0, gnorm=1.255, clip=100, loss_scale=2, train_wall=83, gb_free=9.1, wall=29853
2023-05-20 22:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:23:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:24:27 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 1013.2 | wpb 1392.5 | bsz 52.6 | num_updates 6229 | best_loss 3.138
2023-05-20 22:24:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 6229 updates
2023-05-20 22:24:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint48.pt
2023-05-20 22:24:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint48.pt
2023-05-20 22:24:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint48.pt (epoch 48 @ 6229 updates, score 3.184) (writing took 19.621211044490337 seconds)
2023-05-20 22:24:47 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2023-05-20 22:24:47 | INFO | train | epoch 048 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1302.9 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6229 | lr 0 | gnorm 1.436 | clip 100 | loss_scale 2 | train_wall 548 | gb_free 10.1 | wall 29941
2023-05-20 22:24:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:24:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:24:47 | INFO | fairseq.trainer | begin training epoch 49
2023-05-20 22:24:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:25:32 | INFO | train_inner | epoch 049:     11 / 130 loss=2.712, nll_loss=0.76, ppl=1.69, wps=898.7, ups=0.15, wpb=5958.3, bsz=276.9, num_updates=6240, lr=0, gnorm=1.747, clip=100, loss_scale=2, train_wall=77, gb_free=9.9, wall=29986
2023-05-20 22:26:56 | INFO | train_inner | epoch 049:     31 / 130 loss=2.727, nll_loss=0.775, ppl=1.71, wps=1440, ups=0.24, wpb=6041.4, bsz=283.7, num_updates=6260, lr=0, gnorm=1.332, clip=100, loss_scale=2, train_wall=84, gb_free=10.1, wall=30070
2023-05-20 22:28:23 | INFO | train_inner | epoch 049:     51 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1397.9, ups=0.23, wpb=6094.9, bsz=288.7, num_updates=6280, lr=0, gnorm=1.278, clip=100, loss_scale=2, train_wall=87, gb_free=9.7, wall=30157
2023-05-20 22:29:48 | INFO | train_inner | epoch 049:     71 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1405.1, ups=0.24, wpb=5947.2, bsz=265.9, num_updates=6300, lr=0, gnorm=1.317, clip=100, loss_scale=2, train_wall=85, gb_free=9.8, wall=30242
2023-05-20 22:31:12 | INFO | train_inner | epoch 049:     91 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=1463.3, ups=0.24, wpb=6148.2, bsz=277.1, num_updates=6320, lr=0, gnorm=1.44, clip=100, loss_scale=2, train_wall=84, gb_free=9.3, wall=30326
2023-05-20 22:32:38 | INFO | train_inner | epoch 049:    111 / 130 loss=2.73, nll_loss=0.78, ppl=1.72, wps=1422.4, ups=0.23, wpb=6147.6, bsz=281.8, num_updates=6340, lr=0, gnorm=1.523, clip=100, loss_scale=2, train_wall=86, gb_free=9.4, wall=30412
2023-05-20 22:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:33:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:34:30 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 995.5 | wpb 1392.5 | bsz 52.6 | num_updates 6359 | best_loss 3.138
2023-05-20 22:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 6359 updates
2023-05-20 22:34:30 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint49.pt
2023-05-20 22:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint49.pt
2023-05-20 22:34:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint49.pt (epoch 49 @ 6359 updates, score 3.184) (writing took 19.479996871203184 seconds)
2023-05-20 22:34:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2023-05-20 22:34:50 | INFO | train | epoch 049 | loss 2.722 | nll_loss 0.77 | ppl 1.71 | wps 1306.2 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6359 | lr 0 | gnorm 1.437 | clip 100 | loss_scale 2 | train_wall 547 | gb_free 10.1 | wall 30544
2023-05-20 22:34:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:34:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:34:50 | INFO | fairseq.trainer | begin training epoch 50
2023-05-20 22:34:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:34:53 | INFO | train_inner | epoch 050:      1 / 130 loss=2.729, nll_loss=0.779, ppl=1.72, wps=873.3, ups=0.15, wpb=5904.1, bsz=290.5, num_updates=6360, lr=0, gnorm=1.352, clip=100, loss_scale=2, train_wall=79, gb_free=9.9, wall=30547
2023-05-20 22:36:22 | INFO | train_inner | epoch 050:     21 / 130 loss=2.723, nll_loss=0.772, ppl=1.71, wps=1398.8, ups=0.23, wpb=6166.9, bsz=285.4, num_updates=6380, lr=0, gnorm=1.236, clip=100, loss_scale=2, train_wall=88, gb_free=9.1, wall=30636
2023-05-20 22:37:48 | INFO | train_inner | epoch 050:     41 / 130 loss=2.715, nll_loss=0.764, ppl=1.7, wps=1394.6, ups=0.23, wpb=6018.1, bsz=263.9, num_updates=6400, lr=0, gnorm=1.404, clip=100, loss_scale=2, train_wall=86, gb_free=9.2, wall=30722
2023-05-20 22:39:13 | INFO | train_inner | epoch 050:     61 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1457.3, ups=0.24, wpb=6182.1, bsz=287, num_updates=6420, lr=0, gnorm=1.365, clip=100, loss_scale=2, train_wall=85, gb_free=10, wall=30807
2023-05-20 22:40:34 | INFO | train_inner | epoch 050:     81 / 130 loss=2.732, nll_loss=0.781, ppl=1.72, wps=1497.1, ups=0.24, wpb=6114.7, bsz=283.8, num_updates=6440, lr=0, gnorm=1.351, clip=100, loss_scale=2, train_wall=82, gb_free=8.5, wall=30888
2023-05-20 22:41:56 | INFO | train_inner | epoch 050:    101 / 130 loss=2.737, nll_loss=0.788, ppl=1.73, wps=1490.3, ups=0.24, wpb=6113.8, bsz=302.2, num_updates=6460, lr=0, gnorm=1.559, clip=100, loss_scale=2, train_wall=82, gb_free=10.1, wall=30971
2023-05-20 22:43:20 | INFO | train_inner | epoch 050:    121 / 130 loss=2.716, nll_loss=0.764, ppl=1.7, wps=1441.8, ups=0.24, wpb=6026.9, bsz=279.2, num_updates=6480, lr=0, gnorm=1.513, clip=100, loss_scale=2, train_wall=84, gb_free=9.8, wall=31054
2023-05-20 22:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:43:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:44:31 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 983.8 | wpb 1392.5 | bsz 52.6 | num_updates 6489 | best_loss 3.138
2023-05-20 22:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 6489 updates
2023-05-20 22:44:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint50.pt
2023-05-20 22:44:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint50.pt
2023-05-20 22:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint50.pt (epoch 50 @ 6489 updates, score 3.184) (writing took 19.849170498549938 seconds)
2023-05-20 22:44:51 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2023-05-20 22:44:51 | INFO | train | epoch 050 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1308.6 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6489 | lr 0 | gnorm 1.405 | clip 100 | loss_scale 2 | train_wall 544 | gb_free 10.1 | wall 31145
2023-05-20 22:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:44:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:44:51 | INFO | fairseq.trainer | begin training epoch 51
2023-05-20 22:44:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:45:37 | INFO | train_inner | epoch 051:     11 / 130 loss=2.754, nll_loss=0.805, ppl=1.75, wps=866, ups=0.15, wpb=5914.1, bsz=278.2, num_updates=6500, lr=0, gnorm=1.335, clip=100, loss_scale=2, train_wall=80, gb_free=9.1, wall=31191
2023-05-20 22:46:58 | INFO | train_inner | epoch 051:     31 / 130 loss=2.707, nll_loss=0.755, ppl=1.69, wps=1494.7, ups=0.25, wpb=6061.4, bsz=281.1, num_updates=6520, lr=0, gnorm=1.236, clip=100, loss_scale=2, train_wall=81, gb_free=9.8, wall=31272
2023-05-20 22:48:18 | INFO | train_inner | epoch 051:     51 / 130 loss=2.712, nll_loss=0.76, ppl=1.69, wps=1500.2, ups=0.25, wpb=6014.7, bsz=273.6, num_updates=6540, lr=0, gnorm=1.227, clip=100, loss_scale=2, train_wall=80, gb_free=9.7, wall=31352
2023-05-20 22:49:46 | INFO | train_inner | epoch 051:     71 / 130 loss=2.716, nll_loss=0.763, ppl=1.7, wps=1393.8, ups=0.23, wpb=6161.2, bsz=276.1, num_updates=6560, lr=0, gnorm=1.526, clip=100, loss_scale=2, train_wall=88, gb_free=9, wall=31440
2023-05-20 22:51:09 | INFO | train_inner | epoch 051:     91 / 130 loss=2.727, nll_loss=0.775, ppl=1.71, wps=1482.2, ups=0.24, wpb=6116.3, bsz=279.3, num_updates=6580, lr=0, gnorm=1.432, clip=100, loss_scale=2, train_wall=82, gb_free=9.8, wall=31523
2023-05-20 22:52:35 | INFO | train_inner | epoch 051:    111 / 130 loss=2.706, nll_loss=0.753, ppl=1.68, wps=1407.8, ups=0.23, wpb=6035, bsz=276.9, num_updates=6600, lr=0, gnorm=1.397, clip=100, loss_scale=2, train_wall=86, gb_free=9.8, wall=31609
2023-05-20 22:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 22:53:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:54:28 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 991.1 | wpb 1392.5 | bsz 52.6 | num_updates 6619 | best_loss 3.138
2023-05-20 22:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 6619 updates
2023-05-20 22:54:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint51.pt
2023-05-20 22:54:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint51.pt
2023-05-20 22:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint51.pt (epoch 51 @ 6619 updates, score 3.184) (writing took 19.991950668394566 seconds)
2023-05-20 22:54:48 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2023-05-20 22:54:48 | INFO | train | epoch 051 | loss 2.721 | nll_loss 0.77 | ppl 1.71 | wps 1318.3 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6619 | lr 0 | gnorm 1.369 | clip 100 | loss_scale 2 | train_wall 540 | gb_free 10.1 | wall 31742
2023-05-20 22:54:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 22:54:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 22:54:48 | INFO | fairseq.trainer | begin training epoch 52
2023-05-20 22:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 22:54:52 | INFO | train_inner | epoch 052:      1 / 130 loss=2.724, nll_loss=0.774, ppl=1.71, wps=846.4, ups=0.15, wpb=5812.9, bsz=279.8, num_updates=6620, lr=0, gnorm=1.448, clip=100, loss_scale=2, train_wall=81, gb_free=9.6, wall=31746
2023-05-20 22:56:19 | INFO | train_inner | epoch 052:     21 / 130 loss=2.703, nll_loss=0.749, ppl=1.68, wps=1394.1, ups=0.23, wpb=6038.7, bsz=277.9, num_updates=6640, lr=0, gnorm=1.505, clip=100, loss_scale=2, train_wall=87, gb_free=9.1, wall=31833
2023-05-20 22:57:44 | INFO | train_inner | epoch 052:     41 / 130 loss=2.731, nll_loss=0.781, ppl=1.72, wps=1423.5, ups=0.23, wpb=6085.7, bsz=285.6, num_updates=6660, lr=0, gnorm=1.272, clip=100, loss_scale=2, train_wall=85, gb_free=9.2, wall=31918
2023-05-20 22:59:08 | INFO | train_inner | epoch 052:     61 / 130 loss=2.714, nll_loss=0.763, ppl=1.7, wps=1438.9, ups=0.24, wpb=6002.9, bsz=259.5, num_updates=6680, lr=0, gnorm=1.525, clip=100, loss_scale=2, train_wall=83, gb_free=10.1, wall=32002
2023-05-20 23:00:32 | INFO | train_inner | epoch 052:     81 / 130 loss=2.724, nll_loss=0.772, ppl=1.71, wps=1428, ups=0.24, wpb=6042.6, bsz=262.6, num_updates=6700, lr=0, gnorm=1.525, clip=100, loss_scale=2, train_wall=85, gb_free=9.5, wall=32086
2023-05-20 23:01:59 | INFO | train_inner | epoch 052:    101 / 130 loss=2.737, nll_loss=0.787, ppl=1.73, wps=1404.9, ups=0.23, wpb=6114.5, bsz=287.6, num_updates=6720, lr=0, gnorm=1.22, clip=100, loss_scale=2, train_wall=87, gb_free=9.1, wall=32173
2023-05-20 23:03:25 | INFO | train_inner | epoch 052:    121 / 130 loss=2.72, nll_loss=0.768, ppl=1.7, wps=1441.3, ups=0.23, wpb=6204.4, bsz=307.3, num_updates=6740, lr=0, gnorm=1.495, clip=100, loss_scale=2, train_wall=86, gb_free=9.8, wall=32259
2023-05-20 23:03:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:03:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:04:33 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 998.5 | wpb 1392.5 | bsz 52.6 | num_updates 6749 | best_loss 3.138
2023-05-20 23:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 6749 updates
2023-05-20 23:04:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint52.pt
2023-05-20 23:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint52.pt
2023-05-20 23:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint52.pt (epoch 52 @ 6749 updates, score 3.184) (writing took 20.07160509377718 seconds)
2023-05-20 23:04:53 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2023-05-20 23:04:53 | INFO | train | epoch 052 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1301.6 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 6749 | lr 0 | gnorm 1.437 | clip 100 | loss_scale 2 | train_wall 548 | gb_free 10.1 | wall 32347
2023-05-20 23:04:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:04:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 23:04:53 | INFO | fairseq.trainer | begin training epoch 53
2023-05-20 23:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:05:41 | INFO | train_inner | epoch 053:     11 / 130 loss=2.727, nll_loss=0.776, ppl=1.71, wps=874.3, ups=0.15, wpb=5922.7, bsz=288.3, num_updates=6760, lr=0, gnorm=1.48, clip=100, loss_scale=2, train_wall=79, gb_free=9, wall=32395
2023-05-20 23:07:05 | INFO | train_inner | epoch 053:     31 / 130 loss=2.739, nll_loss=0.79, ppl=1.73, wps=1464, ups=0.24, wpb=6193.7, bsz=294.4, num_updates=6780, lr=0, gnorm=1.86, clip=100, loss_scale=2, train_wall=85, gb_free=9.6, wall=32479
2023-05-20 23:08:26 | INFO | train_inner | epoch 053:     51 / 130 loss=2.72, nll_loss=0.77, ppl=1.71, wps=1531.9, ups=0.25, wpb=6197.4, bsz=277.4, num_updates=6800, lr=0, gnorm=1.281, clip=100, loss_scale=2, train_wall=81, gb_free=8.8, wall=32560
2023-05-20 23:09:55 | INFO | train_inner | epoch 053:     71 / 130 loss=2.712, nll_loss=0.759, ppl=1.69, wps=1338, ups=0.23, wpb=5904.8, bsz=255.2, num_updates=6820, lr=0, gnorm=1.483, clip=100, loss_scale=2, train_wall=88, gb_free=9.2, wall=32649
2023-05-20 23:11:18 | INFO | train_inner | epoch 053:     91 / 130 loss=2.718, nll_loss=0.768, ppl=1.7, wps=1485.8, ups=0.24, wpb=6216, bsz=296.5, num_updates=6840, lr=0, gnorm=1.38, clip=100, loss_scale=2, train_wall=84, gb_free=9.1, wall=32732
2023-05-20 23:12:43 | INFO | train_inner | epoch 053:    111 / 130 loss=2.715, nll_loss=0.762, ppl=1.7, wps=1395.1, ups=0.23, wpb=5948, bsz=277.2, num_updates=6860, lr=0, gnorm=1.439, clip=100, loss_scale=2, train_wall=85, gb_free=9.5, wall=32818
2023-05-20 23:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:13:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:14:34 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 993.4 | wpb 1392.5 | bsz 52.6 | num_updates 6879 | best_loss 3.138
2023-05-20 23:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 6879 updates
2023-05-20 23:14:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint53.pt
2023-05-20 23:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint53.pt
2023-05-20 23:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint53.pt (epoch 53 @ 6879 updates, score 3.184) (writing took 20.705101117491722 seconds)
2023-05-20 23:14:54 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2023-05-20 23:14:54 | INFO | train | epoch 053 | loss 2.721 | nll_loss 0.77 | ppl 1.71 | wps 1309.4 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 6879 | lr 0 | gnorm 1.473 | clip 100 | loss_scale 2 | train_wall 544 | gb_free 10.1 | wall 32949
2023-05-20 23:14:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 23:14:55 | INFO | fairseq.trainer | begin training epoch 54
2023-05-20 23:14:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:14:59 | INFO | train_inner | epoch 054:      1 / 130 loss=2.718, nll_loss=0.766, ppl=1.7, wps=861.3, ups=0.15, wpb=5836.2, bsz=281.9, num_updates=6880, lr=0, gnorm=1.457, clip=100, loss_scale=2, train_wall=79, gb_free=9.7, wall=32953
2023-05-20 23:16:21 | INFO | train_inner | epoch 054:     21 / 130 loss=2.707, nll_loss=0.755, ppl=1.69, wps=1473.4, ups=0.24, wpb=6070.2, bsz=274.2, num_updates=6900, lr=0, gnorm=1.6, clip=100, loss_scale=2, train_wall=82, gb_free=9.9, wall=33035
2023-05-20 23:17:46 | INFO | train_inner | epoch 054:     41 / 130 loss=2.713, nll_loss=0.76, ppl=1.69, wps=1404.6, ups=0.24, wpb=5940.4, bsz=265.1, num_updates=6920, lr=0, gnorm=1.722, clip=100, loss_scale=2, train_wall=84, gb_free=9.7, wall=33120
2023-05-20 23:19:10 | INFO | train_inner | epoch 054:     61 / 130 loss=2.74, nll_loss=0.79, ppl=1.73, wps=1461.5, ups=0.24, wpb=6171.5, bsz=287.8, num_updates=6940, lr=0, gnorm=1.394, clip=100, loss_scale=2, train_wall=84, gb_free=7.7, wall=33205
2023-05-20 23:20:35 | INFO | train_inner | epoch 054:     81 / 130 loss=2.72, nll_loss=0.769, ppl=1.7, wps=1447, ups=0.24, wpb=6094.5, bsz=296.1, num_updates=6960, lr=0, gnorm=1.467, clip=100, loss_scale=2, train_wall=84, gb_free=9.9, wall=33289
2023-05-20 23:21:57 | INFO | train_inner | epoch 054:    101 / 130 loss=2.73, nll_loss=0.779, ppl=1.72, wps=1470.8, ups=0.24, wpb=6087.5, bsz=296.2, num_updates=6980, lr=0, gnorm=1.73, clip=100, loss_scale=2, train_wall=83, gb_free=9.5, wall=33372
2023-05-20 23:23:23 | INFO | train_inner | epoch 054:    121 / 130 loss=2.718, nll_loss=0.767, ppl=1.7, wps=1441.6, ups=0.23, wpb=6159.5, bsz=270.2, num_updates=7000, lr=0, gnorm=1.494, clip=100, loss_scale=2, train_wall=85, gb_free=9.7, wall=33457
2023-05-20 23:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:23:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:24:32 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 996.3 | wpb 1392.5 | bsz 52.6 | num_updates 7009 | best_loss 3.138
2023-05-20 23:24:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 7009 updates
2023-05-20 23:24:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint54.pt
2023-05-20 23:24:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint54.pt
2023-05-20 23:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint54.pt (epoch 54 @ 7009 updates, score 3.184) (writing took 20.929449513554573 seconds)
2023-05-20 23:24:52 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2023-05-20 23:24:52 | INFO | train | epoch 054 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1316.6 | ups 0.22 | wpb 6056.2 | bsz 280.9 | num_updates 7009 | lr 0 | gnorm 1.552 | clip 100 | loss_scale 2 | train_wall 540 | gb_free 10.1 | wall 33547
2023-05-20 23:24:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:24:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 23:24:52 | INFO | fairseq.trainer | begin training epoch 55
2023-05-20 23:24:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:25:43 | INFO | train_inner | epoch 055:     11 / 130 loss=2.727, nll_loss=0.775, ppl=1.71, wps=837.9, ups=0.14, wpb=5850.7, bsz=262.5, num_updates=7020, lr=0, gnorm=1.311, clip=100, loss_scale=2, train_wall=83, gb_free=9.2, wall=33597
2023-05-20 23:27:11 | INFO | train_inner | epoch 055:     31 / 130 loss=2.727, nll_loss=0.776, ppl=1.71, wps=1401.4, ups=0.23, wpb=6167.6, bsz=285.8, num_updates=7040, lr=0, gnorm=1.423, clip=100, loss_scale=2, train_wall=88, gb_free=9.2, wall=33685
2023-05-20 23:28:34 | INFO | train_inner | epoch 055:     51 / 130 loss=2.708, nll_loss=0.756, ppl=1.69, wps=1480.2, ups=0.24, wpb=6173.3, bsz=293.7, num_updates=7060, lr=0, gnorm=1.21, clip=100, loss_scale=2, train_wall=83, gb_free=9.3, wall=33768
2023-05-20 23:30:00 | INFO | train_inner | epoch 055:     71 / 130 loss=2.716, nll_loss=0.764, ppl=1.7, wps=1400.5, ups=0.23, wpb=6000.3, bsz=271.1, num_updates=7080, lr=0, gnorm=1.258, clip=100, loss_scale=2, train_wall=86, gb_free=10.1, wall=33854
2023-05-20 23:31:24 | INFO | train_inner | epoch 055:     91 / 130 loss=2.737, nll_loss=0.789, ppl=1.73, wps=1444.8, ups=0.24, wpb=6115.2, bsz=293.9, num_updates=7100, lr=0, gnorm=1.429, clip=100, loss_scale=2, train_wall=85, gb_free=9.9, wall=33938
2023-05-20 23:32:48 | INFO | train_inner | epoch 055:    111 / 130 loss=2.714, nll_loss=0.762, ppl=1.7, wps=1447.1, ups=0.24, wpb=6037.5, bsz=280.9, num_updates=7120, lr=0, gnorm=1.527, clip=100, loss_scale=4, train_wall=83, gb_free=9.3, wall=34022
2023-05-20 23:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:34:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:34:37 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 979.9 | wpb 1392.5 | bsz 52.6 | num_updates 7139 | best_loss 3.138
2023-05-20 23:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 7139 updates
2023-05-20 23:34:37 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint55.pt
2023-05-20 23:34:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint55.pt
2023-05-20 23:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint55.pt (epoch 55 @ 7139 updates, score 3.184) (writing took 21.16300481930375 seconds)
2023-05-20 23:34:58 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2023-05-20 23:34:58 | INFO | train | epoch 055 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1299.3 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 7139 | lr 0 | gnorm 1.37 | clip 100 | loss_scale 4 | train_wall 547 | gb_free 10.1 | wall 34152
2023-05-20 23:34:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:34:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 23:34:58 | INFO | fairseq.trainer | begin training epoch 56
2023-05-20 23:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:35:03 | INFO | train_inner | epoch 056:      1 / 130 loss=2.732, nll_loss=0.782, ppl=1.72, wps=872, ups=0.15, wpb=5887.2, bsz=273.9, num_updates=7140, lr=0, gnorm=1.431, clip=100, loss_scale=4, train_wall=77, gb_free=9.4, wall=34157
2023-05-20 23:36:29 | INFO | train_inner | epoch 056:     21 / 130 loss=2.724, nll_loss=0.772, ppl=1.71, wps=1429.8, ups=0.23, wpb=6187.4, bsz=284.7, num_updates=7160, lr=0, gnorm=1.338, clip=100, loss_scale=4, train_wall=86, gb_free=8.9, wall=34243
2023-05-20 23:37:56 | INFO | train_inner | epoch 056:     41 / 130 loss=2.698, nll_loss=0.743, ppl=1.67, wps=1382.7, ups=0.23, wpb=5980.2, bsz=268.7, num_updates=7180, lr=0, gnorm=1.278, clip=100, loss_scale=4, train_wall=86, gb_free=9.4, wall=34330
2023-05-20 23:39:23 | INFO | train_inner | epoch 056:     61 / 130 loss=2.736, nll_loss=0.786, ppl=1.72, wps=1409.4, ups=0.23, wpb=6170.8, bsz=304.4, num_updates=7200, lr=0, gnorm=1.445, clip=100, loss_scale=4, train_wall=87, gb_free=9.1, wall=34418
2023-05-20 23:40:45 | INFO | train_inner | epoch 056:     81 / 130 loss=2.736, nll_loss=0.787, ppl=1.73, wps=1512.3, ups=0.25, wpb=6143.3, bsz=288.1, num_updates=7220, lr=0, gnorm=1.369, clip=100, loss_scale=4, train_wall=81, gb_free=9.2, wall=34499
2023-05-20 23:42:11 | INFO | train_inner | epoch 056:    101 / 130 loss=2.733, nll_loss=0.784, ppl=1.72, wps=1424.4, ups=0.23, wpb=6182.4, bsz=295, num_updates=7240, lr=0, gnorm=1.348, clip=100, loss_scale=4, train_wall=87, gb_free=8.7, wall=34586
2023-05-20 23:43:37 | INFO | train_inner | epoch 056:    121 / 130 loss=2.713, nll_loss=0.761, ppl=1.69, wps=1409, ups=0.24, wpb=5991, bsz=259.7, num_updates=7260, lr=0, gnorm=1.263, clip=100, loss_scale=4, train_wall=85, gb_free=9.2, wall=34671
2023-05-20 23:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 23:44:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:44:49 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.184 | nll_loss 1.251 | ppl 2.38 | bleu 49.3 | wps 985.1 | wpb 1392.5 | bsz 52.6 | num_updates 7269 | best_loss 3.138
2023-05-20 23:44:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 7269 updates
2023-05-20 23:44:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint56.pt
2023-05-20 23:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage3_new/checkpoint56.pt
2023-05-20 23:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage3_new/checkpoint56.pt (epoch 56 @ 7269 updates, score 3.184) (writing took 31.352466106414795 seconds)
2023-05-20 23:45:20 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2023-05-20 23:45:20 | INFO | train | epoch 056 | loss 2.722 | nll_loss 0.771 | ppl 1.71 | wps 1266.1 | ups 0.21 | wpb 6056.2 | bsz 280.9 | num_updates 7269 | lr 0 | gnorm 1.406 | clip 100 | loss_scale 4 | train_wall 554 | gb_free 10.1 | wall 34774
2023-05-20 23:45:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 23:45:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130
2023-05-20 23:45:20 | INFO | fairseq.trainer | begin training epoch 57
2023-05-20 23:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 23:46:07 | INFO | train_inner | epoch 057:     11 / 130 loss=2.734, nll_loss=0.785, ppl=1.72, wps=776.1, ups=0.13, wpb=5854.9, bsz=284.3, num_updates=7280, lr=0, gnorm=1.802, clip=100, loss_scale=4, train_wall=83, gb_free=9.8, wall=34821
2023-05-20 23:47:32 | INFO | train_inner | epoch 057:     31 / 130 loss=2.733, nll_loss=0.785, ppl=1.72, wps=1439.2, ups=0.24, wpb=6106.8, bsz=290.2, num_updates=7300, lr=0, gnorm=1.714, clip=100, loss_scale=4, train_wall=85, gb_free=9.6, wall=34906
slurmstepd: error: *** JOB 116924 ON 100server CANCELLED AT 2023-05-20T23:48:53 ***
2023-05-20 23:48:55 | INFO | train_inner | epoch 057:     51 / 130 loss=2.715, nll_loss=0.763, ppl=1.7, wps=1484.4, ups=0.24, wpb=6175.6, bsz=298.8, num_updates=7320, lr=0, gnorm=1.496, clip=100, loss_scale=4, train_wall=83, gb_free=9.3, wall=34990
