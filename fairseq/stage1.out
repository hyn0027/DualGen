2023-05-19 16:36:30 | INFO | fairseq.distributed.utils | Rank 0, device_id: 0
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-05-19 16:36:32 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:42880
2023-05-19 16:36:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 7
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 2
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 4
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 6
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 3
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 5
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 1
2023-05-19 16:36:33 | INFO | fairseq.distributed.utils | initialized host 99server as rank 0
2023-05-19 16:36:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 5, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:42880', 'distributed_port': 42880, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [4], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'training/stage1', 'restore_file': '/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='bartDualEnc_large', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=1, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=5, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='training/stage1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='10000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=200, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': Namespace(_name='graph_to_seq', activation_fn='gelu', adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='bartDualEnc_large', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.1, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/home/hongyining/s_link/dualEnc_virtual/AMR2.0bin', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', debug_param_names=False, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=12, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze=1, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=True, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=5, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=1, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt', save_dir='training/stage1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='graph_to_seq', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, total_num_update='10000', tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[4], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=200, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 200, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 10000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-05-19 16:36:39 | INFO | fairseq.tasks.graph_to_seq | adding dictionary: 50264 types
2023-05-19 16:36:49 | INFO | fairseq_cli.train | BARTDualEncModel(
  (encoder): GraphToTextEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (graph_layers): ModuleList(
      (0): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (1): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (2): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (3): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (4): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (5): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (6): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (7): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (8): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (9): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (10): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
      (11): TransformerGraphEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (structure_w_v): Linear(in_features=64, out_features=1024, bias=True)
        (structure_w_k): Linear(in_features=64, out_features=1024, bias=True)
      )
    )
    (graph_embeddings): Linear(in_features=1024, out_features=64, bias=True)
    (graph_embeddings_inverse): Linear(in_features=1024, out_features=64, bias=True)
    (gamma_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(50264, 1024, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)
    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=1024, out_features=4096, bias=True)
        (fc2): Linear(in_features=4096, out_features=1024, bias=True)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=1024, out_features=50264, bias=False)
  )
  (classification_heads): ModuleDict()
)
2023-05-19 16:36:49 | INFO | fairseq_cli.train | task: GraphToSeq
2023-05-19 16:36:49 | INFO | fairseq_cli.train | model: BARTDualEncModel
2023-05-19 16:36:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2023-05-19 16:36:49 | INFO | fairseq_cli.train | num. shared model params: 559,173,888 (num. trained: 1,728,768)
2023-05-19 16:36:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.source
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.source-target.target
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.info-None.info
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge-None.edge
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node-None.node
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.edge.info-None.edge.info
2023-05-19 16:36:49 | INFO | fairseq.data.data_utils | loaded 1,368 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/valid.node.info-None.node.info
2023-05-19 16:36:49 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin valid 1368 examples
2023-05-19 16:37:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-05-19 16:37:14 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-05-19 16:37:14 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-05-19 16:37:14 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-05-19 16:37:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 23.653 GB ; name = TITAN RTX                               
2023-05-19 16:37:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-05-19 16:37:14 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-05-19 16:37:14 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2023-05-19 16:37:14 | INFO | fairseq.trainer | Preparing to load checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt
2023-05-19 16:38:01 | INFO | fairseq.trainer | Loaded checkpoint /home/hongyining/s_link/dualEnc_virtual/bart.large/model.pt (epoch 41 @ 0 updates)
2023-05-19 16:38:01 | INFO | fairseq.trainer | loading train data for epoch 1
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.source
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.source-target.target
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.info-None.info
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge-None.edge
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node-None.node
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.edge.info-None.edge.info
2023-05-19 16:38:01 | INFO | fairseq.data.data_utils | loaded 36,521 examples from: /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin/train.node.info-None.node.info
2023-05-19 16:38:01 | INFO | fairseq.tasks.graph_to_seq | /home/hongyining/s_link/dualEnc_virtual/AMR2.0bin train 36521 examples
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-19 16:38:01 | INFO | fairseq_cli.train | begin dry-run validation on "valid" subset
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False
2023-05-19 16:38:01 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1
2023-05-19 16:38:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 16:38:05 | INFO | fairseq.trainer | begin training epoch 1
2023-05-19 16:38:05 | INFO | fairseq_cli.train | Start iterating over samples
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1258] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2023-05-19 16:38:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-05-19 16:38:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-05-19 16:38:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-05-19 16:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-05-19 16:38:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-05-19 16:38:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-05-19 16:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-05-19 16:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-05-19 16:38:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-19 16:39:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-19 16:39:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-05-19 16:39:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2023-05-19 16:39:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2023-05-19 16:39:28 | INFO | train_inner | epoch 001:     18 / 65 loss=15.501, nll_loss=14.405, ppl=21688, wps=2000.4, ups=0.17, wpb=11882.8, bsz=550.4, num_updates=5, lr=1.25e-06, gnorm=221.18, clip=100, loss_scale=0.0156, train_wall=80, gb_free=21.7, wall=134
2023-05-19 16:39:44 | INFO | train_inner | epoch 001:     23 / 65 loss=15.473, nll_loss=14.371, ppl=21194.9, wps=3703.3, ups=0.3, wpb=12149.4, bsz=544, num_updates=10, lr=2.5e-06, gnorm=431.185, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=150
2023-05-19 16:40:01 | INFO | train_inner | epoch 001:     28 / 65 loss=15.435, nll_loss=14.323, ppl=20498.8, wps=3852.1, ups=0.31, wpb=12589.6, bsz=568.6, num_updates=15, lr=3.75e-06, gnorm=413.126, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.4, wall=166
2023-05-19 16:40:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2023-05-19 16:40:21 | INFO | train_inner | epoch 001:     34 / 65 loss=15.455, nll_loss=14.334, ppl=20647, wps=2965.7, ups=0.24, wpb=12128, bsz=605.8, num_updates=20, lr=5e-06, gnorm=557.284, clip=100, loss_scale=0.0078, train_wall=20, gb_free=21.4, wall=187
2023-05-19 16:40:38 | INFO | train_inner | epoch 001:     39 / 65 loss=15.332, nll_loss=14.217, ppl=19043.7, wps=3578.3, ups=0.3, wpb=12113, bsz=548.2, num_updates=25, lr=6.25e-06, gnorm=317.223, clip=100, loss_scale=0.0078, train_wall=17, gb_free=20.5, wall=204
2023-05-19 16:40:53 | INFO | train_inner | epoch 001:     44 / 65 loss=15.695, nll_loss=14.588, ppl=24625.1, wps=4079.1, ups=0.33, wpb=12470.4, bsz=650.2, num_updates=30, lr=7.5e-06, gnorm=1589.19, clip=100, loss_scale=0.0078, train_wall=15, gb_free=20.5, wall=219
2023-05-19 16:41:10 | INFO | train_inner | epoch 001:     49 / 65 loss=15.596, nll_loss=14.491, ppl=23027.1, wps=3741.1, ups=0.31, wpb=12225.6, bsz=597.2, num_updates=35, lr=8.75e-06, gnorm=438.623, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=235
2023-05-19 16:41:25 | INFO | train_inner | epoch 001:     54 / 65 loss=15.531, nll_loss=14.424, ppl=21974.5, wps=3951.7, ups=0.32, wpb=12424.2, bsz=617, num_updates=40, lr=1e-05, gnorm=301.302, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=251
2023-05-19 16:41:41 | INFO | train_inner | epoch 001:     59 / 65 loss=15.206, nll_loss=14.097, ppl=17524.5, wps=3923.8, ups=0.32, wpb=12159.6, bsz=569.4, num_updates=45, lr=1.125e-05, gnorm=286.752, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=267
2023-05-19 16:41:57 | INFO | train_inner | epoch 001:     64 / 65 loss=15.068, nll_loss=13.951, ppl=15841, wps=3617.9, ups=0.3, wpb=11928.4, bsz=503.4, num_updates=50, lr=1.25e-05, gnorm=411.063, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=283
2023-05-19 16:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 16:41:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:42:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.653 | nll_loss 12.545 | ppl 5978.01 | bleu 0.01 | wps 863.4 | wpb 2785 | bsz 105.2 | num_updates 51
2023-05-19 16:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 51 updates
2023-05-19 16:42:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint1.pt
2023-05-19 16:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint1.pt
2023-05-19 16:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint1.pt (epoch 1 @ 51 updates, score 13.653) (writing took 12.113759458065033 seconds)
2023-05-19 16:42:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-05-19 16:42:53 | INFO | train | epoch 001 | loss 15.437 | nll_loss 14.328 | ppl 20563.5 | wps 2628.5 | ups 0.22 | wpb 12045.5 | bsz 568.5 | num_updates 51 | lr 1.275e-05 | gnorm 540.175 | clip 100 | loss_scale 0.0078 | train_wall 231 | gb_free 21.4 | wall 339
2023-05-19 16:42:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:42:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 16:42:53 | INFO | fairseq.trainer | begin training epoch 2
2023-05-19 16:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 16:43:06 | INFO | train_inner | epoch 002:      4 / 65 loss=15.339, nll_loss=14.224, ppl=19134.4, wps=774.7, ups=0.07, wpb=10619.2, bsz=516.8, num_updates=55, lr=1.375e-05, gnorm=729.568, clip=100, loss_scale=0.0078, train_wall=14, gb_free=21.5, wall=352
2023-05-19 16:43:25 | INFO | train_inner | epoch 002:      9 / 65 loss=15.286, nll_loss=14.179, ppl=18546.8, wps=3194.1, ups=0.26, wpb=12359.8, bsz=565, num_updates=60, lr=1.5e-05, gnorm=570.912, clip=100, loss_scale=0.0078, train_wall=19, gb_free=21.7, wall=371
2023-05-19 16:43:41 | INFO | train_inner | epoch 002:     14 / 65 loss=15.156, nll_loss=14.038, ppl=16817.1, wps=4125.9, ups=0.34, wpb=12314, bsz=602.8, num_updates=65, lr=1.625e-05, gnorm=258.478, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.8, wall=387
2023-05-19 16:43:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
2023-05-19 16:44:00 | INFO | train_inner | epoch 002:     20 / 65 loss=15.112, nll_loss=13.995, ppl=16331.6, wps=3255.1, ups=0.27, wpb=12218.8, bsz=545.4, num_updates=70, lr=1.75e-05, gnorm=1134.9, clip=100, loss_scale=0.0039, train_wall=19, gb_free=21.7, wall=406
2023-05-19 16:44:15 | INFO | train_inner | epoch 002:     25 / 65 loss=15.04, nll_loss=13.914, ppl=15436.2, wps=3918, ups=0.33, wpb=12047, bsz=567.6, num_updates=75, lr=1.875e-05, gnorm=291.473, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.8, wall=421
2023-05-19 16:44:32 | INFO | train_inner | epoch 002:     30 / 65 loss=14.815, nll_loss=13.685, ppl=13172.5, wps=3401.1, ups=0.29, wpb=11714.2, bsz=523.4, num_updates=80, lr=2e-05, gnorm=206.996, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.5, wall=438
2023-05-19 16:44:49 | INFO | train_inner | epoch 002:     35 / 65 loss=14.722, nll_loss=13.599, ppl=12407.3, wps=3687.2, ups=0.3, wpb=12390, bsz=538.2, num_updates=85, lr=2.125e-05, gnorm=233.582, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.3, wall=455
2023-05-19 16:45:05 | INFO | train_inner | epoch 002:     40 / 65 loss=15.086, nll_loss=13.965, ppl=15988, wps=4015.9, ups=0.33, wpb=12274.2, bsz=639.2, num_updates=90, lr=2.25e-05, gnorm=232.371, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.7, wall=470
2023-05-19 16:45:20 | INFO | train_inner | epoch 002:     45 / 65 loss=14.521, nll_loss=13.382, ppl=10673.7, wps=3856.8, ups=0.32, wpb=12030.8, bsz=533.6, num_updates=95, lr=2.375e-05, gnorm=220.678, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=486
2023-05-19 16:45:36 | INFO | train_inner | epoch 002:     50 / 65 loss=14.702, nll_loss=13.574, ppl=12193.9, wps=3805.5, ups=0.32, wpb=11905.6, bsz=596.6, num_updates=100, lr=2.5e-05, gnorm=264.201, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=502
2023-05-19 16:45:51 | INFO | train_inner | epoch 002:     55 / 65 loss=14.129, nll_loss=12.984, ppl=8104.42, wps=4220.8, ups=0.34, wpb=12544.6, bsz=518.2, num_updates=105, lr=2.625e-05, gnorm=1239.66, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.6, wall=516
2023-05-19 16:46:07 | INFO | train_inner | epoch 002:     60 / 65 loss=14.462, nll_loss=13.326, ppl=10271.1, wps=3736.8, ups=0.31, wpb=12146, bsz=598, num_updates=110, lr=2.75e-05, gnorm=234.436, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=533
2023-05-19 16:46:21 | INFO | train_inner | epoch 002:     65 / 65 loss=13.98, nll_loss=12.824, ppl=7252.14, wps=3792.4, ups=0.37, wpb=10353, bsz=459, num_updates=115, lr=2.875e-05, gnorm=443.892, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.7, wall=546
2023-05-19 16:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 16:46:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:47:04 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.332 | nll_loss 11.161 | ppl 2290.37 | bleu 0.01 | wps 837.2 | wpb 2785 | bsz 105.2 | num_updates 115 | best_loss 12.332
2023-05-19 16:47:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 115 updates
2023-05-19 16:47:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint2.pt
2023-05-19 16:47:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint2.pt
2023-05-19 16:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint2.pt (epoch 2 @ 115 updates, score 12.332) (writing took 9.701291359961033 seconds)
2023-05-19 16:47:13 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-05-19 16:47:13 | INFO | train | epoch 002 | loss 14.792 | nll_loss 13.664 | ppl 12982.2 | wps 2963.2 | ups 0.25 | wpb 12041 | bsz 559.3 | num_updates 115 | lr 2.875e-05 | gnorm 431.117 | clip 100 | loss_scale 0.0039 | train_wall 206 | gb_free 21.7 | wall 599
2023-05-19 16:47:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:47:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 16:47:13 | INFO | fairseq.trainer | begin training epoch 3
2023-05-19 16:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 16:47:31 | INFO | train_inner | epoch 003:      5 / 65 loss=13.858, nll_loss=12.696, ppl=6633.63, wps=887.2, ups=0.07, wpb=12434, bsz=562.4, num_updates=120, lr=3e-05, gnorm=229.811, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.5, wall=616
2023-05-19 16:47:49 | INFO | train_inner | epoch 003:     10 / 65 loss=13.883, nll_loss=12.714, ppl=6720.83, wps=3438.6, ups=0.28, wpb=12350.6, bsz=586.8, num_updates=125, lr=3.125e-05, gnorm=270.693, clip=100, loss_scale=0.0039, train_wall=18, gb_free=21.6, wall=634
2023-05-19 16:48:05 | INFO | train_inner | epoch 003:     15 / 65 loss=13.358, nll_loss=12.176, ppl=4628.7, wps=3633.2, ups=0.31, wpb=11912, bsz=520, num_updates=130, lr=3.25e-05, gnorm=73.579, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=651
2023-05-19 16:48:23 | INFO | train_inner | epoch 003:     20 / 65 loss=13.435, nll_loss=12.247, ppl=4862.5, wps=3391.5, ups=0.28, wpb=12096, bsz=594.8, num_updates=135, lr=3.375e-05, gnorm=89.117, clip=100, loss_scale=0.0039, train_wall=18, gb_free=20.6, wall=669
2023-05-19 16:48:39 | INFO | train_inner | epoch 003:     25 / 65 loss=13.082, nll_loss=11.887, ppl=3788.19, wps=3633.2, ups=0.3, wpb=12064.8, bsz=560.8, num_updates=140, lr=3.5e-05, gnorm=233.098, clip=100, loss_scale=0.0039, train_wall=17, gb_free=20.5, wall=685
2023-05-19 16:48:56 | INFO | train_inner | epoch 003:     30 / 65 loss=12.81, nll_loss=11.605, ppl=3115.65, wps=3676, ups=0.31, wpb=12043, bsz=564, num_updates=145, lr=3.625e-05, gnorm=353.979, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=702
2023-05-19 16:49:11 | INFO | train_inner | epoch 003:     35 / 65 loss=12.632, nll_loss=11.416, ppl=2732.46, wps=3977.7, ups=0.33, wpb=12093.8, bsz=554.6, num_updates=150, lr=3.75e-05, gnorm=135.227, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=717
2023-05-19 16:49:29 | INFO | train_inner | epoch 003:     40 / 65 loss=12.351, nll_loss=11.125, ppl=2233.9, wps=3250.9, ups=0.27, wpb=11961.2, bsz=504.2, num_updates=155, lr=3.875e-05, gnorm=121.245, clip=100, loss_scale=0.0039, train_wall=18, gb_free=21, wall=735
2023-05-19 16:49:45 | INFO | train_inner | epoch 003:     45 / 65 loss=12.17, nll_loss=10.93, ppl=1951.53, wps=3885.2, ups=0.31, wpb=12396.4, bsz=617.8, num_updates=160, lr=4e-05, gnorm=191.82, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.4, wall=751
2023-05-19 16:50:01 | INFO | train_inner | epoch 003:     50 / 65 loss=12.018, nll_loss=10.764, ppl=1739, wps=4106.2, ups=0.32, wpb=12954.4, bsz=650.8, num_updates=165, lr=4.125e-05, gnorm=220.746, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.2, wall=767
2023-05-19 16:50:17 | INFO | train_inner | epoch 003:     55 / 65 loss=11.647, nll_loss=10.4, ppl=1351.05, wps=3876.2, ups=0.33, wpb=11888.6, bsz=533.6, num_updates=170, lr=4.25e-05, gnorm=304.81, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.8, wall=782
2023-05-19 16:50:33 | INFO | train_inner | epoch 003:     60 / 65 loss=11.231, nll_loss=9.969, ppl=1002.28, wps=3574.3, ups=0.3, wpb=12019.8, bsz=469.6, num_updates=175, lr=4.375e-05, gnorm=32.613, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.7, wall=799
2023-05-19 16:50:48 | INFO | train_inner | epoch 003:     65 / 65 loss=11.473, nll_loss=10.215, ppl=1188.89, wps=3677.1, ups=0.35, wpb=10529.2, bsz=547.4, num_updates=180, lr=4.5e-05, gnorm=116.657, clip=100, loss_scale=0.0039, train_wall=14, gb_free=20.5, wall=813
2023-05-19 16:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 16:50:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:51:33 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.707 | nll_loss 8.393 | ppl 336.23 | bleu 0.01 | wps 793 | wpb 2785 | bsz 105.2 | num_updates 180 | best_loss 9.707
2023-05-19 16:51:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 180 updates
2023-05-19 16:51:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint3.pt
2023-05-19 16:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint3.pt
2023-05-19 16:51:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint3.pt (epoch 3 @ 180 updates, score 9.707) (writing took 8.087744500488043 seconds)
2023-05-19 16:51:41 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-05-19 16:51:41 | INFO | train | epoch 003 | loss 12.624 | nll_loss 11.409 | ppl 2720.1 | wps 2930.2 | ups 0.24 | wpb 12057.2 | bsz 559 | num_updates 180 | lr 4.5e-05 | gnorm 182.569 | clip 100 | loss_scale 0.0039 | train_wall 214 | gb_free 20.5 | wall 867
2023-05-19 16:51:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:51:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 16:51:41 | INFO | fairseq.trainer | begin training epoch 4
2023-05-19 16:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 16:51:57 | INFO | train_inner | epoch 004:      5 / 65 loss=11.015, nll_loss=9.761, ppl=867.7, wps=885.1, ups=0.07, wpb=12260.6, bsz=563.2, num_updates=185, lr=4.625e-05, gnorm=93.691, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=883
2023-05-19 16:52:13 | INFO | train_inner | epoch 004:     10 / 65 loss=10.901, nll_loss=9.649, ppl=803.05, wps=3607, ups=0.3, wpb=11858, bsz=563.2, num_updates=190, lr=4.75e-05, gnorm=33.261, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.3, wall=899
2023-05-19 16:52:30 | INFO | train_inner | epoch 004:     15 / 65 loss=10.471, nll_loss=9.198, ppl=587.33, wps=3848.3, ups=0.32, wpb=12071, bsz=478.6, num_updates=195, lr=4.875e-05, gnorm=16.411, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=915
2023-05-19 16:52:46 | INFO | train_inner | epoch 004:     20 / 65 loss=10.441, nll_loss=9.151, ppl=568.48, wps=3662.6, ups=0.31, wpb=11944.2, bsz=525, num_updates=200, lr=5e-05, gnorm=46.516, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=932
2023-05-19 16:53:03 | INFO | train_inner | epoch 004:     25 / 65 loss=10.464, nll_loss=9.151, ppl=568.35, wps=3850.8, ups=0.3, wpb=12981.6, bsz=672.4, num_updates=205, lr=4.99745e-05, gnorm=62.914, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=948
2023-05-19 16:53:19 | INFO | train_inner | epoch 004:     30 / 65 loss=10.283, nll_loss=8.967, ppl=500.52, wps=3640.5, ups=0.3, wpb=11968.2, bsz=565, num_updates=210, lr=4.9949e-05, gnorm=202.504, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=965
2023-05-19 16:53:35 | INFO | train_inner | epoch 004:     35 / 65 loss=9.807, nll_loss=8.458, ppl=351.57, wps=3728.6, ups=0.31, wpb=12150.2, bsz=506.2, num_updates=215, lr=4.99235e-05, gnorm=10.936, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=981
2023-05-19 16:53:53 | INFO | train_inner | epoch 004:     40 / 65 loss=9.59, nll_loss=8.236, ppl=301.41, wps=3413, ups=0.29, wpb=11963.4, bsz=495.8, num_updates=220, lr=4.9898e-05, gnorm=18.112, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=999
2023-05-19 16:54:09 | INFO | train_inner | epoch 004:     45 / 65 loss=9.563, nll_loss=8.198, ppl=293.56, wps=3937.3, ups=0.32, wpb=12297.6, bsz=549.6, num_updates=225, lr=4.98724e-05, gnorm=16.508, clip=100, loss_scale=0.0039, train_wall=16, gb_free=20.6, wall=1014
2023-05-19 16:54:27 | INFO | train_inner | epoch 004:     50 / 65 loss=9.557, nll_loss=8.184, ppl=290.8, wps=3347.6, ups=0.28, wpb=12028.2, bsz=561.8, num_updates=230, lr=4.98469e-05, gnorm=171.119, clip=100, loss_scale=0.0039, train_wall=18, gb_free=21.5, wall=1032
2023-05-19 16:54:42 | INFO | train_inner | epoch 004:     55 / 65 loss=9.879, nll_loss=8.509, ppl=364.3, wps=4236.6, ups=0.33, wpb=12719.4, bsz=715, num_updates=235, lr=4.98214e-05, gnorm=18.834, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.2, wall=1047
2023-05-19 16:54:56 | INFO | train_inner | epoch 004:     60 / 65 loss=9.332, nll_loss=7.944, ppl=246.22, wps=4115.1, ups=0.34, wpb=11990, bsz=547.2, num_updates=240, lr=4.97959e-05, gnorm=21.304, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.8, wall=1062
2023-05-19 16:55:10 | INFO | train_inner | epoch 004:     65 / 65 loss=9.538, nll_loss=8.147, ppl=283.45, wps=3780.7, ups=0.36, wpb=10403.2, bsz=522.4, num_updates=245, lr=4.97704e-05, gnorm=34.541, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.6, wall=1076
2023-05-19 16:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 16:55:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:55:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.745 | nll_loss 7.274 | ppl 154.72 | bleu 0.03 | wps 869.2 | wpb 2785 | bsz 105.2 | num_updates 245 | best_loss 8.745
2023-05-19 16:55:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 245 updates
2023-05-19 16:55:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint4.pt
2023-05-19 16:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint4.pt
2023-05-19 16:56:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint4.pt (epoch 4 @ 245 updates, score 8.745) (writing took 14.049235504120588 seconds)
2023-05-19 16:56:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-05-19 16:56:05 | INFO | train | epoch 004 | loss 10.071 | nll_loss 8.742 | ppl 428.15 | wps 2960.5 | ups 0.25 | wpb 12048.9 | bsz 558.9 | num_updates 245 | lr 4.97704e-05 | gnorm 57.435 | clip 100 | loss_scale 0.0039 | train_wall 208 | gb_free 21.6 | wall 1131
2023-05-19 16:56:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 16:56:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 16:56:06 | INFO | fairseq.trainer | begin training epoch 5
2023-05-19 16:56:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 16:56:21 | INFO | train_inner | epoch 005:      5 / 65 loss=9.177, nll_loss=7.764, ppl=217.3, wps=871.8, ups=0.07, wpb=12321.6, bsz=556.2, num_updates=250, lr=4.97449e-05, gnorm=8.029, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.7, wall=1146
2023-05-19 16:56:38 | INFO | train_inner | epoch 005:     10 / 65 loss=9.133, nll_loss=7.709, ppl=209.3, wps=3426.6, ups=0.28, wpb=12031.4, bsz=528.6, num_updates=255, lr=4.97194e-05, gnorm=10.499, clip=100, loss_scale=0.0039, train_wall=18, gb_free=21.3, wall=1164
2023-05-19 16:56:54 | INFO | train_inner | epoch 005:     15 / 65 loss=9.275, nll_loss=7.846, ppl=230, wps=3792.1, ups=0.32, wpb=11931, bsz=558.6, num_updates=260, lr=4.96939e-05, gnorm=7.415, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=1180
2023-05-19 16:57:12 | INFO | train_inner | epoch 005:     20 / 65 loss=9.205, nll_loss=7.773, ppl=218.77, wps=3575.4, ups=0.29, wpb=12339.8, bsz=591.2, num_updates=265, lr=4.96684e-05, gnorm=39.017, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.8, wall=1197
2023-05-19 16:57:28 | INFO | train_inner | epoch 005:     25 / 65 loss=8.987, nll_loss=7.546, ppl=186.94, wps=3621.6, ups=0.3, wpb=12074.8, bsz=562, num_updates=270, lr=4.96429e-05, gnorm=9.041, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.1, wall=1214
2023-05-19 16:57:45 | INFO | train_inner | epoch 005:     30 / 65 loss=8.836, nll_loss=7.377, ppl=166.24, wps=3538.7, ups=0.3, wpb=11674.2, bsz=468.2, num_updates=275, lr=4.96173e-05, gnorm=18.722, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=1231
2023-05-19 16:58:01 | INFO | train_inner | epoch 005:     35 / 65 loss=9.212, nll_loss=7.766, ppl=217.63, wps=3968.6, ups=0.31, wpb=12622.4, bsz=653.8, num_updates=280, lr=4.95918e-05, gnorm=23.474, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=1246
2023-05-19 16:58:17 | INFO | train_inner | epoch 005:     40 / 65 loss=9.051, nll_loss=7.596, ppl=193.41, wps=3922.5, ups=0.31, wpb=12763.6, bsz=607.4, num_updates=285, lr=4.95663e-05, gnorm=6.858, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21, wall=1263
2023-05-19 16:58:33 | INFO | train_inner | epoch 005:     45 / 65 loss=8.928, nll_loss=7.479, ppl=178.45, wps=3758.2, ups=0.31, wpb=12043, bsz=507, num_updates=290, lr=4.95408e-05, gnorm=8.483, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.4, wall=1279
2023-05-19 16:58:49 | INFO | train_inner | epoch 005:     50 / 65 loss=9.175, nll_loss=7.741, ppl=213.93, wps=3896, ups=0.32, wpb=12083.4, bsz=632.6, num_updates=295, lr=4.95153e-05, gnorm=37.522, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.4, wall=1294
2023-05-19 16:59:04 | INFO | train_inner | epoch 005:     55 / 65 loss=8.955, nll_loss=7.492, ppl=179.97, wps=3945.1, ups=0.31, wpb=12526.4, bsz=602.2, num_updates=300, lr=4.94898e-05, gnorm=20.986, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.8, wall=1310
2023-05-19 16:59:21 | INFO | train_inner | epoch 005:     60 / 65 loss=8.841, nll_loss=7.375, ppl=166, wps=3525.5, ups=0.3, wpb=11943.2, bsz=524.4, num_updates=305, lr=4.94643e-05, gnorm=6.171, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=1327
2023-05-19 16:59:36 | INFO | train_inner | epoch 005:     65 / 65 loss=8.951, nll_loss=7.486, ppl=179.28, wps=3478, ups=0.34, wpb=10306.8, bsz=471.4, num_updates=310, lr=4.94388e-05, gnorm=42.373, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.6, wall=1342
2023-05-19 16:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 16:59:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:00:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.437 | nll_loss 6.912 | ppl 120.44 | bleu 0.06 | wps 790.1 | wpb 2785 | bsz 105.2 | num_updates 310 | best_loss 8.437
2023-05-19 17:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 310 updates
2023-05-19 17:00:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint5.pt
2023-05-19 17:00:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint5.pt
2023-05-19 17:00:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint5.pt (epoch 5 @ 310 updates, score 8.437) (writing took 9.641074478626251 seconds)
2023-05-19 17:00:31 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-05-19 17:00:31 | INFO | train | epoch 005 | loss 9.058 | nll_loss 7.614 | ppl 195.92 | wps 2950.6 | ups 0.24 | wpb 12050.9 | bsz 558.7 | num_updates 310 | lr 4.94388e-05 | gnorm 18.353 | clip 100 | loss_scale 0.0039 | train_wall 210 | gb_free 21.6 | wall 1397
2023-05-19 17:00:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:00:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:00:31 | INFO | fairseq.trainer | begin training epoch 6
2023-05-19 17:00:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:00:48 | INFO | train_inner | epoch 006:      5 / 65 loss=8.792, nll_loss=7.325, ppl=160.35, wps=849, ups=0.07, wpb=12115.2, bsz=542.4, num_updates=315, lr=4.94133e-05, gnorm=2.821, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.7, wall=1413
2023-05-19 17:01:05 | INFO | train_inner | epoch 006:     10 / 65 loss=8.73, nll_loss=7.26, ppl=153.33, wps=3529.4, ups=0.3, wpb=11886.6, bsz=517.6, num_updates=320, lr=4.93878e-05, gnorm=5.603, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.7, wall=1430
2023-05-19 17:01:21 | INFO | train_inner | epoch 006:     15 / 65 loss=9.029, nll_loss=7.574, ppl=190.55, wps=3826.2, ups=0.31, wpb=12476.8, bsz=662.2, num_updates=325, lr=4.93622e-05, gnorm=5.933, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=1447
2023-05-19 17:01:36 | INFO | train_inner | epoch 006:     20 / 65 loss=8.875, nll_loss=7.413, ppl=170.43, wps=4090.8, ups=0.32, wpb=12615.4, bsz=636.4, num_updates=330, lr=4.93367e-05, gnorm=5.41, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.9, wall=1462
2023-05-19 17:01:53 | INFO | train_inner | epoch 006:     25 / 65 loss=8.62, nll_loss=7.147, ppl=141.71, wps=3499.9, ups=0.29, wpb=11946.2, bsz=508, num_updates=335, lr=4.93112e-05, gnorm=4.846, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.4, wall=1479
2023-05-19 17:02:10 | INFO | train_inner | epoch 006:     30 / 65 loss=8.832, nll_loss=7.359, ppl=164.19, wps=3582, ups=0.3, wpb=12140.6, bsz=586, num_updates=340, lr=4.92857e-05, gnorm=9.067, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.9, wall=1496
2023-05-19 17:02:27 | INFO | train_inner | epoch 006:     35 / 65 loss=8.73, nll_loss=7.236, ppl=150.75, wps=3584.5, ups=0.3, wpb=12005.4, bsz=536.8, num_updates=345, lr=4.92602e-05, gnorm=4.613, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=1513
2023-05-19 17:02:42 | INFO | train_inner | epoch 006:     40 / 65 loss=8.65, nll_loss=7.171, ppl=144.11, wps=4116, ups=0.33, wpb=12451.6, bsz=542, num_updates=350, lr=4.92347e-05, gnorm=20.835, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.6, wall=1528
2023-05-19 17:02:58 | INFO | train_inner | epoch 006:     45 / 65 loss=8.569, nll_loss=7.086, ppl=135.83, wps=3736.7, ups=0.31, wpb=11913.6, bsz=502.8, num_updates=355, lr=4.92092e-05, gnorm=1.48, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=1544
2023-05-19 17:03:13 | INFO | train_inner | epoch 006:     50 / 65 loss=8.917, nll_loss=7.443, ppl=174.03, wps=4132, ups=0.33, wpb=12533.4, bsz=594.4, num_updates=360, lr=4.91837e-05, gnorm=4.129, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.7, wall=1559
2023-05-19 17:03:28 | INFO | train_inner | epoch 006:     55 / 65 loss=8.692, nll_loss=7.223, ppl=149.36, wps=4008.3, ups=0.33, wpb=11977.6, bsz=600, num_updates=365, lr=4.91582e-05, gnorm=4.646, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.6, wall=1574
2023-05-19 17:03:48 | INFO | train_inner | epoch 006:     60 / 65 loss=8.741, nll_loss=7.259, ppl=153.16, wps=3065.8, ups=0.26, wpb=11898, bsz=530.6, num_updates=370, lr=4.91327e-05, gnorm=9.132, clip=100, loss_scale=0.0039, train_wall=19, gb_free=21.4, wall=1593
2023-05-19 17:04:02 | INFO | train_inner | epoch 006:     65 / 65 loss=8.693, nll_loss=7.222, ppl=149.25, wps=3863.3, ups=0.36, wpb=10750.4, bsz=519.4, num_updates=375, lr=4.91071e-05, gnorm=4.67, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.8, wall=1607
2023-05-19 17:04:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:04:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:04:35 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.257 | nll_loss 6.722 | ppl 105.55 | bleu 0.12 | wps 1079.4 | wpb 2785 | bsz 105.2 | num_updates 375 | best_loss 8.257
2023-05-19 17:04:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 375 updates
2023-05-19 17:04:35 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint6.pt
2023-05-19 17:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint6.pt
2023-05-19 17:04:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint6.pt (epoch 6 @ 375 updates, score 8.257) (writing took 7.779539152979851 seconds)
2023-05-19 17:04:43 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-05-19 17:04:43 | INFO | train | epoch 006 | loss 8.762 | nll_loss 7.288 | ppl 156.32 | wps 3105.3 | ups 0.26 | wpb 12054.7 | bsz 559.9 | num_updates 375 | lr 4.91071e-05 | gnorm 6.399 | clip 100 | loss_scale 0.0039 | train_wall 210 | gb_free 21.8 | wall 1649
2023-05-19 17:04:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:04:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:04:43 | INFO | fairseq.trainer | begin training epoch 7
2023-05-19 17:04:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:04:59 | INFO | train_inner | epoch 007:      5 / 65 loss=8.63, nll_loss=7.142, ppl=141.22, wps=1066.2, ups=0.09, wpb=12289.2, bsz=571, num_updates=380, lr=4.90816e-05, gnorm=5.941, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=1665
2023-05-19 17:05:16 | INFO | train_inner | epoch 007:     10 / 65 loss=8.522, nll_loss=7.02, ppl=129.83, wps=3437.5, ups=0.29, wpb=11762.2, bsz=467.6, num_updates=385, lr=4.90561e-05, gnorm=2.008, clip=100, loss_scale=0.0039, train_wall=17, gb_free=20.1, wall=1682
2023-05-19 17:05:33 | INFO | train_inner | epoch 007:     15 / 65 loss=8.581, nll_loss=7.105, ppl=137.63, wps=3821.7, ups=0.31, wpb=12269.8, bsz=564.6, num_updates=390, lr=4.90306e-05, gnorm=5.829, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=1698
2023-05-19 17:05:48 | INFO | train_inner | epoch 007:     20 / 65 loss=8.693, nll_loss=7.207, ppl=147.79, wps=4060, ups=0.33, wpb=12424.8, bsz=614.2, num_updates=395, lr=4.90051e-05, gnorm=19.284, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.8, wall=1714
2023-05-19 17:06:04 | INFO | train_inner | epoch 007:     25 / 65 loss=8.423, nll_loss=6.923, ppl=121.38, wps=3793.8, ups=0.32, wpb=11886.6, bsz=462.4, num_updates=400, lr=4.89796e-05, gnorm=7.153, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.8, wall=1729
2023-05-19 17:06:18 | INFO | train_inner | epoch 007:     30 / 65 loss=8.98, nll_loss=7.521, ppl=183.71, wps=4481.9, ups=0.35, wpb=12962.8, bsz=745.2, num_updates=405, lr=4.89541e-05, gnorm=12.795, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.6, wall=1744
2023-05-19 17:06:35 | INFO | train_inner | epoch 007:     35 / 65 loss=8.444, nll_loss=6.951, ppl=123.76, wps=3504.7, ups=0.29, wpb=12142.6, bsz=540, num_updates=410, lr=4.89286e-05, gnorm=8.713, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.3, wall=1761
2023-05-19 17:06:52 | INFO | train_inner | epoch 007:     40 / 65 loss=8.455, nll_loss=6.961, ppl=124.56, wps=3728.6, ups=0.31, wpb=12205.8, bsz=567, num_updates=415, lr=4.89031e-05, gnorm=1.892, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=1778
2023-05-19 17:07:08 | INFO | train_inner | epoch 007:     45 / 65 loss=8.75, nll_loss=7.277, ppl=155.13, wps=3580.3, ups=0.3, wpb=11927.8, bsz=583, num_updates=420, lr=4.88776e-05, gnorm=2.884, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=1794
2023-05-19 17:07:27 | INFO | train_inner | epoch 007:     50 / 65 loss=8.663, nll_loss=7.176, ppl=144.61, wps=3232, ups=0.27, wpb=12060.8, bsz=584.2, num_updates=425, lr=4.8852e-05, gnorm=7.586, clip=100, loss_scale=0.0039, train_wall=19, gb_free=21.7, wall=1813
2023-05-19 17:07:42 | INFO | train_inner | epoch 007:     55 / 65 loss=8.462, nll_loss=6.956, ppl=124.14, wps=3963.4, ups=0.33, wpb=12055.8, bsz=539.6, num_updates=430, lr=4.88265e-05, gnorm=3.323, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.7, wall=1828
2023-05-19 17:07:58 | INFO | train_inner | epoch 007:     60 / 65 loss=8.539, nll_loss=7.036, ppl=131.27, wps=4010, ups=0.33, wpb=12226.2, bsz=561.4, num_updates=435, lr=4.8801e-05, gnorm=6.635, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=1843
2023-05-19 17:08:10 | INFO | train_inner | epoch 007:     65 / 65 loss=8.491, nll_loss=6.997, ppl=127.74, wps=4126.9, ups=0.4, wpb=10446, bsz=477.2, num_updates=440, lr=4.87755e-05, gnorm=8.745, clip=100, loss_scale=0.0039, train_wall=13, gb_free=21.6, wall=1856
2023-05-19 17:08:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:08:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:08:51 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.164 | nll_loss 6.613 | ppl 97.86 | bleu 0.07 | wps 886.5 | wpb 2785 | bsz 105.2 | num_updates 440 | best_loss 8.164
2023-05-19 17:08:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 440 updates
2023-05-19 17:08:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint7.pt
2023-05-19 17:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint7.pt
2023-05-19 17:08:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint7.pt (epoch 7 @ 440 updates, score 8.164) (writing took 8.046624839305878 seconds)
2023-05-19 17:08:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-05-19 17:08:59 | INFO | train | epoch 007 | loss 8.591 | nll_loss 7.102 | ppl 137.35 | wps 3064.9 | ups 0.25 | wpb 12050.8 | bsz 559.8 | num_updates 440 | lr 4.87755e-05 | gnorm 7.137 | clip 100 | loss_scale 0.0039 | train_wall 206 | gb_free 21.6 | wall 1905
2023-05-19 17:08:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:08:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:08:59 | INFO | fairseq.trainer | begin training epoch 8
2023-05-19 17:08:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:09:14 | INFO | train_inner | epoch 008:      5 / 65 loss=8.467, nll_loss=6.966, ppl=125, wps=997.9, ups=0.08, wpb=12639.6, bsz=607, num_updates=445, lr=4.875e-05, gnorm=1.986, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.8, wall=1919
2023-05-19 17:09:32 | INFO | train_inner | epoch 008:     10 / 65 loss=8.486, nll_loss=6.985, ppl=126.63, wps=3345.6, ups=0.27, wpb=12433, bsz=541.2, num_updates=450, lr=4.87245e-05, gnorm=4.585, clip=100, loss_scale=0.0039, train_wall=19, gb_free=20.8, wall=1938
2023-05-19 17:09:47 | INFO | train_inner | epoch 008:     15 / 65 loss=8.417, nll_loss=6.929, ppl=121.84, wps=3916, ups=0.33, wpb=11729.2, bsz=520.2, num_updates=455, lr=4.8699e-05, gnorm=1.174, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.6, wall=1953
2023-05-19 17:10:03 | INFO | train_inner | epoch 008:     20 / 65 loss=8.433, nll_loss=6.938, ppl=122.58, wps=3992, ups=0.33, wpb=12166.4, bsz=575.4, num_updates=460, lr=4.86735e-05, gnorm=2.094, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.8, wall=1969
2023-05-19 17:10:20 | INFO | train_inner | epoch 008:     25 / 65 loss=8.529, nll_loss=7.039, ppl=131.51, wps=3485.2, ups=0.29, wpb=12085.6, bsz=558.4, num_updates=465, lr=4.8648e-05, gnorm=3.255, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.8, wall=1986
2023-05-19 17:10:36 | INFO | train_inner | epoch 008:     30 / 65 loss=8.512, nll_loss=7.005, ppl=128.44, wps=3792.1, ups=0.31, wpb=12149.4, bsz=558.6, num_updates=470, lr=4.86224e-05, gnorm=4.825, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=2002
2023-05-19 17:10:52 | INFO | train_inner | epoch 008:     35 / 65 loss=8.457, nll_loss=6.938, ppl=122.65, wps=3852.7, ups=0.31, wpb=12284.8, bsz=537.8, num_updates=475, lr=4.85969e-05, gnorm=2.859, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.6, wall=2018
2023-05-19 17:11:09 | INFO | train_inner | epoch 008:     40 / 65 loss=8.514, nll_loss=7.022, ppl=129.98, wps=3636.8, ups=0.3, wpb=12065.2, bsz=607.6, num_updates=480, lr=4.85714e-05, gnorm=2.124, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.8, wall=2035
2023-05-19 17:11:24 | INFO | train_inner | epoch 008:     45 / 65 loss=8.489, nll_loss=6.997, ppl=127.74, wps=3917.4, ups=0.32, wpb=12107.6, bsz=609.4, num_updates=485, lr=4.85459e-05, gnorm=2.809, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=2050
2023-05-19 17:11:40 | INFO | train_inner | epoch 008:     50 / 65 loss=8.447, nll_loss=6.953, ppl=123.92, wps=3804.8, ups=0.31, wpb=12210, bsz=561, num_updates=490, lr=4.85204e-05, gnorm=3.386, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.5, wall=2066
2023-05-19 17:11:56 | INFO | train_inner | epoch 008:     55 / 65 loss=8.457, nll_loss=6.954, ppl=123.95, wps=3891.4, ups=0.32, wpb=12052.6, bsz=541.4, num_updates=495, lr=4.84949e-05, gnorm=2.249, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=2082
2023-05-19 17:12:12 | INFO | train_inner | epoch 008:     60 / 65 loss=8.62, nll_loss=7.117, ppl=138.77, wps=3657.1, ups=0.3, wpb=12015.6, bsz=587.6, num_updates=500, lr=4.84694e-05, gnorm=1.898, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.8, wall=2098
2023-05-19 17:12:27 | INFO | train_inner | epoch 008:     65 / 65 loss=8.467, nll_loss=6.951, ppl=123.76, wps=3775.7, ups=0.35, wpb=10802.8, bsz=469.4, num_updates=505, lr=4.84439e-05, gnorm=2.665, clip=100, loss_scale=0.0039, train_wall=14, gb_free=21.6, wall=2112
2023-05-19 17:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:12:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:13:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.079 | nll_loss 6.52 | ppl 91.8 | bleu 0.11 | wps 977 | wpb 2785 | bsz 105.2 | num_updates 505 | best_loss 8.079
2023-05-19 17:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 505 updates
2023-05-19 17:13:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint8.pt
2023-05-19 17:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint8.pt
2023-05-19 17:13:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint8.pt (epoch 8 @ 505 updates, score 8.079) (writing took 8.970434565097094 seconds)
2023-05-19 17:13:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-05-19 17:13:13 | INFO | train | epoch 008 | loss 8.484 | nll_loss 6.984 | ppl 126.61 | wps 3086.6 | ups 0.26 | wpb 12057.1 | bsz 559.6 | num_updates 505 | lr 4.84439e-05 | gnorm 2.762 | clip 100 | loss_scale 0.0039 | train_wall 207 | gb_free 21.6 | wall 2158
2023-05-19 17:13:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:13:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:13:13 | INFO | fairseq.trainer | begin training epoch 9
2023-05-19 17:13:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:13:28 | INFO | train_inner | epoch 009:      5 / 65 loss=8.409, nll_loss=6.912, ppl=120.42, wps=1000.9, ups=0.08, wpb=12226.8, bsz=564.6, num_updates=510, lr=4.84184e-05, gnorm=10.074, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=2173
2023-05-19 17:13:45 | INFO | train_inner | epoch 009:     10 / 65 loss=8.42, nll_loss=6.917, ppl=120.82, wps=3530.5, ups=0.29, wpb=12253.4, bsz=545, num_updates=515, lr=4.83929e-05, gnorm=1.947, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=2191
2023-05-19 17:14:02 | INFO | train_inner | epoch 009:     15 / 65 loss=8.624, nll_loss=7.125, ppl=139.57, wps=3545.5, ups=0.29, wpb=12286.2, bsz=604.8, num_updates=520, lr=4.83673e-05, gnorm=10.168, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.2, wall=2208
2023-05-19 17:14:19 | INFO | train_inner | epoch 009:     20 / 65 loss=8.352, nll_loss=6.832, ppl=113.91, wps=3632.4, ups=0.3, wpb=12305.6, bsz=548.4, num_updates=525, lr=4.83418e-05, gnorm=2.158, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.4, wall=2225
2023-05-19 17:14:37 | INFO | train_inner | epoch 009:     25 / 65 loss=8.36, nll_loss=6.845, ppl=114.94, wps=3425.4, ups=0.29, wpb=11885.4, bsz=514.8, num_updates=530, lr=4.83163e-05, gnorm=1.427, clip=100, loss_scale=0.0039, train_wall=17, gb_free=20.5, wall=2242
2023-05-19 17:14:52 | INFO | train_inner | epoch 009:     30 / 65 loss=8.398, nll_loss=6.891, ppl=118.69, wps=3870.6, ups=0.33, wpb=11756.4, bsz=544.2, num_updates=535, lr=4.82908e-05, gnorm=4.058, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.5, wall=2258
2023-05-19 17:15:07 | INFO | train_inner | epoch 009:     35 / 65 loss=8.301, nll_loss=6.785, ppl=110.27, wps=3868.3, ups=0.32, wpb=12035.8, bsz=528.4, num_updates=540, lr=4.82653e-05, gnorm=1.469, clip=100, loss_scale=0.0039, train_wall=16, gb_free=20.7, wall=2273
2023-05-19 17:15:23 | INFO | train_inner | epoch 009:     40 / 65 loss=8.493, nll_loss=6.992, ppl=127.28, wps=3976.9, ups=0.33, wpb=12185.6, bsz=610.8, num_updates=545, lr=4.82398e-05, gnorm=2.146, clip=100, loss_scale=0.0039, train_wall=15, gb_free=21.7, wall=2289
2023-05-19 17:15:40 | INFO | train_inner | epoch 009:     45 / 65 loss=8.333, nll_loss=6.81, ppl=112.18, wps=3416.6, ups=0.29, wpb=11892.4, bsz=503.6, num_updates=550, lr=4.82143e-05, gnorm=4.644, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.7, wall=2306
2023-05-19 17:15:56 | INFO | train_inner | epoch 009:     50 / 65 loss=8.309, nll_loss=6.792, ppl=110.85, wps=3881.4, ups=0.32, wpb=12144.2, bsz=590.8, num_updates=555, lr=4.81888e-05, gnorm=1.75, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.8, wall=2322
2023-05-19 17:16:13 | INFO | train_inner | epoch 009:     55 / 65 loss=8.352, nll_loss=6.849, ppl=115.3, wps=3802.5, ups=0.3, wpb=12654.6, bsz=618.8, num_updates=560, lr=4.81633e-05, gnorm=1.541, clip=100, loss_scale=0.0039, train_wall=17, gb_free=21.6, wall=2338
2023-05-19 17:16:28 | INFO | train_inner | epoch 009:     60 / 65 loss=8.555, nll_loss=7.034, ppl=131.08, wps=3894, ups=0.32, wpb=12357.2, bsz=590, num_updates=565, lr=4.81378e-05, gnorm=6.477, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.7, wall=2354
2023-05-19 17:16:41 | INFO | train_inner | epoch 009:     65 / 65 loss=8.312, nll_loss=6.782, ppl=110.07, wps=4242.5, ups=0.4, wpb=10692.2, bsz=499.6, num_updates=570, lr=4.81122e-05, gnorm=3.26, clip=100, loss_scale=0.0039, train_wall=13, gb_free=21.7, wall=2367
2023-05-19 17:16:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:16:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:17:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.038 | nll_loss 6.463 | ppl 88.23 | bleu 0.15 | wps 862 | wpb 2785 | bsz 105.2 | num_updates 570 | best_loss 8.038
2023-05-19 17:17:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 570 updates
2023-05-19 17:17:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint9.pt
2023-05-19 17:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint9.pt
2023-05-19 17:17:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint9.pt (epoch 9 @ 570 updates, score 8.038) (writing took 8.840211559087038 seconds)
2023-05-19 17:17:31 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-05-19 17:17:31 | INFO | train | epoch 009 | loss 8.403 | nll_loss 6.891 | ppl 118.7 | wps 3029.2 | ups 0.25 | wpb 12052 | bsz 558.8 | num_updates 570 | lr 4.81122e-05 | gnorm 3.932 | clip 100 | loss_scale 0.0039 | train_wall 208 | gb_free 21.7 | wall 2417
2023-05-19 17:17:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:17:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:17:31 | INFO | fairseq.trainer | begin training epoch 10
2023-05-19 17:17:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:17:47 | INFO | train_inner | epoch 010:      5 / 65 loss=8.321, nll_loss=6.798, ppl=111.29, wps=920.7, ups=0.08, wpb=12248.4, bsz=611.8, num_updates=575, lr=4.80867e-05, gnorm=3.972, clip=100, loss_scale=0.0039, train_wall=16, gb_free=21.3, wall=2433
2023-05-19 17:18:03 | INFO | train_inner | epoch 010:     10 / 65 loss=8.312, nll_loss=6.797, ppl=111.18, wps=3915.1, ups=0.32, wpb=12107, bsz=542.4, num_updates=580, lr=4.80612e-05, gnorm=1.535, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=2449
2023-05-19 17:18:18 | INFO | train_inner | epoch 010:     15 / 65 loss=8.251, nll_loss=6.731, ppl=106.21, wps=3955.4, ups=0.32, wpb=12249.8, bsz=552, num_updates=585, lr=4.80357e-05, gnorm=1.144, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.7, wall=2464
2023-05-19 17:18:35 | INFO | train_inner | epoch 010:     20 / 65 loss=8.265, nll_loss=6.745, ppl=107.23, wps=3766.6, ups=0.31, wpb=12298, bsz=531.6, num_updates=590, lr=4.80102e-05, gnorm=1.205, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.4, wall=2481
2023-05-19 17:18:52 | INFO | train_inner | epoch 010:     25 / 65 loss=8.227, nll_loss=6.703, ppl=104.15, wps=3533.2, ups=0.29, wpb=12214, bsz=522.4, num_updates=595, lr=4.79847e-05, gnorm=1.326, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.6, wall=2498
2023-05-19 17:19:09 | INFO | train_inner | epoch 010:     30 / 65 loss=8.363, nll_loss=6.855, ppl=115.75, wps=3806.9, ups=0.3, wpb=12553.2, bsz=602.4, num_updates=600, lr=4.79592e-05, gnorm=2.111, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=2514
2023-05-19 17:19:25 | INFO | train_inner | epoch 010:     35 / 65 loss=8.227, nll_loss=6.703, ppl=104.16, wps=3713.2, ups=0.3, wpb=12421.4, bsz=539.2, num_updates=605, lr=4.79337e-05, gnorm=1.739, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=2531
2023-05-19 17:19:41 | INFO | train_inner | epoch 010:     40 / 65 loss=8.35, nll_loss=6.834, ppl=114.08, wps=3653, ups=0.32, wpb=11422, bsz=496.8, num_updates=610, lr=4.79082e-05, gnorm=4.245, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=2547
2023-05-19 17:19:57 | INFO | train_inner | epoch 010:     45 / 65 loss=8.28, nll_loss=6.751, ppl=107.71, wps=3894.7, ups=0.32, wpb=12165, bsz=549.4, num_updates=615, lr=4.78827e-05, gnorm=1.458, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.8, wall=2562
2023-05-19 17:20:12 | INFO | train_inner | epoch 010:     50 / 65 loss=8.395, nll_loss=6.89, ppl=118.57, wps=3922, ups=0.32, wpb=12379.8, bsz=640, num_updates=620, lr=4.78571e-05, gnorm=3.63, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=2578
2023-05-19 17:20:29 | INFO | train_inner | epoch 010:     55 / 65 loss=8.209, nll_loss=6.695, ppl=103.61, wps=3586.3, ups=0.29, wpb=12170.2, bsz=560.2, num_updates=625, lr=4.78316e-05, gnorm=1.304, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.8, wall=2595
2023-05-19 17:20:46 | INFO | train_inner | epoch 010:     60 / 65 loss=8.52, nll_loss=7.003, ppl=128.27, wps=3572.1, ups=0.29, wpb=12189.6, bsz=603.4, num_updates=630, lr=4.78061e-05, gnorm=4.045, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.4, wall=2612
2023-05-19 17:21:00 | INFO | train_inner | epoch 010:     65 / 65 loss=8.559, nll_loss=7.049, ppl=132.42, wps=3836.6, ups=0.37, wpb=10296.8, bsz=521.6, num_updates=635, lr=4.77806e-05, gnorm=7.785, clip=100, loss_scale=0.0078, train_wall=13, gb_free=21.7, wall=2626
2023-05-19 17:21:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:21:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:21:41 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.973 | nll_loss 6.399 | ppl 84.41 | bleu 0.16 | wps 870.4 | wpb 2785 | bsz 105.2 | num_updates 635 | best_loss 7.973
2023-05-19 17:21:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 635 updates
2023-05-19 17:21:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint10.pt
2023-05-19 17:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint10.pt
2023-05-19 17:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint10.pt (epoch 10 @ 635 updates, score 7.973) (writing took 21.935359839349985 seconds)
2023-05-19 17:22:03 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-05-19 17:22:03 | INFO | train | epoch 010 | loss 8.326 | nll_loss 6.809 | ppl 112.1 | wps 2882.5 | ups 0.24 | wpb 12055 | bsz 559.5 | num_updates 635 | lr 4.77806e-05 | gnorm 2.731 | clip 100 | loss_scale 0.0078 | train_wall 208 | gb_free 21.7 | wall 2689
2023-05-19 17:22:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:22:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:22:03 | INFO | fairseq.trainer | begin training epoch 11
2023-05-19 17:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:22:19 | INFO | train_inner | epoch 011:      5 / 65 loss=8.253, nll_loss=6.733, ppl=106.36, wps=775, ups=0.06, wpb=12305.4, bsz=580.6, num_updates=640, lr=4.77551e-05, gnorm=1.614, clip=100, loss_scale=0.0078, train_wall=15, gb_free=20.8, wall=2705
2023-05-19 17:22:35 | INFO | train_inner | epoch 011:     10 / 65 loss=8.286, nll_loss=6.758, ppl=108.26, wps=4068.7, ups=0.32, wpb=12599.2, bsz=583.6, num_updates=645, lr=4.77296e-05, gnorm=2.682, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=2720
2023-05-19 17:22:52 | INFO | train_inner | epoch 011:     15 / 65 loss=8.259, nll_loss=6.741, ppl=106.94, wps=3559.3, ups=0.29, wpb=12177, bsz=555.4, num_updates=650, lr=4.77041e-05, gnorm=3.737, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=2738
2023-05-19 17:23:08 | INFO | train_inner | epoch 011:     20 / 65 loss=8.241, nll_loss=6.713, ppl=104.94, wps=3598.8, ups=0.31, wpb=11764.6, bsz=503.4, num_updates=655, lr=4.76786e-05, gnorm=3.367, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.8, wall=2754
2023-05-19 17:23:24 | INFO | train_inner | epoch 011:     25 / 65 loss=8.291, nll_loss=6.762, ppl=108.54, wps=3993.3, ups=0.33, wpb=12277.8, bsz=581.4, num_updates=660, lr=4.76531e-05, gnorm=2.84, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=2769
2023-05-19 17:23:40 | INFO | train_inner | epoch 011:     30 / 65 loss=8.298, nll_loss=6.783, ppl=110.13, wps=3796.6, ups=0.32, wpb=12022.8, bsz=578.8, num_updates=665, lr=4.76276e-05, gnorm=1.354, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=2785
2023-05-19 17:23:55 | INFO | train_inner | epoch 011:     35 / 65 loss=8.334, nll_loss=6.821, ppl=113.09, wps=3971.9, ups=0.32, wpb=12385.4, bsz=615.4, num_updates=670, lr=4.7602e-05, gnorm=1.705, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=2801
2023-05-19 17:24:12 | INFO | train_inner | epoch 011:     40 / 65 loss=8.161, nll_loss=6.631, ppl=99.08, wps=3508.4, ups=0.29, wpb=12004.6, bsz=480.2, num_updates=675, lr=4.75765e-05, gnorm=2.239, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.7, wall=2818
2023-05-19 17:24:29 | INFO | train_inner | epoch 011:     45 / 65 loss=8.658, nll_loss=7.152, ppl=142.26, wps=3873.1, ups=0.31, wpb=12596.2, bsz=680, num_updates=680, lr=4.7551e-05, gnorm=2.765, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.5, wall=2834
2023-05-19 17:24:44 | INFO | train_inner | epoch 011:     50 / 65 loss=8.255, nll_loss=6.735, ppl=106.53, wps=3923.1, ups=0.33, wpb=11800.8, bsz=546.8, num_updates=685, lr=4.75255e-05, gnorm=2.823, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.8, wall=2849
2023-05-19 17:24:59 | INFO | train_inner | epoch 011:     55 / 65 loss=8.217, nll_loss=6.692, ppl=103.4, wps=3977.4, ups=0.33, wpb=12049.8, bsz=547.2, num_updates=690, lr=4.75e-05, gnorm=4.27, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=2865
2023-05-19 17:25:16 | INFO | train_inner | epoch 011:     60 / 65 loss=8.182, nll_loss=6.649, ppl=100.37, wps=3445, ups=0.28, wpb=12193.2, bsz=539.4, num_updates=695, lr=4.74745e-05, gnorm=1.124, clip=100, loss_scale=0.0078, train_wall=18, gb_free=21.5, wall=2882
2023-05-19 17:25:31 | INFO | train_inner | epoch 011:     65 / 65 loss=8.143, nll_loss=6.607, ppl=97.49, wps=3635.9, ups=0.35, wpb=10386.4, bsz=457, num_updates=700, lr=4.7449e-05, gnorm=2.098, clip=100, loss_scale=0.0078, train_wall=14, gb_free=21.4, wall=2897
2023-05-19 17:25:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:25:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:26:15 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.951 | nll_loss 6.376 | ppl 83.04 | bleu 0.15 | wps 811.8 | wpb 2785 | bsz 105.2 | num_updates 700 | best_loss 7.951
2023-05-19 17:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 700 updates
2023-05-19 17:26:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint11.pt
2023-05-19 17:26:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint11.pt
2023-05-19 17:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint11.pt (epoch 11 @ 700 updates, score 7.951) (writing took 25.493486277759075 seconds)
2023-05-19 17:26:40 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-05-19 17:26:40 | INFO | train | epoch 011 | loss 8.278 | nll_loss 6.755 | ppl 108.03 | wps 2824.4 | ups 0.23 | wpb 12043.3 | bsz 557.6 | num_updates 700 | lr 4.7449e-05 | gnorm 2.509 | clip 100 | loss_scale 0.0078 | train_wall 206 | gb_free 21.4 | wall 2966
2023-05-19 17:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:26:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:26:40 | INFO | fairseq.trainer | begin training epoch 12
2023-05-19 17:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:26:57 | INFO | train_inner | epoch 012:      5 / 65 loss=8.164, nll_loss=6.635, ppl=99.38, wps=714.2, ups=0.06, wpb=12294.6, bsz=582.2, num_updates=705, lr=4.74235e-05, gnorm=1.002, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=2983
2023-05-19 17:27:13 | INFO | train_inner | epoch 012:     10 / 65 loss=8.177, nll_loss=6.641, ppl=99.83, wps=4024.2, ups=0.32, wpb=12702.8, bsz=619, num_updates=710, lr=4.7398e-05, gnorm=3.881, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.9, wall=2998
2023-05-19 17:27:28 | INFO | train_inner | epoch 012:     15 / 65 loss=8.221, nll_loss=6.693, ppl=103.49, wps=3868.5, ups=0.32, wpb=12140.8, bsz=581.2, num_updates=715, lr=4.73724e-05, gnorm=1.335, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3014
2023-05-19 17:27:45 | INFO | train_inner | epoch 012:     20 / 65 loss=8.154, nll_loss=6.621, ppl=98.44, wps=3686.5, ups=0.31, wpb=11963.4, bsz=487.6, num_updates=720, lr=4.73469e-05, gnorm=1.233, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3030
2023-05-19 17:28:01 | INFO | train_inner | epoch 012:     25 / 65 loss=8.238, nll_loss=6.707, ppl=104.51, wps=3653.6, ups=0.31, wpb=11909.2, bsz=558.2, num_updates=725, lr=4.73214e-05, gnorm=1.023, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=3047
2023-05-19 17:28:16 | INFO | train_inner | epoch 012:     30 / 65 loss=8.476, nll_loss=6.958, ppl=124.34, wps=3983, ups=0.33, wpb=12239.6, bsz=612.6, num_updates=730, lr=4.72959e-05, gnorm=1.866, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.8, wall=3062
2023-05-19 17:28:34 | INFO | train_inner | epoch 012:     35 / 65 loss=8.255, nll_loss=6.737, ppl=106.66, wps=3511.5, ups=0.28, wpb=12328.4, bsz=577.2, num_updates=735, lr=4.72704e-05, gnorm=2.402, clip=100, loss_scale=0.0078, train_wall=18, gb_free=21.8, wall=3080
2023-05-19 17:28:50 | INFO | train_inner | epoch 012:     40 / 65 loss=8.12, nll_loss=6.59, ppl=96.35, wps=3828.4, ups=0.31, wpb=12323, bsz=560, num_updates=740, lr=4.72449e-05, gnorm=0.815, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3096
2023-05-19 17:29:06 | INFO | train_inner | epoch 012:     45 / 65 loss=8.274, nll_loss=6.746, ppl=107.31, wps=3890.1, ups=0.31, wpb=12351.2, bsz=602.2, num_updates=745, lr=4.72194e-05, gnorm=0.985, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3112
2023-05-19 17:29:21 | INFO | train_inner | epoch 012:     50 / 65 loss=8.209, nll_loss=6.673, ppl=102.03, wps=3844.9, ups=0.32, wpb=12017, bsz=536, num_updates=750, lr=4.71939e-05, gnorm=18.572, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3127
2023-05-19 17:29:36 | INFO | train_inner | epoch 012:     55 / 65 loss=8.29, nll_loss=6.759, ppl=108.27, wps=4028.2, ups=0.34, wpb=11877.6, bsz=541.6, num_updates=755, lr=4.71684e-05, gnorm=6.747, clip=100, loss_scale=0.0078, train_wall=15, gb_free=20.1, wall=3142
2023-05-19 17:29:52 | INFO | train_inner | epoch 012:     60 / 65 loss=8.179, nll_loss=6.653, ppl=100.64, wps=3855.5, ups=0.32, wpb=12227, bsz=539, num_updates=760, lr=4.71429e-05, gnorm=1.546, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.2, wall=3158
2023-05-19 17:30:05 | INFO | train_inner | epoch 012:     65 / 65 loss=8.156, nll_loss=6.615, ppl=98.02, wps=3838.6, ups=0.38, wpb=10200.8, bsz=434.6, num_updates=765, lr=4.71173e-05, gnorm=1.872, clip=100, loss_scale=0.0078, train_wall=13, gb_free=21.7, wall=3171
2023-05-19 17:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:30:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:30:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.901 | nll_loss 6.29 | ppl 78.22 | bleu 0.16 | wps 828.7 | wpb 2785 | bsz 105.2 | num_updates 765 | best_loss 7.901
2023-05-19 17:30:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 765 updates
2023-05-19 17:30:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint12.pt
2023-05-19 17:30:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint12.pt
2023-05-19 17:30:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint12.pt (epoch 12 @ 765 updates, score 7.901) (writing took 8.79361055791378 seconds)
2023-05-19 17:30:57 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-05-19 17:30:57 | INFO | train | epoch 012 | loss 8.225 | nll_loss 6.695 | ppl 103.64 | wps 3046.7 | ups 0.25 | wpb 12044.3 | bsz 556.3 | num_updates 765 | lr 4.71173e-05 | gnorm 3.329 | clip 100 | loss_scale 0.0078 | train_wall 204 | gb_free 21.7 | wall 3223
2023-05-19 17:30:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:30:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:30:57 | INFO | fairseq.trainer | begin training epoch 13
2023-05-19 17:30:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:31:14 | INFO | train_inner | epoch 013:      5 / 65 loss=8.255, nll_loss=6.719, ppl=105.37, wps=914.1, ups=0.07, wpb=12579.4, bsz=643.4, num_updates=770, lr=4.70918e-05, gnorm=1.13, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.8, wall=3240
2023-05-19 17:31:30 | INFO | train_inner | epoch 013:     10 / 65 loss=8.131, nll_loss=6.6, ppl=96.99, wps=3867.6, ups=0.32, wpb=12143.8, bsz=502.6, num_updates=775, lr=4.70663e-05, gnorm=1.016, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=3256
2023-05-19 17:31:45 | INFO | train_inner | epoch 013:     15 / 65 loss=8.434, nll_loss=6.9, ppl=119.42, wps=4063.5, ups=0.33, wpb=12474.4, bsz=610.4, num_updates=780, lr=4.70408e-05, gnorm=2.509, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.2, wall=3271
2023-05-19 17:32:01 | INFO | train_inner | epoch 013:     20 / 65 loss=8.146, nll_loss=6.606, ppl=97.39, wps=3735.6, ups=0.32, wpb=11568.2, bsz=466.2, num_updates=785, lr=4.70153e-05, gnorm=0.746, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=3287
2023-05-19 17:32:17 | INFO | train_inner | epoch 013:     25 / 65 loss=8.148, nll_loss=6.618, ppl=98.24, wps=3722.5, ups=0.32, wpb=11745, bsz=507.4, num_updates=790, lr=4.69898e-05, gnorm=0.998, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3302
2023-05-19 17:32:32 | INFO | train_inner | epoch 013:     30 / 65 loss=8.147, nll_loss=6.618, ppl=98.24, wps=3970.6, ups=0.33, wpb=12016, bsz=588.6, num_updates=795, lr=4.69643e-05, gnorm=0.755, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.3, wall=3317
2023-05-19 17:32:47 | INFO | train_inner | epoch 013:     35 / 65 loss=8.144, nll_loss=6.604, ppl=97.28, wps=3989.5, ups=0.33, wpb=12078.6, bsz=590, num_updates=800, lr=4.69388e-05, gnorm=2.242, clip=100, loss_scale=0.0078, train_wall=15, gb_free=20.1, wall=3333
2023-05-19 17:33:04 | INFO | train_inner | epoch 013:     40 / 65 loss=8.375, nll_loss=6.851, ppl=115.4, wps=3742.5, ups=0.3, wpb=12660.8, bsz=654.4, num_updates=805, lr=4.69133e-05, gnorm=2.229, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.6, wall=3350
2023-05-19 17:33:20 | INFO | train_inner | epoch 013:     45 / 65 loss=8.165, nll_loss=6.616, ppl=98.11, wps=3653.8, ups=0.3, wpb=12155.2, bsz=547.6, num_updates=810, lr=4.68878e-05, gnorm=1.519, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.6, wall=3366
2023-05-19 17:33:36 | INFO | train_inner | epoch 013:     50 / 65 loss=8.128, nll_loss=6.594, ppl=96.62, wps=3750, ups=0.32, wpb=11895, bsz=526, num_updates=815, lr=4.68622e-05, gnorm=1.272, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.3, wall=3382
2023-05-19 17:33:51 | INFO | train_inner | epoch 013:     55 / 65 loss=8.152, nll_loss=6.613, ppl=97.92, wps=4162, ups=0.34, wpb=12419.8, bsz=579.6, num_updates=820, lr=4.68367e-05, gnorm=1.798, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=3397
2023-05-19 17:34:07 | INFO | train_inner | epoch 013:     60 / 65 loss=8.138, nll_loss=6.6, ppl=97.03, wps=3830.2, ups=0.31, wpb=12450, bsz=590.4, num_updates=825, lr=4.68112e-05, gnorm=1.106, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3413
2023-05-19 17:34:22 | INFO | train_inner | epoch 013:     65 / 65 loss=8.13, nll_loss=6.593, ppl=96.54, wps=3623.3, ups=0.35, wpb=10457.2, bsz=470.6, num_updates=830, lr=4.67857e-05, gnorm=0.799, clip=100, loss_scale=0.0078, train_wall=14, gb_free=21.7, wall=3428
2023-05-19 17:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:34:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:35:06 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.852 | nll_loss 6.245 | ppl 75.86 | bleu 0.14 | wps 805.1 | wpb 2785 | bsz 105.2 | num_updates 830 | best_loss 7.852
2023-05-19 17:35:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 830 updates
2023-05-19 17:35:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint13.pt
2023-05-19 17:35:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint13.pt
2023-05-19 17:35:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint13.pt (epoch 13 @ 830 updates, score 7.852) (writing took 8.85825840011239 seconds)
2023-05-19 17:35:15 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-05-19 17:35:15 | INFO | train | epoch 013 | loss 8.194 | nll_loss 6.659 | ppl 101.03 | wps 3039 | ups 0.25 | wpb 12049.5 | bsz 559.8 | num_updates 830 | lr 4.67857e-05 | gnorm 1.394 | clip 100 | loss_scale 0.0078 | train_wall 204 | gb_free 21.7 | wall 3481
2023-05-19 17:35:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:35:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:35:15 | INFO | fairseq.trainer | begin training epoch 14
2023-05-19 17:35:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:35:31 | INFO | train_inner | epoch 014:      5 / 65 loss=8.15, nll_loss=6.6, ppl=97.01, wps=883.2, ups=0.07, wpb=12197.2, bsz=519.2, num_updates=835, lr=4.67602e-05, gnorm=1.718, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.5, wall=3497
2023-05-19 17:35:47 | INFO | train_inner | epoch 014:     10 / 65 loss=8.053, nll_loss=6.497, ppl=90.35, wps=3738.6, ups=0.31, wpb=12085, bsz=502.6, num_updates=840, lr=4.67347e-05, gnorm=0.764, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3513
2023-05-19 17:36:03 | INFO | train_inner | epoch 014:     15 / 65 loss=8.167, nll_loss=6.629, ppl=98.99, wps=4007, ups=0.32, wpb=12512, bsz=587, num_updates=845, lr=4.67092e-05, gnorm=1.024, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3528
2023-05-19 17:36:19 | INFO | train_inner | epoch 014:     20 / 65 loss=8.125, nll_loss=6.591, ppl=96.4, wps=3747.5, ups=0.31, wpb=11959.2, bsz=553, num_updates=850, lr=4.66837e-05, gnorm=1.203, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=3544
2023-05-19 17:36:35 | INFO | train_inner | epoch 014:     25 / 65 loss=8.37, nll_loss=6.835, ppl=114.13, wps=3781.8, ups=0.3, wpb=12654.2, bsz=636.8, num_updates=855, lr=4.66582e-05, gnorm=2.209, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.9, wall=3561
2023-05-19 17:36:51 | INFO | train_inner | epoch 014:     30 / 65 loss=8.059, nll_loss=6.505, ppl=90.82, wps=3832.3, ups=0.31, wpb=12297, bsz=568.4, num_updates=860, lr=4.66327e-05, gnorm=0.64, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3577
2023-05-19 17:37:08 | INFO | train_inner | epoch 014:     35 / 65 loss=8.163, nll_loss=6.625, ppl=98.67, wps=3501.9, ups=0.3, wpb=11837.8, bsz=517.4, num_updates=865, lr=4.66071e-05, gnorm=1.937, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=3594
2023-05-19 17:37:25 | INFO | train_inner | epoch 014:     40 / 65 loss=8.15, nll_loss=6.608, ppl=97.52, wps=3735, ups=0.31, wpb=12084.4, bsz=568.2, num_updates=870, lr=4.65816e-05, gnorm=1.68, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.4, wall=3610
2023-05-19 17:37:42 | INFO | train_inner | epoch 014:     45 / 65 loss=8.108, nll_loss=6.565, ppl=94.65, wps=3621.6, ups=0.29, wpb=12500.6, bsz=583.4, num_updates=875, lr=4.65561e-05, gnorm=2.475, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=3628
2023-05-19 17:37:59 | INFO | train_inner | epoch 014:     50 / 65 loss=8.15, nll_loss=6.609, ppl=97.6, wps=3401.6, ups=0.29, wpb=11577.8, bsz=512.6, num_updates=880, lr=4.65306e-05, gnorm=1.101, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.7, wall=3645
2023-05-19 17:38:14 | INFO | train_inner | epoch 014:     55 / 65 loss=8.233, nll_loss=6.697, ppl=103.77, wps=3849.2, ups=0.32, wpb=12020.6, bsz=626.4, num_updates=885, lr=4.65051e-05, gnorm=1.763, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=3660
2023-05-19 17:38:31 | INFO | train_inner | epoch 014:     60 / 65 loss=8.115, nll_loss=6.576, ppl=95.41, wps=3625.8, ups=0.3, wpb=12235.2, bsz=567, num_updates=890, lr=4.64796e-05, gnorm=0.963, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=3677
2023-05-19 17:38:46 | INFO | train_inner | epoch 014:     65 / 65 loss=8.216, nll_loss=6.678, ppl=102.4, wps=3664.9, ups=0.34, wpb=10697.2, bsz=512, num_updates=895, lr=4.64541e-05, gnorm=2.306, clip=100, loss_scale=0.0078, train_wall=15, gb_free=20.9, wall=3692
2023-05-19 17:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:38:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:39:28 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.826 | nll_loss 6.209 | ppl 73.98 | bleu 0.17 | wps 844.8 | wpb 2785 | bsz 105.2 | num_updates 895 | best_loss 7.826
2023-05-19 17:39:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 895 updates
2023-05-19 17:39:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint14.pt
2023-05-19 17:39:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint14.pt
2023-05-19 17:39:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint14.pt (epoch 14 @ 895 updates, score 7.826) (writing took 8.275707345455885 seconds)
2023-05-19 17:39:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-05-19 17:39:37 | INFO | train | epoch 014 | loss 8.158 | nll_loss 6.616 | ppl 98.12 | wps 2994.2 | ups 0.25 | wpb 12050.6 | bsz 558 | num_updates 895 | lr 4.64541e-05 | gnorm 1.522 | clip 100 | loss_scale 0.0078 | train_wall 210 | gb_free 20.9 | wall 3742
2023-05-19 17:39:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:39:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:39:37 | INFO | fairseq.trainer | begin training epoch 15
2023-05-19 17:39:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:39:54 | INFO | train_inner | epoch 015:      5 / 65 loss=8.058, nll_loss=6.504, ppl=90.79, wps=888.7, ups=0.07, wpb=12059.4, bsz=567.6, num_updates=900, lr=4.64286e-05, gnorm=1.672, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=3760
2023-05-19 17:40:09 | INFO | train_inner | epoch 015:     10 / 65 loss=8.104, nll_loss=6.562, ppl=94.46, wps=3839.5, ups=0.32, wpb=11985.4, bsz=542.6, num_updates=905, lr=4.64031e-05, gnorm=0.977, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.6, wall=3775
2023-05-19 17:40:26 | INFO | train_inner | epoch 015:     15 / 65 loss=8.112, nll_loss=6.568, ppl=94.87, wps=3664.9, ups=0.3, wpb=12124.2, bsz=593.6, num_updates=910, lr=4.63776e-05, gnorm=0.673, clip=100, loss_scale=0.0078, train_wall=17, gb_free=20.4, wall=3792
2023-05-19 17:40:43 | INFO | train_inner | epoch 015:     20 / 65 loss=8.119, nll_loss=6.573, ppl=95.23, wps=3640.9, ups=0.3, wpb=12239.8, bsz=521.2, num_updates=915, lr=4.6352e-05, gnorm=1.349, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.5, wall=3809
2023-05-19 17:40:59 | INFO | train_inner | epoch 015:     25 / 65 loss=8.36, nll_loss=6.818, ppl=112.8, wps=3807.3, ups=0.31, wpb=12404.4, bsz=624.4, num_updates=920, lr=4.63265e-05, gnorm=1.728, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=3825
2023-05-19 17:41:14 | INFO | train_inner | epoch 015:     30 / 65 loss=8.212, nll_loss=6.665, ppl=101.46, wps=4411.1, ups=0.35, wpb=12748.6, bsz=640.2, num_updates=925, lr=4.6301e-05, gnorm=6.58, clip=100, loss_scale=0.0078, train_wall=14, gb_free=21.6, wall=3839
2023-05-19 17:41:29 | INFO | train_inner | epoch 015:     35 / 65 loss=8.225, nll_loss=6.686, ppl=102.99, wps=3894.6, ups=0.33, wpb=11864.6, bsz=522.2, num_updates=930, lr=4.62755e-05, gnorm=1.833, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=3855
2023-05-19 17:41:47 | INFO | train_inner | epoch 015:     40 / 65 loss=8.067, nll_loss=6.524, ppl=92.04, wps=3391, ups=0.28, wpb=12161, bsz=537.4, num_updates=935, lr=4.625e-05, gnorm=0.881, clip=100, loss_scale=0.0078, train_wall=18, gb_free=21.5, wall=3872
2023-05-19 17:42:02 | INFO | train_inner | epoch 015:     45 / 65 loss=8.042, nll_loss=6.495, ppl=90.2, wps=4079.3, ups=0.33, wpb=12272.2, bsz=537.2, num_updates=940, lr=4.62245e-05, gnorm=0.842, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.8, wall=3888
2023-05-19 17:42:18 | INFO | train_inner | epoch 015:     50 / 65 loss=8.079, nll_loss=6.53, ppl=92.43, wps=3815.4, ups=0.32, wpb=12097.4, bsz=554.2, num_updates=945, lr=4.6199e-05, gnorm=2.333, clip=100, loss_scale=0.0078, train_wall=16, gb_free=20.5, wall=3903
2023-05-19 17:42:34 | INFO | train_inner | epoch 015:     55 / 65 loss=8.129, nll_loss=6.581, ppl=95.76, wps=3678.3, ups=0.31, wpb=11983, bsz=546.2, num_updates=950, lr=4.61735e-05, gnorm=2.008, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=3920
2023-05-19 17:42:52 | INFO | train_inner | epoch 015:     60 / 65 loss=8.114, nll_loss=6.571, ppl=95.04, wps=3502.3, ups=0.28, wpb=12362.4, bsz=583, num_updates=955, lr=4.6148e-05, gnorm=0.788, clip=100, loss_scale=0.0078, train_wall=18, gb_free=21.2, wall=3937
2023-05-19 17:43:04 | INFO | train_inner | epoch 015:     65 / 65 loss=8.064, nll_loss=6.511, ppl=91.2, wps=4310.5, ups=0.41, wpb=10513.4, bsz=502.8, num_updates=960, lr=4.61224e-05, gnorm=2.794, clip=100, loss_scale=0.0078, train_wall=12, gb_free=21.8, wall=3950
2023-05-19 17:43:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:43:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:43:44 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.791 | nll_loss 6.171 | ppl 72.07 | bleu 0.25 | wps 905.4 | wpb 2785 | bsz 105.2 | num_updates 960 | best_loss 7.791
2023-05-19 17:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 960 updates
2023-05-19 17:43:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint15.pt
2023-05-19 17:43:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint15.pt
2023-05-19 17:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint15.pt (epoch 15 @ 960 updates, score 7.791) (writing took 9.246852658689022 seconds)
2023-05-19 17:43:53 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-05-19 17:43:53 | INFO | train | epoch 015 | loss 8.131 | nll_loss 6.585 | ppl 96 | wps 3060.5 | ups 0.25 | wpb 12062.8 | bsz 559.4 | num_updates 960 | lr 4.61224e-05 | gnorm 1.881 | clip 100 | loss_scale 0.0078 | train_wall 206 | gb_free 21.8 | wall 3999
2023-05-19 17:43:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:43:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:43:53 | INFO | fairseq.trainer | begin training epoch 16
2023-05-19 17:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:44:08 | INFO | train_inner | epoch 016:      5 / 65 loss=8.104, nll_loss=6.557, ppl=94.13, wps=957.5, ups=0.08, wpb=12342.8, bsz=556.2, num_updates=965, lr=4.60969e-05, gnorm=3.073, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=4014
2023-05-19 17:44:25 | INFO | train_inner | epoch 016:     10 / 65 loss=8.072, nll_loss=6.523, ppl=91.95, wps=3624.4, ups=0.3, wpb=12196.4, bsz=575.6, num_updates=970, lr=4.60714e-05, gnorm=1.153, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.6, wall=4031
2023-05-19 17:44:42 | INFO | train_inner | epoch 016:     15 / 65 loss=8.375, nll_loss=6.834, ppl=114.08, wps=3611.5, ups=0.3, wpb=12077, bsz=633, num_updates=975, lr=4.60459e-05, gnorm=45.837, clip=100, loss_scale=0.0078, train_wall=17, gb_free=20.8, wall=4048
2023-05-19 17:44:58 | INFO | train_inner | epoch 016:     20 / 65 loss=8.093, nll_loss=6.549, ppl=93.61, wps=3609.5, ups=0.3, wpb=11891.6, bsz=589.6, num_updates=980, lr=4.60204e-05, gnorm=2.485, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=4064
2023-05-19 17:45:14 | INFO | train_inner | epoch 016:     25 / 65 loss=8.073, nll_loss=6.525, ppl=92.07, wps=3798.3, ups=0.32, wpb=11960.6, bsz=531.8, num_updates=985, lr=4.59949e-05, gnorm=0.979, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=4080
2023-05-19 17:45:31 | INFO | train_inner | epoch 016:     30 / 65 loss=8.161, nll_loss=6.618, ppl=98.22, wps=3804.7, ups=0.3, wpb=12714, bsz=646, num_updates=990, lr=4.59694e-05, gnorm=1.918, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.7, wall=4097
2023-05-19 17:45:48 | INFO | train_inner | epoch 016:     35 / 65 loss=8.085, nll_loss=6.529, ppl=92.33, wps=3493.1, ups=0.29, wpb=12078.2, bsz=555, num_updates=995, lr=4.59439e-05, gnorm=3.252, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.6, wall=4114
2023-05-19 17:46:05 | INFO | train_inner | epoch 016:     40 / 65 loss=8.05, nll_loss=6.495, ppl=90.23, wps=3662.3, ups=0.3, wpb=12065.2, bsz=571, num_updates=1000, lr=4.59184e-05, gnorm=1.453, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.3, wall=4130
2023-05-19 17:46:19 | INFO | train_inner | epoch 016:     45 / 65 loss=8.022, nll_loss=6.468, ppl=88.53, wps=4165.7, ups=0.34, wpb=12352.4, bsz=550.8, num_updates=1005, lr=4.58929e-05, gnorm=0.991, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=4145
2023-05-19 17:46:36 | INFO | train_inner | epoch 016:     50 / 65 loss=8.074, nll_loss=6.528, ppl=92.26, wps=3585.9, ups=0.29, wpb=12259.4, bsz=564.2, num_updates=1010, lr=4.58673e-05, gnorm=3.262, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21, wall=4162
2023-05-19 17:46:53 | INFO | train_inner | epoch 016:     55 / 65 loss=8.003, nll_loss=6.45, ppl=87.45, wps=3660.8, ups=0.3, wpb=12231.6, bsz=512.2, num_updates=1015, lr=4.58418e-05, gnorm=0.762, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.4, wall=4179
2023-05-19 17:47:09 | INFO | train_inner | epoch 016:     60 / 65 loss=8.15, nll_loss=6.594, ppl=96.57, wps=3978.5, ups=0.33, wpb=12220.6, bsz=547.8, num_updates=1020, lr=4.58163e-05, gnorm=2.065, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.4, wall=4194
2023-05-19 17:47:22 | INFO | train_inner | epoch 016:     65 / 65 loss=8.05, nll_loss=6.499, ppl=90.45, wps=3881.4, ups=0.38, wpb=10299.4, bsz=442.8, num_updates=1025, lr=4.57908e-05, gnorm=0.92, clip=100, loss_scale=0.0078, train_wall=13, gb_free=21.6, wall=4208
2023-05-19 17:47:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:47:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:48:04 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.768 | nll_loss 6.141 | ppl 70.56 | bleu 0.28 | wps 854.4 | wpb 2785 | bsz 105.2 | num_updates 1025 | best_loss 7.768
2023-05-19 17:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1025 updates
2023-05-19 17:48:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint16.pt
2023-05-19 17:48:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint16.pt
2023-05-19 17:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint16.pt (epoch 16 @ 1025 updates, score 7.768) (writing took 9.955878764390945 seconds)
2023-05-19 17:48:14 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-05-19 17:48:14 | INFO | train | epoch 016 | loss 8.102 | nll_loss 6.552 | ppl 93.83 | wps 3003.4 | ups 0.25 | wpb 12053 | bsz 559.7 | num_updates 1025 | lr 4.57908e-05 | gnorm 5.242 | clip 100 | loss_scale 0.0078 | train_wall 208 | gb_free 21.6 | wall 4259
2023-05-19 17:48:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:48:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:48:14 | INFO | fairseq.trainer | begin training epoch 17
2023-05-19 17:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:48:31 | INFO | train_inner | epoch 017:      5 / 65 loss=8.021, nll_loss=6.46, ppl=88.03, wps=854.5, ups=0.07, wpb=11735.8, bsz=502.4, num_updates=1030, lr=4.57653e-05, gnorm=0.896, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.7, wall=4276
2023-05-19 17:48:47 | INFO | train_inner | epoch 017:     10 / 65 loss=8.058, nll_loss=6.505, ppl=90.82, wps=3675.3, ups=0.29, wpb=12473.4, bsz=592.8, num_updates=1035, lr=4.57398e-05, gnorm=0.944, clip=100, loss_scale=0.0078, train_wall=17, gb_free=21.7, wall=4293
2023-05-19 17:49:03 | INFO | train_inner | epoch 017:     15 / 65 loss=8.236, nll_loss=6.683, ppl=102.78, wps=4119.8, ups=0.34, wpb=12294.2, bsz=622.2, num_updates=1040, lr=4.57143e-05, gnorm=0.794, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=4309
2023-05-19 17:49:18 | INFO | train_inner | epoch 017:     20 / 65 loss=8.061, nll_loss=6.503, ppl=90.68, wps=4209.3, ups=0.33, wpb=12754.8, bsz=629, num_updates=1045, lr=4.56888e-05, gnorm=2.258, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.7, wall=4324
2023-05-19 17:49:34 | INFO | train_inner | epoch 017:     25 / 65 loss=7.971, nll_loss=6.411, ppl=85.07, wps=3934.5, ups=0.32, wpb=12187.6, bsz=554, num_updates=1050, lr=4.56633e-05, gnorm=0.67, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.7, wall=4340
2023-05-19 17:49:49 | INFO | train_inner | epoch 017:     30 / 65 loss=8.189, nll_loss=6.641, ppl=99.84, wps=3947.1, ups=0.32, wpb=12210.6, bsz=617, num_updates=1055, lr=4.56378e-05, gnorm=0.955, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.6, wall=4355
2023-05-19 17:50:06 | INFO | train_inner | epoch 017:     35 / 65 loss=8.144, nll_loss=6.597, ppl=96.79, wps=3803.6, ups=0.31, wpb=12277.6, bsz=545, num_updates=1060, lr=4.56122e-05, gnorm=1.193, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=4371
2023-05-19 17:50:21 | INFO | train_inner | epoch 017:     40 / 65 loss=8.006, nll_loss=6.456, ppl=87.81, wps=4057.3, ups=0.33, wpb=12413.8, bsz=538.8, num_updates=1065, lr=4.55867e-05, gnorm=0.617, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=4387
2023-05-19 17:50:38 | INFO | train_inner | epoch 017:     45 / 65 loss=8.087, nll_loss=6.536, ppl=92.79, wps=3658.6, ups=0.3, wpb=12046, bsz=545.8, num_updates=1070, lr=4.55612e-05, gnorm=0.954, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.5, wall=4403
2023-05-19 17:50:54 | INFO | train_inner | epoch 017:     50 / 65 loss=8.07, nll_loss=6.514, ppl=91.38, wps=3796.5, ups=0.31, wpb=12107.2, bsz=565.4, num_updates=1075, lr=4.55357e-05, gnorm=1.019, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=4419
2023-05-19 17:51:10 | INFO | train_inner | epoch 017:     55 / 65 loss=8.024, nll_loss=6.466, ppl=88.37, wps=3709, ups=0.31, wpb=12049.4, bsz=527.6, num_updates=1080, lr=4.55102e-05, gnorm=0.908, clip=100, loss_scale=0.0078, train_wall=16, gb_free=21.6, wall=4436
2023-05-19 17:51:27 | INFO | train_inner | epoch 017:     60 / 65 loss=8.056, nll_loss=6.504, ppl=90.78, wps=3655.7, ups=0.3, wpb=12144.4, bsz=591.6, num_updates=1085, lr=4.54847e-05, gnorm=0.778, clip=100, loss_scale=0.0078, train_wall=17, gb_free=20.5, wall=4453
2023-05-19 17:51:41 | INFO | train_inner | epoch 017:     65 / 65 loss=8.027, nll_loss=6.475, ppl=88.97, wps=3425.5, ups=0.34, wpb=10010.2, bsz=443.2, num_updates=1090, lr=4.54592e-05, gnorm=1.009, clip=100, loss_scale=0.0078, train_wall=15, gb_free=21.5, wall=4467
2023-05-19 17:51:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:51:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:52:21 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.72 | nll_loss 6.086 | ppl 67.95 | bleu 0.33 | wps 905.8 | wpb 2785 | bsz 105.2 | num_updates 1090 | best_loss 7.72
2023-05-19 17:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1090 updates
2023-05-19 17:52:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint17.pt
2023-05-19 17:52:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint17.pt
2023-05-19 17:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint17.pt (epoch 17 @ 1090 updates, score 7.72) (writing took 8.945170350372791 seconds)
2023-05-19 17:52:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-05-19 17:52:30 | INFO | train | epoch 017 | loss 8.074 | nll_loss 6.52 | ppl 91.78 | wps 3055.7 | ups 0.25 | wpb 12054.2 | bsz 559.6 | num_updates 1090 | lr 4.54592e-05 | gnorm 1 | clip 100 | loss_scale 0.0078 | train_wall 205 | gb_free 21.5 | wall 4516
2023-05-19 17:52:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:52:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:52:30 | INFO | fairseq.trainer | begin training epoch 18
2023-05-19 17:52:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:52:46 | INFO | train_inner | epoch 018:      5 / 65 loss=8.029, nll_loss=6.475, ppl=88.94, wps=927, ups=0.08, wpb=12051.6, bsz=500.6, num_updates=1095, lr=4.54337e-05, gnorm=7.327, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=4532
2023-05-19 17:53:01 | INFO | train_inner | epoch 018:     10 / 65 loss=8.076, nll_loss=6.539, ppl=93, wps=4232.2, ups=0.34, wpb=12444.6, bsz=583.6, num_updates=1100, lr=4.54082e-05, gnorm=3.698, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.7, wall=4547
2023-05-19 17:53:19 | INFO | train_inner | epoch 018:     15 / 65 loss=7.931, nll_loss=6.396, ppl=84.24, wps=3497.2, ups=0.28, wpb=12351, bsz=560.4, num_updates=1105, lr=4.53827e-05, gnorm=2.609, clip=100, loss_scale=0.0156, train_wall=18, gb_free=21.6, wall=4565
2023-05-19 17:53:36 | INFO | train_inner | epoch 018:     20 / 65 loss=8.208, nll_loss=6.706, ppl=104.38, wps=3537.8, ups=0.29, wpb=12198.2, bsz=602.4, num_updates=1110, lr=4.53571e-05, gnorm=1.073, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=4582
2023-05-19 17:53:52 | INFO | train_inner | epoch 018:     25 / 65 loss=8.011, nll_loss=6.501, ppl=90.6, wps=3823.4, ups=0.32, wpb=11972.8, bsz=523.2, num_updates=1115, lr=4.53316e-05, gnorm=2.2, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=4597
2023-05-19 17:54:08 | INFO | train_inner | epoch 018:     30 / 65 loss=8.071, nll_loss=6.571, ppl=95.04, wps=3671.5, ups=0.31, wpb=11903.6, bsz=564.8, num_updates=1120, lr=4.53061e-05, gnorm=2.846, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.4, wall=4614
2023-05-19 17:54:25 | INFO | train_inner | epoch 018:     35 / 65 loss=8.004, nll_loss=6.486, ppl=89.66, wps=3593.6, ups=0.3, wpb=12020.8, bsz=518.2, num_updates=1125, lr=4.52806e-05, gnorm=0.951, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=4630
2023-05-19 17:54:40 | INFO | train_inner | epoch 018:     40 / 65 loss=7.97, nll_loss=6.436, ppl=86.59, wps=3893.5, ups=0.32, wpb=12116.8, bsz=523.6, num_updates=1130, lr=4.52551e-05, gnorm=8.365, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=4646
2023-05-19 17:54:55 | INFO | train_inner | epoch 018:     45 / 65 loss=8.128, nll_loss=6.627, ppl=98.85, wps=4189.5, ups=0.33, wpb=12749.2, bsz=672.4, num_updates=1135, lr=4.52296e-05, gnorm=1.239, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.7, wall=4661
2023-05-19 17:55:11 | INFO | train_inner | epoch 018:     50 / 65 loss=8.023, nll_loss=6.518, ppl=91.66, wps=3768, ups=0.32, wpb=11908.6, bsz=541.8, num_updates=1140, lr=4.52041e-05, gnorm=1.042, clip=100, loss_scale=0.0156, train_wall=16, gb_free=20.5, wall=4677
2023-05-19 17:55:28 | INFO | train_inner | epoch 018:     55 / 65 loss=8.142, nll_loss=6.644, ppl=100.03, wps=3711.4, ups=0.31, wpb=12101.4, bsz=609, num_updates=1145, lr=4.51786e-05, gnorm=5.625, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=4693
2023-05-19 17:55:44 | INFO | train_inner | epoch 018:     60 / 65 loss=7.986, nll_loss=6.468, ppl=88.55, wps=3771.5, ups=0.3, wpb=12689.4, bsz=614.8, num_updates=1150, lr=4.51531e-05, gnorm=1.196, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=4710
2023-05-19 17:55:57 | INFO | train_inner | epoch 018:     65 / 65 loss=7.988, nll_loss=6.485, ppl=89.56, wps=4132.2, ups=0.41, wpb=10140.4, bsz=457.6, num_updates=1155, lr=4.51276e-05, gnorm=1.068, clip=100, loss_scale=0.0156, train_wall=12, gb_free=21.7, wall=4722
2023-05-19 17:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 17:55:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:56:39 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.653 | nll_loss 6.069 | ppl 67.13 | bleu 0.35 | wps 852.7 | wpb 2785 | bsz 105.2 | num_updates 1155 | best_loss 7.653
2023-05-19 17:56:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1155 updates
2023-05-19 17:56:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint18.pt
2023-05-19 17:56:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint18.pt
2023-05-19 17:56:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint18.pt (epoch 18 @ 1155 updates, score 7.653) (writing took 8.683350265026093 seconds)
2023-05-19 17:56:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-05-19 17:56:47 | INFO | train | epoch 018 | loss 8.045 | nll_loss 6.528 | ppl 92.27 | wps 3045 | ups 0.25 | wpb 12049.9 | bsz 559.4 | num_updates 1155 | lr 4.51276e-05 | gnorm 3.018 | clip 100 | loss_scale 0.0156 | train_wall 206 | gb_free 21.7 | wall 4773
2023-05-19 17:56:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 17:56:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 17:56:47 | INFO | fairseq.trainer | begin training epoch 19
2023-05-19 17:56:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 17:57:03 | INFO | train_inner | epoch 019:      5 / 65 loss=7.978, nll_loss=6.475, ppl=88.96, wps=929, ups=0.08, wpb=12381.6, bsz=650.4, num_updates=1160, lr=4.5102e-05, gnorm=0.975, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.2, wall=4789
2023-05-19 17:57:20 | INFO | train_inner | epoch 019:     10 / 65 loss=8.023, nll_loss=6.512, ppl=91.29, wps=3752.2, ups=0.31, wpb=12277.2, bsz=552.4, num_updates=1165, lr=4.50765e-05, gnorm=1.773, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=4805
2023-05-19 17:57:36 | INFO | train_inner | epoch 019:     15 / 65 loss=7.956, nll_loss=6.438, ppl=86.71, wps=3715.1, ups=0.31, wpb=12163.8, bsz=554.4, num_updates=1170, lr=4.5051e-05, gnorm=2.473, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.1, wall=4822
2023-05-19 17:57:51 | INFO | train_inner | epoch 019:     20 / 65 loss=7.975, nll_loss=6.453, ppl=87.58, wps=4104.6, ups=0.33, wpb=12353.4, bsz=599.2, num_updates=1175, lr=4.50255e-05, gnorm=1.424, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.5, wall=4837
2023-05-19 17:58:07 | INFO | train_inner | epoch 019:     25 / 65 loss=7.865, nll_loss=6.331, ppl=80.51, wps=3900.2, ups=0.31, wpb=12405.6, bsz=573.4, num_updates=1180, lr=4.5e-05, gnorm=1.32, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=4853
2023-05-19 17:58:23 | INFO | train_inner | epoch 019:     30 / 65 loss=7.839, nll_loss=6.3, ppl=78.82, wps=3813.7, ups=0.31, wpb=12334.2, bsz=554.4, num_updates=1185, lr=4.49745e-05, gnorm=2.054, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=4869
2023-05-19 17:58:39 | INFO | train_inner | epoch 019:     35 / 65 loss=8.283, nll_loss=6.758, ppl=108.26, wps=3644.9, ups=0.31, wpb=11842.6, bsz=591, num_updates=1190, lr=4.4949e-05, gnorm=1.818, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=4885
2023-05-19 17:58:56 | INFO | train_inner | epoch 019:     40 / 65 loss=7.902, nll_loss=6.361, ppl=82.22, wps=3579.2, ups=0.3, wpb=11868, bsz=538, num_updates=1195, lr=4.49235e-05, gnorm=1.139, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.7, wall=4902
2023-05-19 17:59:10 | INFO | train_inner | epoch 019:     45 / 65 loss=7.811, nll_loss=6.27, ppl=77.17, wps=4371, ups=0.36, wpb=12304, bsz=635.2, num_updates=1200, lr=4.4898e-05, gnorm=1.284, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=4916
2023-05-19 17:59:28 | INFO | train_inner | epoch 019:     50 / 65 loss=7.788, nll_loss=6.232, ppl=75.15, wps=3375.3, ups=0.28, wpb=11996, bsz=476.6, num_updates=1205, lr=4.48724e-05, gnorm=1.138, clip=100, loss_scale=0.0156, train_wall=18, gb_free=20.4, wall=4934
2023-05-19 17:59:45 | INFO | train_inner | epoch 019:     55 / 65 loss=7.685, nll_loss=6.113, ppl=69.22, wps=3609.8, ups=0.29, wpb=12353.2, bsz=527.8, num_updates=1210, lr=4.48469e-05, gnorm=1.815, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=4951
2023-05-19 18:00:01 | INFO | train_inner | epoch 019:     60 / 65 loss=7.838, nll_loss=6.288, ppl=78.12, wps=4003.9, ups=0.32, wpb=12459.6, bsz=613.8, num_updates=1215, lr=4.48214e-05, gnorm=3.096, clip=100, loss_scale=0.0156, train_wall=16, gb_free=20.7, wall=4966
2023-05-19 18:00:14 | INFO | train_inner | epoch 019:     65 / 65 loss=7.794, nll_loss=6.244, ppl=75.77, wps=3585, ups=0.36, wpb=9975.8, bsz=409, num_updates=1220, lr=4.47959e-05, gnorm=1.337, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=4980
2023-05-19 18:00:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:00:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:00:58 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.356 | nll_loss 5.717 | ppl 52.6 | bleu 0.83 | wps 815 | wpb 2785 | bsz 105.2 | num_updates 1220 | best_loss 7.356
2023-05-19 18:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1220 updates
2023-05-19 18:00:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint19.pt
2023-05-19 18:01:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint19.pt
2023-05-19 18:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint19.pt (epoch 19 @ 1220 updates, score 7.356) (writing took 8.884830448776484 seconds)
2023-05-19 18:01:07 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-05-19 18:01:07 | INFO | train | epoch 019 | loss 7.903 | nll_loss 6.368 | ppl 82.6 | wps 3015 | ups 0.25 | wpb 12055 | bsz 559.7 | num_updates 1220 | lr 4.47959e-05 | gnorm 1.665 | clip 100 | loss_scale 0.0156 | train_wall 206 | gb_free 21.7 | wall 5033
2023-05-19 18:01:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:01:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:01:07 | INFO | fairseq.trainer | begin training epoch 20
2023-05-19 18:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:01:24 | INFO | train_inner | epoch 020:      5 / 65 loss=7.827, nll_loss=6.273, ppl=77.32, wps=878.6, ups=0.07, wpb=12194, bsz=595.6, num_updates=1225, lr=4.47704e-05, gnorm=1.234, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=5050
2023-05-19 18:01:40 | INFO | train_inner | epoch 020:     10 / 65 loss=7.739, nll_loss=6.177, ppl=72.38, wps=3594.7, ups=0.3, wpb=11905.8, bsz=483.4, num_updates=1230, lr=4.47449e-05, gnorm=1.194, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=5066
2023-05-19 18:01:58 | INFO | train_inner | epoch 020:     15 / 65 loss=7.825, nll_loss=6.281, ppl=77.76, wps=3324.5, ups=0.28, wpb=11744.2, bsz=525.6, num_updates=1235, lr=4.47194e-05, gnorm=2.236, clip=100, loss_scale=0.0156, train_wall=18, gb_free=21.2, wall=5084
2023-05-19 18:02:14 | INFO | train_inner | epoch 020:     20 / 65 loss=7.737, nll_loss=6.179, ppl=72.46, wps=3986.7, ups=0.32, wpb=12523.4, bsz=616.4, num_updates=1240, lr=4.46939e-05, gnorm=1.063, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=5100
2023-05-19 18:02:30 | INFO | train_inner | epoch 020:     25 / 65 loss=7.777, nll_loss=6.216, ppl=74.36, wps=3804.7, ups=0.32, wpb=11952, bsz=546.2, num_updates=1245, lr=4.46684e-05, gnorm=1.916, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5115
2023-05-19 18:02:45 | INFO | train_inner | epoch 020:     30 / 65 loss=7.805, nll_loss=6.248, ppl=76.03, wps=3949.2, ups=0.32, wpb=12344.4, bsz=574.4, num_updates=1250, lr=4.46429e-05, gnorm=1.733, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5131
2023-05-19 18:03:00 | INFO | train_inner | epoch 020:     35 / 65 loss=7.703, nll_loss=6.137, ppl=70.38, wps=4045.8, ups=0.33, wpb=12230.4, bsz=546.8, num_updates=1255, lr=4.46173e-05, gnorm=2.979, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.7, wall=5146
2023-05-19 18:03:17 | INFO | train_inner | epoch 020:     40 / 65 loss=7.686, nll_loss=6.115, ppl=69.3, wps=3789, ups=0.31, wpb=12312, bsz=550.6, num_updates=1260, lr=4.45918e-05, gnorm=1.352, clip=100, loss_scale=0.0156, train_wall=16, gb_free=20.6, wall=5162
2023-05-19 18:03:31 | INFO | train_inner | epoch 020:     45 / 65 loss=7.629, nll_loss=6.051, ppl=66.31, wps=4373.6, ups=0.35, wpb=12558.4, bsz=594.8, num_updates=1265, lr=4.45663e-05, gnorm=1.274, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=5177
2023-05-19 18:03:48 | INFO | train_inner | epoch 020:     50 / 65 loss=7.656, nll_loss=6.086, ppl=67.91, wps=3659.2, ups=0.3, wpb=12174.6, bsz=534.4, num_updates=1270, lr=4.45408e-05, gnorm=7.006, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.2, wall=5193
2023-05-19 18:04:04 | INFO | train_inner | epoch 020:     55 / 65 loss=7.648, nll_loss=6.076, ppl=67.46, wps=3639.6, ups=0.31, wpb=11931, bsz=536.8, num_updates=1275, lr=4.45153e-05, gnorm=1.15, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.9, wall=5210
2023-05-19 18:04:21 | INFO | train_inner | epoch 020:     60 / 65 loss=7.852, nll_loss=6.288, ppl=78.13, wps=3582.3, ups=0.29, wpb=12223, bsz=594.4, num_updates=1280, lr=4.44898e-05, gnorm=1.22, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=5227
2023-05-19 18:04:36 | INFO | train_inner | epoch 020:     65 / 65 loss=7.85, nll_loss=6.285, ppl=77.99, wps=3600.1, ups=0.34, wpb=10504.2, bsz=558.4, num_updates=1285, lr=4.44643e-05, gnorm=2.239, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.8, wall=5241
2023-05-19 18:04:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:04:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:05:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.205 | nll_loss 5.533 | ppl 46.31 | bleu 1.06 | wps 876.7 | wpb 2785 | bsz 105.2 | num_updates 1285 | best_loss 7.205
2023-05-19 18:05:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1285 updates
2023-05-19 18:05:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint20.pt
2023-05-19 18:05:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint20.pt
2023-05-19 18:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint20.pt (epoch 20 @ 1285 updates, score 7.205) (writing took 13.595295924693346 seconds)
2023-05-19 18:05:33 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-05-19 18:05:33 | INFO | train | epoch 020 | loss 7.747 | nll_loss 6.184 | ppl 72.71 | wps 2945.3 | ups 0.24 | wpb 12046 | bsz 558.3 | num_updates 1285 | lr 4.44643e-05 | gnorm 2.046 | clip 100 | loss_scale 0.0156 | train_wall 208 | gb_free 21.8 | wall 5299
2023-05-19 18:05:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:05:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:05:39 | INFO | fairseq.trainer | begin training epoch 21
2023-05-19 18:05:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:05:54 | INFO | train_inner | epoch 021:      5 / 65 loss=7.978, nll_loss=6.408, ppl=84.92, wps=787.5, ups=0.06, wpb=12323.8, bsz=593.4, num_updates=1290, lr=4.44388e-05, gnorm=3.413, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.3, wall=5320
2023-05-19 18:06:10 | INFO | train_inner | epoch 021:     10 / 65 loss=7.63, nll_loss=6.053, ppl=66.38, wps=3787.9, ups=0.31, wpb=12339.8, bsz=544.6, num_updates=1295, lr=4.44133e-05, gnorm=1.537, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=5336
2023-05-19 18:06:27 | INFO | train_inner | epoch 021:     15 / 65 loss=7.711, nll_loss=6.15, ppl=71.03, wps=3639.7, ups=0.3, wpb=12134.8, bsz=615, num_updates=1300, lr=4.43878e-05, gnorm=1.115, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=5353
2023-05-19 18:06:43 | INFO | train_inner | epoch 021:     20 / 65 loss=7.735, nll_loss=6.168, ppl=71.93, wps=3906.7, ups=0.32, wpb=12360.6, bsz=586.6, num_updates=1305, lr=4.43622e-05, gnorm=5.222, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5368
2023-05-19 18:06:59 | INFO | train_inner | epoch 021:     25 / 65 loss=7.601, nll_loss=6.029, ppl=65.29, wps=3842.1, ups=0.31, wpb=12417.4, bsz=561.6, num_updates=1310, lr=4.43367e-05, gnorm=1.141, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5385
2023-05-19 18:07:16 | INFO | train_inner | epoch 021:     30 / 65 loss=7.638, nll_loss=6.066, ppl=67.01, wps=3684.7, ups=0.3, wpb=12380.8, bsz=560.6, num_updates=1315, lr=4.43112e-05, gnorm=1.851, clip=100, loss_scale=0.0156, train_wall=17, gb_free=20.1, wall=5401
2023-05-19 18:07:33 | INFO | train_inner | epoch 021:     35 / 65 loss=7.603, nll_loss=6.012, ppl=64.51, wps=3492.3, ups=0.29, wpb=12116.6, bsz=571.2, num_updates=1320, lr=4.42857e-05, gnorm=1.202, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.2, wall=5419
2023-05-19 18:07:48 | INFO | train_inner | epoch 021:     40 / 65 loss=7.585, nll_loss=6.006, ppl=64.28, wps=4066.2, ups=0.34, wpb=11952.8, bsz=538.6, num_updates=1325, lr=4.42602e-05, gnorm=1.135, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.6, wall=5433
2023-05-19 18:08:04 | INFO | train_inner | epoch 021:     45 / 65 loss=7.668, nll_loss=6.088, ppl=68.04, wps=3680.7, ups=0.31, wpb=12037.2, bsz=555.8, num_updates=1330, lr=4.42347e-05, gnorm=1.092, clip=100, loss_scale=0.0156, train_wall=16, gb_free=20.4, wall=5450
2023-05-19 18:08:20 | INFO | train_inner | epoch 021:     50 / 65 loss=7.604, nll_loss=6.025, ppl=65.12, wps=3755.8, ups=0.32, wpb=11906.6, bsz=565.6, num_updates=1335, lr=4.42092e-05, gnorm=1.371, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=5466
2023-05-19 18:08:37 | INFO | train_inner | epoch 021:     55 / 65 loss=7.603, nll_loss=6.025, ppl=65.14, wps=3597.6, ups=0.3, wpb=11949.8, bsz=591.6, num_updates=1340, lr=4.41837e-05, gnorm=3.828, clip=100, loss_scale=0.0156, train_wall=17, gb_free=20.1, wall=5482
2023-05-19 18:08:52 | INFO | train_inner | epoch 021:     60 / 65 loss=7.675, nll_loss=6.093, ppl=68.24, wps=4015.1, ups=0.32, wpb=12518.4, bsz=576.2, num_updates=1345, lr=4.41582e-05, gnorm=1.761, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=5498
2023-05-19 18:09:07 | INFO | train_inner | epoch 021:     65 / 65 loss=7.535, nll_loss=5.941, ppl=61.42, wps=3546.9, ups=0.35, wpb=10272.6, bsz=417.8, num_updates=1350, lr=4.41327e-05, gnorm=1.304, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=5512
2023-05-19 18:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:09:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:09:51 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.087 | nll_loss 5.398 | ppl 42.17 | bleu 1.14 | wps 810.5 | wpb 2785 | bsz 105.2 | num_updates 1350 | best_loss 7.087
2023-05-19 18:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1350 updates
2023-05-19 18:09:51 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint21.pt
2023-05-19 18:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint21.pt
2023-05-19 18:10:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint21.pt (epoch 21 @ 1350 updates, score 7.087) (writing took 24.868771690875292 seconds)
2023-05-19 18:10:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-05-19 18:10:16 | INFO | train | epoch 021 | loss 7.661 | nll_loss 6.084 | ppl 67.85 | wps 2773.6 | ups 0.23 | wpb 12054.7 | bsz 559.9 | num_updates 1350 | lr 4.41327e-05 | gnorm 1.998 | clip 100 | loss_scale 0.0156 | train_wall 207 | gb_free 21.7 | wall 5581
2023-05-19 18:10:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:10:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:10:16 | INFO | fairseq.trainer | begin training epoch 22
2023-05-19 18:10:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:10:32 | INFO | train_inner | epoch 022:      5 / 65 loss=7.529, nll_loss=5.939, ppl=61.34, wps=720.2, ups=0.06, wpb=12324.4, bsz=585.4, num_updates=1355, lr=4.41071e-05, gnorm=1.079, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5598
2023-05-19 18:10:47 | INFO | train_inner | epoch 022:     10 / 65 loss=7.571, nll_loss=5.974, ppl=62.86, wps=4293.3, ups=0.35, wpb=12398.4, bsz=611.8, num_updates=1360, lr=4.40816e-05, gnorm=15.225, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=5612
2023-05-19 18:11:03 | INFO | train_inner | epoch 022:     15 / 65 loss=7.525, nll_loss=5.922, ppl=60.63, wps=3737.8, ups=0.31, wpb=11884.2, bsz=472.4, num_updates=1365, lr=4.40561e-05, gnorm=3.088, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.3, wall=5628
2023-05-19 18:11:18 | INFO | train_inner | epoch 022:     20 / 65 loss=7.594, nll_loss=6.003, ppl=64.15, wps=3944.8, ups=0.32, wpb=12385.4, bsz=570.8, num_updates=1370, lr=4.40306e-05, gnorm=2.377, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.1, wall=5644
2023-05-19 18:11:35 | INFO | train_inner | epoch 022:     25 / 65 loss=7.788, nll_loss=6.2, ppl=73.53, wps=3652.1, ups=0.3, wpb=12306, bsz=607.6, num_updates=1375, lr=4.40051e-05, gnorm=2.264, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=5661
2023-05-19 18:11:52 | INFO | train_inner | epoch 022:     30 / 65 loss=7.553, nll_loss=5.966, ppl=62.52, wps=3678, ups=0.3, wpb=12216.4, bsz=561.4, num_updates=1380, lr=4.39796e-05, gnorm=10.358, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=5677
2023-05-19 18:12:08 | INFO | train_inner | epoch 022:     35 / 65 loss=7.514, nll_loss=5.916, ppl=60.38, wps=3692.1, ups=0.31, wpb=11893.8, bsz=549.2, num_updates=1385, lr=4.39541e-05, gnorm=2.001, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=5694
2023-05-19 18:12:24 | INFO | train_inner | epoch 022:     40 / 65 loss=7.625, nll_loss=6.028, ppl=65.24, wps=3733.2, ups=0.3, wpb=12340.6, bsz=580.4, num_updates=1390, lr=4.39286e-05, gnorm=2.257, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=5710
2023-05-19 18:12:41 | INFO | train_inner | epoch 022:     45 / 65 loss=7.531, nll_loss=5.932, ppl=61.05, wps=3721.4, ups=0.31, wpb=12113.6, bsz=526.8, num_updates=1395, lr=4.39031e-05, gnorm=2.014, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=5726
2023-05-19 18:12:57 | INFO | train_inner | epoch 022:     50 / 65 loss=7.499, nll_loss=5.903, ppl=59.84, wps=3608, ups=0.31, wpb=11735, bsz=531.6, num_updates=1400, lr=4.38776e-05, gnorm=1.376, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.7, wall=5743
2023-05-19 18:13:12 | INFO | train_inner | epoch 022:     55 / 65 loss=7.631, nll_loss=6.048, ppl=66.18, wps=4027.1, ups=0.32, wpb=12438, bsz=628.8, num_updates=1405, lr=4.3852e-05, gnorm=1.212, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.5, wall=5758
2023-05-19 18:13:28 | INFO | train_inner | epoch 022:     60 / 65 loss=7.441, nll_loss=5.831, ppl=56.92, wps=3825, ups=0.31, wpb=12199.2, bsz=516.6, num_updates=1410, lr=4.38265e-05, gnorm=1.774, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=5774
2023-05-19 18:13:42 | INFO | train_inner | epoch 022:     65 / 65 loss=7.559, nll_loss=5.962, ppl=62.35, wps=3835.7, ups=0.36, wpb=10544.2, bsz=523.4, num_updates=1415, lr=4.3801e-05, gnorm=7.899, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.7, wall=5788
2023-05-19 18:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:13:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:14:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.972 | nll_loss 5.267 | ppl 38.5 | bleu 1.27 | wps 805.2 | wpb 2785 | bsz 105.2 | num_updates 1415 | best_loss 6.972
2023-05-19 18:14:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1415 updates
2023-05-19 18:14:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint22.pt
2023-05-19 18:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint22.pt
2023-05-19 18:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint22.pt (epoch 22 @ 1415 updates, score 6.972) (writing took 26.47116056457162 seconds)
2023-05-19 18:14:53 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-05-19 18:14:53 | INFO | train | epoch 022 | loss 7.567 | nll_loss 5.972 | ppl 62.77 | wps 2828.8 | ups 0.23 | wpb 12059.9 | bsz 558.9 | num_updates 1415 | lr 4.3801e-05 | gnorm 4.071 | clip 100 | loss_scale 0.0156 | train_wall 206 | gb_free 21.7 | wall 5859
2023-05-19 18:14:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:14:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:14:53 | INFO | fairseq.trainer | begin training epoch 23
2023-05-19 18:14:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:15:08 | INFO | train_inner | epoch 023:      5 / 65 loss=7.418, nll_loss=5.807, ppl=55.97, wps=724.2, ups=0.06, wpb=12452.4, bsz=572, num_updates=1420, lr=4.37755e-05, gnorm=1.749, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.6, wall=5874
2023-05-19 18:15:24 | INFO | train_inner | epoch 023:     10 / 65 loss=7.598, nll_loss=6.008, ppl=64.35, wps=3701.8, ups=0.31, wpb=12060, bsz=585.4, num_updates=1425, lr=4.375e-05, gnorm=1.461, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.2, wall=5890
2023-05-19 18:15:42 | INFO | train_inner | epoch 023:     15 / 65 loss=7.451, nll_loss=5.844, ppl=57.43, wps=3502.6, ups=0.29, wpb=12105.2, bsz=479.8, num_updates=1430, lr=4.37245e-05, gnorm=1.196, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.1, wall=5907
2023-05-19 18:15:58 | INFO | train_inner | epoch 023:     20 / 65 loss=7.5, nll_loss=5.889, ppl=59.25, wps=3705.6, ups=0.31, wpb=12079.2, bsz=532.2, num_updates=1435, lr=4.3699e-05, gnorm=3.351, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=5924
2023-05-19 18:16:13 | INFO | train_inner | epoch 023:     25 / 65 loss=7.418, nll_loss=5.806, ppl=55.95, wps=3976.5, ups=0.32, wpb=12351, bsz=523.4, num_updates=1440, lr=4.36735e-05, gnorm=1.303, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=5939
2023-05-19 18:16:30 | INFO | train_inner | epoch 023:     30 / 65 loss=7.457, nll_loss=5.849, ppl=57.63, wps=3778.8, ups=0.31, wpb=12371.6, bsz=595.2, num_updates=1445, lr=4.3648e-05, gnorm=3.756, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=5956
2023-05-19 18:16:45 | INFO | train_inner | epoch 023:     35 / 65 loss=7.452, nll_loss=5.852, ppl=57.78, wps=3927.8, ups=0.33, wpb=11989.8, bsz=569.6, num_updates=1450, lr=4.36224e-05, gnorm=1.343, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.5, wall=5971
2023-05-19 18:17:02 | INFO | train_inner | epoch 023:     40 / 65 loss=7.669, nll_loss=6.071, ppl=67.25, wps=3449.2, ups=0.29, wpb=11994.8, bsz=571.2, num_updates=1455, lr=4.35969e-05, gnorm=1.389, clip=100, loss_scale=0.0156, train_wall=17, gb_free=20.5, wall=5988
2023-05-19 18:17:17 | INFO | train_inner | epoch 023:     45 / 65 loss=7.515, nll_loss=5.902, ppl=59.79, wps=4144.4, ups=0.34, wpb=12344, bsz=644.4, num_updates=1460, lr=4.35714e-05, gnorm=2.407, clip=100, loss_scale=0.0156, train_wall=15, gb_free=20.5, wall=6003
2023-05-19 18:17:33 | INFO | train_inner | epoch 023:     50 / 65 loss=7.43, nll_loss=5.805, ppl=55.91, wps=3716.2, ups=0.31, wpb=11833.6, bsz=521.4, num_updates=1465, lr=4.35459e-05, gnorm=2.577, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=6019
2023-05-19 18:17:50 | INFO | train_inner | epoch 023:     55 / 65 loss=7.466, nll_loss=5.847, ppl=57.56, wps=3720.9, ups=0.31, wpb=12110, bsz=521.4, num_updates=1470, lr=4.35204e-05, gnorm=2.657, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.4, wall=6035
2023-05-19 18:18:06 | INFO | train_inner | epoch 023:     60 / 65 loss=7.633, nll_loss=6.038, ppl=65.72, wps=3855, ups=0.31, wpb=12448.4, bsz=662.8, num_updates=1475, lr=4.34949e-05, gnorm=1.858, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=6051
2023-05-19 18:18:20 | INFO | train_inner | epoch 023:     65 / 65 loss=7.424, nll_loss=5.82, ppl=56.49, wps=3637.9, ups=0.34, wpb=10564.6, bsz=496, num_updates=1480, lr=4.34694e-05, gnorm=3.944, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.8, wall=6066
2023-05-19 18:18:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:18:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:19:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.905 | nll_loss 5.185 | ppl 36.39 | bleu 1.59 | wps 904.2 | wpb 2785 | bsz 105.2 | num_updates 1480 | best_loss 6.905
2023-05-19 18:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1480 updates
2023-05-19 18:19:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint23.pt
2023-05-19 18:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint23.pt
2023-05-19 18:19:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint23.pt (epoch 23 @ 1480 updates, score 6.905) (writing took 11.440114710479975 seconds)
2023-05-19 18:19:14 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-05-19 18:19:15 | INFO | train | epoch 023 | loss 7.495 | nll_loss 5.888 | ppl 59.22 | wps 2990.4 | ups 0.25 | wpb 12054.2 | bsz 559.6 | num_updates 1480 | lr 4.34694e-05 | gnorm 2.23 | clip 100 | loss_scale 0.0156 | train_wall 207 | gb_free 21.8 | wall 6121
2023-05-19 18:19:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:19:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:19:16 | INFO | fairseq.trainer | begin training epoch 24
2023-05-19 18:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:19:33 | INFO | train_inner | epoch 024:      5 / 65 loss=7.418, nll_loss=5.795, ppl=55.51, wps=813.4, ups=0.07, wpb=11880.4, bsz=472.2, num_updates=1485, lr=4.34439e-05, gnorm=1.707, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.7, wall=6139
2023-05-19 18:19:50 | INFO | train_inner | epoch 024:     10 / 65 loss=7.567, nll_loss=5.959, ppl=62.19, wps=3575.1, ups=0.3, wpb=12079.2, bsz=556.8, num_updates=1490, lr=4.34184e-05, gnorm=1.711, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=6156
2023-05-19 18:20:06 | INFO | train_inner | epoch 024:     15 / 65 loss=7.42, nll_loss=5.801, ppl=55.76, wps=3737.7, ups=0.31, wpb=11878.6, bsz=587, num_updates=1495, lr=4.33929e-05, gnorm=1.957, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.4, wall=6172
2023-05-19 18:20:21 | INFO | train_inner | epoch 024:     20 / 65 loss=7.37, nll_loss=5.749, ppl=53.76, wps=4274.7, ups=0.34, wpb=12544.6, bsz=566.8, num_updates=1500, lr=4.33673e-05, gnorm=1.676, clip=100, loss_scale=0.0156, train_wall=15, gb_free=20.5, wall=6187
2023-05-19 18:20:37 | INFO | train_inner | epoch 024:     25 / 65 loss=7.46, nll_loss=5.839, ppl=57.25, wps=3775.4, ups=0.31, wpb=12220.6, bsz=531.6, num_updates=1505, lr=4.33418e-05, gnorm=2.361, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.3, wall=6203
2023-05-19 18:20:52 | INFO | train_inner | epoch 024:     30 / 65 loss=7.447, nll_loss=5.83, ppl=56.9, wps=3987.3, ups=0.33, wpb=12152.4, bsz=608.8, num_updates=1510, lr=4.33163e-05, gnorm=5.767, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.7, wall=6218
2023-05-19 18:21:10 | INFO | train_inner | epoch 024:     35 / 65 loss=7.365, nll_loss=5.745, ppl=53.61, wps=3490.2, ups=0.28, wpb=12496.2, bsz=562, num_updates=1515, lr=4.32908e-05, gnorm=2.233, clip=100, loss_scale=0.0156, train_wall=18, gb_free=21.5, wall=6236
2023-05-19 18:21:26 | INFO | train_inner | epoch 024:     40 / 65 loss=7.634, nll_loss=6.022, ppl=64.97, wps=3922.2, ups=0.33, wpb=12031.4, bsz=631.2, num_updates=1520, lr=4.32653e-05, gnorm=3.272, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.7, wall=6251
2023-05-19 18:21:41 | INFO | train_inner | epoch 024:     45 / 65 loss=7.335, nll_loss=5.708, ppl=52.29, wps=3949.7, ups=0.32, wpb=12286.2, bsz=542.8, num_updates=1525, lr=4.32398e-05, gnorm=1.533, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=6267
2023-05-19 18:21:57 | INFO | train_inner | epoch 024:     50 / 65 loss=7.377, nll_loss=5.754, ppl=53.98, wps=3662, ups=0.31, wpb=11918, bsz=529, num_updates=1530, lr=4.32143e-05, gnorm=3.249, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=6283
2023-05-19 18:22:15 | INFO | train_inner | epoch 024:     55 / 65 loss=7.494, nll_loss=5.881, ppl=58.92, wps=3503.6, ups=0.29, wpb=12172.8, bsz=593.6, num_updates=1535, lr=4.31888e-05, gnorm=3.643, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.8, wall=6301
2023-05-19 18:22:31 | INFO | train_inner | epoch 024:     60 / 65 loss=7.387, nll_loss=5.769, ppl=54.54, wps=3766.5, ups=0.3, wpb=12530, bsz=594.6, num_updates=1540, lr=4.31633e-05, gnorm=2.636, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.6, wall=6317
2023-05-19 18:22:45 | INFO | train_inner | epoch 024:     65 / 65 loss=7.391, nll_loss=5.767, ppl=54.45, wps=3886, ups=0.37, wpb=10557.4, bsz=503.2, num_updates=1545, lr=4.31378e-05, gnorm=4.452, clip=100, loss_scale=0.0156, train_wall=14, gb_free=21.9, wall=6331
2023-05-19 18:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:22:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:23:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.813 | nll_loss 5.073 | ppl 33.66 | bleu 1.84 | wps 894.7 | wpb 2785 | bsz 105.2 | num_updates 1545 | best_loss 6.813
2023-05-19 18:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1545 updates
2023-05-19 18:23:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint24.pt
2023-05-19 18:23:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint24.pt
2023-05-19 18:23:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint24.pt (epoch 24 @ 1545 updates, score 6.813) (writing took 32.45764219760895 seconds)
2023-05-19 18:23:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-05-19 18:23:58 | INFO | train | epoch 024 | loss 7.436 | nll_loss 5.817 | ppl 56.36 | wps 2774.4 | ups 0.23 | wpb 12057.5 | bsz 560 | num_updates 1545 | lr 4.31378e-05 | gnorm 2.784 | clip 100 | loss_scale 0.0156 | train_wall 208 | gb_free 21.9 | wall 6403
2023-05-19 18:23:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:23:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:23:58 | INFO | fairseq.trainer | begin training epoch 25
2023-05-19 18:23:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:24:14 | INFO | train_inner | epoch 025:      5 / 65 loss=7.448, nll_loss=5.83, ppl=56.87, wps=679.9, ups=0.06, wpb=12157.4, bsz=541.8, num_updates=1550, lr=4.31122e-05, gnorm=3.485, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.5, wall=6420
2023-05-19 18:24:30 | INFO | train_inner | epoch 025:     10 / 65 loss=7.471, nll_loss=5.855, ppl=57.87, wps=3907.2, ups=0.33, wpb=11940, bsz=589, num_updates=1555, lr=4.30867e-05, gnorm=2.596, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.5, wall=6435
2023-05-19 18:24:46 | INFO | train_inner | epoch 025:     15 / 65 loss=7.345, nll_loss=5.718, ppl=52.64, wps=3740.8, ups=0.31, wpb=12031.2, bsz=562.8, num_updates=1560, lr=4.30612e-05, gnorm=2.431, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=6451
2023-05-19 18:25:02 | INFO | train_inner | epoch 025:     20 / 65 loss=7.322, nll_loss=5.689, ppl=51.6, wps=3672.9, ups=0.31, wpb=11970, bsz=518, num_updates=1565, lr=4.30357e-05, gnorm=3.048, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.6, wall=6468
2023-05-19 18:25:18 | INFO | train_inner | epoch 025:     25 / 65 loss=7.371, nll_loss=5.746, ppl=53.66, wps=3716.9, ups=0.31, wpb=11930.8, bsz=561.8, num_updates=1570, lr=4.30102e-05, gnorm=2.037, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=6484
2023-05-19 18:25:33 | INFO | train_inner | epoch 025:     30 / 65 loss=7.349, nll_loss=5.709, ppl=52.31, wps=4037.8, ups=0.34, wpb=12023.4, bsz=503.8, num_updates=1575, lr=4.29847e-05, gnorm=3.041, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.3, wall=6499
2023-05-19 18:25:49 | INFO | train_inner | epoch 025:     35 / 65 loss=7.312, nll_loss=5.674, ppl=51.04, wps=3733.6, ups=0.31, wpb=12065.6, bsz=517.4, num_updates=1580, lr=4.29592e-05, gnorm=2.277, clip=100, loss_scale=0.0156, train_wall=16, gb_free=21.8, wall=6515
2023-05-19 18:26:06 | INFO | train_inner | epoch 025:     40 / 65 loss=7.29, nll_loss=5.65, ppl=50.21, wps=3576.8, ups=0.29, wpb=12384.2, bsz=543.8, num_updates=1585, lr=4.29337e-05, gnorm=3.031, clip=100, loss_scale=0.0156, train_wall=17, gb_free=21.5, wall=6532
2023-05-19 18:26:22 | INFO | train_inner | epoch 025:     45 / 65 loss=7.358, nll_loss=5.732, ppl=53.16, wps=4096.8, ups=0.33, wpb=12449, bsz=625.4, num_updates=1590, lr=4.29082e-05, gnorm=2.066, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.8, wall=6547
2023-05-19 18:26:36 | INFO | train_inner | epoch 025:     50 / 65 loss=7.318, nll_loss=5.693, ppl=51.73, wps=4103.1, ups=0.34, wpb=12178, bsz=555.6, num_updates=1595, lr=4.28827e-05, gnorm=7.012, clip=100, loss_scale=0.0156, train_wall=15, gb_free=21.4, wall=6562
2023-05-19 18:26:55 | INFO | train_inner | epoch 025:     55 / 65 loss=7.563, nll_loss=5.951, ppl=61.84, wps=3401.5, ups=0.28, wpb=12287.2, bsz=601.2, num_updates=1600, lr=4.28571e-05, gnorm=2.132, clip=100, loss_scale=0.0156, train_wall=18, gb_free=21.5, wall=6580
2023-05-19 18:27:11 | INFO | train_inner | epoch 025:     60 / 65 loss=7.638, nll_loss=6.02, ppl=64.88, wps=3849.4, ups=0.3, wpb=12730.8, bsz=659.6, num_updates=1605, lr=4.28316e-05, gnorm=2.932, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.2, wall=6597
2023-05-19 18:27:25 | INFO | train_inner | epoch 025:     65 / 65 loss=7.387, nll_loss=5.764, ppl=54.34, wps=3700, ups=0.35, wpb=10596.4, bsz=503.8, num_updates=1610, lr=4.28061e-05, gnorm=3.208, clip=100, loss_scale=0.0312, train_wall=14, gb_free=21.5, wall=6611
2023-05-19 18:27:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:27:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:28:06 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.785 | nll_loss 5.037 | ppl 32.84 | bleu 1.82 | wps 898 | wpb 2785 | bsz 105.2 | num_updates 1610 | best_loss 6.785
2023-05-19 18:28:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1610 updates
2023-05-19 18:28:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint25.pt
2023-05-19 18:28:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint25.pt
2023-05-19 18:28:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint25.pt (epoch 25 @ 1610 updates, score 6.785) (writing took 20.280932568013668 seconds)
2023-05-19 18:28:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-05-19 18:28:26 | INFO | train | epoch 025 | loss 7.399 | nll_loss 5.773 | ppl 54.67 | wps 2922.3 | ups 0.24 | wpb 12057.2 | bsz 560.3 | num_updates 1610 | lr 4.28061e-05 | gnorm 3.023 | clip 100 | loss_scale 0.0312 | train_wall 207 | gb_free 21.5 | wall 6672
2023-05-19 18:28:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:28:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:28:26 | INFO | fairseq.trainer | begin training epoch 26
2023-05-19 18:28:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:28:42 | INFO | train_inner | epoch 026:      5 / 65 loss=7.42, nll_loss=5.784, ppl=55.11, wps=826.8, ups=0.06, wpb=12739.8, bsz=658, num_updates=1615, lr=4.27806e-05, gnorm=1.85, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=6688
2023-05-19 18:28:58 | INFO | train_inner | epoch 026:     10 / 65 loss=7.595, nll_loss=5.963, ppl=62.4, wps=3979.5, ups=0.32, wpb=12339.4, bsz=617.6, num_updates=1620, lr=4.27551e-05, gnorm=3.671, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=6704
2023-05-19 18:29:14 | INFO | train_inner | epoch 026:     15 / 65 loss=7.295, nll_loss=5.646, ppl=50.08, wps=3623.4, ups=0.3, wpb=11935.4, bsz=557, num_updates=1625, lr=4.27296e-05, gnorm=2.527, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=6720
2023-05-19 18:29:31 | INFO | train_inner | epoch 026:     20 / 65 loss=7.372, nll_loss=5.726, ppl=52.95, wps=3688.8, ups=0.3, wpb=12104.4, bsz=556.4, num_updates=1630, lr=4.27041e-05, gnorm=6.367, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=6737
2023-05-19 18:29:47 | INFO | train_inner | epoch 026:     25 / 65 loss=7.296, nll_loss=5.643, ppl=49.99, wps=3711.4, ups=0.32, wpb=11705.2, bsz=542, num_updates=1635, lr=4.26786e-05, gnorm=2.567, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=6752
2023-05-19 18:30:04 | INFO | train_inner | epoch 026:     30 / 65 loss=7.39, nll_loss=5.746, ppl=53.68, wps=3432.7, ups=0.28, wpb=12230, bsz=573.8, num_updates=1640, lr=4.26531e-05, gnorm=2.294, clip=100, loss_scale=0.0312, train_wall=18, gb_free=21.7, wall=6770
2023-05-19 18:30:20 | INFO | train_inner | epoch 026:     35 / 65 loss=7.299, nll_loss=5.653, ppl=50.3, wps=4098.8, ups=0.33, wpb=12457.2, bsz=597.6, num_updates=1645, lr=4.26276e-05, gnorm=2.02, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.4, wall=6785
2023-05-19 18:30:36 | INFO | train_inner | epoch 026:     40 / 65 loss=7.323, nll_loss=5.676, ppl=51.11, wps=3745.6, ups=0.31, wpb=12199.6, bsz=564, num_updates=1650, lr=4.2602e-05, gnorm=88.009, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=6802
2023-05-19 18:30:53 | INFO | train_inner | epoch 026:     45 / 65 loss=7.351, nll_loss=5.703, ppl=52.09, wps=3467.3, ups=0.29, wpb=11828.8, bsz=501, num_updates=1655, lr=4.25765e-05, gnorm=3.699, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.7, wall=6819
2023-05-19 18:31:09 | INFO | train_inner | epoch 026:     50 / 65 loss=7.285, nll_loss=5.637, ppl=49.78, wps=3707, ups=0.3, wpb=12176.8, bsz=536.4, num_updates=1660, lr=4.2551e-05, gnorm=2.001, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.4, wall=6835
2023-05-19 18:31:25 | INFO | train_inner | epoch 026:     55 / 65 loss=7.384, nll_loss=5.742, ppl=53.51, wps=3944.3, ups=0.32, wpb=12350.8, bsz=562.4, num_updates=1665, lr=4.25255e-05, gnorm=3.139, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=6851
2023-05-19 18:31:41 | INFO | train_inner | epoch 026:     60 / 65 loss=7.301, nll_loss=5.657, ppl=50.47, wps=3932.6, ups=0.32, wpb=12379.8, bsz=590.8, num_updates=1670, lr=4.25e-05, gnorm=6.386, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21, wall=6867
2023-05-19 18:31:55 | INFO | train_inner | epoch 026:     65 / 65 loss=7.337, nll_loss=5.688, ppl=51.55, wps=3519.4, ups=0.34, wpb=10286.6, bsz=419.6, num_updates=1675, lr=4.24745e-05, gnorm=5.744, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=6881
2023-05-19 18:31:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:32:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:32:44 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.712 | nll_loss 4.941 | ppl 30.72 | bleu 2.06 | wps 878 | wpb 2785 | bsz 105.2 | num_updates 1675 | best_loss 6.712
2023-05-19 18:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1675 updates
2023-05-19 18:32:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint26.pt
2023-05-19 18:32:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint26.pt
2023-05-19 18:32:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint26.pt (epoch 26 @ 1675 updates, score 6.712) (writing took 10.052481401711702 seconds)
2023-05-19 18:32:57 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-05-19 18:32:57 | INFO | train | epoch 026 | loss 7.358 | nll_loss 5.714 | ppl 52.48 | wps 2892.5 | ups 0.24 | wpb 12056.4 | bsz 559.7 | num_updates 1675 | lr 4.24745e-05 | gnorm 10.021 | clip 100 | loss_scale 0.0312 | train_wall 209 | gb_free 21.5 | wall 6943
2023-05-19 18:32:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:32:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:32:57 | INFO | fairseq.trainer | begin training epoch 27
2023-05-19 18:32:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:33:13 | INFO | train_inner | epoch 027:      5 / 65 loss=7.325, nll_loss=5.682, ppl=51.33, wps=804.4, ups=0.07, wpb=12151.2, bsz=592.2, num_updates=1680, lr=4.2449e-05, gnorm=3.525, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.8, wall=6959
2023-05-19 18:33:28 | INFO | train_inner | epoch 027:     10 / 65 loss=7.337, nll_loss=5.686, ppl=51.49, wps=4235.3, ups=0.34, wpb=12567.2, bsz=580.2, num_updates=1685, lr=4.24235e-05, gnorm=3.94, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.6, wall=6974
2023-05-19 18:33:44 | INFO | train_inner | epoch 027:     15 / 65 loss=7.351, nll_loss=5.702, ppl=52.04, wps=3776.2, ups=0.3, wpb=12396.6, bsz=605.6, num_updates=1690, lr=4.2398e-05, gnorm=3.811, clip=100, loss_scale=0.0312, train_wall=16, gb_free=20.7, wall=6990
2023-05-19 18:34:00 | INFO | train_inner | epoch 027:     20 / 65 loss=7.309, nll_loss=5.651, ppl=50.25, wps=3796.6, ups=0.32, wpb=12029.4, bsz=546.2, num_updates=1695, lr=4.23724e-05, gnorm=2.201, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=7006
2023-05-19 18:34:17 | INFO | train_inner | epoch 027:     25 / 65 loss=7.266, nll_loss=5.604, ppl=48.64, wps=3536.9, ups=0.3, wpb=11813.6, bsz=503.4, num_updates=1700, lr=4.23469e-05, gnorm=3.159, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.6, wall=7023
2023-05-19 18:34:34 | INFO | train_inner | epoch 027:     30 / 65 loss=7.29, nll_loss=5.638, ppl=49.79, wps=3688, ups=0.3, wpb=12209.4, bsz=536, num_updates=1705, lr=4.23214e-05, gnorm=3.053, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.6, wall=7039
2023-05-19 18:34:49 | INFO | train_inner | epoch 027:     35 / 65 loss=7.307, nll_loss=5.638, ppl=49.79, wps=4103.4, ups=0.33, wpb=12324.6, bsz=544.4, num_updates=1710, lr=4.22959e-05, gnorm=2.418, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=7054
2023-05-19 18:35:04 | INFO | train_inner | epoch 027:     40 / 65 loss=7.263, nll_loss=5.603, ppl=48.6, wps=3830.4, ups=0.33, wpb=11690.6, bsz=513.8, num_updates=1715, lr=4.22704e-05, gnorm=2.32, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.8, wall=7070
2023-05-19 18:35:20 | INFO | train_inner | epoch 027:     45 / 65 loss=7.322, nll_loss=5.66, ppl=50.58, wps=4013.6, ups=0.31, wpb=12750.2, bsz=609.2, num_updates=1720, lr=4.22449e-05, gnorm=7.403, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=7085
2023-05-19 18:35:36 | INFO | train_inner | epoch 027:     50 / 65 loss=7.362, nll_loss=5.715, ppl=52.52, wps=3616.9, ups=0.31, wpb=11762.4, bsz=535.8, num_updates=1725, lr=4.22194e-05, gnorm=5.204, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.2, wall=7102
2023-05-19 18:35:52 | INFO | train_inner | epoch 027:     55 / 65 loss=7.662, nll_loss=6.029, ppl=65.32, wps=3966.5, ups=0.32, wpb=12474.4, bsz=684.6, num_updates=1730, lr=4.21939e-05, gnorm=2.89, clip=100, loss_scale=0.0312, train_wall=16, gb_free=20.4, wall=7117
2023-05-19 18:36:07 | INFO | train_inner | epoch 027:     60 / 65 loss=7.194, nll_loss=5.524, ppl=46.02, wps=3892.7, ups=0.32, wpb=12108.8, bsz=547.2, num_updates=1735, lr=4.21684e-05, gnorm=2.66, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=7133
2023-05-19 18:36:22 | INFO | train_inner | epoch 027:     65 / 65 loss=7.247, nll_loss=5.591, ppl=48.21, wps=3468.5, ups=0.34, wpb=10301.8, bsz=444.8, num_updates=1740, lr=4.21429e-05, gnorm=10.22, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.4, wall=7148
2023-05-19 18:36:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:36:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:37:04 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.682 | nll_loss 4.898 | ppl 29.82 | bleu 2.16 | wps 849.4 | wpb 2785 | bsz 105.2 | num_updates 1740 | best_loss 6.682
2023-05-19 18:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1740 updates
2023-05-19 18:37:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint27.pt
2023-05-19 18:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint27.pt
2023-05-19 18:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint27.pt (epoch 27 @ 1740 updates, score 6.682) (writing took 28.29260327294469 seconds)
2023-05-19 18:37:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-05-19 18:37:33 | INFO | train | epoch 027 | loss 7.328 | nll_loss 5.673 | ppl 51.02 | wps 2839.3 | ups 0.24 | wpb 12044.6 | bsz 557.2 | num_updates 1740 | lr 4.21429e-05 | gnorm 4.062 | clip 100 | loss_scale 0.0312 | train_wall 205 | gb_free 21.4 | wall 7218
2023-05-19 18:37:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:37:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:37:33 | INFO | fairseq.trainer | begin training epoch 28
2023-05-19 18:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:37:48 | INFO | train_inner | epoch 028:      5 / 65 loss=7.248, nll_loss=5.593, ppl=48.26, wps=725.7, ups=0.06, wpb=12475, bsz=547.4, num_updates=1745, lr=4.21173e-05, gnorm=3.785, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.7, wall=7234
2023-05-19 18:38:05 | INFO | train_inner | epoch 028:     10 / 65 loss=7.488, nll_loss=5.834, ppl=57.04, wps=3672.3, ups=0.3, wpb=12331.4, bsz=592, num_updates=1750, lr=4.20918e-05, gnorm=2.638, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=7251
2023-05-19 18:38:21 | INFO | train_inner | epoch 028:     15 / 65 loss=7.543, nll_loss=5.889, ppl=59.25, wps=3701.4, ups=0.31, wpb=11926.4, bsz=591.6, num_updates=1755, lr=4.20663e-05, gnorm=3.983, clip=100, loss_scale=0.0312, train_wall=16, gb_free=20.6, wall=7267
2023-05-19 18:38:39 | INFO | train_inner | epoch 028:     20 / 65 loss=7.23, nll_loss=5.561, ppl=47.2, wps=3314.3, ups=0.28, wpb=12036.6, bsz=508.6, num_updates=1760, lr=4.20408e-05, gnorm=3.23, clip=100, loss_scale=0.0312, train_wall=18, gb_free=21.5, wall=7285
2023-05-19 18:38:55 | INFO | train_inner | epoch 028:     25 / 65 loss=7.239, nll_loss=5.582, ppl=47.9, wps=3769.9, ups=0.31, wpb=12215, bsz=538.8, num_updates=1765, lr=4.20153e-05, gnorm=1.686, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=7301
2023-05-19 18:39:12 | INFO | train_inner | epoch 028:     30 / 65 loss=7.246, nll_loss=5.584, ppl=47.97, wps=3633.2, ups=0.3, wpb=12158, bsz=557, num_updates=1770, lr=4.19898e-05, gnorm=2.801, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.7, wall=7318
2023-05-19 18:39:30 | INFO | train_inner | epoch 028:     35 / 65 loss=7.326, nll_loss=5.671, ppl=50.94, wps=3301.5, ups=0.28, wpb=11680.6, bsz=531.8, num_updates=1775, lr=4.19643e-05, gnorm=2.671, clip=100, loss_scale=0.0312, train_wall=18, gb_free=21.5, wall=7336
2023-05-19 18:39:45 | INFO | train_inner | epoch 028:     40 / 65 loss=7.201, nll_loss=5.535, ppl=46.36, wps=4180.5, ups=0.34, wpb=12322, bsz=581.8, num_updates=1780, lr=4.19388e-05, gnorm=4.143, clip=100, loss_scale=0.0312, train_wall=15, gb_free=20.7, wall=7350
2023-05-19 18:40:01 | INFO | train_inner | epoch 028:     45 / 65 loss=7.319, nll_loss=5.668, ppl=50.83, wps=3799.1, ups=0.31, wpb=12229.6, bsz=571.6, num_updates=1785, lr=4.19133e-05, gnorm=3.729, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=7366
2023-05-19 18:40:17 | INFO | train_inner | epoch 028:     50 / 65 loss=7.208, nll_loss=5.545, ppl=46.67, wps=3813.3, ups=0.31, wpb=12465, bsz=613.4, num_updates=1790, lr=4.18878e-05, gnorm=2.398, clip=100, loss_scale=0.0312, train_wall=16, gb_free=20.4, wall=7383
2023-05-19 18:40:34 | INFO | train_inner | epoch 028:     55 / 65 loss=7.287, nll_loss=5.631, ppl=49.55, wps=3600.1, ups=0.3, wpb=12085.8, bsz=558.8, num_updates=1795, lr=4.18622e-05, gnorm=2.869, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.8, wall=7400
2023-05-19 18:40:49 | INFO | train_inner | epoch 028:     60 / 65 loss=7.195, nll_loss=5.523, ppl=45.98, wps=4065.6, ups=0.33, wpb=12273.4, bsz=558, num_updates=1800, lr=4.18367e-05, gnorm=5.108, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.7, wall=7415
2023-05-19 18:41:03 | INFO | train_inner | epoch 028:     65 / 65 loss=7.246, nll_loss=5.586, ppl=48.03, wps=3682.7, ups=0.35, wpb=10426.8, bsz=500.4, num_updates=1805, lr=4.18112e-05, gnorm=3.464, clip=100, loss_scale=0.0312, train_wall=14, gb_free=21.8, wall=7429
2023-05-19 18:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:41:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:41:42 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.652 | nll_loss 4.865 | ppl 29.14 | bleu 2.32 | wps 928.9 | wpb 2785 | bsz 105.2 | num_updates 1805 | best_loss 6.652
2023-05-19 18:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1805 updates
2023-05-19 18:41:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint28.pt
2023-05-19 18:41:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint28.pt
2023-05-19 18:42:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint28.pt (epoch 28 @ 1805 updates, score 6.652) (writing took 22.18655626475811 seconds)
2023-05-19 18:42:04 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-05-19 18:42:04 | INFO | train | epoch 028 | loss 7.29 | nll_loss 5.631 | ppl 49.54 | wps 2884 | ups 0.24 | wpb 12048.1 | bsz 557.8 | num_updates 1805 | lr 4.18112e-05 | gnorm 3.27 | clip 100 | loss_scale 0.0312 | train_wall 210 | gb_free 21.8 | wall 7490
2023-05-19 18:42:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:42:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:42:04 | INFO | fairseq.trainer | begin training epoch 29
2023-05-19 18:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:42:20 | INFO | train_inner | epoch 029:      5 / 65 loss=7.254, nll_loss=5.594, ppl=48.3, wps=773.1, ups=0.06, wpb=11903.4, bsz=529, num_updates=1810, lr=4.17857e-05, gnorm=3.493, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=7506
2023-05-19 18:42:36 | INFO | train_inner | epoch 029:     10 / 65 loss=7.212, nll_loss=5.55, ppl=46.84, wps=3668.6, ups=0.31, wpb=11959.6, bsz=502.6, num_updates=1815, lr=4.17602e-05, gnorm=3.275, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.8, wall=7522
2023-05-19 18:42:53 | INFO | train_inner | epoch 029:     15 / 65 loss=7.215, nll_loss=5.545, ppl=46.7, wps=3655, ups=0.3, wpb=12270.8, bsz=549.6, num_updates=1820, lr=4.17347e-05, gnorm=4.684, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.2, wall=7539
2023-05-19 18:43:09 | INFO | train_inner | epoch 029:     20 / 65 loss=7.227, nll_loss=5.552, ppl=46.92, wps=3855.5, ups=0.32, wpb=12122.8, bsz=544.8, num_updates=1825, lr=4.17092e-05, gnorm=2.938, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=7555
2023-05-19 18:43:25 | INFO | train_inner | epoch 029:     25 / 65 loss=7.487, nll_loss=5.826, ppl=56.72, wps=3664.7, ups=0.3, wpb=12206.2, bsz=616.6, num_updates=1830, lr=4.16837e-05, gnorm=5.289, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.7, wall=7571
2023-05-19 18:43:41 | INFO | train_inner | epoch 029:     30 / 65 loss=7.225, nll_loss=5.555, ppl=47.01, wps=4061, ups=0.33, wpb=12412.4, bsz=599.4, num_updates=1835, lr=4.16582e-05, gnorm=1.894, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.8, wall=7587
2023-05-19 18:43:57 | INFO | train_inner | epoch 029:     35 / 65 loss=7.214, nll_loss=5.546, ppl=46.71, wps=3547.4, ups=0.3, wpb=11845.2, bsz=485, num_updates=1840, lr=4.16327e-05, gnorm=3.01, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.7, wall=7603
2023-05-19 18:44:14 | INFO | train_inner | epoch 029:     40 / 65 loss=7.277, nll_loss=5.619, ppl=49.16, wps=3783.5, ups=0.31, wpb=12338, bsz=593.4, num_updates=1845, lr=4.16071e-05, gnorm=3.918, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=7620
2023-05-19 18:44:29 | INFO | train_inner | epoch 029:     45 / 65 loss=7.218, nll_loss=5.552, ppl=46.92, wps=3980.4, ups=0.32, wpb=12346.4, bsz=585.2, num_updates=1850, lr=4.15816e-05, gnorm=3.402, clip=100, loss_scale=0.0312, train_wall=15, gb_free=20.7, wall=7635
2023-05-19 18:44:45 | INFO | train_inner | epoch 029:     50 / 65 loss=7.201, nll_loss=5.536, ppl=46.4, wps=4095.2, ups=0.32, wpb=12611, bsz=604.8, num_updates=1855, lr=4.15561e-05, gnorm=6.02, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=7650
2023-05-19 18:45:02 | INFO | train_inner | epoch 029:     55 / 65 loss=7.442, nll_loss=5.782, ppl=55.01, wps=3575, ups=0.29, wpb=12203.8, bsz=581.6, num_updates=1860, lr=4.15306e-05, gnorm=3.959, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=7667
2023-05-19 18:45:17 | INFO | train_inner | epoch 029:     60 / 65 loss=7.323, nll_loss=5.67, ppl=50.93, wps=3918.3, ups=0.32, wpb=12344.4, bsz=648, num_updates=1865, lr=4.15051e-05, gnorm=4.918, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=7683
2023-05-19 18:45:31 | INFO | train_inner | epoch 029:     65 / 65 loss=7.256, nll_loss=5.586, ppl=48.02, wps=3891.2, ups=0.38, wpb=10201.2, bsz=436.8, num_updates=1870, lr=4.14796e-05, gnorm=3.933, clip=100, loss_scale=0.0312, train_wall=13, gb_free=20.5, wall=7696
2023-05-19 18:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:45:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:46:13 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.693 | nll_loss 4.912 | ppl 30.12 | bleu 2.42 | wps 838 | wpb 2785 | bsz 105.2 | num_updates 1870 | best_loss 6.652
2023-05-19 18:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1870 updates
2023-05-19 18:46:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint29.pt
2023-05-19 18:46:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint29.pt
2023-05-19 18:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint29.pt (epoch 29 @ 1870 updates, score 6.693) (writing took 6.816880814731121 seconds)
2023-05-19 18:46:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-05-19 18:46:20 | INFO | train | epoch 029 | loss 7.273 | nll_loss 5.609 | ppl 48.81 | wps 3061.8 | ups 0.25 | wpb 12058.9 | bsz 559.8 | num_updates 1870 | lr 4.14796e-05 | gnorm 3.903 | clip 100 | loss_scale 0.0312 | train_wall 206 | gb_free 20.5 | wall 7746
2023-05-19 18:46:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:46:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:46:20 | INFO | fairseq.trainer | begin training epoch 30
2023-05-19 18:46:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:46:35 | INFO | train_inner | epoch 030:      5 / 65 loss=7.254, nll_loss=5.588, ppl=48.11, wps=919.6, ups=0.08, wpb=11873.4, bsz=543, num_updates=1875, lr=4.14541e-05, gnorm=3.451, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.7, wall=7761
2023-05-19 18:46:51 | INFO | train_inner | epoch 030:     10 / 65 loss=7.306, nll_loss=5.657, ppl=50.47, wps=4073.6, ups=0.32, wpb=12612, bsz=665.6, num_updates=1880, lr=4.14286e-05, gnorm=4.063, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.4, wall=7776
2023-05-19 18:47:08 | INFO | train_inner | epoch 030:     15 / 65 loss=7.232, nll_loss=5.566, ppl=47.37, wps=3794.5, ups=0.32, wpb=11809, bsz=511.8, num_updates=1885, lr=4.14031e-05, gnorm=3.977, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=7794
2023-05-19 18:47:24 | INFO | train_inner | epoch 030:     20 / 65 loss=7.25, nll_loss=5.594, ppl=48.29, wps=3823.9, ups=0.31, wpb=12176.4, bsz=548.8, num_updates=1890, lr=4.13776e-05, gnorm=2.938, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=7810
2023-05-19 18:47:40 | INFO | train_inner | epoch 030:     25 / 65 loss=7.18, nll_loss=5.512, ppl=45.63, wps=3682.5, ups=0.31, wpb=11941.6, bsz=529.2, num_updates=1895, lr=4.1352e-05, gnorm=3.732, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=7826
2023-05-19 18:47:57 | INFO | train_inner | epoch 030:     30 / 65 loss=7.221, nll_loss=5.564, ppl=47.31, wps=3649.6, ups=0.29, wpb=12448.4, bsz=579, num_updates=1900, lr=4.13265e-05, gnorm=3.173, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=7843
2023-05-19 18:48:13 | INFO | train_inner | epoch 030:     35 / 65 loss=7.24, nll_loss=5.579, ppl=47.79, wps=3857.7, ups=0.32, wpb=12226.8, bsz=548.6, num_updates=1905, lr=4.1301e-05, gnorm=2.747, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=7859
2023-05-19 18:48:29 | INFO | train_inner | epoch 030:     40 / 65 loss=7.414, nll_loss=5.761, ppl=54.24, wps=4034.9, ups=0.31, wpb=12816, bsz=612.6, num_updates=1910, lr=4.12755e-05, gnorm=4.595, clip=100, loss_scale=0.0312, train_wall=16, gb_free=20.6, wall=7875
2023-05-19 18:48:46 | INFO | train_inner | epoch 030:     45 / 65 loss=7.2, nll_loss=5.528, ppl=46.15, wps=3567.2, ups=0.3, wpb=12087.6, bsz=534.2, num_updates=1915, lr=4.125e-05, gnorm=4.486, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.8, wall=7892
2023-05-19 18:49:01 | INFO | train_inner | epoch 030:     50 / 65 loss=7.433, nll_loss=5.772, ppl=54.65, wps=3925.1, ups=0.32, wpb=12132.8, bsz=604.8, num_updates=1920, lr=4.12245e-05, gnorm=4.211, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=7907
2023-05-19 18:49:16 | INFO | train_inner | epoch 030:     55 / 65 loss=7.302, nll_loss=5.636, ppl=49.72, wps=4281.7, ups=0.35, wpb=12219.6, bsz=554.6, num_updates=1925, lr=4.1199e-05, gnorm=11.797, clip=100, loss_scale=0.0312, train_wall=14, gb_free=21.3, wall=7921
2023-05-19 18:49:31 | INFO | train_inner | epoch 030:     60 / 65 loss=7.22, nll_loss=5.558, ppl=47.13, wps=3908, ups=0.33, wpb=11843, bsz=574.4, num_updates=1930, lr=4.11735e-05, gnorm=3.343, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.6, wall=7936
2023-05-19 18:49:45 | INFO | train_inner | epoch 030:     65 / 65 loss=7.083, nll_loss=5.393, ppl=42.01, wps=3621.3, ups=0.34, wpb=10527, bsz=459.6, num_updates=1935, lr=4.1148e-05, gnorm=4.554, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.7, wall=7951
2023-05-19 18:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:49:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:50:26 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.641 | nll_loss 4.851 | ppl 28.86 | bleu 2.51 | wps 874.3 | wpb 2785 | bsz 105.2 | num_updates 1935 | best_loss 6.641
2023-05-19 18:50:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1935 updates
2023-05-19 18:50:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint30.pt
2023-05-19 18:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint30.pt
2023-05-19 18:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint30.pt (epoch 30 @ 1935 updates, score 6.641) (writing took 28.645683128386736 seconds)
2023-05-19 18:50:55 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-05-19 18:50:55 | INFO | train | epoch 030 | loss 7.259 | nll_loss 5.596 | ppl 48.37 | wps 2851 | ups 0.24 | wpb 12054.9 | bsz 558.9 | num_updates 1935 | lr 4.1148e-05 | gnorm 4.39 | clip 100 | loss_scale 0.0312 | train_wall 203 | gb_free 21.7 | wall 8021
2023-05-19 18:50:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:50:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:50:55 | INFO | fairseq.trainer | begin training epoch 31
2023-05-19 18:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:51:12 | INFO | train_inner | epoch 031:      5 / 65 loss=7.193, nll_loss=5.521, ppl=45.92, wps=704.8, ups=0.06, wpb=12278.6, bsz=525, num_updates=1940, lr=4.11224e-05, gnorm=4.301, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.4, wall=8038
2023-05-19 18:51:29 | INFO | train_inner | epoch 031:     10 / 65 loss=7.242, nll_loss=5.566, ppl=47.38, wps=3897.9, ups=0.31, wpb=12590.2, bsz=580.8, num_updates=1945, lr=4.10969e-05, gnorm=4.214, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.3, wall=8054
2023-05-19 18:51:44 | INFO | train_inner | epoch 031:     15 / 65 loss=7.301, nll_loss=5.644, ppl=49.99, wps=3798.8, ups=0.31, wpb=12061.2, bsz=590, num_updates=1950, lr=4.10714e-05, gnorm=4.882, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=8070
2023-05-19 18:52:02 | INFO | train_inner | epoch 031:     20 / 65 loss=7.158, nll_loss=5.482, ppl=44.68, wps=3407.8, ups=0.29, wpb=11697.4, bsz=504.2, num_updates=1955, lr=4.10459e-05, gnorm=2.978, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=8087
2023-05-19 18:52:17 | INFO | train_inner | epoch 031:     25 / 65 loss=7.107, nll_loss=5.425, ppl=42.97, wps=3914.6, ups=0.31, wpb=12453.8, bsz=563.2, num_updates=1960, lr=4.10204e-05, gnorm=2.807, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=8103
2023-05-19 18:52:36 | INFO | train_inner | epoch 031:     30 / 65 loss=7.37, nll_loss=5.708, ppl=52.29, wps=3345.7, ups=0.27, wpb=12437.8, bsz=606.6, num_updates=1965, lr=4.09949e-05, gnorm=3.486, clip=100, loss_scale=0.0312, train_wall=19, gb_free=20.9, wall=8122
2023-05-19 18:52:51 | INFO | train_inner | epoch 031:     35 / 65 loss=7.194, nll_loss=5.524, ppl=46.02, wps=4204.9, ups=0.34, wpb=12329.4, bsz=599, num_updates=1970, lr=4.09694e-05, gnorm=2.764, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.4, wall=8136
2023-05-19 18:53:08 | INFO | train_inner | epoch 031:     40 / 65 loss=7.181, nll_loss=5.503, ppl=45.35, wps=3423.5, ups=0.29, wpb=11623, bsz=477.6, num_updates=1975, lr=4.09439e-05, gnorm=2.585, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=8153
2023-05-19 18:53:25 | INFO | train_inner | epoch 031:     45 / 65 loss=7.453, nll_loss=5.79, ppl=55.32, wps=3525.6, ups=0.29, wpb=12205.8, bsz=598.6, num_updates=1980, lr=4.09184e-05, gnorm=3.695, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.2, wall=8171
2023-05-19 18:53:40 | INFO | train_inner | epoch 031:     50 / 65 loss=7.234, nll_loss=5.569, ppl=47.48, wps=4254.4, ups=0.34, wpb=12539, bsz=626.6, num_updates=1985, lr=4.08929e-05, gnorm=9.204, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.6, wall=8186
2023-05-19 18:53:56 | INFO | train_inner | epoch 031:     55 / 65 loss=7.221, nll_loss=5.557, ppl=47.09, wps=3808.3, ups=0.31, wpb=12114.6, bsz=555.2, num_updates=1990, lr=4.08673e-05, gnorm=2.395, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.3, wall=8201
2023-05-19 18:54:11 | INFO | train_inner | epoch 031:     60 / 65 loss=7.192, nll_loss=5.523, ppl=45.99, wps=3807.8, ups=0.32, wpb=11923.8, bsz=536, num_updates=1995, lr=4.08418e-05, gnorm=4.766, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.9, wall=8217
2023-05-19 18:54:24 | INFO | train_inner | epoch 031:     65 / 65 loss=7.125, nll_loss=5.447, ppl=43.64, wps=4071.6, ups=0.39, wpb=10449.4, bsz=489.4, num_updates=2000, lr=4.08163e-05, gnorm=5.966, clip=100, loss_scale=0.0312, train_wall=13, gb_free=20.6, wall=8230
2023-05-19 18:54:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:54:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:55:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.599 | nll_loss 4.796 | ppl 27.79 | bleu 2.66 | wps 838.4 | wpb 2785 | bsz 105.2 | num_updates 2000 | best_loss 6.599
2023-05-19 18:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2000 updates
2023-05-19 18:55:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint31.pt
2023-05-19 18:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint31.pt
2023-05-19 18:55:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint31.pt (epoch 31 @ 2000 updates, score 6.599) (writing took 25.6961094327271 seconds)
2023-05-19 18:55:33 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-05-19 18:55:33 | INFO | train | epoch 031 | loss 7.23 | nll_loss 5.56 | ppl 47.18 | wps 2821.4 | ups 0.23 | wpb 12054.2 | bsz 557.9 | num_updates 2000 | lr 4.08163e-05 | gnorm 4.157 | clip 100 | loss_scale 0.0312 | train_wall 209 | gb_free 20.6 | wall 8298
2023-05-19 18:55:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:55:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 18:55:33 | INFO | fairseq.trainer | begin training epoch 32
2023-05-19 18:55:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 18:55:49 | INFO | train_inner | epoch 032:      5 / 65 loss=7.16, nll_loss=5.483, ppl=44.73, wps=715.9, ups=0.06, wpb=12093, bsz=541, num_updates=2005, lr=4.07908e-05, gnorm=4.52, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.8, wall=8314
2023-05-19 18:56:05 | INFO | train_inner | epoch 032:     10 / 65 loss=7.157, nll_loss=5.481, ppl=44.67, wps=3681.7, ups=0.3, wpb=12200.8, bsz=552.6, num_updates=2010, lr=4.07653e-05, gnorm=4.591, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.6, wall=8331
2023-05-19 18:56:21 | INFO | train_inner | epoch 032:     15 / 65 loss=7.218, nll_loss=5.543, ppl=46.61, wps=3953.6, ups=0.32, wpb=12329.6, bsz=572.8, num_updates=2015, lr=4.07398e-05, gnorm=6.785, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=8347
2023-05-19 18:56:36 | INFO | train_inner | epoch 032:     20 / 65 loss=7.228, nll_loss=5.559, ppl=47.14, wps=3974.5, ups=0.32, wpb=12473.8, bsz=563.8, num_updates=2020, lr=4.07143e-05, gnorm=3.122, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=8362
2023-05-19 18:56:52 | INFO | train_inner | epoch 032:     25 / 65 loss=7.085, nll_loss=5.397, ppl=42.14, wps=3686.6, ups=0.31, wpb=11802.6, bsz=551.6, num_updates=2025, lr=4.06888e-05, gnorm=4.175, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.5, wall=8378
2023-05-19 18:57:09 | INFO | train_inner | epoch 032:     30 / 65 loss=7.175, nll_loss=5.487, ppl=44.85, wps=3649.8, ups=0.31, wpb=11898.2, bsz=501.4, num_updates=2030, lr=4.06633e-05, gnorm=2.664, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=8395
2023-05-19 18:57:25 | INFO | train_inner | epoch 032:     35 / 65 loss=7.12, nll_loss=5.433, ppl=43.19, wps=3713.7, ups=0.3, wpb=12212.4, bsz=529.8, num_updates=2035, lr=4.06378e-05, gnorm=4.865, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=8411
2023-05-19 18:57:43 | INFO | train_inner | epoch 032:     40 / 65 loss=7.423, nll_loss=5.757, ppl=54.1, wps=3404.9, ups=0.27, wpb=12409.6, bsz=592.2, num_updates=2040, lr=4.06122e-05, gnorm=4.738, clip=100, loss_scale=0.0312, train_wall=18, gb_free=21.7, wall=8429
2023-05-19 18:58:01 | INFO | train_inner | epoch 032:     45 / 65 loss=7.245, nll_loss=5.58, ppl=47.83, wps=3442.5, ups=0.29, wpb=12072, bsz=566, num_updates=2045, lr=4.05867e-05, gnorm=7.917, clip=100, loss_scale=0.0312, train_wall=18, gb_free=20.4, wall=8447
2023-05-19 18:58:16 | INFO | train_inner | epoch 032:     50 / 65 loss=7.35, nll_loss=5.684, ppl=51.42, wps=3914.2, ups=0.33, wpb=11837.2, bsz=562.6, num_updates=2050, lr=4.05612e-05, gnorm=3.038, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.4, wall=8462
2023-05-19 18:58:34 | INFO | train_inner | epoch 032:     55 / 65 loss=7.198, nll_loss=5.52, ppl=45.88, wps=3511.9, ups=0.28, wpb=12396.4, bsz=571, num_updates=2055, lr=4.05357e-05, gnorm=5.09, clip=100, loss_scale=0.0312, train_wall=18, gb_free=20.6, wall=8480
2023-05-19 18:58:51 | INFO | train_inner | epoch 032:     60 / 65 loss=7.182, nll_loss=5.508, ppl=45.49, wps=3426.4, ups=0.28, wpb=12081, bsz=608.2, num_updates=2060, lr=4.05102e-05, gnorm=7.54, clip=100, loss_scale=0.0312, train_wall=18, gb_free=21.7, wall=8497
2023-05-19 18:59:04 | INFO | train_inner | epoch 032:     65 / 65 loss=7.225, nll_loss=5.559, ppl=47.14, wps=4188.3, ups=0.38, wpb=10941.6, bsz=567.6, num_updates=2065, lr=4.04847e-05, gnorm=7.138, clip=100, loss_scale=0.0312, train_wall=13, gb_free=21.6, wall=8510
2023-05-19 18:59:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 18:59:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 18:59:44 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.561 | nll_loss 4.762 | ppl 27.13 | bleu 2.44 | wps 900.8 | wpb 2785 | bsz 105.2 | num_updates 2065 | best_loss 6.561
2023-05-19 18:59:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2065 updates
2023-05-19 18:59:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint32.pt
2023-05-19 18:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint32.pt
2023-05-19 19:00:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint32.pt (epoch 32 @ 2065 updates, score 6.561) (writing took 26.4716756939888 seconds)
2023-05-19 19:00:11 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-05-19 19:00:11 | INFO | train | epoch 032 | loss 7.213 | nll_loss 5.538 | ppl 46.46 | wps 2815.9 | ups 0.23 | wpb 12057.6 | bsz 560 | num_updates 2065 | lr 4.04847e-05 | gnorm 5.091 | clip 100 | loss_scale 0.0312 | train_wall 211 | gb_free 21.6 | wall 8577
2023-05-19 19:00:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:00:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:00:11 | INFO | fairseq.trainer | begin training epoch 33
2023-05-19 19:00:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:00:27 | INFO | train_inner | epoch 033:      5 / 65 loss=7.134, nll_loss=5.454, ppl=43.85, wps=731.8, ups=0.06, wpb=12086, bsz=549.8, num_updates=2070, lr=4.04592e-05, gnorm=6.872, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.4, wall=8593
2023-05-19 19:00:43 | INFO | train_inner | epoch 033:     10 / 65 loss=7.15, nll_loss=5.474, ppl=44.46, wps=3750.4, ups=0.31, wpb=12261.6, bsz=537.6, num_updates=2075, lr=4.04337e-05, gnorm=3.905, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=8609
2023-05-19 19:01:00 | INFO | train_inner | epoch 033:     15 / 65 loss=7.225, nll_loss=5.548, ppl=46.8, wps=3642.9, ups=0.31, wpb=11834.4, bsz=530.2, num_updates=2080, lr=4.04082e-05, gnorm=4.265, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.6, wall=8625
2023-05-19 19:01:16 | INFO | train_inner | epoch 033:     20 / 65 loss=7.189, nll_loss=5.51, ppl=45.57, wps=3873, ups=0.31, wpb=12339.4, bsz=540.6, num_updates=2085, lr=4.03827e-05, gnorm=6.792, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.8, wall=8641
2023-05-19 19:01:31 | INFO | train_inner | epoch 033:     25 / 65 loss=7.376, nll_loss=5.702, ppl=52.07, wps=3965, ups=0.32, wpb=12311.6, bsz=620.6, num_updates=2090, lr=4.03571e-05, gnorm=13.514, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.7, wall=8657
2023-05-19 19:01:46 | INFO | train_inner | epoch 033:     30 / 65 loss=7.174, nll_loss=5.501, ppl=45.28, wps=4072.6, ups=0.34, wpb=12147.4, bsz=620.6, num_updates=2095, lr=4.03316e-05, gnorm=5.45, clip=100, loss_scale=0.0312, train_wall=15, gb_free=21.5, wall=8672
2023-05-19 19:02:01 | INFO | train_inner | epoch 033:     35 / 65 loss=7.234, nll_loss=5.566, ppl=47.37, wps=4180.5, ups=0.35, wpb=12100.8, bsz=593.4, num_updates=2100, lr=4.03061e-05, gnorm=3.631, clip=100, loss_scale=0.0312, train_wall=14, gb_free=20.4, wall=8686
2023-05-19 19:02:17 | INFO | train_inner | epoch 033:     40 / 65 loss=7.194, nll_loss=5.525, ppl=46.04, wps=3693.3, ups=0.3, wpb=12205.8, bsz=521.4, num_updates=2105, lr=4.02806e-05, gnorm=3.263, clip=100, loss_scale=0.0312, train_wall=17, gb_free=21.5, wall=8703
2023-05-19 19:02:33 | INFO | train_inner | epoch 033:     45 / 65 loss=7.139, nll_loss=5.464, ppl=44.13, wps=3741.2, ups=0.31, wpb=12085.6, bsz=550.2, num_updates=2110, lr=4.02551e-05, gnorm=9.59, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.4, wall=8719
2023-05-19 19:02:49 | INFO | train_inner | epoch 033:     50 / 65 loss=7.254, nll_loss=5.581, ppl=47.87, wps=3803.9, ups=0.31, wpb=12302.4, bsz=568.4, num_updates=2115, lr=4.02296e-05, gnorm=5.526, clip=100, loss_scale=0.0312, train_wall=16, gb_free=21.2, wall=8735
2023-05-19 19:03:04 | INFO | train_inner | epoch 033:     55 / 65 loss=7.13, nll_loss=5.446, ppl=43.6, wps=4085, ups=0.33, wpb=12285, bsz=577.8, num_updates=2120, lr=4.02041e-05, gnorm=4.922, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.4, wall=8750
2023-05-19 19:03:21 | INFO | train_inner | epoch 033:     60 / 65 loss=7.124, nll_loss=5.441, ppl=43.44, wps=3751.3, ups=0.31, wpb=12198.4, bsz=521.6, num_updates=2125, lr=4.01786e-05, gnorm=11.225, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=8766
2023-05-19 19:03:34 | INFO | train_inner | epoch 033:     65 / 65 loss=7.319, nll_loss=5.65, ppl=50.23, wps=4089.1, ups=0.39, wpb=10528.2, bsz=546.2, num_updates=2130, lr=4.01531e-05, gnorm=10.408, clip=100, loss_scale=0.0625, train_wall=13, gb_free=20.7, wall=8779
2023-05-19 19:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:03:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:04:13 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.542 | nll_loss 4.738 | ppl 26.69 | bleu 2.63 | wps 902.7 | wpb 2785 | bsz 105.2 | num_updates 2130 | best_loss 6.542
2023-05-19 19:04:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2130 updates
2023-05-19 19:04:13 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint33.pt
2023-05-19 19:04:18 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint33.pt
2023-05-19 19:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint33.pt (epoch 33 @ 2130 updates, score 6.542) (writing took 10.324361439794302 seconds)
2023-05-19 19:04:28 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-05-19 19:04:30 | INFO | train | epoch 033 | loss 7.202 | nll_loss 5.527 | ppl 46.1 | wps 3027.5 | ups 0.25 | wpb 12052.8 | bsz 559.9 | num_updates 2130 | lr 4.01531e-05 | gnorm 6.874 | clip 100 | loss_scale 0.0625 | train_wall 202 | gb_free 20.7 | wall 8836
2023-05-19 19:04:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:04:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:04:32 | INFO | fairseq.trainer | begin training epoch 34
2023-05-19 19:04:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:04:48 | INFO | train_inner | epoch 034:      5 / 65 loss=7.152, nll_loss=5.476, ppl=44.52, wps=794.7, ups=0.07, wpb=11903.4, bsz=538.2, num_updates=2135, lr=4.01276e-05, gnorm=5.359, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=8854
2023-05-19 19:05:04 | INFO | train_inner | epoch 034:     10 / 65 loss=7.166, nll_loss=5.489, ppl=44.93, wps=4092.7, ups=0.33, wpb=12502.2, bsz=607, num_updates=2140, lr=4.0102e-05, gnorm=6.901, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.9, wall=8870
2023-05-19 19:05:20 | INFO | train_inner | epoch 034:     15 / 65 loss=7.185, nll_loss=5.509, ppl=45.55, wps=3616.6, ups=0.3, wpb=11894, bsz=586, num_updates=2145, lr=4.00765e-05, gnorm=3.842, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=8886
2023-05-19 19:05:37 | INFO | train_inner | epoch 034:     20 / 65 loss=7.149, nll_loss=5.467, ppl=44.24, wps=3802.3, ups=0.3, wpb=12609.2, bsz=600.8, num_updates=2150, lr=4.0051e-05, gnorm=5.305, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=8903
2023-05-19 19:05:53 | INFO | train_inner | epoch 034:     25 / 65 loss=7.131, nll_loss=5.447, ppl=43.64, wps=3606.4, ups=0.31, wpb=11753.6, bsz=512.2, num_updates=2155, lr=4.00255e-05, gnorm=16.303, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.7, wall=8919
2023-05-19 19:06:09 | INFO | train_inner | epoch 034:     30 / 65 loss=7.235, nll_loss=5.559, ppl=47.13, wps=3832, ups=0.31, wpb=12358.6, bsz=555.8, num_updates=2160, lr=4e-05, gnorm=4.903, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=8935
2023-05-19 19:06:26 | INFO | train_inner | epoch 034:     35 / 65 loss=7.113, nll_loss=5.431, ppl=43.13, wps=3681.8, ups=0.3, wpb=12431, bsz=579, num_updates=2165, lr=3.99745e-05, gnorm=6.734, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=8952
2023-05-19 19:06:41 | INFO | train_inner | epoch 034:     40 / 65 loss=7.164, nll_loss=5.486, ppl=44.81, wps=4102.8, ups=0.33, wpb=12466.8, bsz=557.4, num_updates=2170, lr=3.9949e-05, gnorm=5.332, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=8967
2023-05-19 19:06:57 | INFO | train_inner | epoch 034:     45 / 65 loss=7.569, nll_loss=5.902, ppl=59.82, wps=3900.5, ups=0.32, wpb=12226.4, bsz=656.2, num_updates=2175, lr=3.99235e-05, gnorm=6.528, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=8983
2023-05-19 19:07:12 | INFO | train_inner | epoch 034:     50 / 65 loss=7.213, nll_loss=5.535, ppl=46.36, wps=3899.9, ups=0.33, wpb=11945.4, bsz=582.8, num_updates=2180, lr=3.9898e-05, gnorm=3.119, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=8998
2023-05-19 19:07:28 | INFO | train_inner | epoch 034:     55 / 65 loss=7.136, nll_loss=5.46, ppl=44.02, wps=3752.7, ups=0.31, wpb=12014.6, bsz=546.8, num_updates=2185, lr=3.98724e-05, gnorm=5.995, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=9014
2023-05-19 19:07:45 | INFO | train_inner | epoch 034:     60 / 65 loss=7.126, nll_loss=5.447, ppl=43.62, wps=3691.9, ups=0.3, wpb=12465.6, bsz=547.2, num_updates=2190, lr=3.98469e-05, gnorm=8.261, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=9031
2023-05-19 19:08:02 | INFO | train_inner | epoch 034:     65 / 65 loss=7.111, nll_loss=5.42, ppl=42.81, wps=3043.6, ups=0.3, wpb=10162, bsz=398.4, num_updates=2195, lr=3.98214e-05, gnorm=3.972, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=9048
2023-05-19 19:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:08:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:08:43 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.568 | nll_loss 4.762 | ppl 27.14 | bleu 2.49 | wps 863.7 | wpb 2785 | bsz 105.2 | num_updates 2195 | best_loss 6.542
2023-05-19 19:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2195 updates
2023-05-19 19:08:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint34.pt
2023-05-19 19:08:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint34.pt
2023-05-19 19:08:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint34.pt (epoch 34 @ 2195 updates, score 6.568) (writing took 9.390712223947048 seconds)
2023-05-19 19:08:53 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-05-19 19:08:53 | INFO | train | epoch 034 | loss 7.19 | nll_loss 5.511 | ppl 45.6 | wps 2989.1 | ups 0.25 | wpb 12056.4 | bsz 559.1 | num_updates 2195 | lr 3.98214e-05 | gnorm 6.35 | clip 100 | loss_scale 0.0625 | train_wall 209 | gb_free 21.8 | wall 9099
2023-05-19 19:08:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:08:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:08:53 | INFO | fairseq.trainer | begin training epoch 35
2023-05-19 19:08:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:09:10 | INFO | train_inner | epoch 035:      5 / 65 loss=7.137, nll_loss=5.45, ppl=43.71, wps=857.9, ups=0.07, wpb=11751.6, bsz=501.4, num_updates=2200, lr=3.97959e-05, gnorm=5.977, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=9116
2023-05-19 19:09:26 | INFO | train_inner | epoch 035:     10 / 65 loss=7.074, nll_loss=5.381, ppl=41.68, wps=4018.7, ups=0.33, wpb=12251, bsz=533, num_updates=2205, lr=3.97704e-05, gnorm=3.115, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=9131
2023-05-19 19:09:40 | INFO | train_inner | epoch 035:     15 / 65 loss=7.176, nll_loss=5.503, ppl=45.34, wps=4198.3, ups=0.34, wpb=12470.4, bsz=630, num_updates=2210, lr=3.97449e-05, gnorm=3.464, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=9146
2023-05-19 19:09:57 | INFO | train_inner | epoch 035:     20 / 65 loss=7.081, nll_loss=5.398, ppl=42.15, wps=3681.1, ups=0.31, wpb=11948.8, bsz=526.2, num_updates=2215, lr=3.97194e-05, gnorm=3.636, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=9162
2023-05-19 19:10:12 | INFO | train_inner | epoch 035:     25 / 65 loss=7.267, nll_loss=5.598, ppl=48.44, wps=4022.8, ups=0.32, wpb=12499.6, bsz=642, num_updates=2220, lr=3.96939e-05, gnorm=5.544, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.8, wall=9178
2023-05-19 19:10:28 | INFO | train_inner | epoch 035:     30 / 65 loss=7.12, nll_loss=5.44, ppl=43.42, wps=3879.1, ups=0.32, wpb=12140, bsz=544.8, num_updates=2225, lr=3.96684e-05, gnorm=4.68, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=9194
2023-05-19 19:10:46 | INFO | train_inner | epoch 035:     35 / 65 loss=7.182, nll_loss=5.5, ppl=45.26, wps=3294.3, ups=0.27, wpb=12019.8, bsz=522.8, num_updates=2230, lr=3.96429e-05, gnorm=2.463, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.6, wall=9212
2023-05-19 19:11:02 | INFO | train_inner | epoch 035:     40 / 65 loss=7.12, nll_loss=5.434, ppl=43.24, wps=3777.1, ups=0.31, wpb=12120, bsz=541.4, num_updates=2235, lr=3.96173e-05, gnorm=7.176, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.5, wall=9228
2023-05-19 19:11:19 | INFO | train_inner | epoch 035:     45 / 65 loss=7.128, nll_loss=5.444, ppl=43.53, wps=3482.5, ups=0.29, wpb=11978.4, bsz=503.6, num_updates=2240, lr=3.95918e-05, gnorm=3.692, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.1, wall=9245
2023-05-19 19:11:35 | INFO | train_inner | epoch 035:     50 / 65 loss=7.47, nll_loss=5.813, ppl=56.2, wps=3972.5, ups=0.31, wpb=12763.2, bsz=739, num_updates=2245, lr=3.95663e-05, gnorm=3.52, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21, wall=9261
2023-05-19 19:11:52 | INFO | train_inner | epoch 035:     55 / 65 loss=7.169, nll_loss=5.493, ppl=45.03, wps=3801.5, ups=0.31, wpb=12248.4, bsz=551.2, num_updates=2250, lr=3.95408e-05, gnorm=3.287, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=9277
2023-05-19 19:12:07 | INFO | train_inner | epoch 035:     60 / 65 loss=7.303, nll_loss=5.638, ppl=49.79, wps=3868.4, ups=0.32, wpb=12246, bsz=579.4, num_updates=2255, lr=3.95153e-05, gnorm=4.345, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=9293
2023-05-19 19:12:23 | INFO | train_inner | epoch 035:     65 / 65 loss=7.132, nll_loss=5.443, ppl=43.51, wps=3363.5, ups=0.33, wpb=10271.2, bsz=458.2, num_updates=2260, lr=3.94898e-05, gnorm=5.34, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=9308
2023-05-19 19:12:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:12:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:13:04 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.537 | nll_loss 4.724 | ppl 26.43 | bleu 2.47 | wps 879.1 | wpb 2785 | bsz 105.2 | num_updates 2260 | best_loss 6.537
2023-05-19 19:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2260 updates
2023-05-19 19:13:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint35.pt
2023-05-19 19:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint35.pt
2023-05-19 19:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint35.pt (epoch 35 @ 2260 updates, score 6.537) (writing took 31.829436887055635 seconds)
2023-05-19 19:13:35 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2023-05-19 19:13:35 | INFO | train | epoch 035 | loss 7.184 | nll_loss 5.505 | ppl 45.42 | wps 2773.3 | ups 0.23 | wpb 12054.5 | bsz 559.5 | num_updates 2260 | lr 3.94898e-05 | gnorm 4.326 | clip 100 | loss_scale 0.0625 | train_wall 209 | gb_free 21.7 | wall 9381
2023-05-19 19:13:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:13:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:13:35 | INFO | fairseq.trainer | begin training epoch 36
2023-05-19 19:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:13:50 | INFO | train_inner | epoch 036:      5 / 65 loss=7.126, nll_loss=5.442, ppl=43.46, wps=708.9, ups=0.06, wpb=12377.6, bsz=633.6, num_updates=2265, lr=3.94643e-05, gnorm=7.092, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.6, wall=9396
2023-05-19 19:14:05 | INFO | train_inner | epoch 036:     10 / 65 loss=7.189, nll_loss=5.508, ppl=45.52, wps=3869.3, ups=0.32, wpb=12011.6, bsz=566.4, num_updates=2270, lr=3.94388e-05, gnorm=2.562, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=9411
2023-05-19 19:14:21 | INFO | train_inner | epoch 036:     15 / 65 loss=7.22, nll_loss=5.546, ppl=46.71, wps=3871.8, ups=0.32, wpb=12033, bsz=564.2, num_updates=2275, lr=3.94133e-05, gnorm=3.013, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=9427
2023-05-19 19:14:37 | INFO | train_inner | epoch 036:     20 / 65 loss=7.333, nll_loss=5.656, ppl=50.44, wps=3839.2, ups=0.31, wpb=12375.2, bsz=593.6, num_updates=2280, lr=3.93878e-05, gnorm=3.477, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=9443
2023-05-19 19:14:53 | INFO | train_inner | epoch 036:     25 / 65 loss=7.048, nll_loss=5.354, ppl=40.89, wps=3625.3, ups=0.31, wpb=11732.2, bsz=471, num_updates=2285, lr=3.93622e-05, gnorm=7.494, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=9459
2023-05-19 19:15:08 | INFO | train_inner | epoch 036:     30 / 65 loss=7.095, nll_loss=5.414, ppl=42.63, wps=4018.8, ups=0.33, wpb=12165.8, bsz=570.8, num_updates=2290, lr=3.93367e-05, gnorm=7.405, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=9474
2023-05-19 19:15:24 | INFO | train_inner | epoch 036:     35 / 65 loss=7.266, nll_loss=5.589, ppl=48.12, wps=3784.8, ups=0.31, wpb=12130.6, bsz=561.8, num_updates=2295, lr=3.93112e-05, gnorm=3.536, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=9490
2023-05-19 19:15:41 | INFO | train_inner | epoch 036:     40 / 65 loss=7.112, nll_loss=5.428, ppl=43.04, wps=3709.9, ups=0.3, wpb=12575, bsz=607, num_updates=2300, lr=3.92857e-05, gnorm=3.683, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=9507
2023-05-19 19:15:58 | INFO | train_inner | epoch 036:     45 / 65 loss=7.126, nll_loss=5.444, ppl=43.54, wps=3716, ups=0.3, wpb=12419.2, bsz=551, num_updates=2305, lr=3.92602e-05, gnorm=6.102, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=9524
2023-05-19 19:16:13 | INFO | train_inner | epoch 036:     50 / 65 loss=7.094, nll_loss=5.402, ppl=42.28, wps=4096.6, ups=0.33, wpb=12295.2, bsz=532.6, num_updates=2310, lr=3.92347e-05, gnorm=4.45, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=9539
2023-05-19 19:16:31 | INFO | train_inner | epoch 036:     55 / 65 loss=7.136, nll_loss=5.453, ppl=43.8, wps=3363, ups=0.28, wpb=12102, bsz=556, num_updates=2315, lr=3.92092e-05, gnorm=3.292, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.8, wall=9557
2023-05-19 19:16:47 | INFO | train_inner | epoch 036:     60 / 65 loss=7.227, nll_loss=5.549, ppl=46.82, wps=3906.3, ups=0.32, wpb=12239, bsz=577.2, num_updates=2320, lr=3.91837e-05, gnorm=9.868, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=9573
2023-05-19 19:17:01 | INFO | train_inner | epoch 036:     65 / 65 loss=7.192, nll_loss=5.518, ppl=45.83, wps=3692.3, ups=0.36, wpb=10269.8, bsz=484.6, num_updates=2325, lr=3.91582e-05, gnorm=8.242, clip=100, loss_scale=0.0625, train_wall=14, gb_free=20.4, wall=9586
2023-05-19 19:17:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:17:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:17:41 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.574 | nll_loss 4.772 | ppl 27.33 | bleu 2.68 | wps 890.9 | wpb 2785 | bsz 105.2 | num_updates 2325 | best_loss 6.537
2023-05-19 19:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2325 updates
2023-05-19 19:17:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint36.pt
2023-05-19 19:17:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint36.pt
2023-05-19 19:17:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint36.pt (epoch 36 @ 2325 updates, score 6.574) (writing took 6.859850078821182 seconds)
2023-05-19 19:17:48 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2023-05-19 19:17:48 | INFO | train | epoch 036 | loss 7.166 | nll_loss 5.485 | ppl 44.77 | wps 3102.6 | ups 0.26 | wpb 12055.9 | bsz 559.2 | num_updates 2325 | lr 3.91582e-05 | gnorm 5.401 | clip 100 | loss_scale 0.0625 | train_wall 205 | gb_free 20.4 | wall 9634
2023-05-19 19:17:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:17:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:17:48 | INFO | fairseq.trainer | begin training epoch 37
2023-05-19 19:17:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:18:05 | INFO | train_inner | epoch 037:      5 / 65 loss=7.156, nll_loss=5.474, ppl=44.44, wps=980.5, ups=0.08, wpb=12521.8, bsz=541, num_updates=2330, lr=3.91327e-05, gnorm=2.729, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=9650
2023-05-19 19:18:21 | INFO | train_inner | epoch 037:     10 / 65 loss=7.347, nll_loss=5.687, ppl=51.53, wps=3860.1, ups=0.31, wpb=12439.8, bsz=651.2, num_updates=2335, lr=3.91071e-05, gnorm=5.767, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=9666
2023-05-19 19:18:37 | INFO | train_inner | epoch 037:     15 / 65 loss=7.137, nll_loss=5.463, ppl=44.1, wps=3831, ups=0.31, wpb=12202.2, bsz=533.2, num_updates=2340, lr=3.90816e-05, gnorm=3.136, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=9682
2023-05-19 19:18:54 | INFO | train_inner | epoch 037:     20 / 65 loss=7.075, nll_loss=5.392, ppl=42, wps=3327.7, ups=0.28, wpb=11825.4, bsz=524.8, num_updates=2345, lr=3.90561e-05, gnorm=4.431, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.1, wall=9700
2023-05-19 19:19:10 | INFO | train_inner | epoch 037:     25 / 65 loss=7.054, nll_loss=5.365, ppl=41.2, wps=3861, ups=0.32, wpb=12228.4, bsz=572, num_updates=2350, lr=3.90306e-05, gnorm=6.614, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.4, wall=9716
2023-05-19 19:19:25 | INFO | train_inner | epoch 037:     30 / 65 loss=7.072, nll_loss=5.389, ppl=41.9, wps=4035.8, ups=0.33, wpb=12327, bsz=581.6, num_updates=2355, lr=3.90051e-05, gnorm=4.536, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=9731
2023-05-19 19:19:43 | INFO | train_inner | epoch 037:     35 / 65 loss=7.297, nll_loss=5.627, ppl=49.42, wps=3432, ups=0.28, wpb=12195.8, bsz=588.2, num_updates=2360, lr=3.89796e-05, gnorm=8.685, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.4, wall=9749
2023-05-19 19:19:59 | INFO | train_inner | epoch 037:     40 / 65 loss=7.078, nll_loss=5.383, ppl=41.72, wps=3612.7, ups=0.31, wpb=11721.2, bsz=537, num_updates=2365, lr=3.89541e-05, gnorm=3.602, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=9765
2023-05-19 19:20:16 | INFO | train_inner | epoch 037:     45 / 65 loss=7.185, nll_loss=5.51, ppl=45.57, wps=3828.4, ups=0.31, wpb=12495.6, bsz=573.4, num_updates=2370, lr=3.89286e-05, gnorm=3.791, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=9782
2023-05-19 19:20:31 | INFO | train_inner | epoch 037:     50 / 65 loss=7.037, nll_loss=5.346, ppl=40.68, wps=3905.7, ups=0.32, wpb=12094, bsz=544, num_updates=2375, lr=3.89031e-05, gnorm=4.151, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=9797
2023-05-19 19:20:48 | INFO | train_inner | epoch 037:     55 / 65 loss=7.126, nll_loss=5.445, ppl=43.56, wps=3620.7, ups=0.31, wpb=11863.4, bsz=553.4, num_updates=2380, lr=3.88776e-05, gnorm=4.742, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=9813
2023-05-19 19:21:03 | INFO | train_inner | epoch 037:     60 / 65 loss=7.06, nll_loss=5.367, ppl=41.27, wps=3895.8, ups=0.32, wpb=12104.6, bsz=489.4, num_updates=2385, lr=3.8852e-05, gnorm=3.23, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=9829
2023-05-19 19:21:16 | INFO | train_inner | epoch 037:     65 / 65 loss=7.384, nll_loss=5.717, ppl=52.61, wps=4293.1, ups=0.4, wpb=10729.2, bsz=595.6, num_updates=2390, lr=3.88265e-05, gnorm=4.766, clip=100, loss_scale=0.0625, train_wall=12, gb_free=21.7, wall=9841
2023-05-19 19:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:21:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:22:00 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.487 | nll_loss 4.675 | ppl 25.54 | bleu 2.56 | wps 799.5 | wpb 2785 | bsz 105.2 | num_updates 2390 | best_loss 6.487
2023-05-19 19:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2390 updates
2023-05-19 19:22:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint37.pt
2023-05-19 19:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint37.pt
2023-05-19 19:22:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint37.pt (epoch 37 @ 2390 updates, score 6.487) (writing took 28.404714915901423 seconds)
2023-05-19 19:22:29 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2023-05-19 19:22:29 | INFO | train | epoch 037 | loss 7.153 | nll_loss 5.473 | ppl 44.41 | wps 2792.3 | ups 0.23 | wpb 12057.6 | bsz 560.4 | num_updates 2390 | lr 3.88265e-05 | gnorm 4.629 | clip 100 | loss_scale 0.0625 | train_wall 207 | gb_free 21.7 | wall 9914
2023-05-19 19:22:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:22:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:22:29 | INFO | fairseq.trainer | begin training epoch 38
2023-05-19 19:22:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:22:44 | INFO | train_inner | epoch 038:      5 / 65 loss=7.061, nll_loss=5.369, ppl=41.31, wps=694.5, ups=0.06, wpb=12247.6, bsz=547.2, num_updates=2395, lr=3.8801e-05, gnorm=4.044, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=9930
2023-05-19 19:23:00 | INFO | train_inner | epoch 038:     10 / 65 loss=7.32, nll_loss=5.646, ppl=50.06, wps=3598.2, ups=0.3, wpb=11935.6, bsz=562.4, num_updates=2400, lr=3.87755e-05, gnorm=3.536, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=9946
2023-05-19 19:23:17 | INFO | train_inner | epoch 038:     15 / 65 loss=7.064, nll_loss=5.375, ppl=41.5, wps=3761.4, ups=0.31, wpb=12294, bsz=521.6, num_updates=2405, lr=3.875e-05, gnorm=6.623, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=9963
2023-05-19 19:23:31 | INFO | train_inner | epoch 038:     20 / 65 loss=7.147, nll_loss=5.462, ppl=44.09, wps=4340, ups=0.35, wpb=12325.6, bsz=641, num_updates=2410, lr=3.87245e-05, gnorm=9.171, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.1, wall=9977
2023-05-19 19:23:46 | INFO | train_inner | epoch 038:     25 / 65 loss=7.137, nll_loss=5.45, ppl=43.72, wps=3990.8, ups=0.32, wpb=12350.2, bsz=616, num_updates=2415, lr=3.8699e-05, gnorm=4.084, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=9992
2023-05-19 19:24:03 | INFO | train_inner | epoch 038:     30 / 65 loss=7.083, nll_loss=5.393, ppl=42.01, wps=3531, ups=0.3, wpb=11909.8, bsz=510.2, num_updates=2420, lr=3.86735e-05, gnorm=3.597, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=10009
2023-05-19 19:24:20 | INFO | train_inner | epoch 038:     35 / 65 loss=7.202, nll_loss=5.53, ppl=46.21, wps=3863.3, ups=0.31, wpb=12606.2, bsz=636.2, num_updates=2425, lr=3.8648e-05, gnorm=4.594, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=10025
2023-05-19 19:24:35 | INFO | train_inner | epoch 038:     40 / 65 loss=7.078, nll_loss=5.389, ppl=41.9, wps=3884.7, ups=0.32, wpb=12195.6, bsz=548.2, num_updates=2430, lr=3.86224e-05, gnorm=18.526, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=10041
2023-05-19 19:24:52 | INFO | train_inner | epoch 038:     45 / 65 loss=7.085, nll_loss=5.393, ppl=42.02, wps=3614.8, ups=0.31, wpb=11839.6, bsz=513, num_updates=2435, lr=3.85969e-05, gnorm=36.019, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=10057
2023-05-19 19:25:09 | INFO | train_inner | epoch 038:     50 / 65 loss=7.175, nll_loss=5.488, ppl=44.89, wps=3555.6, ups=0.3, wpb=11975.4, bsz=543.6, num_updates=2440, lr=3.85714e-05, gnorm=4.431, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=10074
2023-05-19 19:25:22 | INFO | train_inner | epoch 038:     55 / 65 loss=7.049, nll_loss=5.356, ppl=40.96, wps=4529, ups=0.37, wpb=12360.4, bsz=607.8, num_updates=2445, lr=3.85459e-05, gnorm=3.695, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.6, wall=10088
2023-05-19 19:25:40 | INFO | train_inner | epoch 038:     60 / 65 loss=7.032, nll_loss=5.336, ppl=40.4, wps=3540.5, ups=0.29, wpb=12263, bsz=516, num_updates=2450, lr=3.85204e-05, gnorm=5.525, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=10105
2023-05-19 19:25:53 | INFO | train_inner | epoch 038:     65 / 65 loss=7.367, nll_loss=5.692, ppl=51.71, wps=3756.3, ups=0.36, wpb=10396.8, bsz=497.4, num_updates=2455, lr=3.84949e-05, gnorm=4.674, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.8, wall=10119
2023-05-19 19:25:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:25:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:26:34 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.513 | nll_loss 4.701 | ppl 26.01 | bleu 2.67 | wps 894.7 | wpb 2785 | bsz 105.2 | num_updates 2455 | best_loss 6.487
2023-05-19 19:26:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2455 updates
2023-05-19 19:26:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint38.pt
2023-05-19 19:26:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint38.pt
2023-05-19 19:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint38.pt (epoch 38 @ 2455 updates, score 6.513) (writing took 6.844306088984013 seconds)
2023-05-19 19:26:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2023-05-19 19:26:40 | INFO | train | epoch 038 | loss 7.136 | nll_loss 5.449 | ppl 43.69 | wps 3111.9 | ups 0.26 | wpb 12053.8 | bsz 558.5 | num_updates 2455 | lr 3.84949e-05 | gnorm 8.348 | clip 100 | loss_scale 0.0625 | train_wall 204 | gb_free 21.8 | wall 10166
2023-05-19 19:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:26:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:26:40 | INFO | fairseq.trainer | begin training epoch 39
2023-05-19 19:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:26:56 | INFO | train_inner | epoch 039:      5 / 65 loss=7.073, nll_loss=5.382, ppl=41.7, wps=933.2, ups=0.08, wpb=11681.4, bsz=505.2, num_updates=2460, lr=3.84694e-05, gnorm=3.253, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=10182
2023-05-19 19:27:10 | INFO | train_inner | epoch 039:     10 / 65 loss=7.084, nll_loss=5.394, ppl=42.03, wps=4160.7, ups=0.34, wpb=12111.2, bsz=535, num_updates=2465, lr=3.84439e-05, gnorm=2.381, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=10196
2023-05-19 19:27:28 | INFO | train_inner | epoch 039:     15 / 65 loss=7.473, nll_loss=5.802, ppl=55.78, wps=3985.2, ups=0.33, wpb=12113, bsz=638.2, num_updates=2470, lr=3.84184e-05, gnorm=4.127, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=10213
2023-05-19 19:27:43 | INFO | train_inner | epoch 039:     20 / 65 loss=7.076, nll_loss=5.381, ppl=41.68, wps=4129.3, ups=0.33, wpb=12650.4, bsz=608.6, num_updates=2475, lr=3.83929e-05, gnorm=4.137, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=10229
2023-05-19 19:27:59 | INFO | train_inner | epoch 039:     25 / 65 loss=7.097, nll_loss=5.414, ppl=42.64, wps=3663.3, ups=0.3, wpb=12016.4, bsz=577.4, num_updates=2480, lr=3.83673e-05, gnorm=5.414, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=10245
2023-05-19 19:28:14 | INFO | train_inner | epoch 039:     30 / 65 loss=7.286, nll_loss=5.619, ppl=49.15, wps=4351.9, ups=0.35, wpb=12456.2, bsz=671.4, num_updates=2485, lr=3.83418e-05, gnorm=6.706, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.6, wall=10260
2023-05-19 19:28:31 | INFO | train_inner | epoch 039:     35 / 65 loss=7.04, nll_loss=5.344, ppl=40.61, wps=3566.3, ups=0.29, wpb=12117, bsz=525.4, num_updates=2490, lr=3.83163e-05, gnorm=3.098, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=10276
2023-05-19 19:28:47 | INFO | train_inner | epoch 039:     40 / 65 loss=7.074, nll_loss=5.382, ppl=41.71, wps=3652.2, ups=0.31, wpb=11718.4, bsz=500.6, num_updates=2495, lr=3.82908e-05, gnorm=5.483, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21, wall=10293
2023-05-19 19:29:03 | INFO | train_inner | epoch 039:     45 / 65 loss=7.081, nll_loss=5.396, ppl=42.1, wps=3905.3, ups=0.31, wpb=12526.6, bsz=587, num_updates=2500, lr=3.82653e-05, gnorm=4.481, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=10309
2023-05-19 19:29:18 | INFO | train_inner | epoch 039:     50 / 65 loss=7.014, nll_loss=5.314, ppl=39.77, wps=4022, ups=0.32, wpb=12384.2, bsz=524.4, num_updates=2505, lr=3.82398e-05, gnorm=5.482, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=10324
2023-05-19 19:29:34 | INFO | train_inner | epoch 039:     55 / 65 loss=7.145, nll_loss=5.463, ppl=44.09, wps=3875.1, ups=0.31, wpb=12500.6, bsz=615.8, num_updates=2510, lr=3.82143e-05, gnorm=7.31, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.2, wall=10340
2023-05-19 19:29:51 | INFO | train_inner | epoch 039:     60 / 65 loss=7.047, nll_loss=5.354, ppl=40.89, wps=3662.2, ups=0.3, wpb=12127.8, bsz=546.8, num_updates=2515, lr=3.81888e-05, gnorm=7.09, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.4, wall=10357
2023-05-19 19:30:06 | INFO | train_inner | epoch 039:     65 / 65 loss=7.053, nll_loss=5.359, ppl=41.04, wps=3415.8, ups=0.33, wpb=10308, bsz=439, num_updates=2520, lr=3.81633e-05, gnorm=6.138, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=10372
2023-05-19 19:30:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:30:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:30:46 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.451 | nll_loss 4.624 | ppl 24.66 | bleu 3.03 | wps 910.4 | wpb 2785 | bsz 105.2 | num_updates 2520 | best_loss 6.451
2023-05-19 19:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2520 updates
2023-05-19 19:30:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint39.pt
2023-05-19 19:30:51 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint39.pt
2023-05-19 19:31:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint39.pt (epoch 39 @ 2520 updates, score 6.451) (writing took 25.558287229388952 seconds)
2023-05-19 19:31:11 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2023-05-19 19:31:11 | INFO | train | epoch 039 | loss 7.12 | nll_loss 5.432 | ppl 43.17 | wps 2893.3 | ups 0.24 | wpb 12054.7 | bsz 559.6 | num_updates 2520 | lr 3.81633e-05 | gnorm 5.008 | clip 100 | loss_scale 0.0625 | train_wall 203 | gb_free 21.7 | wall 10437
2023-05-19 19:31:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:31:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:31:11 | INFO | fairseq.trainer | begin training epoch 40
2023-05-19 19:31:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:31:27 | INFO | train_inner | epoch 040:      5 / 65 loss=7.237, nll_loss=5.542, ppl=46.6, wps=739, ups=0.06, wpb=12027.2, bsz=566.4, num_updates=2525, lr=3.81378e-05, gnorm=3.296, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=10453
2023-05-19 19:31:44 | INFO | train_inner | epoch 040:     10 / 65 loss=7.099, nll_loss=5.407, ppl=42.44, wps=3699.1, ups=0.3, wpb=12284, bsz=560.2, num_updates=2530, lr=3.81122e-05, gnorm=7.402, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=10470
2023-05-19 19:32:00 | INFO | train_inner | epoch 040:     15 / 65 loss=7.194, nll_loss=5.509, ppl=45.55, wps=3700, ups=0.31, wpb=11983.8, bsz=528.8, num_updates=2535, lr=3.80867e-05, gnorm=7.533, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=10486
2023-05-19 19:32:16 | INFO | train_inner | epoch 040:     20 / 65 loss=7.027, nll_loss=5.327, ppl=40.14, wps=3935.8, ups=0.32, wpb=12239.2, bsz=547.4, num_updates=2540, lr=3.80612e-05, gnorm=9.331, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=10501
2023-05-19 19:32:31 | INFO | train_inner | epoch 040:     25 / 65 loss=7.103, nll_loss=5.42, ppl=42.81, wps=4102.1, ups=0.32, wpb=12707.2, bsz=615, num_updates=2545, lr=3.80357e-05, gnorm=4.014, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=10517
2023-05-19 19:32:47 | INFO | train_inner | epoch 040:     30 / 65 loss=7.229, nll_loss=5.532, ppl=46.28, wps=3869.9, ups=0.32, wpb=12211, bsz=558.6, num_updates=2550, lr=3.80102e-05, gnorm=5.102, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=10533
2023-05-19 19:33:03 | INFO | train_inner | epoch 040:     35 / 65 loss=7.033, nll_loss=5.34, ppl=40.49, wps=3848.3, ups=0.32, wpb=11952.6, bsz=541.4, num_updates=2555, lr=3.79847e-05, gnorm=7.296, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=10548
2023-05-19 19:33:19 | INFO | train_inner | epoch 040:     40 / 65 loss=7.058, nll_loss=5.366, ppl=41.23, wps=3830.6, ups=0.31, wpb=12324, bsz=563.6, num_updates=2560, lr=3.79592e-05, gnorm=4.118, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=10564
2023-05-19 19:33:36 | INFO | train_inner | epoch 040:     45 / 65 loss=7.143, nll_loss=5.452, ppl=43.77, wps=3383.1, ups=0.29, wpb=11844.8, bsz=563.2, num_updates=2565, lr=3.79337e-05, gnorm=5.341, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=10582
2023-05-19 19:33:51 | INFO | train_inner | epoch 040:     50 / 65 loss=6.997, nll_loss=5.286, ppl=39.03, wps=4143.3, ups=0.34, wpb=12330.6, bsz=565.4, num_updates=2570, lr=3.79082e-05, gnorm=8.88, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.7, wall=10597
2023-05-19 19:34:07 | INFO | train_inner | epoch 040:     55 / 65 loss=7.104, nll_loss=5.413, ppl=42.6, wps=3816.8, ups=0.31, wpb=12276.8, bsz=620.2, num_updates=2575, lr=3.78827e-05, gnorm=7.21, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=10613
2023-05-19 19:34:22 | INFO | train_inner | epoch 040:     60 / 65 loss=7.071, nll_loss=5.38, ppl=41.65, wps=4177, ups=0.34, wpb=12233.8, bsz=598.4, num_updates=2580, lr=3.78571e-05, gnorm=7.118, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.2, wall=10627
2023-05-19 19:34:36 | INFO | train_inner | epoch 040:     65 / 65 loss=7.025, nll_loss=5.321, ppl=39.97, wps=3611.7, ups=0.36, wpb=10159, bsz=429.4, num_updates=2585, lr=3.78316e-05, gnorm=7.165, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.7, wall=10642
2023-05-19 19:34:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:34:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:35:17 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.445 | nll_loss 4.622 | ppl 24.63 | bleu 2.9 | wps 876.1 | wpb 2785 | bsz 105.2 | num_updates 2585 | best_loss 6.445
2023-05-19 19:35:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2585 updates
2023-05-19 19:35:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint40.pt
2023-05-19 19:35:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint40.pt
2023-05-19 19:35:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint40.pt (epoch 40 @ 2585 updates, score 6.445) (writing took 25.988303631544113 seconds)
2023-05-19 19:35:43 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2023-05-19 19:35:43 | INFO | train | epoch 040 | loss 7.102 | nll_loss 5.408 | ppl 42.46 | wps 2883.6 | ups 0.24 | wpb 12044.2 | bsz 558.3 | num_updates 2585 | lr 3.78316e-05 | gnorm 6.447 | clip 100 | loss_scale 0.0625 | train_wall 204 | gb_free 21.7 | wall 10708
2023-05-19 19:35:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:35:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:35:43 | INFO | fairseq.trainer | begin training epoch 41
2023-05-19 19:35:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:35:59 | INFO | train_inner | epoch 041:      5 / 65 loss=7.046, nll_loss=5.351, ppl=40.82, wps=727.3, ups=0.06, wpb=12129, bsz=551.2, num_updates=2590, lr=3.78061e-05, gnorm=5.816, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=10725
2023-05-19 19:36:16 | INFO | train_inner | epoch 041:     10 / 65 loss=7.017, nll_loss=5.321, ppl=39.98, wps=3624.2, ups=0.3, wpb=12256.8, bsz=540.2, num_updates=2595, lr=3.77806e-05, gnorm=4.385, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.5, wall=10742
2023-05-19 19:36:32 | INFO | train_inner | epoch 041:     15 / 65 loss=7.033, nll_loss=5.337, ppl=40.41, wps=3979.4, ups=0.32, wpb=12599.4, bsz=565.2, num_updates=2600, lr=3.77551e-05, gnorm=3.515, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=10758
2023-05-19 19:36:50 | INFO | train_inner | epoch 041:     20 / 65 loss=7.041, nll_loss=5.344, ppl=40.62, wps=3408.6, ups=0.28, wpb=12115.4, bsz=549, num_updates=2605, lr=3.77296e-05, gnorm=6.266, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.7, wall=10775
2023-05-19 19:37:05 | INFO | train_inner | epoch 041:     25 / 65 loss=7.083, nll_loss=5.392, ppl=41.98, wps=4103.7, ups=0.34, wpb=12240, bsz=605.2, num_updates=2610, lr=3.77041e-05, gnorm=10.877, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=10790
2023-05-19 19:37:20 | INFO | train_inner | epoch 041:     30 / 65 loss=7.053, nll_loss=5.358, ppl=41, wps=3829.9, ups=0.31, wpb=12165.8, bsz=495.4, num_updates=2615, lr=3.76786e-05, gnorm=3.837, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=10806
2023-05-19 19:37:37 | INFO | train_inner | epoch 041:     35 / 65 loss=7.105, nll_loss=5.418, ppl=42.76, wps=3730.2, ups=0.3, wpb=12512.4, bsz=635.8, num_updates=2620, lr=3.76531e-05, gnorm=6.182, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=10823
2023-05-19 19:37:52 | INFO | train_inner | epoch 041:     40 / 65 loss=7.446, nll_loss=5.776, ppl=54.8, wps=4193.2, ups=0.34, wpb=12337, bsz=714, num_updates=2625, lr=3.76276e-05, gnorm=8.053, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.7, wall=10838
2023-05-19 19:38:07 | INFO | train_inner | epoch 041:     45 / 65 loss=7.044, nll_loss=5.339, ppl=40.48, wps=3889.5, ups=0.32, wpb=12006, bsz=514, num_updates=2630, lr=3.7602e-05, gnorm=10.454, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=10853
2023-05-19 19:38:24 | INFO | train_inner | epoch 041:     50 / 65 loss=7.065, nll_loss=5.374, ppl=41.48, wps=3581.9, ups=0.29, wpb=12153.8, bsz=561.8, num_updates=2635, lr=3.75765e-05, gnorm=10.028, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=10870
2023-05-19 19:38:40 | INFO | train_inner | epoch 041:     55 / 65 loss=7.098, nll_loss=5.4, ppl=42.22, wps=3801, ups=0.31, wpb=12068.4, bsz=549.2, num_updates=2640, lr=3.7551e-05, gnorm=6.201, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.9, wall=10886
2023-05-19 19:38:58 | INFO | train_inner | epoch 041:     60 / 65 loss=7.141, nll_loss=5.441, ppl=43.45, wps=3302.1, ups=0.29, wpb=11461.6, bsz=463, num_updates=2645, lr=3.75255e-05, gnorm=7.628, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=10903
2023-05-19 19:39:12 | INFO | train_inner | epoch 041:     65 / 65 loss=7.102, nll_loss=5.416, ppl=42.69, wps=3821, ups=0.36, wpb=10652.4, bsz=532, num_updates=2650, lr=3.75e-05, gnorm=6.116, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=10917
2023-05-19 19:39:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:39:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:39:50 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.429 | nll_loss 4.602 | ppl 24.28 | bleu 2.9 | wps 933.5 | wpb 2785 | bsz 105.2 | num_updates 2650 | best_loss 6.429
2023-05-19 19:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 2650 updates
2023-05-19 19:39:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint41.pt
2023-05-19 19:39:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint41.pt
2023-05-19 19:40:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint41.pt (epoch 41 @ 2650 updates, score 6.429) (writing took 9.81477952748537 seconds)
2023-05-19 19:40:04 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2023-05-19 19:40:04 | INFO | train | epoch 041 | loss 7.098 | nll_loss 5.405 | ppl 42.38 | wps 2993.8 | ups 0.25 | wpb 12053.7 | bsz 559.7 | num_updates 2650 | lr 3.75e-05 | gnorm 6.874 | clip 100 | loss_scale 0.125 | train_wall 208 | gb_free 21.7 | wall 10970
2023-05-19 19:40:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:40:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:40:04 | INFO | fairseq.trainer | begin training epoch 42
2023-05-19 19:40:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:40:21 | INFO | train_inner | epoch 042:      5 / 65 loss=7.013, nll_loss=5.313, ppl=39.76, wps=840.5, ups=0.07, wpb=11670.2, bsz=514.6, num_updates=2655, lr=3.74745e-05, gnorm=3.098, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=10987
2023-05-19 19:40:38 | INFO | train_inner | epoch 042:     10 / 65 loss=7.03, nll_loss=5.329, ppl=40.19, wps=3480.8, ups=0.29, wpb=12101, bsz=533, num_updates=2660, lr=3.7449e-05, gnorm=4.33, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.9, wall=11004
2023-05-19 19:40:53 | INFO | train_inner | epoch 042:     15 / 65 loss=6.996, nll_loss=5.29, ppl=39.11, wps=4030.9, ups=0.33, wpb=12152.4, bsz=534.4, num_updates=2665, lr=3.74235e-05, gnorm=7.713, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=11019
2023-05-19 19:41:09 | INFO | train_inner | epoch 042:     20 / 65 loss=7.351, nll_loss=5.665, ppl=50.74, wps=3941.8, ups=0.32, wpb=12449, bsz=665.4, num_updates=2670, lr=3.7398e-05, gnorm=6.166, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11035
2023-05-19 19:41:24 | INFO | train_inner | epoch 042:     25 / 65 loss=7.006, nll_loss=5.306, ppl=39.56, wps=4118.5, ups=0.34, wpb=12137.6, bsz=531.6, num_updates=2675, lr=3.73724e-05, gnorm=5.346, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=11050
2023-05-19 19:41:41 | INFO | train_inner | epoch 042:     30 / 65 loss=7.202, nll_loss=5.516, ppl=45.75, wps=3418.9, ups=0.29, wpb=11750.4, bsz=533.4, num_updates=2680, lr=3.73469e-05, gnorm=4.826, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=11067
2023-05-19 19:41:55 | INFO | train_inner | epoch 042:     35 / 65 loss=7.09, nll_loss=5.397, ppl=42.14, wps=4213.3, ups=0.35, wpb=12018.4, bsz=528.2, num_updates=2685, lr=3.73214e-05, gnorm=7.528, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=11081
2023-05-19 19:42:10 | INFO | train_inner | epoch 042:     40 / 65 loss=7.028, nll_loss=5.329, ppl=40.19, wps=4195.6, ups=0.34, wpb=12402.4, bsz=567.4, num_updates=2690, lr=3.72959e-05, gnorm=4.343, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=11096
2023-05-19 19:42:27 | INFO | train_inner | epoch 042:     45 / 65 loss=7.156, nll_loss=5.475, ppl=44.48, wps=3835.3, ups=0.3, wpb=12793.8, bsz=635.8, num_updates=2695, lr=3.72704e-05, gnorm=6.975, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=11113
2023-05-19 19:42:44 | INFO | train_inner | epoch 042:     50 / 65 loss=7.112, nll_loss=5.427, ppl=43.03, wps=3580, ups=0.29, wpb=12169, bsz=609.8, num_updates=2700, lr=3.72449e-05, gnorm=6.687, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.1, wall=11130
2023-05-19 19:43:00 | INFO | train_inner | epoch 042:     55 / 65 loss=7.05, nll_loss=5.355, ppl=40.94, wps=3762.1, ups=0.31, wpb=12137.6, bsz=543.6, num_updates=2705, lr=3.72194e-05, gnorm=9.432, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=11146
2023-05-19 19:43:15 | INFO | train_inner | epoch 042:     60 / 65 loss=7.087, nll_loss=5.395, ppl=42.07, wps=4070.2, ups=0.33, wpb=12424.8, bsz=615.4, num_updates=2710, lr=3.71939e-05, gnorm=5.797, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=11161
2023-05-19 19:43:30 | INFO | train_inner | epoch 042:     65 / 65 loss=7.006, nll_loss=5.305, ppl=39.52, wps=3523.9, ups=0.34, wpb=10466.2, bsz=462.8, num_updates=2715, lr=3.71684e-05, gnorm=5.857, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=11176
2023-05-19 19:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:43:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:44:09 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.429 | nll_loss 4.603 | ppl 24.29 | bleu 3.09 | wps 933.4 | wpb 2785 | bsz 105.2 | num_updates 2715 | best_loss 6.429
2023-05-19 19:44:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2715 updates
2023-05-19 19:44:09 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint42.pt
2023-05-19 19:44:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint42.pt
2023-05-19 19:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint42.pt (epoch 42 @ 2715 updates, score 6.429) (writing took 19.37587295845151 seconds)
2023-05-19 19:44:29 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2023-05-19 19:44:29 | INFO | train | epoch 042 | loss 7.088 | nll_loss 5.394 | ppl 42.05 | wps 2959.6 | ups 0.25 | wpb 12051.8 | bsz 559.6 | num_updates 2715 | lr 3.71684e-05 | gnorm 6.008 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.4 | wall 11235
2023-05-19 19:44:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:44:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:44:29 | INFO | fairseq.trainer | begin training epoch 43
2023-05-19 19:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:44:45 | INFO | train_inner | epoch 043:      5 / 65 loss=7.007, nll_loss=5.301, ppl=39.42, wps=808.1, ups=0.07, wpb=12091.2, bsz=508.6, num_updates=2720, lr=3.71429e-05, gnorm=7.057, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11251
2023-05-19 19:45:00 | INFO | train_inner | epoch 043:     10 / 65 loss=7.181, nll_loss=5.501, ppl=45.29, wps=3883.8, ups=0.33, wpb=11814.6, bsz=604, num_updates=2725, lr=3.71173e-05, gnorm=7.907, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=11266
2023-05-19 19:45:16 | INFO | train_inner | epoch 043:     15 / 65 loss=7.077, nll_loss=5.379, ppl=41.61, wps=3589.7, ups=0.31, wpb=11636.6, bsz=533.2, num_updates=2730, lr=3.70918e-05, gnorm=3.855, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=11282
2023-05-19 19:45:30 | INFO | train_inner | epoch 043:     20 / 65 loss=7.088, nll_loss=5.399, ppl=42.2, wps=4570.1, ups=0.36, wpb=12644, bsz=610.6, num_updates=2735, lr=3.70663e-05, gnorm=5.482, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.5, wall=11296
2023-05-19 19:45:47 | INFO | train_inner | epoch 043:     25 / 65 loss=7.099, nll_loss=5.408, ppl=42.46, wps=3611.8, ups=0.3, wpb=12202.2, bsz=563.8, num_updates=2740, lr=3.70408e-05, gnorm=3.37, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.6, wall=11313
2023-05-19 19:46:06 | INFO | train_inner | epoch 043:     30 / 65 loss=6.988, nll_loss=5.286, ppl=39.01, wps=3267.1, ups=0.27, wpb=12275.8, bsz=519.4, num_updates=2745, lr=3.70153e-05, gnorm=4.787, clip=100, loss_scale=0.125, train_wall=19, gb_free=21.5, wall=11332
2023-05-19 19:46:21 | INFO | train_inner | epoch 043:     35 / 65 loss=7.378, nll_loss=5.695, ppl=51.82, wps=3974.7, ups=0.32, wpb=12309.2, bsz=637.6, num_updates=2750, lr=3.69898e-05, gnorm=3.719, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.2, wall=11347
2023-05-19 19:46:39 | INFO | train_inner | epoch 043:     40 / 65 loss=7.127, nll_loss=5.443, ppl=43.49, wps=3477.7, ups=0.28, wpb=12313, bsz=605.4, num_updates=2755, lr=3.69643e-05, gnorm=3.543, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=11365
2023-05-19 19:46:55 | INFO | train_inner | epoch 043:     45 / 65 loss=7, nll_loss=5.301, ppl=39.41, wps=3952, ups=0.32, wpb=12501, bsz=584.6, num_updates=2760, lr=3.69388e-05, gnorm=7.022, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11381
2023-05-19 19:47:12 | INFO | train_inner | epoch 043:     50 / 65 loss=7.076, nll_loss=5.385, ppl=41.77, wps=3669.1, ups=0.29, wpb=12516.8, bsz=583.6, num_updates=2765, lr=3.69133e-05, gnorm=5.987, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=11398
2023-05-19 19:47:28 | INFO | train_inner | epoch 043:     55 / 65 loss=6.896, nll_loss=5.177, ppl=36.17, wps=3658.5, ups=0.31, wpb=11801.4, bsz=507.8, num_updates=2770, lr=3.68878e-05, gnorm=4.896, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=11414
2023-05-19 19:47:43 | INFO | train_inner | epoch 043:     60 / 65 loss=7.059, nll_loss=5.361, ppl=41.09, wps=4087.2, ups=0.34, wpb=11934.4, bsz=526.2, num_updates=2775, lr=3.68622e-05, gnorm=3.494, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=11428
2023-05-19 19:47:56 | INFO | train_inner | epoch 043:     65 / 65 loss=7.082, nll_loss=5.393, ppl=42.02, wps=3924.8, ups=0.37, wpb=10559.2, bsz=476, num_updates=2780, lr=3.68367e-05, gnorm=4.962, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.9, wall=11442
2023-05-19 19:47:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:47:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:48:38 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.405 | nll_loss 4.576 | ppl 23.86 | bleu 3.09 | wps 860.8 | wpb 2785 | bsz 105.2 | num_updates 2780 | best_loss 6.405
2023-05-19 19:48:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2780 updates
2023-05-19 19:48:38 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint43.pt
2023-05-19 19:48:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint43.pt
2023-05-19 19:49:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint43.pt (epoch 43 @ 2780 updates, score 6.405) (writing took 27.11732578650117 seconds)
2023-05-19 19:49:05 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2023-05-19 19:49:05 | INFO | train | epoch 043 | loss 7.082 | nll_loss 5.387 | ppl 41.85 | wps 2838.7 | ups 0.24 | wpb 12046.1 | bsz 558.5 | num_updates 2780 | lr 3.68367e-05 | gnorm 5.083 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.9 | wall 11511
2023-05-19 19:49:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:49:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:49:05 | INFO | fairseq.trainer | begin training epoch 44
2023-05-19 19:49:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:49:20 | INFO | train_inner | epoch 044:      5 / 65 loss=7.227, nll_loss=5.54, ppl=46.51, wps=746.7, ups=0.06, wpb=12555, bsz=604.6, num_updates=2785, lr=3.68112e-05, gnorm=5.586, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.2, wall=11526
2023-05-19 19:49:36 | INFO | train_inner | epoch 044:     10 / 65 loss=7.313, nll_loss=5.63, ppl=49.51, wps=3807.8, ups=0.31, wpb=12292, bsz=647.2, num_updates=2790, lr=3.67857e-05, gnorm=5.726, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=11542
2023-05-19 19:49:52 | INFO | train_inner | epoch 044:     15 / 65 loss=7.105, nll_loss=5.419, ppl=42.77, wps=3983.2, ups=0.33, wpb=12219.6, bsz=609.6, num_updates=2795, lr=3.67602e-05, gnorm=4.26, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=11557
2023-05-19 19:50:07 | INFO | train_inner | epoch 044:     20 / 65 loss=7.023, nll_loss=5.327, ppl=40.15, wps=3875.4, ups=0.32, wpb=12131, bsz=542.6, num_updates=2800, lr=3.67347e-05, gnorm=4.032, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=11573
2023-05-19 19:50:23 | INFO | train_inner | epoch 044:     25 / 65 loss=6.981, nll_loss=5.274, ppl=38.69, wps=3758.1, ups=0.32, wpb=11895, bsz=496.4, num_updates=2805, lr=3.67092e-05, gnorm=4.854, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=11589
2023-05-19 19:50:38 | INFO | train_inner | epoch 044:     30 / 65 loss=7.004, nll_loss=5.303, ppl=39.49, wps=4154.6, ups=0.33, wpb=12412, bsz=588.2, num_updates=2810, lr=3.66837e-05, gnorm=4.185, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=11604
2023-05-19 19:50:55 | INFO | train_inner | epoch 044:     35 / 65 loss=6.966, nll_loss=5.255, ppl=38.19, wps=3435.2, ups=0.29, wpb=11926.2, bsz=505.4, num_updates=2815, lr=3.66582e-05, gnorm=4.936, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=11621
2023-05-19 19:51:12 | INFO | train_inner | epoch 044:     40 / 65 loss=7.093, nll_loss=5.405, ppl=42.37, wps=3642.4, ups=0.3, wpb=11992.4, bsz=620, num_updates=2820, lr=3.66327e-05, gnorm=6.226, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11638
2023-05-19 19:51:29 | INFO | train_inner | epoch 044:     45 / 65 loss=7.034, nll_loss=5.339, ppl=40.47, wps=3706.7, ups=0.3, wpb=12369, bsz=592.8, num_updates=2825, lr=3.66071e-05, gnorm=3.217, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=11654
2023-05-19 19:51:44 | INFO | train_inner | epoch 044:     50 / 65 loss=7.026, nll_loss=5.328, ppl=40.16, wps=3910.4, ups=0.32, wpb=12214, bsz=567.2, num_updates=2830, lr=3.65816e-05, gnorm=9.043, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=11670
2023-05-19 19:52:02 | INFO | train_inner | epoch 044:     55 / 65 loss=7.142, nll_loss=5.441, ppl=43.45, wps=3552.7, ups=0.28, wpb=12490.2, bsz=536.4, num_updates=2835, lr=3.65561e-05, gnorm=4.47, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=11688
2023-05-19 19:52:19 | INFO | train_inner | epoch 044:     60 / 65 loss=6.988, nll_loss=5.276, ppl=38.73, wps=3346.2, ups=0.29, wpb=11661.2, bsz=470.6, num_updates=2840, lr=3.65306e-05, gnorm=9.076, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=11705
2023-05-19 19:52:34 | INFO | train_inner | epoch 044:     65 / 65 loss=7.023, nll_loss=5.319, ppl=39.92, wps=3423.6, ups=0.33, wpb=10368.2, bsz=460.6, num_updates=2845, lr=3.65051e-05, gnorm=4.53, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=11720
2023-05-19 19:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:52:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:53:15 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.409 | nll_loss 4.577 | ppl 23.87 | bleu 3.15 | wps 892.7 | wpb 2785 | bsz 105.2 | num_updates 2845 | best_loss 6.405
2023-05-19 19:53:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2845 updates
2023-05-19 19:53:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint44.pt
2023-05-19 19:53:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint44.pt
2023-05-19 19:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint44.pt (epoch 44 @ 2845 updates, score 6.409) (writing took 7.088228639215231 seconds)
2023-05-19 19:53:22 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2023-05-19 19:53:22 | INFO | train | epoch 044 | loss 7.073 | nll_loss 5.375 | ppl 41.51 | wps 3048.8 | ups 0.25 | wpb 12040.4 | bsz 557 | num_updates 2845 | lr 3.65051e-05 | gnorm 5.395 | clip 100 | loss_scale 0.125 | train_wall 209 | gb_free 21.5 | wall 11767
2023-05-19 19:53:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:53:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:53:22 | INFO | fairseq.trainer | begin training epoch 45
2023-05-19 19:53:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:53:38 | INFO | train_inner | epoch 045:      5 / 65 loss=7.082, nll_loss=5.385, ppl=41.78, wps=939.6, ups=0.08, wpb=11912.6, bsz=570, num_updates=2850, lr=3.64796e-05, gnorm=6.525, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11783
2023-05-19 19:53:54 | INFO | train_inner | epoch 045:     10 / 65 loss=7.266, nll_loss=5.578, ppl=47.76, wps=3678.2, ups=0.3, wpb=12330.8, bsz=567, num_updates=2855, lr=3.64541e-05, gnorm=6.529, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=11800
2023-05-19 19:54:09 | INFO | train_inner | epoch 045:     15 / 65 loss=6.999, nll_loss=5.292, ppl=39.18, wps=4160.7, ups=0.35, wpb=11966.2, bsz=546.4, num_updates=2860, lr=3.64286e-05, gnorm=3.301, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.5, wall=11815
2023-05-19 19:54:27 | INFO | train_inner | epoch 045:     20 / 65 loss=7.144, nll_loss=5.459, ppl=43.99, wps=3363.8, ups=0.28, wpb=12134.8, bsz=540.4, num_updates=2865, lr=3.64031e-05, gnorm=8.919, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=11833
2023-05-19 19:54:43 | INFO | train_inner | epoch 045:     25 / 65 loss=7.074, nll_loss=5.386, ppl=41.81, wps=3843.7, ups=0.31, wpb=12281.4, bsz=641.4, num_updates=2870, lr=3.63776e-05, gnorm=6.679, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=11849
2023-05-19 19:54:58 | INFO | train_inner | epoch 045:     30 / 65 loss=7.05, nll_loss=5.348, ppl=40.72, wps=4112.8, ups=0.34, wpb=12197.8, bsz=593.4, num_updates=2875, lr=3.6352e-05, gnorm=5.592, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=11863
2023-05-19 19:55:13 | INFO | train_inner | epoch 045:     35 / 65 loss=7.02, nll_loss=5.321, ppl=39.99, wps=3867.2, ups=0.32, wpb=12118.2, bsz=529.6, num_updates=2880, lr=3.63265e-05, gnorm=6.025, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.4, wall=11879
2023-05-19 19:55:28 | INFO | train_inner | epoch 045:     40 / 65 loss=7.018, nll_loss=5.32, ppl=39.96, wps=4262.2, ups=0.35, wpb=12296, bsz=556.4, num_updates=2885, lr=3.6301e-05, gnorm=5.749, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=11894
2023-05-19 19:55:45 | INFO | train_inner | epoch 045:     45 / 65 loss=7.046, nll_loss=5.354, ppl=40.89, wps=3494.9, ups=0.29, wpb=11945.8, bsz=540.2, num_updates=2890, lr=3.62755e-05, gnorm=5.402, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=11911
2023-05-19 19:56:01 | INFO | train_inner | epoch 045:     50 / 65 loss=7.058, nll_loss=5.368, ppl=41.29, wps=3884.1, ups=0.31, wpb=12434.4, bsz=571, num_updates=2895, lr=3.625e-05, gnorm=9.023, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=11927
2023-05-19 19:56:17 | INFO | train_inner | epoch 045:     55 / 65 loss=7.175, nll_loss=5.489, ppl=44.91, wps=3666.8, ups=0.31, wpb=11982.8, bsz=549.4, num_updates=2900, lr=3.62245e-05, gnorm=4.979, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=11943
2023-05-19 19:56:32 | INFO | train_inner | epoch 045:     60 / 65 loss=6.982, nll_loss=5.276, ppl=38.76, wps=4142.4, ups=0.33, wpb=12412.6, bsz=568.2, num_updates=2905, lr=3.6199e-05, gnorm=6.675, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=11958
2023-05-19 19:56:45 | INFO | train_inner | epoch 045:     65 / 65 loss=7.027, nll_loss=5.336, ppl=40.38, wps=4061, ups=0.38, wpb=10753.4, bsz=498.6, num_updates=2910, lr=3.61735e-05, gnorm=3.644, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.6, wall=11971
2023-05-19 19:56:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 19:56:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:57:28 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.475 | nll_loss 4.657 | ppl 25.22 | bleu 2.82 | wps 835.7 | wpb 2785 | bsz 105.2 | num_updates 2910 | best_loss 6.405
2023-05-19 19:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2910 updates
2023-05-19 19:57:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint45.pt
2023-05-19 19:57:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint45.pt
2023-05-19 19:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint45.pt (epoch 45 @ 2910 updates, score 6.475) (writing took 8.351030170917511 seconds)
2023-05-19 19:57:36 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2023-05-19 19:57:36 | INFO | train | epoch 045 | loss 7.073 | nll_loss 5.378 | ppl 41.59 | wps 3075.2 | ups 0.26 | wpb 12059 | bsz 559.4 | num_updates 2910 | lr 3.61735e-05 | gnorm 6.08 | clip 100 | loss_scale 0.125 | train_wall 203 | gb_free 21.6 | wall 12022
2023-05-19 19:57:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 19:57:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 19:57:37 | INFO | fairseq.trainer | begin training epoch 46
2023-05-19 19:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 19:57:51 | INFO | train_inner | epoch 046:      5 / 65 loss=7.262, nll_loss=5.578, ppl=47.76, wps=940.5, ups=0.08, wpb=12363.6, bsz=641.6, num_updates=2915, lr=3.6148e-05, gnorm=8.381, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=12037
2023-05-19 19:58:08 | INFO | train_inner | epoch 046:     10 / 65 loss=7.054, nll_loss=5.36, ppl=41.07, wps=3481.2, ups=0.3, wpb=11668.6, bsz=534, num_updates=2920, lr=3.61224e-05, gnorm=8.698, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=12054
2023-05-19 19:58:23 | INFO | train_inner | epoch 046:     15 / 65 loss=7.067, nll_loss=5.384, ppl=41.75, wps=4057.6, ups=0.33, wpb=12314.6, bsz=588.6, num_updates=2925, lr=3.60969e-05, gnorm=10.339, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=12069
2023-05-19 19:58:38 | INFO | train_inner | epoch 046:     20 / 65 loss=7.018, nll_loss=5.32, ppl=39.96, wps=4038.3, ups=0.34, wpb=12003.4, bsz=565.6, num_updates=2930, lr=3.60714e-05, gnorm=3.868, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=12084
2023-05-19 19:58:53 | INFO | train_inner | epoch 046:     25 / 65 loss=7.09, nll_loss=5.398, ppl=42.16, wps=3936.7, ups=0.33, wpb=12072.6, bsz=556.8, num_updates=2935, lr=3.60459e-05, gnorm=4.64, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=12099
2023-05-19 19:59:09 | INFO | train_inner | epoch 046:     30 / 65 loss=7.041, nll_loss=5.344, ppl=40.62, wps=3702.6, ups=0.31, wpb=11956.8, bsz=485, num_updates=2940, lr=3.60204e-05, gnorm=4.416, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=12115
2023-05-19 19:59:25 | INFO | train_inner | epoch 046:     35 / 65 loss=7.161, nll_loss=5.471, ppl=44.36, wps=3818.7, ups=0.32, wpb=11936.2, bsz=588.6, num_updates=2945, lr=3.59949e-05, gnorm=5.332, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=12131
2023-05-19 19:59:41 | INFO | train_inner | epoch 046:     40 / 65 loss=7.082, nll_loss=5.389, ppl=41.9, wps=3876.8, ups=0.31, wpb=12353.6, bsz=564, num_updates=2950, lr=3.59694e-05, gnorm=3.711, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=12147
2023-05-19 19:59:57 | INFO | train_inner | epoch 046:     45 / 65 loss=7.103, nll_loss=5.415, ppl=42.68, wps=3770.9, ups=0.31, wpb=12346.6, bsz=586, num_updates=2955, lr=3.59439e-05, gnorm=5.136, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=12163
2023-05-19 20:00:15 | INFO | train_inner | epoch 046:     50 / 65 loss=7.036, nll_loss=5.344, ppl=40.61, wps=3584.9, ups=0.29, wpb=12362, bsz=538.6, num_updates=2960, lr=3.59184e-05, gnorm=3.52, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=12180
2023-05-19 20:00:31 | INFO | train_inner | epoch 046:     55 / 65 loss=7.077, nll_loss=5.386, ppl=41.81, wps=3650.1, ups=0.3, wpb=12214.6, bsz=586.4, num_updates=2965, lr=3.58929e-05, gnorm=4.447, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=12197
2023-05-19 20:00:48 | INFO | train_inner | epoch 046:     60 / 65 loss=7.08, nll_loss=5.394, ppl=42.03, wps=3766.3, ups=0.29, wpb=12809.4, bsz=624.4, num_updates=2970, lr=3.58673e-05, gnorm=3.312, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=12214
2023-05-19 20:01:03 | INFO | train_inner | epoch 046:     65 / 65 loss=6.951, nll_loss=5.244, ppl=37.91, wps=3448.4, ups=0.34, wpb=10140, bsz=408, num_updates=2975, lr=3.58418e-05, gnorm=6.132, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=12229
2023-05-19 20:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:01:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:01:42 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.431 | nll_loss 4.603 | ppl 24.3 | bleu 3.31 | wps 911.4 | wpb 2785 | bsz 105.2 | num_updates 2975 | best_loss 6.405
2023-05-19 20:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2975 updates
2023-05-19 20:01:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint46.pt
2023-05-19 20:01:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint46.pt
2023-05-19 20:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint46.pt (epoch 46 @ 2975 updates, score 6.431) (writing took 6.561417866498232 seconds)
2023-05-19 20:01:49 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2023-05-19 20:01:49 | INFO | train | epoch 046 | loss 7.08 | nll_loss 5.389 | ppl 41.9 | wps 3100.5 | ups 0.26 | wpb 12041.7 | bsz 559 | num_updates 2975 | lr 3.58418e-05 | gnorm 5.533 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.7 | wall 12275
2023-05-19 20:01:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:01:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:01:49 | INFO | fairseq.trainer | begin training epoch 47
2023-05-19 20:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:02:06 | INFO | train_inner | epoch 047:      5 / 65 loss=7.074, nll_loss=5.376, ppl=41.53, wps=941.6, ups=0.08, wpb=11807.8, bsz=485.2, num_updates=2980, lr=3.58163e-05, gnorm=2.838, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=12292
2023-05-19 20:02:22 | INFO | train_inner | epoch 047:     10 / 65 loss=7.091, nll_loss=5.396, ppl=42.11, wps=3790.6, ups=0.31, wpb=12150.6, bsz=618, num_updates=2985, lr=3.57908e-05, gnorm=5.978, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=12308
2023-05-19 20:02:37 | INFO | train_inner | epoch 047:     15 / 65 loss=7.176, nll_loss=5.49, ppl=44.96, wps=4079.9, ups=0.33, wpb=12216, bsz=567.6, num_updates=2990, lr=3.57653e-05, gnorm=6.697, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=12323
2023-05-19 20:02:52 | INFO | train_inner | epoch 047:     20 / 65 loss=6.985, nll_loss=5.289, ppl=39.08, wps=3989, ups=0.33, wpb=12146.6, bsz=515.4, num_updates=2995, lr=3.57398e-05, gnorm=7.34, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=12338
2023-05-19 20:03:09 | INFO | train_inner | epoch 047:     25 / 65 loss=6.988, nll_loss=5.287, ppl=39.04, wps=3709.9, ups=0.3, wpb=12259.6, bsz=516.2, num_updates=3000, lr=3.57143e-05, gnorm=9.371, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=12354
2023-05-19 20:03:24 | INFO | train_inner | epoch 047:     30 / 65 loss=6.987, nll_loss=5.28, ppl=38.84, wps=3789.7, ups=0.32, wpb=12017.4, bsz=573, num_updates=3005, lr=3.56888e-05, gnorm=4.515, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=12370
2023-05-19 20:03:41 | INFO | train_inner | epoch 047:     35 / 65 loss=7.032, nll_loss=5.338, ppl=40.45, wps=3799.5, ups=0.3, wpb=12483.8, bsz=591.4, num_updates=3010, lr=3.56633e-05, gnorm=4.084, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.1, wall=12387
2023-05-19 20:03:58 | INFO | train_inner | epoch 047:     40 / 65 loss=6.987, nll_loss=5.288, ppl=39.07, wps=3668.5, ups=0.3, wpb=12357.4, bsz=575.2, num_updates=3015, lr=3.56378e-05, gnorm=7.943, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=12403
2023-05-19 20:04:14 | INFO | train_inner | epoch 047:     45 / 65 loss=7.184, nll_loss=5.487, ppl=44.86, wps=3692.2, ups=0.31, wpb=11900.6, bsz=577.6, num_updates=3020, lr=3.56122e-05, gnorm=7.105, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=12420
2023-05-19 20:04:30 | INFO | train_inner | epoch 047:     50 / 65 loss=7.091, nll_loss=5.399, ppl=42.19, wps=3998.7, ups=0.32, wpb=12639.2, bsz=618.6, num_updates=3025, lr=3.55867e-05, gnorm=7.102, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.6, wall=12435
2023-05-19 20:04:45 | INFO | train_inner | epoch 047:     55 / 65 loss=6.999, nll_loss=5.297, ppl=39.32, wps=3943.5, ups=0.32, wpb=12168.8, bsz=586, num_updates=3030, lr=3.55612e-05, gnorm=7.718, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=12451
2023-05-19 20:05:02 | INFO | train_inner | epoch 047:     60 / 65 loss=7.101, nll_loss=5.41, ppl=42.52, wps=3757, ups=0.3, wpb=12439.6, bsz=581.8, num_updates=3035, lr=3.55357e-05, gnorm=7.932, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=12467
2023-05-19 20:05:15 | INFO | train_inner | epoch 047:     65 / 65 loss=7.08, nll_loss=5.393, ppl=42.03, wps=3663.6, ups=0.36, wpb=10191.4, bsz=467.8, num_updates=3040, lr=3.55102e-05, gnorm=22.588, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=12481
2023-05-19 20:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:05:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:05:57 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.406 | nll_loss 4.579 | ppl 23.9 | bleu 2.82 | wps 872 | wpb 2785 | bsz 105.2 | num_updates 3040 | best_loss 6.405
2023-05-19 20:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3040 updates
2023-05-19 20:05:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint47.pt
2023-05-19 20:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint47.pt
2023-05-19 20:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint47.pt (epoch 47 @ 3040 updates, score 6.406) (writing took 6.628325462341309 seconds)
2023-05-19 20:06:03 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2023-05-19 20:06:03 | INFO | train | epoch 047 | loss 7.059 | nll_loss 5.363 | ppl 41.17 | wps 3083.1 | ups 0.26 | wpb 12059.9 | bsz 559.5 | num_updates 3040 | lr 3.55102e-05 | gnorm 7.785 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.6 | wall 12529
2023-05-19 20:06:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:06:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:06:03 | INFO | fairseq.trainer | begin training epoch 48
2023-05-19 20:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:06:19 | INFO | train_inner | epoch 048:      5 / 65 loss=7.094, nll_loss=5.405, ppl=42.37, wps=990.6, ups=0.08, wpb=12531, bsz=641.8, num_updates=3045, lr=3.54847e-05, gnorm=6.935, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.1, wall=12544
2023-05-19 20:06:34 | INFO | train_inner | epoch 048:     10 / 65 loss=7.016, nll_loss=5.318, ppl=39.9, wps=4101.3, ups=0.34, wpb=12160.8, bsz=569.8, num_updates=3050, lr=3.54592e-05, gnorm=16.809, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=12559
2023-05-19 20:06:52 | INFO | train_inner | epoch 048:     15 / 65 loss=6.972, nll_loss=5.269, ppl=38.56, wps=3724.5, ups=0.31, wpb=12062.6, bsz=522, num_updates=3055, lr=3.54337e-05, gnorm=9.454, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=12578
2023-05-19 20:07:07 | INFO | train_inner | epoch 048:     20 / 65 loss=7.017, nll_loss=5.321, ppl=39.97, wps=4014.9, ups=0.33, wpb=12183.2, bsz=543.8, num_updates=3060, lr=3.54082e-05, gnorm=7.167, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=12593
2023-05-19 20:07:23 | INFO | train_inner | epoch 048:     25 / 65 loss=7.065, nll_loss=5.372, ppl=41.41, wps=3851.2, ups=0.32, wpb=12199.6, bsz=538.4, num_updates=3065, lr=3.53827e-05, gnorm=7.478, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=12609
2023-05-19 20:07:40 | INFO | train_inner | epoch 048:     30 / 65 loss=6.943, nll_loss=5.233, ppl=37.62, wps=3471.8, ups=0.29, wpb=11947.6, bsz=520, num_updates=3070, lr=3.53571e-05, gnorm=8.068, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=12626
2023-05-19 20:07:57 | INFO | train_inner | epoch 048:     35 / 65 loss=6.922, nll_loss=5.205, ppl=36.9, wps=3625.5, ups=0.3, wpb=12173.8, bsz=562.4, num_updates=3075, lr=3.53316e-05, gnorm=24.317, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.9, wall=12643
2023-05-19 20:08:15 | INFO | train_inner | epoch 048:     40 / 65 loss=7.128, nll_loss=5.432, ppl=43.19, wps=3559.6, ups=0.29, wpb=12338, bsz=561.2, num_updates=3080, lr=3.53061e-05, gnorm=5.488, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=12660
2023-05-19 20:08:30 | INFO | train_inner | epoch 048:     45 / 65 loss=7.052, nll_loss=5.349, ppl=40.75, wps=3972.2, ups=0.32, wpb=12247.2, bsz=600, num_updates=3085, lr=3.52806e-05, gnorm=3.507, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=12676
2023-05-19 20:08:47 | INFO | train_inner | epoch 048:     50 / 65 loss=6.968, nll_loss=5.265, ppl=38.46, wps=3557.3, ups=0.3, wpb=11923.6, bsz=559.8, num_updates=3090, lr=3.52551e-05, gnorm=5.609, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=12693
2023-05-19 20:09:03 | INFO | train_inner | epoch 048:     55 / 65 loss=6.98, nll_loss=5.268, ppl=38.54, wps=3751.7, ups=0.32, wpb=11845, bsz=496.6, num_updates=3095, lr=3.52296e-05, gnorm=5.751, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=12708
2023-05-19 20:09:17 | INFO | train_inner | epoch 048:     60 / 65 loss=7.243, nll_loss=5.547, ppl=46.75, wps=4143.1, ups=0.34, wpb=12332.4, bsz=644.2, num_updates=3100, lr=3.52041e-05, gnorm=3.695, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=12723
2023-05-19 20:09:31 | INFO | train_inner | epoch 048:     65 / 65 loss=6.99, nll_loss=5.294, ppl=39.23, wps=4096.9, ups=0.38, wpb=10786.6, bsz=513.4, num_updates=3105, lr=3.51786e-05, gnorm=7.873, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.7, wall=12736
2023-05-19 20:09:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:09:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:10:11 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.345 | nll_loss 4.509 | ppl 22.77 | bleu 3.33 | wps 892.8 | wpb 2785 | bsz 105.2 | num_updates 3105 | best_loss 6.345
2023-05-19 20:10:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3105 updates
2023-05-19 20:10:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint48.pt
2023-05-19 20:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint48.pt
2023-05-19 20:10:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint48.pt (epoch 48 @ 3105 updates, score 6.345) (writing took 15.927445363253355 seconds)
2023-05-19 20:10:27 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2023-05-19 20:10:27 | INFO | train | epoch 048 | loss 7.031 | nll_loss 5.33 | ppl 40.24 | wps 2973.3 | ups 0.25 | wpb 12056.3 | bsz 559.5 | num_updates 3105 | lr 3.51786e-05 | gnorm 8.627 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.7 | wall 12793
2023-05-19 20:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:10:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:10:27 | INFO | fairseq.trainer | begin training epoch 49
2023-05-19 20:10:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:10:42 | INFO | train_inner | epoch 049:      5 / 65 loss=7.029, nll_loss=5.326, ppl=40.13, wps=867.2, ups=0.07, wpb=12377.6, bsz=587.6, num_updates=3110, lr=3.51531e-05, gnorm=4.971, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=12808
2023-05-19 20:10:58 | INFO | train_inner | epoch 049:     10 / 65 loss=7.031, nll_loss=5.33, ppl=40.23, wps=3739.9, ups=0.31, wpb=12109.2, bsz=552.4, num_updates=3115, lr=3.51276e-05, gnorm=4.153, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=12824
2023-05-19 20:11:14 | INFO | train_inner | epoch 049:     15 / 65 loss=6.997, nll_loss=5.289, ppl=39.09, wps=3772.9, ups=0.32, wpb=11833.8, bsz=523.8, num_updates=3120, lr=3.5102e-05, gnorm=2.815, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=12840
2023-05-19 20:11:32 | INFO | train_inner | epoch 049:     20 / 65 loss=7.124, nll_loss=5.438, ppl=43.35, wps=3502, ups=0.28, wpb=12565.6, bsz=679.8, num_updates=3125, lr=3.50765e-05, gnorm=8.169, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=12858
2023-05-19 20:11:49 | INFO | train_inner | epoch 049:     25 / 65 loss=6.98, nll_loss=5.266, ppl=38.49, wps=3580.3, ups=0.3, wpb=12034, bsz=532.4, num_updates=3130, lr=3.5051e-05, gnorm=9.671, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=12874
2023-05-19 20:12:05 | INFO | train_inner | epoch 049:     30 / 65 loss=6.982, nll_loss=5.278, ppl=38.81, wps=3686.7, ups=0.31, wpb=11790.8, bsz=549.8, num_updates=3135, lr=3.50255e-05, gnorm=7.092, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.4, wall=12890
2023-05-19 20:12:21 | INFO | train_inner | epoch 049:     35 / 65 loss=7.006, nll_loss=5.301, ppl=39.42, wps=3605.2, ups=0.3, wpb=11921.2, bsz=489.6, num_updates=3140, lr=3.5e-05, gnorm=4.071, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=12907
2023-05-19 20:12:38 | INFO | train_inner | epoch 049:     40 / 65 loss=7.048, nll_loss=5.346, ppl=40.69, wps=3636, ups=0.3, wpb=12058.8, bsz=573.8, num_updates=3145, lr=3.49745e-05, gnorm=4.702, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=12923
2023-05-19 20:12:53 | INFO | train_inner | epoch 049:     45 / 65 loss=6.969, nll_loss=5.253, ppl=38.13, wps=4084.4, ups=0.33, wpb=12561.6, bsz=551.4, num_updates=3150, lr=3.4949e-05, gnorm=5.853, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=12939
2023-05-19 20:13:09 | INFO | train_inner | epoch 049:     50 / 65 loss=7.11, nll_loss=5.408, ppl=42.45, wps=3948.9, ups=0.32, wpb=12427, bsz=594.4, num_updates=3155, lr=3.49235e-05, gnorm=6.303, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=12955
2023-05-19 20:13:25 | INFO | train_inner | epoch 049:     55 / 65 loss=6.951, nll_loss=5.242, ppl=37.85, wps=3742.3, ups=0.3, wpb=12273, bsz=540, num_updates=3160, lr=3.4898e-05, gnorm=9.716, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=12971
2023-05-19 20:13:41 | INFO | train_inner | epoch 049:     60 / 65 loss=6.906, nll_loss=5.187, ppl=36.43, wps=3932.7, ups=0.33, wpb=12061.8, bsz=550, num_updates=3165, lr=3.48724e-05, gnorm=4.771, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=12986
2023-05-19 20:13:56 | INFO | train_inner | epoch 049:     65 / 65 loss=7.243, nll_loss=5.541, ppl=46.57, wps=3443.9, ups=0.33, wpb=10569.2, bsz=525.6, num_updates=3170, lr=3.48469e-05, gnorm=8.609, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=13002
2023-05-19 20:13:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:13:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:14:36 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.382 | nll_loss 4.543 | ppl 23.31 | bleu 3.47 | wps 893.2 | wpb 2785 | bsz 105.2 | num_updates 3170 | best_loss 6.345
2023-05-19 20:14:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3170 updates
2023-05-19 20:14:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint49.pt
2023-05-19 20:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint49.pt
2023-05-19 20:14:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint49.pt (epoch 49 @ 3170 updates, score 6.382) (writing took 6.409409508109093 seconds)
2023-05-19 20:14:42 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2023-05-19 20:14:42 | INFO | train | epoch 049 | loss 7.027 | nll_loss 5.322 | ppl 40 | wps 3061.5 | ups 0.25 | wpb 12044.9 | bsz 557.7 | num_updates 3170 | lr 3.48469e-05 | gnorm 6.223 | clip 100 | loss_scale 0.25 | train_wall 209 | gb_free 21.6 | wall 13048
2023-05-19 20:14:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:14:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:14:43 | INFO | fairseq.trainer | begin training epoch 50
2023-05-19 20:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:14:58 | INFO | train_inner | epoch 050:      5 / 65 loss=6.967, nll_loss=5.259, ppl=38.31, wps=989.4, ups=0.08, wpb=12321, bsz=572.8, num_updates=3175, lr=3.48214e-05, gnorm=3.038, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=13064
2023-05-19 20:15:14 | INFO | train_inner | epoch 050:     10 / 65 loss=7.022, nll_loss=5.323, ppl=40.04, wps=3785.2, ups=0.31, wpb=12316.8, bsz=585, num_updates=3180, lr=3.47959e-05, gnorm=4.109, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=13080
2023-05-19 20:15:32 | INFO | train_inner | epoch 050:     15 / 65 loss=7.071, nll_loss=5.376, ppl=41.53, wps=3501.8, ups=0.29, wpb=12161, bsz=579.2, num_updates=3185, lr=3.47704e-05, gnorm=15.655, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=13098
2023-05-19 20:15:50 | INFO | train_inner | epoch 050:     20 / 65 loss=6.91, nll_loss=5.192, ppl=36.56, wps=3285.2, ups=0.28, wpb=11879.4, bsz=478.2, num_updates=3190, lr=3.47449e-05, gnorm=6.721, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=13116
2023-05-19 20:16:06 | INFO | train_inner | epoch 050:     25 / 65 loss=6.897, nll_loss=5.182, ppl=36.29, wps=3829.1, ups=0.31, wpb=12425.4, bsz=557.8, num_updates=3195, lr=3.47194e-05, gnorm=3.236, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=13132
2023-05-19 20:16:22 | INFO | train_inner | epoch 050:     30 / 65 loss=6.99, nll_loss=5.282, ppl=38.92, wps=4009.7, ups=0.32, wpb=12371.8, bsz=597.8, num_updates=3200, lr=3.46939e-05, gnorm=4.638, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=13147
2023-05-19 20:16:37 | INFO | train_inner | epoch 050:     35 / 65 loss=7.026, nll_loss=5.325, ppl=40.1, wps=4019.7, ups=0.32, wpb=12474.2, bsz=608.8, num_updates=3205, lr=3.46684e-05, gnorm=4.49, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=13163
2023-05-19 20:16:52 | INFO | train_inner | epoch 050:     40 / 65 loss=6.974, nll_loss=5.273, ppl=38.67, wps=3897, ups=0.32, wpb=12005, bsz=529.8, num_updates=3210, lr=3.46429e-05, gnorm=4.798, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=13178
2023-05-19 20:17:08 | INFO | train_inner | epoch 050:     45 / 65 loss=7.001, nll_loss=5.297, ppl=39.3, wps=3931.8, ups=0.33, wpb=12064.8, bsz=567.4, num_updates=3215, lr=3.46173e-05, gnorm=3.468, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=13194
2023-05-19 20:17:23 | INFO | train_inner | epoch 050:     50 / 65 loss=7.248, nll_loss=5.542, ppl=46.6, wps=4018, ups=0.33, wpb=12341, bsz=607.8, num_updates=3220, lr=3.45918e-05, gnorm=4.848, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=13209
2023-05-19 20:17:38 | INFO | train_inner | epoch 050:     55 / 65 loss=7.144, nll_loss=5.436, ppl=43.28, wps=4041.3, ups=0.33, wpb=12113.2, bsz=593.2, num_updates=3225, lr=3.45663e-05, gnorm=10.786, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=13224
2023-05-19 20:17:53 | INFO | train_inner | epoch 050:     60 / 65 loss=7.001, nll_loss=5.3, ppl=39.39, wps=4007.7, ups=0.33, wpb=12003.4, bsz=543, num_updates=3230, lr=3.45408e-05, gnorm=9.316, clip=100, loss_scale=0.25, train_wall=15, gb_free=21, wall=13239
2023-05-19 20:18:06 | INFO | train_inner | epoch 050:     65 / 65 loss=7.04, nll_loss=5.337, ppl=40.43, wps=3855.3, ups=0.38, wpb=10259.8, bsz=455.2, num_updates=3235, lr=3.45153e-05, gnorm=24.994, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=13252
2023-05-19 20:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:18:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:18:48 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.373 | nll_loss 4.536 | ppl 23.2 | bleu 3.37 | wps 856.1 | wpb 2785 | bsz 105.2 | num_updates 3235 | best_loss 6.345
2023-05-19 20:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3235 updates
2023-05-19 20:18:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint50.pt
2023-05-19 20:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint50.pt
2023-05-19 20:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint50.pt (epoch 50 @ 3235 updates, score 6.373) (writing took 9.846210934221745 seconds)
2023-05-19 20:18:58 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2023-05-19 20:18:58 | INFO | train | epoch 050 | loss 7.022 | nll_loss 5.317 | ppl 39.87 | wps 3067.4 | ups 0.25 | wpb 12056.7 | bsz 559.7 | num_updates 3235 | lr 3.45153e-05 | gnorm 7.7 | clip 100 | loss_scale 0.25 | train_wall 203 | gb_free 21.6 | wall 13304
2023-05-19 20:18:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:18:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:18:58 | INFO | fairseq.trainer | begin training epoch 51
2023-05-19 20:18:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:19:14 | INFO | train_inner | epoch 051:      5 / 65 loss=7.218, nll_loss=5.526, ppl=46.08, wps=936.1, ups=0.07, wpb=12585.6, bsz=638.4, num_updates=3240, lr=3.44898e-05, gnorm=4.246, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=13319
2023-05-19 20:19:29 | INFO | train_inner | epoch 051:     10 / 65 loss=7.014, nll_loss=5.309, ppl=39.63, wps=4094.8, ups=0.33, wpb=12444.8, bsz=620.6, num_updates=3245, lr=3.44643e-05, gnorm=4.578, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.8, wall=13335
2023-05-19 20:19:44 | INFO | train_inner | epoch 051:     15 / 65 loss=6.885, nll_loss=5.166, ppl=35.9, wps=3802.4, ups=0.32, wpb=11840.2, bsz=492, num_updates=3250, lr=3.44388e-05, gnorm=6.4, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=13350
2023-05-19 20:20:00 | INFO | train_inner | epoch 051:     20 / 65 loss=6.949, nll_loss=5.241, ppl=37.82, wps=3856.5, ups=0.32, wpb=12171.8, bsz=568.4, num_updates=3255, lr=3.44133e-05, gnorm=4.4, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=13366
2023-05-19 20:20:16 | INFO | train_inner | epoch 051:     25 / 65 loss=7.024, nll_loss=5.321, ppl=39.99, wps=3831, ups=0.32, wpb=11998.4, bsz=540.2, num_updates=3260, lr=3.43878e-05, gnorm=7.101, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=13382
2023-05-19 20:20:33 | INFO | train_inner | epoch 051:     30 / 65 loss=6.969, nll_loss=5.263, ppl=38.4, wps=3440.2, ups=0.28, wpb=12106, bsz=513.2, num_updates=3265, lr=3.43622e-05, gnorm=5.993, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=13399
2023-05-19 20:20:50 | INFO | train_inner | epoch 051:     35 / 65 loss=7.021, nll_loss=5.318, ppl=39.88, wps=3730.5, ups=0.3, wpb=12509.4, bsz=582.2, num_updates=3270, lr=3.43367e-05, gnorm=3.664, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=13416
2023-05-19 20:21:06 | INFO | train_inner | epoch 051:     40 / 65 loss=6.99, nll_loss=5.289, ppl=39.11, wps=3823, ups=0.32, wpb=12119.4, bsz=530.4, num_updates=3275, lr=3.43112e-05, gnorm=2.642, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=13432
2023-05-19 20:21:23 | INFO | train_inner | epoch 051:     45 / 65 loss=7.105, nll_loss=5.411, ppl=42.54, wps=3711.4, ups=0.3, wpb=12296.6, bsz=562.6, num_updates=3280, lr=3.42857e-05, gnorm=4.579, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=13448
2023-05-19 20:21:39 | INFO | train_inner | epoch 051:     50 / 65 loss=6.995, nll_loss=5.293, ppl=39.21, wps=3771.8, ups=0.31, wpb=12026.2, bsz=557.8, num_updates=3285, lr=3.42602e-05, gnorm=6.484, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=13464
2023-05-19 20:21:56 | INFO | train_inner | epoch 051:     55 / 65 loss=7.038, nll_loss=5.334, ppl=40.35, wps=3537, ups=0.29, wpb=12227.6, bsz=571, num_updates=3290, lr=3.42347e-05, gnorm=3.187, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=13482
2023-05-19 20:22:11 | INFO | train_inner | epoch 051:     60 / 65 loss=7.04, nll_loss=5.341, ppl=40.54, wps=3891.5, ups=0.32, wpb=12103.8, bsz=631.8, num_updates=3295, lr=3.42092e-05, gnorm=3.555, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=13497
2023-05-19 20:22:25 | INFO | train_inner | epoch 051:     65 / 65 loss=7.007, nll_loss=5.308, ppl=39.62, wps=3600.5, ups=0.36, wpb=10124.8, bsz=449.8, num_updates=3300, lr=3.41837e-05, gnorm=9.711, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.9, wall=13511
2023-05-19 20:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:22:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:23:05 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.388 | nll_loss 4.556 | ppl 23.51 | bleu 3.4 | wps 895 | wpb 2785 | bsz 105.2 | num_updates 3300 | best_loss 6.345
2023-05-19 20:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 3300 updates
2023-05-19 20:23:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint51.pt
2023-05-19 20:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint51.pt
2023-05-19 20:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint51.pt (epoch 51 @ 3300 updates, score 6.388) (writing took 6.841265458613634 seconds)
2023-05-19 20:23:12 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2023-05-19 20:23:12 | INFO | train | epoch 051 | loss 7.021 | nll_loss 5.318 | ppl 39.89 | wps 3078.1 | ups 0.26 | wpb 12042.7 | bsz 558.3 | num_updates 3300 | lr 3.41837e-05 | gnorm 5.118 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.9 | wall 13558
2023-05-19 20:23:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:23:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:23:12 | INFO | fairseq.trainer | begin training epoch 52
2023-05-19 20:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:23:30 | INFO | train_inner | epoch 052:      5 / 65 loss=7.002, nll_loss=5.299, ppl=39.36, wps=942.6, ups=0.08, wpb=12107.2, bsz=559.8, num_updates=3305, lr=3.41582e-05, gnorm=2.946, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=13575
2023-05-19 20:23:45 | INFO | train_inner | epoch 052:     10 / 65 loss=6.955, nll_loss=5.249, ppl=38.04, wps=3920.4, ups=0.32, wpb=12099.8, bsz=555.8, num_updates=3310, lr=3.41327e-05, gnorm=3.943, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=13591
2023-05-19 20:24:02 | INFO | train_inner | epoch 052:     15 / 65 loss=6.926, nll_loss=5.216, ppl=37.17, wps=3639.5, ups=0.3, wpb=11980.8, bsz=519.2, num_updates=3315, lr=3.41071e-05, gnorm=9.704, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=13607
2023-05-19 20:24:17 | INFO | train_inner | epoch 052:     20 / 65 loss=7.244, nll_loss=5.555, ppl=47.02, wps=3983.1, ups=0.32, wpb=12471.2, bsz=626.6, num_updates=3320, lr=3.40816e-05, gnorm=3.739, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=13623
2023-05-19 20:24:33 | INFO | train_inner | epoch 052:     25 / 65 loss=6.886, nll_loss=5.174, ppl=36.09, wps=3690.5, ups=0.31, wpb=11895.4, bsz=483.6, num_updates=3325, lr=3.40561e-05, gnorm=5.96, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=13639
2023-05-19 20:24:49 | INFO | train_inner | epoch 052:     30 / 65 loss=6.943, nll_loss=5.233, ppl=37.61, wps=3795, ups=0.32, wpb=11924, bsz=516.2, num_updates=3330, lr=3.40306e-05, gnorm=12.329, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=13655
2023-05-19 20:25:05 | INFO | train_inner | epoch 052:     35 / 65 loss=7.054, nll_loss=5.345, ppl=40.66, wps=3914.5, ups=0.32, wpb=12228, bsz=555.2, num_updates=3335, lr=3.40051e-05, gnorm=4.259, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=13670
2023-05-19 20:25:21 | INFO | train_inner | epoch 052:     40 / 65 loss=6.967, nll_loss=5.251, ppl=38.08, wps=3677.7, ups=0.31, wpb=12010, bsz=533, num_updates=3340, lr=3.39796e-05, gnorm=5.691, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=13687
2023-05-19 20:25:38 | INFO | train_inner | epoch 052:     45 / 65 loss=7.008, nll_loss=5.295, ppl=39.26, wps=3404.3, ups=0.29, wpb=11858.4, bsz=551.8, num_updates=3345, lr=3.39541e-05, gnorm=6.012, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=13704
2023-05-19 20:25:54 | INFO | train_inner | epoch 052:     50 / 65 loss=7.02, nll_loss=5.313, ppl=39.75, wps=3961.4, ups=0.31, wpb=12639, bsz=598.8, num_updates=3350, lr=3.39286e-05, gnorm=3.914, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=13720
2023-05-19 20:26:10 | INFO | train_inner | epoch 052:     55 / 65 loss=6.988, nll_loss=5.277, ppl=38.76, wps=3892.9, ups=0.32, wpb=12345, bsz=601.4, num_updates=3355, lr=3.39031e-05, gnorm=6.474, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=13736
2023-05-19 20:26:27 | INFO | train_inner | epoch 052:     60 / 65 loss=7.049, nll_loss=5.36, ppl=41.06, wps=3800.3, ups=0.31, wpb=12453.2, bsz=616.8, num_updates=3360, lr=3.38776e-05, gnorm=5.823, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=13752
2023-05-19 20:26:40 | INFO | train_inner | epoch 052:     65 / 65 loss=7.011, nll_loss=5.315, ppl=39.81, wps=4007.1, ups=0.37, wpb=10694.8, bsz=549.2, num_updates=3365, lr=3.3852e-05, gnorm=5.378, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.2, wall=13766
2023-05-19 20:26:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:27:20 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.344 | nll_loss 4.506 | ppl 22.73 | bleu 3.21 | wps 893.9 | wpb 2785 | bsz 105.2 | num_updates 3365 | best_loss 6.344
2023-05-19 20:27:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 3365 updates
2023-05-19 20:27:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint52.pt
2023-05-19 20:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint52.pt
2023-05-19 20:27:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint52.pt (epoch 52 @ 3365 updates, score 6.344) (writing took 26.600961804389954 seconds)
2023-05-19 20:27:47 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2023-05-19 20:27:47 | INFO | train | epoch 052 | loss 7.005 | nll_loss 5.3 | ppl 39.39 | wps 2855.1 | ups 0.24 | wpb 12054.4 | bsz 559 | num_updates 3365 | lr 3.3852e-05 | gnorm 5.859 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.2 | wall 13832
2023-05-19 20:27:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:27:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:27:47 | INFO | fairseq.trainer | begin training epoch 53
2023-05-19 20:27:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:28:04 | INFO | train_inner | epoch 053:      5 / 65 loss=6.995, nll_loss=5.294, ppl=39.24, wps=719.5, ups=0.06, wpb=12080.4, bsz=552.8, num_updates=3370, lr=3.38265e-05, gnorm=5.738, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.3, wall=13850
2023-05-19 20:28:20 | INFO | train_inner | epoch 053:     10 / 65 loss=7.266, nll_loss=5.569, ppl=47.48, wps=3833.1, ups=0.31, wpb=12461.8, bsz=628.8, num_updates=3375, lr=3.3801e-05, gnorm=3.753, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=13866
2023-05-19 20:28:37 | INFO | train_inner | epoch 053:     15 / 65 loss=7.036, nll_loss=5.337, ppl=40.42, wps=3560.9, ups=0.29, wpb=12343.6, bsz=556, num_updates=3380, lr=3.37755e-05, gnorm=6.579, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=13883
2023-05-19 20:28:52 | INFO | train_inner | epoch 053:     20 / 65 loss=6.922, nll_loss=5.205, ppl=36.88, wps=4314.8, ups=0.35, wpb=12349.6, bsz=562.6, num_updates=3385, lr=3.375e-05, gnorm=3.965, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=13898
2023-05-19 20:29:07 | INFO | train_inner | epoch 053:     25 / 65 loss=6.902, nll_loss=5.186, ppl=36.41, wps=4002.2, ups=0.32, wpb=12526.6, bsz=559.2, num_updates=3390, lr=3.37245e-05, gnorm=14.679, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=13913
2023-05-19 20:29:24 | INFO | train_inner | epoch 053:     30 / 65 loss=6.946, nll_loss=5.235, ppl=37.67, wps=3605.9, ups=0.3, wpb=11848.4, bsz=511, num_updates=3395, lr=3.3699e-05, gnorm=5.442, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=13930
2023-05-19 20:29:42 | INFO | train_inner | epoch 053:     35 / 65 loss=6.939, nll_loss=5.232, ppl=37.57, wps=3232.4, ups=0.28, wpb=11721, bsz=503.4, num_updates=3400, lr=3.36735e-05, gnorm=3.855, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.4, wall=13948
2023-05-19 20:29:57 | INFO | train_inner | epoch 053:     40 / 65 loss=6.874, nll_loss=5.156, ppl=35.66, wps=4065.4, ups=0.33, wpb=12366.8, bsz=552.8, num_updates=3405, lr=3.3648e-05, gnorm=5.605, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=13963
2023-05-19 20:30:13 | INFO | train_inner | epoch 053:     45 / 65 loss=6.992, nll_loss=5.292, ppl=39.17, wps=3883, ups=0.31, wpb=12401.8, bsz=592.2, num_updates=3410, lr=3.36224e-05, gnorm=6.106, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=13979
2023-05-19 20:30:29 | INFO | train_inner | epoch 053:     50 / 65 loss=7.059, nll_loss=5.362, ppl=41.12, wps=3882.8, ups=0.32, wpb=12258.2, bsz=617.2, num_updates=3415, lr=3.35969e-05, gnorm=6.29, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=13995
2023-05-19 20:30:45 | INFO | train_inner | epoch 053:     55 / 65 loss=7.013, nll_loss=5.312, ppl=39.72, wps=3604.2, ups=0.31, wpb=11757.6, bsz=539.4, num_updates=3420, lr=3.35714e-05, gnorm=3.858, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=14011
2023-05-19 20:31:00 | INFO | train_inner | epoch 053:     60 / 65 loss=6.965, nll_loss=5.255, ppl=38.18, wps=4166.2, ups=0.34, wpb=12291.4, bsz=576.8, num_updates=3425, lr=3.35459e-05, gnorm=4.193, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=14026
2023-05-19 20:31:13 | INFO | train_inner | epoch 053:     65 / 65 loss=7.05, nll_loss=5.358, ppl=41, wps=4096.3, ups=0.4, wpb=10297.4, bsz=511.4, num_updates=3430, lr=3.35204e-05, gnorm=7.105, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=14038
2023-05-19 20:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:31:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:31:54 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.343 | nll_loss 4.499 | ppl 22.62 | bleu 3.35 | wps 873.3 | wpb 2785 | bsz 105.2 | num_updates 3430 | best_loss 6.343
2023-05-19 20:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 3430 updates
2023-05-19 20:31:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint53.pt
2023-05-19 20:31:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint53.pt
2023-05-19 20:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint53.pt (epoch 53 @ 3430 updates, score 6.343) (writing took 25.534596107900143 seconds)
2023-05-19 20:32:19 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2023-05-19 20:32:19 | INFO | train | epoch 053 | loss 6.996 | nll_loss 5.291 | ppl 39.16 | wps 2876.6 | ups 0.24 | wpb 12054.2 | bsz 558.7 | num_updates 3430 | lr 3.35204e-05 | gnorm 5.936 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.6 | wall 14105
2023-05-19 20:32:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:32:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:32:19 | INFO | fairseq.trainer | begin training epoch 54
2023-05-19 20:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:32:34 | INFO | train_inner | epoch 054:      5 / 65 loss=6.959, nll_loss=5.253, ppl=38.14, wps=735.8, ups=0.06, wpb=11965.8, bsz=538.8, num_updates=3435, lr=3.34949e-05, gnorm=2.896, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=14120
2023-05-19 20:32:51 | INFO | train_inner | epoch 054:     10 / 65 loss=6.933, nll_loss=5.225, ppl=37.41, wps=3533.2, ups=0.29, wpb=12021.4, bsz=527.4, num_updates=3440, lr=3.34694e-05, gnorm=8.354, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=14137
2023-05-19 20:33:07 | INFO | train_inner | epoch 054:     15 / 65 loss=6.92, nll_loss=5.197, ppl=36.69, wps=3708.4, ups=0.3, wpb=12282.4, bsz=549, num_updates=3445, lr=3.34439e-05, gnorm=5.459, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.9, wall=14153
2023-05-19 20:33:23 | INFO | train_inner | epoch 054:     20 / 65 loss=6.943, nll_loss=5.227, ppl=37.46, wps=3796.9, ups=0.32, wpb=11801.2, bsz=530.8, num_updates=3450, lr=3.34184e-05, gnorm=9.157, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=14169
2023-05-19 20:33:39 | INFO | train_inner | epoch 054:     25 / 65 loss=7.165, nll_loss=5.465, ppl=44.16, wps=3940.3, ups=0.32, wpb=12154.2, bsz=616.6, num_updates=3455, lr=3.33929e-05, gnorm=8.046, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=14185
2023-05-19 20:33:56 | INFO | train_inner | epoch 054:     30 / 65 loss=6.918, nll_loss=5.206, ppl=36.92, wps=3643.2, ups=0.29, wpb=12384.4, bsz=533.4, num_updates=3460, lr=3.33673e-05, gnorm=3.717, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=14202
2023-05-19 20:34:12 | INFO | train_inner | epoch 054:     35 / 65 loss=6.993, nll_loss=5.292, ppl=39.17, wps=3910.1, ups=0.32, wpb=12357.4, bsz=628, num_updates=3465, lr=3.33418e-05, gnorm=9.679, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=14217
2023-05-19 20:34:28 | INFO | train_inner | epoch 054:     40 / 65 loss=7.04, nll_loss=5.341, ppl=40.54, wps=3598, ups=0.3, wpb=12066.6, bsz=557.4, num_updates=3470, lr=3.33163e-05, gnorm=4.319, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=14234
2023-05-19 20:34:43 | INFO | train_inner | epoch 054:     45 / 65 loss=7.117, nll_loss=5.416, ppl=42.68, wps=4040.2, ups=0.34, wpb=11793.4, bsz=579.8, num_updates=3475, lr=3.32908e-05, gnorm=13.708, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=14249
2023-05-19 20:34:58 | INFO | train_inner | epoch 054:     50 / 65 loss=6.99, nll_loss=5.282, ppl=38.92, wps=4021.9, ups=0.32, wpb=12407.6, bsz=584.6, num_updates=3480, lr=3.32653e-05, gnorm=3.774, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=14264
2023-05-19 20:35:14 | INFO | train_inner | epoch 054:     55 / 65 loss=6.975, nll_loss=5.273, ppl=38.67, wps=3919.4, ups=0.32, wpb=12387.8, bsz=540, num_updates=3485, lr=3.32398e-05, gnorm=3.305, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=14280
2023-05-19 20:35:31 | INFO | train_inner | epoch 054:     60 / 65 loss=6.964, nll_loss=5.267, ppl=38.51, wps=3732.6, ups=0.3, wpb=12404.2, bsz=583, num_updates=3490, lr=3.32143e-05, gnorm=4.372, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=14297
2023-05-19 20:35:43 | INFO | train_inner | epoch 054:     65 / 65 loss=6.992, nll_loss=5.296, ppl=39.29, wps=4256.9, ups=0.4, wpb=10726.8, bsz=512.4, num_updates=3495, lr=3.31888e-05, gnorm=4.949, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.7, wall=14309
2023-05-19 20:35:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:35:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:36:26 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.352 | nll_loss 4.515 | ppl 22.87 | bleu 3.54 | wps 841.8 | wpb 2785 | bsz 105.2 | num_updates 3495 | best_loss 6.343
2023-05-19 20:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 3495 updates
2023-05-19 20:36:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint54.pt
2023-05-19 20:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint54.pt
2023-05-19 20:36:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint54.pt (epoch 54 @ 3495 updates, score 6.352) (writing took 8.810253109782934 seconds)
2023-05-19 20:36:35 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2023-05-19 20:36:35 | INFO | train | epoch 054 | loss 6.993 | nll_loss 5.287 | ppl 39.05 | wps 3066.5 | ups 0.25 | wpb 12057.9 | bsz 560.1 | num_updates 3495 | lr 3.31888e-05 | gnorm 6.287 | clip 100 | loss_scale 0.25 | train_wall 203 | gb_free 21.7 | wall 14360
2023-05-19 20:36:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:36:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:36:35 | INFO | fairseq.trainer | begin training epoch 55
2023-05-19 20:36:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:36:54 | INFO | train_inner | epoch 055:      5 / 65 loss=6.955, nll_loss=5.249, ppl=38.02, wps=853.8, ups=0.07, wpb=11998.4, bsz=507.8, num_updates=3500, lr=3.31633e-05, gnorm=4.227, clip=100, loss_scale=0.25, train_wall=19, gb_free=20.6, wall=14379
2023-05-19 20:37:11 | INFO | train_inner | epoch 055:     10 / 65 loss=6.953, nll_loss=5.251, ppl=38.09, wps=3483, ups=0.28, wpb=12257.4, bsz=559.2, num_updates=3505, lr=3.31378e-05, gnorm=7.537, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=14397
2023-05-19 20:37:27 | INFO | train_inner | epoch 055:     15 / 65 loss=6.952, nll_loss=5.247, ppl=37.97, wps=3889.3, ups=0.31, wpb=12364.2, bsz=576.8, num_updates=3510, lr=3.31122e-05, gnorm=5.722, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=14413
2023-05-19 20:37:43 | INFO | train_inner | epoch 055:     20 / 65 loss=6.96, nll_loss=5.255, ppl=38.18, wps=3661.5, ups=0.31, wpb=11925.2, bsz=544.2, num_updates=3515, lr=3.30867e-05, gnorm=5.127, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=14429
2023-05-19 20:38:00 | INFO | train_inner | epoch 055:     25 / 65 loss=7.034, nll_loss=5.337, ppl=40.41, wps=3947.4, ups=0.31, wpb=12713.6, bsz=619.4, num_updates=3520, lr=3.30612e-05, gnorm=26.927, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=14445
2023-05-19 20:38:15 | INFO | train_inner | epoch 055:     30 / 65 loss=7.035, nll_loss=5.335, ppl=40.35, wps=3874.6, ups=0.32, wpb=12266, bsz=574.8, num_updates=3525, lr=3.30357e-05, gnorm=3.886, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=14461
2023-05-19 20:38:31 | INFO | train_inner | epoch 055:     35 / 65 loss=6.891, nll_loss=5.177, ppl=36.19, wps=3827.3, ups=0.33, wpb=11766.8, bsz=503.2, num_updates=3530, lr=3.30102e-05, gnorm=5.066, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=14477
2023-05-19 20:38:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-19 20:38:50 | INFO | train_inner | epoch 055:     41 / 65 loss=7.022, nll_loss=5.32, ppl=39.94, wps=3089.7, ups=0.26, wpb=11803.2, bsz=530, num_updates=3535, lr=3.29847e-05, gnorm=2.638, clip=100, loss_scale=0.125, train_wall=19, gb_free=21.5, wall=14496
2023-05-19 20:39:05 | INFO | train_inner | epoch 055:     46 / 65 loss=7.209, nll_loss=5.525, ppl=46.06, wps=4097.4, ups=0.32, wpb=12710.8, bsz=654.4, num_updates=3540, lr=3.29592e-05, gnorm=3.64, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.2, wall=14511
2023-05-19 20:39:22 | INFO | train_inner | epoch 055:     51 / 65 loss=6.974, nll_loss=5.279, ppl=38.82, wps=3678.3, ups=0.31, wpb=11875.2, bsz=532, num_updates=3545, lr=3.29337e-05, gnorm=4.59, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=14527
2023-05-19 20:39:36 | INFO | train_inner | epoch 055:     56 / 65 loss=7.008, nll_loss=5.313, ppl=39.74, wps=4235.1, ups=0.34, wpb=12377.6, bsz=601.2, num_updates=3550, lr=3.29082e-05, gnorm=3.279, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=14542
2023-05-19 20:39:51 | INFO | train_inner | epoch 055:     61 / 65 loss=7.108, nll_loss=5.418, ppl=42.76, wps=4138.1, ups=0.34, wpb=12277.2, bsz=572.8, num_updates=3555, lr=3.28827e-05, gnorm=3.614, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=14557
2023-05-19 20:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:40:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:40:42 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.397 | nll_loss 4.571 | ppl 23.77 | bleu 3.27 | wps 878.9 | wpb 2785 | bsz 105.2 | num_updates 3559 | best_loss 6.343
2023-05-19 20:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 3559 updates
2023-05-19 20:40:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint55.pt
2023-05-19 20:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint55.pt
2023-05-19 20:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint55.pt (epoch 55 @ 3559 updates, score 6.397) (writing took 6.672433253377676 seconds)
2023-05-19 20:40:49 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2023-05-19 20:40:49 | INFO | train | epoch 055 | loss 7.006 | nll_loss 5.307 | ppl 39.58 | wps 3036.6 | ups 0.25 | wpb 12051.3 | bsz 558.4 | num_updates 3559 | lr 3.28622e-05 | gnorm 6.319 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.8 | wall 14614
2023-05-19 20:40:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:40:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:40:49 | INFO | fairseq.trainer | begin training epoch 56
2023-05-19 20:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:40:52 | INFO | train_inner | epoch 056:      1 / 65 loss=6.936, nll_loss=5.237, ppl=37.71, wps=854.7, ups=0.08, wpb=10389.6, bsz=479, num_updates=3560, lr=3.28571e-05, gnorm=8.798, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.5, wall=14618
2023-05-19 20:41:09 | INFO | train_inner | epoch 056:      6 / 65 loss=6.992, nll_loss=5.294, ppl=39.23, wps=3379.6, ups=0.28, wpb=11930.4, bsz=523.4, num_updates=3565, lr=3.28316e-05, gnorm=6.302, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=14635
2023-05-19 20:41:25 | INFO | train_inner | epoch 056:     11 / 65 loss=6.969, nll_loss=5.27, ppl=38.58, wps=4168.7, ups=0.33, wpb=12687.4, bsz=598.4, num_updates=3570, lr=3.28061e-05, gnorm=4.612, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=14650
2023-05-19 20:41:40 | INFO | train_inner | epoch 056:     16 / 65 loss=6.958, nll_loss=5.248, ppl=38.01, wps=4009.2, ups=0.33, wpb=12282.2, bsz=558.2, num_updates=3575, lr=3.27806e-05, gnorm=3.379, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.1, wall=14666
2023-05-19 20:41:55 | INFO | train_inner | epoch 056:     21 / 65 loss=6.916, nll_loss=5.201, ppl=36.79, wps=3898.2, ups=0.33, wpb=11840, bsz=523.4, num_updates=3580, lr=3.27551e-05, gnorm=3.715, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=14681
2023-05-19 20:42:12 | INFO | train_inner | epoch 056:     26 / 65 loss=6.986, nll_loss=5.289, ppl=39.09, wps=3714, ups=0.3, wpb=12272.2, bsz=597, num_updates=3585, lr=3.27296e-05, gnorm=11.194, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=14697
2023-05-19 20:42:29 | INFO | train_inner | epoch 056:     31 / 65 loss=7.118, nll_loss=5.424, ppl=42.92, wps=3576.5, ups=0.29, wpb=12323.4, bsz=626.8, num_updates=3590, lr=3.27041e-05, gnorm=4.379, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=14715
2023-05-19 20:42:44 | INFO | train_inner | epoch 056:     36 / 65 loss=7.176, nll_loss=5.491, ppl=44.96, wps=4186.4, ups=0.33, wpb=12646.4, bsz=623, num_updates=3595, lr=3.26786e-05, gnorm=6.353, clip=100, loss_scale=0.125, train_wall=15, gb_free=21, wall=14730
2023-05-19 20:43:01 | INFO | train_inner | epoch 056:     41 / 65 loss=7.077, nll_loss=5.376, ppl=41.53, wps=3560.5, ups=0.3, wpb=11956.2, bsz=551.6, num_updates=3600, lr=3.26531e-05, gnorm=5.039, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.6, wall=14747
2023-05-19 20:43:17 | INFO | train_inner | epoch 056:     46 / 65 loss=6.905, nll_loss=5.186, ppl=36.4, wps=3781.1, ups=0.31, wpb=12170.6, bsz=520.4, num_updates=3605, lr=3.26276e-05, gnorm=3.606, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=14763
2023-05-19 20:43:34 | INFO | train_inner | epoch 056:     51 / 65 loss=7.04, nll_loss=5.35, ppl=40.79, wps=3738.2, ups=0.3, wpb=12437, bsz=616.8, num_updates=3610, lr=3.2602e-05, gnorm=6.722, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=14779
2023-05-19 20:43:49 | INFO | train_inner | epoch 056:     56 / 65 loss=6.898, nll_loss=5.192, ppl=36.57, wps=3915, ups=0.32, wpb=12148, bsz=523.8, num_updates=3615, lr=3.25765e-05, gnorm=6.567, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=14795
2023-05-19 20:44:05 | INFO | train_inner | epoch 056:     61 / 65 loss=6.944, nll_loss=5.234, ppl=37.63, wps=3739.3, ups=0.31, wpb=11872, bsz=536.6, num_updates=3620, lr=3.2551e-05, gnorm=4.601, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=14811
2023-05-19 20:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:44:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:44:55 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.296 | nll_loss 4.44 | ppl 21.7 | bleu 4.02 | wps 904 | wpb 2785 | bsz 105.2 | num_updates 3624 | best_loss 6.296
2023-05-19 20:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 3624 updates
2023-05-19 20:44:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint56.pt
2023-05-19 20:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint56.pt
2023-05-19 20:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint56.pt (epoch 56 @ 3624 updates, score 6.296) (writing took 15.159939423203468 seconds)
2023-05-19 20:45:10 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2023-05-19 20:45:10 | INFO | train | epoch 056 | loss 6.993 | nll_loss 5.291 | ppl 39.14 | wps 2994 | ups 0.25 | wpb 12055.3 | bsz 560 | num_updates 3624 | lr 3.25306e-05 | gnorm 5.826 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.6 | wall 14876
2023-05-19 20:45:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:45:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:45:10 | INFO | fairseq.trainer | begin training epoch 57
2023-05-19 20:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:45:13 | INFO | train_inner | epoch 057:      1 / 65 loss=6.911, nll_loss=5.199, ppl=36.72, wps=739.2, ups=0.07, wpb=10064.4, bsz=490.4, num_updates=3625, lr=3.25255e-05, gnorm=5.852, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.5, wall=14879
2023-05-19 20:45:28 | INFO | train_inner | epoch 057:      6 / 65 loss=7.06, nll_loss=5.352, ppl=40.85, wps=4063.9, ups=0.32, wpb=12590.2, bsz=609.8, num_updates=3630, lr=3.25e-05, gnorm=3.871, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=14894
2023-05-19 20:45:45 | INFO | train_inner | epoch 057:     11 / 65 loss=7.189, nll_loss=5.486, ppl=44.82, wps=3835.6, ups=0.31, wpb=12332.6, bsz=622.8, num_updates=3635, lr=3.24745e-05, gnorm=5.207, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=14910
2023-05-19 20:46:03 | INFO | train_inner | epoch 057:     16 / 65 loss=6.96, nll_loss=5.249, ppl=38.03, wps=3312.8, ups=0.27, wpb=12146.2, bsz=556, num_updates=3640, lr=3.2449e-05, gnorm=5.006, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=14929
2023-05-19 20:46:19 | INFO | train_inner | epoch 057:     21 / 65 loss=6.939, nll_loss=5.227, ppl=37.47, wps=3784.7, ups=0.3, wpb=12412.8, bsz=620, num_updates=3645, lr=3.24235e-05, gnorm=4.103, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=14945
2023-05-19 20:46:35 | INFO | train_inner | epoch 057:     26 / 65 loss=6.903, nll_loss=5.191, ppl=36.54, wps=3947.6, ups=0.32, wpb=12448, bsz=562.6, num_updates=3650, lr=3.2398e-05, gnorm=5.956, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=14961
2023-05-19 20:46:51 | INFO | train_inner | epoch 057:     31 / 65 loss=6.853, nll_loss=5.132, ppl=35.06, wps=3858.3, ups=0.32, wpb=12071.6, bsz=529.2, num_updates=3655, lr=3.23724e-05, gnorm=4.235, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=14976
2023-05-19 20:47:07 | INFO | train_inner | epoch 057:     36 / 65 loss=7.036, nll_loss=5.335, ppl=40.35, wps=3599.6, ups=0.3, wpb=11914.4, bsz=535.4, num_updates=3660, lr=3.23469e-05, gnorm=5.043, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=14993
2023-05-19 20:47:23 | INFO | train_inner | epoch 057:     41 / 65 loss=6.96, nll_loss=5.254, ppl=38.15, wps=3860, ups=0.32, wpb=11899.8, bsz=588.8, num_updates=3665, lr=3.23214e-05, gnorm=4.902, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=15008
2023-05-19 20:47:38 | INFO | train_inner | epoch 057:     46 / 65 loss=6.912, nll_loss=5.196, ppl=36.64, wps=3953.7, ups=0.33, wpb=12066, bsz=519.8, num_updates=3670, lr=3.22959e-05, gnorm=8.008, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=15024
2023-05-19 20:47:53 | INFO | train_inner | epoch 057:     51 / 65 loss=6.94, nll_loss=5.231, ppl=37.56, wps=4071, ups=0.33, wpb=12245.8, bsz=560.4, num_updates=3675, lr=3.22704e-05, gnorm=6.148, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=15039
2023-05-19 20:48:08 | INFO | train_inner | epoch 057:     56 / 65 loss=6.852, nll_loss=5.129, ppl=35, wps=3990.3, ups=0.33, wpb=12172.4, bsz=571.2, num_updates=3680, lr=3.22449e-05, gnorm=5.459, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=15054
2023-05-19 20:48:26 | INFO | train_inner | epoch 057:     61 / 65 loss=7.07, nll_loss=5.366, ppl=41.25, wps=3550, ups=0.29, wpb=12409.4, bsz=575.6, num_updates=3685, lr=3.22194e-05, gnorm=8.577, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=15071
2023-05-19 20:48:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:48:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:49:17 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.287 | nll_loss 4.433 | ppl 21.6 | bleu 3.52 | wps 864.5 | wpb 2785 | bsz 105.2 | num_updates 3689 | best_loss 6.287
2023-05-19 20:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 3689 updates
2023-05-19 20:49:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint57.pt
2023-05-19 20:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint57.pt
2023-05-19 20:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint57.pt (epoch 57 @ 3689 updates, score 6.287) (writing took 10.69870512187481 seconds)
2023-05-19 20:49:28 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2023-05-19 20:49:28 | INFO | train | epoch 057 | loss 6.969 | nll_loss 5.258 | ppl 38.28 | wps 3041.9 | ups 0.25 | wpb 12056.3 | bsz 559.9 | num_updates 3689 | lr 3.2199e-05 | gnorm 5.447 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.6 | wall 15134
2023-05-19 20:49:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:49:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:49:28 | INFO | fairseq.trainer | begin training epoch 58
2023-05-19 20:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:49:30 | INFO | train_inner | epoch 058:      1 / 65 loss=6.899, nll_loss=5.182, ppl=36.29, wps=785, ups=0.08, wpb=10157.2, bsz=424.6, num_updates=3690, lr=3.21939e-05, gnorm=4.811, clip=100, loss_scale=0.125, train_wall=12, gb_free=20.4, wall=15136
2023-05-19 20:49:45 | INFO | train_inner | epoch 058:      6 / 65 loss=7.22, nll_loss=5.517, ppl=45.8, wps=4136.9, ups=0.34, wpb=12252.2, bsz=621.4, num_updates=3695, lr=3.21684e-05, gnorm=6.385, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=15151
2023-05-19 20:50:02 | INFO | train_inner | epoch 058:     11 / 65 loss=7.011, nll_loss=5.31, ppl=39.66, wps=3701.8, ups=0.3, wpb=12149, bsz=645.2, num_updates=3700, lr=3.21429e-05, gnorm=8.317, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=15167
2023-05-19 20:50:18 | INFO | train_inner | epoch 058:     16 / 65 loss=6.87, nll_loss=5.145, ppl=35.37, wps=3696.3, ups=0.31, wpb=11958.2, bsz=481.4, num_updates=3705, lr=3.21173e-05, gnorm=6.181, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.9, wall=15184
2023-05-19 20:50:34 | INFO | train_inner | epoch 058:     21 / 65 loss=6.933, nll_loss=5.226, ppl=37.42, wps=3738.5, ups=0.31, wpb=12140.8, bsz=563.6, num_updates=3710, lr=3.20918e-05, gnorm=11.569, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=15200
2023-05-19 20:50:52 | INFO | train_inner | epoch 058:     26 / 65 loss=6.944, nll_loss=5.235, ppl=37.67, wps=3461.5, ups=0.28, wpb=12226, bsz=582.6, num_updates=3715, lr=3.20663e-05, gnorm=6.745, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=15217
2023-05-19 20:51:10 | INFO | train_inner | epoch 058:     31 / 65 loss=7.008, nll_loss=5.308, ppl=39.62, wps=3315.3, ups=0.27, wpb=12143, bsz=563.2, num_updates=3720, lr=3.20408e-05, gnorm=5.231, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=15236
2023-05-19 20:51:25 | INFO | train_inner | epoch 058:     36 / 65 loss=6.94, nll_loss=5.231, ppl=37.56, wps=3945.4, ups=0.33, wpb=11938.8, bsz=541.2, num_updates=3725, lr=3.20153e-05, gnorm=5.344, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=15251
2023-05-19 20:51:42 | INFO | train_inner | epoch 058:     41 / 65 loss=6.927, nll_loss=5.214, ppl=37.11, wps=3567.3, ups=0.29, wpb=12157.4, bsz=539.4, num_updates=3730, lr=3.19898e-05, gnorm=2.557, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.5, wall=15268
2023-05-19 20:51:58 | INFO | train_inner | epoch 058:     46 / 65 loss=7.063, nll_loss=5.353, ppl=40.87, wps=3913.9, ups=0.32, wpb=12346, bsz=574.2, num_updates=3735, lr=3.19643e-05, gnorm=7.863, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=15284
2023-05-19 20:52:14 | INFO | train_inner | epoch 058:     51 / 65 loss=6.959, nll_loss=5.241, ppl=37.81, wps=3632.6, ups=0.3, wpb=11921.2, bsz=532.4, num_updates=3740, lr=3.19388e-05, gnorm=5.914, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=15300
2023-05-19 20:52:29 | INFO | train_inner | epoch 058:     56 / 65 loss=6.859, nll_loss=5.13, ppl=35.02, wps=4158.4, ups=0.34, wpb=12216, bsz=548.8, num_updates=3745, lr=3.19133e-05, gnorm=7.675, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=15315
2023-05-19 20:52:46 | INFO | train_inner | epoch 058:     61 / 65 loss=6.869, nll_loss=5.149, ppl=35.47, wps=3602.8, ups=0.29, wpb=12374.4, bsz=545.6, num_updates=3750, lr=3.18878e-05, gnorm=5.553, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=15332
2023-05-19 20:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:52:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:53:37 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.288 | nll_loss 4.437 | ppl 21.65 | bleu 3.68 | wps 892 | wpb 2785 | bsz 105.2 | num_updates 3754 | best_loss 6.287
2023-05-19 20:53:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 3754 updates
2023-05-19 20:53:37 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint58.pt
2023-05-19 20:53:42 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint58.pt
2023-05-19 20:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint58.pt (epoch 58 @ 3754 updates, score 6.288) (writing took 8.770603954792023 seconds)
2023-05-19 20:53:46 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2023-05-19 20:53:46 | INFO | train | epoch 058 | loss 6.967 | nll_loss 5.255 | ppl 38.2 | wps 3034.1 | ups 0.25 | wpb 12050.4 | bsz 559.6 | num_updates 3754 | lr 3.18673e-05 | gnorm 6.791 | clip 100 | loss_scale 0.125 | train_wall 209 | gb_free 21.7 | wall 15392
2023-05-19 20:53:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:53:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:53:46 | INFO | fairseq.trainer | begin training epoch 59
2023-05-19 20:53:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:53:49 | INFO | train_inner | epoch 059:      1 / 65 loss=6.939, nll_loss=5.229, ppl=37.51, wps=867.1, ups=0.08, wpb=10938.6, bsz=556, num_updates=3755, lr=3.18622e-05, gnorm=8.343, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.2, wall=15395
2023-05-19 20:54:06 | INFO | train_inner | epoch 059:      6 / 65 loss=6.937, nll_loss=5.227, ppl=37.46, wps=3519.9, ups=0.3, wpb=11729.4, bsz=535.2, num_updates=3760, lr=3.18367e-05, gnorm=4.314, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=15412
2023-05-19 20:54:21 | INFO | train_inner | epoch 059:     11 / 65 loss=7, nll_loss=5.289, ppl=39.1, wps=4000.9, ups=0.33, wpb=12181.4, bsz=511.8, num_updates=3765, lr=3.18112e-05, gnorm=3.626, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=15427
2023-05-19 20:54:39 | INFO | train_inner | epoch 059:     16 / 65 loss=6.934, nll_loss=5.22, ppl=37.28, wps=3489.2, ups=0.29, wpb=12126.4, bsz=572.8, num_updates=3770, lr=3.17857e-05, gnorm=3.643, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=15444
2023-05-19 20:54:54 | INFO | train_inner | epoch 059:     21 / 65 loss=6.941, nll_loss=5.223, ppl=37.34, wps=3875.9, ups=0.32, wpb=12099.8, bsz=525.6, num_updates=3775, lr=3.17602e-05, gnorm=5.628, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=15460
2023-05-19 20:55:10 | INFO | train_inner | epoch 059:     26 / 65 loss=7.059, nll_loss=5.357, ppl=40.99, wps=3783.9, ups=0.31, wpb=12250.2, bsz=619, num_updates=3780, lr=3.17347e-05, gnorm=6.427, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=15476
2023-05-19 20:55:25 | INFO | train_inner | epoch 059:     31 / 65 loss=6.869, nll_loss=5.153, ppl=35.59, wps=4069.4, ups=0.33, wpb=12318.8, bsz=543.6, num_updates=3785, lr=3.17092e-05, gnorm=18.757, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=15491
2023-05-19 20:55:41 | INFO | train_inner | epoch 059:     36 / 65 loss=6.861, nll_loss=5.137, ppl=35.2, wps=3745.3, ups=0.31, wpb=11931, bsz=508, num_updates=3790, lr=3.16837e-05, gnorm=16.621, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=15507
2023-05-19 20:55:58 | INFO | train_inner | epoch 059:     41 / 65 loss=7.099, nll_loss=5.388, ppl=41.88, wps=3526.5, ups=0.3, wpb=11933.4, bsz=577, num_updates=3795, lr=3.16582e-05, gnorm=4.366, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.8, wall=15524
2023-05-19 20:56:14 | INFO | train_inner | epoch 059:     46 / 65 loss=6.938, nll_loss=5.22, ppl=37.28, wps=3740.4, ups=0.31, wpb=11918.2, bsz=523.6, num_updates=3800, lr=3.16327e-05, gnorm=12.977, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=15540
2023-05-19 20:56:30 | INFO | train_inner | epoch 059:     51 / 65 loss=7.014, nll_loss=5.31, ppl=39.68, wps=4179.9, ups=0.33, wpb=12784.2, bsz=634, num_updates=3805, lr=3.16071e-05, gnorm=3.288, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.7, wall=15555
2023-05-19 20:56:45 | INFO | train_inner | epoch 059:     56 / 65 loss=6.95, nll_loss=5.246, ppl=37.94, wps=3993.8, ups=0.33, wpb=12240.6, bsz=613, num_updates=3810, lr=3.15816e-05, gnorm=7.065, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=15571
2023-05-19 20:57:00 | INFO | train_inner | epoch 059:     61 / 65 loss=6.932, nll_loss=5.224, ppl=37.36, wps=4196.6, ups=0.34, wpb=12524.4, bsz=635, num_updates=3815, lr=3.15561e-05, gnorm=4.624, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=15586
2023-05-19 20:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 20:57:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:57:52 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.308 | nll_loss 4.457 | ppl 21.96 | bleu 3.99 | wps 878.8 | wpb 2785 | bsz 105.2 | num_updates 3819 | best_loss 6.287
2023-05-19 20:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 3819 updates
2023-05-19 20:57:52 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint59.pt
2023-05-19 20:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint59.pt
2023-05-19 20:57:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint59.pt (epoch 59 @ 3819 updates, score 6.308) (writing took 6.522812396287918 seconds)
2023-05-19 20:57:58 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2023-05-19 20:57:58 | INFO | train | epoch 059 | loss 6.954 | nll_loss 5.242 | ppl 37.83 | wps 3104.1 | ups 0.26 | wpb 12047.8 | bsz 558.5 | num_updates 3819 | lr 3.15357e-05 | gnorm 7.436 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.6 | wall 15644
2023-05-19 20:57:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 20:57:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 20:57:59 | INFO | fairseq.trainer | begin training epoch 60
2023-05-19 20:57:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 20:58:02 | INFO | train_inner | epoch 060:      1 / 65 loss=6.865, nll_loss=5.14, ppl=35.26, wps=858.7, ups=0.08, wpb=10601.8, bsz=447.8, num_updates=3820, lr=3.15306e-05, gnorm=5.08, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.3, wall=15647
2023-05-19 20:58:17 | INFO | train_inner | epoch 060:      6 / 65 loss=6.9, nll_loss=5.187, ppl=36.43, wps=3778.2, ups=0.32, wpb=11932.8, bsz=520.2, num_updates=3825, lr=3.15051e-05, gnorm=8.098, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=15663
2023-05-19 20:58:33 | INFO | train_inner | epoch 060:     11 / 65 loss=6.84, nll_loss=5.112, ppl=34.58, wps=3948, ups=0.32, wpb=12273.2, bsz=548.4, num_updates=3830, lr=3.14796e-05, gnorm=2.986, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=15679
2023-05-19 20:58:49 | INFO | train_inner | epoch 060:     16 / 65 loss=6.989, nll_loss=5.286, ppl=39.02, wps=3863.6, ups=0.32, wpb=12222.2, bsz=612, num_updates=3835, lr=3.14541e-05, gnorm=2.938, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=15694
2023-05-19 20:59:04 | INFO | train_inner | epoch 060:     21 / 65 loss=6.955, nll_loss=5.248, ppl=38.01, wps=4021.2, ups=0.32, wpb=12417.6, bsz=572, num_updates=3840, lr=3.14286e-05, gnorm=4.19, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=15710
2023-05-19 20:59:20 | INFO | train_inner | epoch 060:     26 / 65 loss=6.903, nll_loss=5.188, ppl=36.45, wps=3818.7, ups=0.31, wpb=12323.2, bsz=575, num_updates=3845, lr=3.14031e-05, gnorm=8.053, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=15726
2023-05-19 20:59:37 | INFO | train_inner | epoch 060:     31 / 65 loss=7.311, nll_loss=5.622, ppl=49.25, wps=3913.9, ups=0.31, wpb=12816.8, bsz=670.2, num_updates=3850, lr=3.13776e-05, gnorm=5.511, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=15742
2023-05-19 20:59:54 | INFO | train_inner | epoch 060:     36 / 65 loss=6.965, nll_loss=5.264, ppl=38.42, wps=3519.7, ups=0.29, wpb=12139.6, bsz=568, num_updates=3855, lr=3.1352e-05, gnorm=5.074, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=15760
2023-05-19 21:00:10 | INFO | train_inner | epoch 060:     41 / 65 loss=6.91, nll_loss=5.198, ppl=36.71, wps=3662.4, ups=0.31, wpb=11763.6, bsz=521.2, num_updates=3860, lr=3.13265e-05, gnorm=6.002, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=15776
2023-05-19 21:00:26 | INFO | train_inner | epoch 060:     46 / 65 loss=6.993, nll_loss=5.285, ppl=39, wps=3847, ups=0.32, wpb=12178.6, bsz=561.4, num_updates=3865, lr=3.1301e-05, gnorm=10.384, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=15792
2023-05-19 21:00:41 | INFO | train_inner | epoch 060:     51 / 65 loss=7.009, nll_loss=5.307, ppl=39.6, wps=4077, ups=0.33, wpb=12328.8, bsz=602, num_updates=3870, lr=3.12755e-05, gnorm=4.653, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=15807
2023-05-19 21:00:59 | INFO | train_inner | epoch 060:     56 / 65 loss=6.894, nll_loss=5.173, ppl=36.07, wps=3337, ups=0.28, wpb=11981.6, bsz=512.8, num_updates=3875, lr=3.125e-05, gnorm=2.85, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=15825
2023-05-19 21:01:14 | INFO | train_inner | epoch 060:     61 / 65 loss=6.901, nll_loss=5.186, ppl=36.39, wps=3886.7, ups=0.33, wpb=11907.4, bsz=545, num_updates=3880, lr=3.12245e-05, gnorm=3.757, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=15840
2023-05-19 21:01:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:01:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:02:06 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.322 | nll_loss 4.477 | ppl 22.27 | bleu 3.78 | wps 876.9 | wpb 2785 | bsz 105.2 | num_updates 3884 | best_loss 6.287
2023-05-19 21:02:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 3884 updates
2023-05-19 21:02:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint60.pt
2023-05-19 21:02:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint60.pt
2023-05-19 21:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint60.pt (epoch 60 @ 3884 updates, score 6.322) (writing took 6.6592599749565125 seconds)
2023-05-19 21:02:13 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2023-05-19 21:02:13 | INFO | train | epoch 060 | loss 6.961 | nll_loss 5.251 | ppl 38.08 | wps 3079.3 | ups 0.26 | wpb 12056.3 | bsz 559.7 | num_updates 3884 | lr 3.12041e-05 | gnorm 5.347 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.7 | wall 15899
2023-05-19 21:02:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:02:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:02:13 | INFO | fairseq.trainer | begin training epoch 61
2023-05-19 21:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:02:16 | INFO | train_inner | epoch 061:      1 / 65 loss=6.914, nll_loss=5.204, ppl=36.87, wps=836.1, ups=0.08, wpb=10292.6, bsz=469.4, num_updates=3885, lr=3.1199e-05, gnorm=5.286, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=15901
2023-05-19 21:02:30 | INFO | train_inner | epoch 061:      6 / 65 loss=6.961, nll_loss=5.254, ppl=38.17, wps=4053.1, ups=0.34, wpb=11907.2, bsz=558.2, num_updates=3890, lr=3.11735e-05, gnorm=2.962, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=15916
2023-05-19 21:02:46 | INFO | train_inner | epoch 061:     11 / 65 loss=6.967, nll_loss=5.256, ppl=38.22, wps=3807.5, ups=0.31, wpb=12252.8, bsz=549, num_updates=3895, lr=3.1148e-05, gnorm=3.7, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=15932
2023-05-19 21:03:01 | INFO | train_inner | epoch 061:     16 / 65 loss=6.874, nll_loss=5.156, ppl=35.66, wps=4320.3, ups=0.34, wpb=12528, bsz=573, num_updates=3900, lr=3.11224e-05, gnorm=9.428, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=15947
2023-05-19 21:03:16 | INFO | train_inner | epoch 061:     21 / 65 loss=6.893, nll_loss=5.183, ppl=36.32, wps=3894.3, ups=0.32, wpb=12004.4, bsz=538, num_updates=3905, lr=3.10969e-05, gnorm=3.296, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=15962
2023-05-19 21:03:33 | INFO | train_inner | epoch 061:     26 / 65 loss=6.945, nll_loss=5.24, ppl=37.8, wps=3696.6, ups=0.31, wpb=11942.8, bsz=544.6, num_updates=3910, lr=3.10714e-05, gnorm=3.727, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=15978
2023-05-19 21:03:49 | INFO | train_inner | epoch 061:     31 / 65 loss=6.946, nll_loss=5.237, ppl=37.72, wps=3930.3, ups=0.31, wpb=12537.6, bsz=571.4, num_updates=3915, lr=3.10459e-05, gnorm=8.177, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=15994
2023-05-19 21:04:06 | INFO | train_inner | epoch 061:     36 / 65 loss=7.143, nll_loss=5.443, ppl=43.5, wps=3632.2, ups=0.29, wpb=12403.2, bsz=581, num_updates=3920, lr=3.10204e-05, gnorm=13.523, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=16011
2023-05-19 21:04:22 | INFO | train_inner | epoch 061:     41 / 65 loss=6.905, nll_loss=5.19, ppl=36.5, wps=3779.6, ups=0.31, wpb=12130.4, bsz=570, num_updates=3925, lr=3.09949e-05, gnorm=4.476, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=16027
2023-05-19 21:04:37 | INFO | train_inner | epoch 061:     46 / 65 loss=6.892, nll_loss=5.175, ppl=36.12, wps=3923.8, ups=0.33, wpb=11827.4, bsz=519.4, num_updates=3930, lr=3.09694e-05, gnorm=3.342, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=16042
2023-05-19 21:04:54 | INFO | train_inner | epoch 061:     51 / 65 loss=6.917, nll_loss=5.202, ppl=36.8, wps=3529.9, ups=0.29, wpb=12173, bsz=590, num_updates=3935, lr=3.09439e-05, gnorm=3.748, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=16060
2023-05-19 21:05:11 | INFO | train_inner | epoch 061:     56 / 65 loss=6.958, nll_loss=5.253, ppl=38.13, wps=3640, ups=0.3, wpb=12133.2, bsz=562.2, num_updates=3940, lr=3.09184e-05, gnorm=2.441, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=16076
2023-05-19 21:05:26 | INFO | train_inner | epoch 061:     61 / 65 loss=6.989, nll_loss=5.28, ppl=38.87, wps=4200.4, ups=0.33, wpb=12707.6, bsz=628.4, num_updates=3945, lr=3.08929e-05, gnorm=4.68, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=16091
2023-05-19 21:05:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:05:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:06:15 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.315 | nll_loss 4.463 | ppl 22.06 | bleu 3.99 | wps 941.8 | wpb 2785 | bsz 105.2 | num_updates 3949 | best_loss 6.287
2023-05-19 21:06:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 3949 updates
2023-05-19 21:06:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint61.pt
2023-05-19 21:06:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint61.pt
2023-05-19 21:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint61.pt (epoch 61 @ 3949 updates, score 6.315) (writing took 6.710493724793196 seconds)
2023-05-19 21:06:22 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2023-05-19 21:06:22 | INFO | train | epoch 061 | loss 6.949 | nll_loss 5.239 | ppl 37.78 | wps 3150.1 | ups 0.26 | wpb 12051.7 | bsz 557.1 | num_updates 3949 | lr 3.08724e-05 | gnorm 5.369 | clip 100 | loss_scale 0.125 | train_wall 203 | gb_free 21.8 | wall 16147
2023-05-19 21:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:06:22 | INFO | fairseq.trainer | begin training epoch 62
2023-05-19 21:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:06:24 | INFO | train_inner | epoch 062:      1 / 65 loss=6.938, nll_loss=5.227, ppl=37.46, wps=869.4, ups=0.09, wpb=10177.8, bsz=452.4, num_updates=3950, lr=3.08673e-05, gnorm=6.683, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.6, wall=16150
2023-05-19 21:06:41 | INFO | train_inner | epoch 062:      6 / 65 loss=6.832, nll_loss=5.101, ppl=34.32, wps=3775.7, ups=0.3, wpb=12417, bsz=521.4, num_updates=3955, lr=3.08418e-05, gnorm=4.059, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=16166
2023-05-19 21:06:57 | INFO | train_inner | epoch 062:     11 / 65 loss=6.9, nll_loss=5.186, ppl=36.4, wps=3623.2, ups=0.31, wpb=11871.6, bsz=542.8, num_updates=3960, lr=3.08163e-05, gnorm=6.553, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=16183
2023-05-19 21:07:14 | INFO | train_inner | epoch 062:     16 / 65 loss=6.982, nll_loss=5.275, ppl=38.71, wps=3795.6, ups=0.3, wpb=12457.4, bsz=592, num_updates=3965, lr=3.07908e-05, gnorm=2.772, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=16199
2023-05-19 21:07:28 | INFO | train_inner | epoch 062:     21 / 65 loss=6.918, nll_loss=5.2, ppl=36.76, wps=4231.1, ups=0.34, wpb=12266.8, bsz=625.4, num_updates=3970, lr=3.07653e-05, gnorm=6.514, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=16214
2023-05-19 21:07:44 | INFO | train_inner | epoch 062:     26 / 65 loss=7.075, nll_loss=5.371, ppl=41.4, wps=3759.7, ups=0.31, wpb=12141.8, bsz=563.8, num_updates=3975, lr=3.07398e-05, gnorm=5.447, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.9, wall=16230
2023-05-19 21:07:59 | INFO | train_inner | epoch 062:     31 / 65 loss=6.961, nll_loss=5.255, ppl=38.18, wps=4151.5, ups=0.34, wpb=12194, bsz=578.8, num_updates=3980, lr=3.07143e-05, gnorm=4.135, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.1, wall=16245
2023-05-19 21:08:14 | INFO | train_inner | epoch 062:     36 / 65 loss=6.844, nll_loss=5.126, ppl=34.93, wps=3860.7, ups=0.32, wpb=12016.2, bsz=513, num_updates=3985, lr=3.06888e-05, gnorm=5.261, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=16260
2023-05-19 21:08:29 | INFO | train_inner | epoch 062:     41 / 65 loss=7.033, nll_loss=5.329, ppl=40.21, wps=4166.4, ups=0.33, wpb=12518, bsz=605.4, num_updates=3990, lr=3.06633e-05, gnorm=5.084, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=16275
2023-05-19 21:08:47 | INFO | train_inner | epoch 062:     46 / 65 loss=7.023, nll_loss=5.314, ppl=39.79, wps=3451.5, ups=0.28, wpb=12244, bsz=576.6, num_updates=3995, lr=3.06378e-05, gnorm=6.711, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=16293
2023-05-19 21:09:05 | INFO | train_inner | epoch 062:     51 / 65 loss=6.844, nll_loss=5.113, ppl=34.61, wps=3371.9, ups=0.29, wpb=11807.4, bsz=514, num_updates=4000, lr=3.06122e-05, gnorm=5.147, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.1, wall=16310
2023-05-19 21:09:20 | INFO | train_inner | epoch 062:     56 / 65 loss=6.936, nll_loss=5.221, ppl=37.3, wps=3949.4, ups=0.32, wpb=12316.6, bsz=574.2, num_updates=4005, lr=3.05867e-05, gnorm=4.038, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=16326
2023-05-19 21:09:36 | INFO | train_inner | epoch 062:     61 / 65 loss=6.965, nll_loss=5.261, ppl=38.35, wps=3915.7, ups=0.32, wpb=12146.8, bsz=602.8, num_updates=4010, lr=3.05612e-05, gnorm=5.578, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=16342
2023-05-19 21:09:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:09:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:10:28 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.29 | nll_loss 4.443 | ppl 21.76 | bleu 4.06 | wps 875.9 | wpb 2785 | bsz 105.2 | num_updates 4014 | best_loss 6.287
2023-05-19 21:10:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 4014 updates
2023-05-19 21:10:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint62.pt
2023-05-19 21:10:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint62.pt
2023-05-19 21:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint62.pt (epoch 62 @ 4014 updates, score 6.29) (writing took 6.709284126758575 seconds)
2023-05-19 21:10:35 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2023-05-19 21:10:35 | INFO | train | epoch 062 | loss 6.942 | nll_loss 5.229 | ppl 37.51 | wps 3090 | ups 0.26 | wpb 12051.4 | bsz 559.8 | num_updates 4014 | lr 3.05408e-05 | gnorm 5.081 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.6 | wall 16401
2023-05-19 21:10:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:10:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:10:35 | INFO | fairseq.trainer | begin training epoch 63
2023-05-19 21:10:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:10:38 | INFO | train_inner | epoch 063:      1 / 65 loss=6.903, nll_loss=5.192, ppl=36.55, wps=828.3, ups=0.08, wpb=10275.4, bsz=449.6, num_updates=4015, lr=3.05357e-05, gnorm=6.175, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=16404
2023-05-19 21:10:53 | INFO | train_inner | epoch 063:      6 / 65 loss=6.915, nll_loss=5.211, ppl=37.03, wps=4209.2, ups=0.34, wpb=12418, bsz=605.4, num_updates=4020, lr=3.05102e-05, gnorm=26.26, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.4, wall=16418
2023-05-19 21:11:09 | INFO | train_inner | epoch 063:     11 / 65 loss=6.91, nll_loss=5.199, ppl=36.73, wps=3727, ups=0.3, wpb=12226.4, bsz=552, num_updates=4025, lr=3.04847e-05, gnorm=6.94, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=16435
2023-05-19 21:11:25 | INFO | train_inner | epoch 063:     16 / 65 loss=6.845, nll_loss=5.117, ppl=34.7, wps=3694.1, ups=0.3, wpb=12125.8, bsz=527, num_updates=4030, lr=3.04592e-05, gnorm=5.53, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=16451
2023-05-19 21:11:41 | INFO | train_inner | epoch 063:     21 / 65 loss=6.855, nll_loss=5.136, ppl=35.16, wps=3720.1, ups=0.33, wpb=11410.8, bsz=523.6, num_updates=4035, lr=3.04337e-05, gnorm=5.52, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16466
2023-05-19 21:11:56 | INFO | train_inner | epoch 063:     26 / 65 loss=6.936, nll_loss=5.227, ppl=37.47, wps=3904.4, ups=0.32, wpb=12138.4, bsz=599.6, num_updates=4040, lr=3.04082e-05, gnorm=5.472, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=16482
2023-05-19 21:12:13 | INFO | train_inner | epoch 063:     31 / 65 loss=7.171, nll_loss=5.47, ppl=44.32, wps=3720.3, ups=0.3, wpb=12203.6, bsz=596.6, num_updates=4045, lr=3.03827e-05, gnorm=3.056, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=16498
2023-05-19 21:12:31 | INFO | train_inner | epoch 063:     36 / 65 loss=6.873, nll_loss=5.161, ppl=35.77, wps=3394.1, ups=0.28, wpb=12315.2, bsz=555.4, num_updates=4050, lr=3.03571e-05, gnorm=3.016, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.8, wall=16517
2023-05-19 21:12:46 | INFO | train_inner | epoch 063:     41 / 65 loss=6.887, nll_loss=5.172, ppl=36.06, wps=4012.3, ups=0.33, wpb=12201.4, bsz=591.4, num_updates=4055, lr=3.03316e-05, gnorm=6.999, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.5, wall=16532
2023-05-19 21:13:02 | INFO | train_inner | epoch 063:     46 / 65 loss=6.91, nll_loss=5.187, ppl=36.44, wps=3868.3, ups=0.31, wpb=12313.2, bsz=525.6, num_updates=4060, lr=3.03061e-05, gnorm=4.169, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=16548
2023-05-19 21:13:19 | INFO | train_inner | epoch 063:     51 / 65 loss=6.916, nll_loss=5.196, ppl=36.66, wps=3566.2, ups=0.29, wpb=12161.2, bsz=549.6, num_updates=4065, lr=3.02806e-05, gnorm=5.489, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.2, wall=16565
2023-05-19 21:13:34 | INFO | train_inner | epoch 063:     56 / 65 loss=6.9, nll_loss=5.186, ppl=36.41, wps=3911.8, ups=0.32, wpb=12081, bsz=582.4, num_updates=4070, lr=3.02551e-05, gnorm=2.95, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=16580
2023-05-19 21:13:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-19 21:13:54 | INFO | train_inner | epoch 063:     62 / 65 loss=7.119, nll_loss=5.417, ppl=42.73, wps=3225.4, ups=0.25, wpb=12719.6, bsz=641.8, num_updates=4075, lr=3.02296e-05, gnorm=2.967, clip=100, loss_scale=0.125, train_wall=20, gb_free=21.7, wall=16600
2023-05-19 21:14:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:14:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:14:44 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.248 | nll_loss 4.394 | ppl 21.03 | bleu 3.83 | wps 851.6 | wpb 2785 | bsz 105.2 | num_updates 4078 | best_loss 6.248
2023-05-19 21:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 4078 updates
2023-05-19 21:14:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint63.pt
2023-05-19 21:14:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint63.pt
2023-05-19 21:15:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint63.pt (epoch 63 @ 4078 updates, score 6.248) (writing took 36.99409552291036 seconds)
2023-05-19 21:15:21 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2023-05-19 21:15:21 | INFO | train | epoch 063 | loss 6.931 | nll_loss 5.217 | ppl 37.2 | wps 2698.6 | ups 0.22 | wpb 12049.6 | bsz 560 | num_updates 4078 | lr 3.02143e-05 | gnorm 6.583 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.6 | wall 16687
2023-05-19 21:15:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:15:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:15:21 | INFO | fairseq.trainer | begin training epoch 64
2023-05-19 21:15:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:15:27 | INFO | train_inner | epoch 064:      2 / 65 loss=6.797, nll_loss=5.073, ppl=33.67, wps=547.7, ups=0.05, wpb=10184.4, bsz=433, num_updates=4080, lr=3.02041e-05, gnorm=5.532, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=16693
2023-05-19 21:15:42 | INFO | train_inner | epoch 064:      7 / 65 loss=6.857, nll_loss=5.137, ppl=35.18, wps=4279.6, ups=0.34, wpb=12740.2, bsz=586.2, num_updates=4085, lr=3.01786e-05, gnorm=5.028, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=16708
2023-05-19 21:15:59 | INFO | train_inner | epoch 064:     12 / 65 loss=6.943, nll_loss=5.233, ppl=37.61, wps=3727.2, ups=0.3, wpb=12311.2, bsz=594, num_updates=4090, lr=3.01531e-05, gnorm=5.605, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=16724
2023-05-19 21:16:14 | INFO | train_inner | epoch 064:     17 / 65 loss=6.896, nll_loss=5.173, ppl=36.08, wps=3741, ups=0.32, wpb=11792.2, bsz=521.8, num_updates=4095, lr=3.01276e-05, gnorm=11.595, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.9, wall=16740
2023-05-19 21:16:31 | INFO | train_inner | epoch 064:     22 / 65 loss=7.018, nll_loss=5.313, ppl=39.75, wps=3628.1, ups=0.3, wpb=12180.8, bsz=554.6, num_updates=4100, lr=3.0102e-05, gnorm=6.097, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=16757
2023-05-19 21:16:46 | INFO | train_inner | epoch 064:     27 / 65 loss=6.84, nll_loss=5.112, ppl=34.58, wps=4052.3, ups=0.34, wpb=12046.4, bsz=562.2, num_updates=4105, lr=3.00765e-05, gnorm=4.522, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=16772
2023-05-19 21:17:02 | INFO | train_inner | epoch 064:     32 / 65 loss=6.86, nll_loss=5.137, ppl=35.18, wps=3736.2, ups=0.31, wpb=12012.2, bsz=552.8, num_updates=4110, lr=3.0051e-05, gnorm=6.246, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=16788
2023-05-19 21:17:17 | INFO | train_inner | epoch 064:     37 / 65 loss=6.936, nll_loss=5.225, ppl=37.39, wps=4167.7, ups=0.34, wpb=12408.8, bsz=620.2, num_updates=4115, lr=3.00255e-05, gnorm=4.463, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16803
2023-05-19 21:17:33 | INFO | train_inner | epoch 064:     42 / 65 loss=6.898, nll_loss=5.179, ppl=36.24, wps=3635, ups=0.3, wpb=12078.8, bsz=533.8, num_updates=4120, lr=3e-05, gnorm=5.516, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.9, wall=16819
2023-05-19 21:17:50 | INFO | train_inner | epoch 064:     47 / 65 loss=6.844, nll_loss=5.125, ppl=34.91, wps=3530.5, ups=0.3, wpb=11944.4, bsz=561.2, num_updates=4125, lr=2.99745e-05, gnorm=4.487, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=16836
2023-05-19 21:18:08 | INFO | train_inner | epoch 064:     52 / 65 loss=7.245, nll_loss=5.545, ppl=46.7, wps=3385.8, ups=0.28, wpb=12164.8, bsz=586.2, num_updates=4130, lr=2.9949e-05, gnorm=7.529, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=16854
2023-05-19 21:18:23 | INFO | train_inner | epoch 064:     57 / 65 loss=6.924, nll_loss=5.214, ppl=37.11, wps=4275.2, ups=0.34, wpb=12470.8, bsz=583.4, num_updates=4135, lr=2.99235e-05, gnorm=4.053, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16869
2023-05-19 21:18:38 | INFO | train_inner | epoch 064:     62 / 65 loss=6.894, nll_loss=5.174, ppl=36.11, wps=4007.6, ups=0.33, wpb=11993.4, bsz=537.2, num_updates=4140, lr=2.9898e-05, gnorm=3.838, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=16884
2023-05-19 21:18:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:18:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:19:25 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.275 | nll_loss 4.426 | ppl 21.49 | bleu 4.1 | wps 896.9 | wpb 2785 | bsz 105.2 | num_updates 4143 | best_loss 6.248
2023-05-19 21:19:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 4143 updates
2023-05-19 21:19:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint64.pt
2023-05-19 21:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint64.pt
2023-05-19 21:19:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint64.pt (epoch 64 @ 4143 updates, score 6.275) (writing took 6.826021708548069 seconds)
2023-05-19 21:19:32 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2023-05-19 21:19:32 | INFO | train | epoch 064 | loss 6.925 | nll_loss 5.209 | ppl 37 | wps 3116.8 | ups 0.26 | wpb 12051.9 | bsz 559.4 | num_updates 4143 | lr 2.98827e-05 | gnorm 5.752 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.8 | wall 16938
2023-05-19 21:19:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:19:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:19:32 | INFO | fairseq.trainer | begin training epoch 65
2023-05-19 21:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:19:39 | INFO | train_inner | epoch 065:      2 / 65 loss=6.878, nll_loss=5.165, ppl=35.88, wps=871.3, ups=0.08, wpb=10650.6, bsz=445.8, num_updates=4145, lr=2.98724e-05, gnorm=6.365, clip=100, loss_scale=0.125, train_wall=14, gb_free=20.4, wall=16945
2023-05-19 21:19:55 | INFO | train_inner | epoch 065:      7 / 65 loss=6.914, nll_loss=5.207, ppl=36.95, wps=4002.4, ups=0.32, wpb=12410, bsz=614.6, num_updates=4150, lr=2.98469e-05, gnorm=12.206, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16960
2023-05-19 21:20:09 | INFO | train_inner | epoch 065:     12 / 65 loss=6.922, nll_loss=5.21, ppl=37.02, wps=4197.2, ups=0.34, wpb=12418.4, bsz=591.4, num_updates=4155, lr=2.98214e-05, gnorm=4.603, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16975
2023-05-19 21:20:24 | INFO | train_inner | epoch 065:     17 / 65 loss=6.866, nll_loss=5.149, ppl=35.47, wps=4078.4, ups=0.33, wpb=12276.6, bsz=541.8, num_updates=4160, lr=2.97959e-05, gnorm=8.759, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=16990
2023-05-19 21:20:41 | INFO | train_inner | epoch 065:     22 / 65 loss=7.109, nll_loss=5.405, ppl=42.38, wps=3601.7, ups=0.3, wpb=11955.2, bsz=562, num_updates=4165, lr=2.97704e-05, gnorm=19.413, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=17007
2023-05-19 21:20:59 | INFO | train_inner | epoch 065:     27 / 65 loss=7.024, nll_loss=5.31, ppl=39.68, wps=3477, ups=0.28, wpb=12307.2, bsz=599, num_updates=4170, lr=2.97449e-05, gnorm=6.218, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=17024
2023-05-19 21:21:16 | INFO | train_inner | epoch 065:     32 / 65 loss=6.934, nll_loss=5.213, ppl=37.1, wps=3582.7, ups=0.29, wpb=12387.6, bsz=575.8, num_updates=4175, lr=2.97194e-05, gnorm=3.997, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=17042
2023-05-19 21:21:32 | INFO | train_inner | epoch 065:     37 / 65 loss=6.925, nll_loss=5.215, ppl=37.15, wps=3813.1, ups=0.3, wpb=12580.2, bsz=602.4, num_updates=4180, lr=2.96939e-05, gnorm=5.069, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=17058
2023-05-19 21:21:49 | INFO | train_inner | epoch 065:     42 / 65 loss=6.888, nll_loss=5.175, ppl=36.13, wps=3716.5, ups=0.31, wpb=12176.2, bsz=608.4, num_updates=4185, lr=2.96684e-05, gnorm=7.251, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=17075
2023-05-19 21:22:03 | INFO | train_inner | epoch 065:     47 / 65 loss=6.873, nll_loss=5.154, ppl=35.61, wps=4236.9, ups=0.36, wpb=11857.2, bsz=561.8, num_updates=4190, lr=2.96429e-05, gnorm=7.349, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=17089
2023-05-19 21:22:20 | INFO | train_inner | epoch 065:     52 / 65 loss=6.854, nll_loss=5.123, ppl=34.84, wps=3595.5, ups=0.3, wpb=12056.2, bsz=518.6, num_updates=4195, lr=2.96173e-05, gnorm=7.1, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=17105
2023-05-19 21:22:35 | INFO | train_inner | epoch 065:     57 / 65 loss=6.899, nll_loss=5.176, ppl=36.14, wps=3978.8, ups=0.33, wpb=12055.4, bsz=554, num_updates=4200, lr=2.95918e-05, gnorm=3.492, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.2, wall=17121
2023-05-19 21:22:52 | INFO | train_inner | epoch 065:     62 / 65 loss=6.85, nll_loss=5.128, ppl=34.98, wps=3396.2, ups=0.29, wpb=11876.6, bsz=519.8, num_updates=4205, lr=2.95663e-05, gnorm=10.725, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=17138
2023-05-19 21:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:22:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:23:41 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.279 | nll_loss 4.423 | ppl 21.46 | bleu 3.92 | wps 848.2 | wpb 2785 | bsz 105.2 | num_updates 4208 | best_loss 6.248
2023-05-19 21:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 4208 updates
2023-05-19 21:23:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint65.pt
2023-05-19 21:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint65.pt
2023-05-19 21:23:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint65.pt (epoch 65 @ 4208 updates, score 6.279) (writing took 9.323575403541327 seconds)
2023-05-19 21:23:51 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2023-05-19 21:23:51 | INFO | train | epoch 065 | loss 6.914 | nll_loss 5.198 | ppl 36.7 | wps 3029.9 | ups 0.25 | wpb 12044.6 | bsz 558 | num_updates 4208 | lr 2.9551e-05 | gnorm 7.829 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.8 | wall 17196
2023-05-19 21:23:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:23:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:23:51 | INFO | fairseq.trainer | begin training epoch 66
2023-05-19 21:23:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:23:57 | INFO | train_inner | epoch 066:      2 / 65 loss=6.823, nll_loss=5.096, ppl=34.21, wps=781.2, ups=0.08, wpb=10084.6, bsz=422.6, num_updates=4210, lr=2.95408e-05, gnorm=5.242, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.6, wall=17203
2023-05-19 21:24:15 | INFO | train_inner | epoch 066:      7 / 65 loss=6.911, nll_loss=5.2, ppl=36.76, wps=3344.9, ups=0.28, wpb=12052.4, bsz=549, num_updates=4215, lr=2.95153e-05, gnorm=3.789, clip=100, loss_scale=0.125, train_wall=18, gb_free=21, wall=17221
2023-05-19 21:24:32 | INFO | train_inner | epoch 066:     12 / 65 loss=6.978, nll_loss=5.271, ppl=38.62, wps=3583.9, ups=0.3, wpb=12016, bsz=585, num_updates=4220, lr=2.94898e-05, gnorm=12.049, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=17237
2023-05-19 21:24:47 | INFO | train_inner | epoch 066:     17 / 65 loss=6.919, nll_loss=5.208, ppl=36.96, wps=3983.5, ups=0.32, wpb=12332.2, bsz=614.6, num_updates=4225, lr=2.94643e-05, gnorm=3.351, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=17253
2023-05-19 21:25:03 | INFO | train_inner | epoch 066:     22 / 65 loss=6.876, nll_loss=5.157, ppl=35.67, wps=3724.3, ups=0.31, wpb=12010.2, bsz=575.8, num_updates=4230, lr=2.94388e-05, gnorm=10.946, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=17269
2023-05-19 21:25:19 | INFO | train_inner | epoch 066:     27 / 65 loss=6.836, nll_loss=5.113, ppl=34.61, wps=3802.7, ups=0.31, wpb=12201.2, bsz=533, num_updates=4235, lr=2.94133e-05, gnorm=5.461, clip=100, loss_scale=0.125, train_wall=16, gb_free=21, wall=17285
2023-05-19 21:25:34 | INFO | train_inner | epoch 066:     32 / 65 loss=7.33, nll_loss=5.634, ppl=49.67, wps=4131.9, ups=0.33, wpb=12425, bsz=664.4, num_updates=4240, lr=2.93878e-05, gnorm=7.611, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17300
2023-05-19 21:25:51 | INFO | train_inner | epoch 066:     37 / 65 loss=6.815, nll_loss=5.088, ppl=34.01, wps=3586.3, ups=0.3, wpb=11812.4, bsz=502.2, num_updates=4245, lr=2.93622e-05, gnorm=6.37, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=17316
2023-05-19 21:26:05 | INFO | train_inner | epoch 066:     42 / 65 loss=6.858, nll_loss=5.138, ppl=35.2, wps=4128.6, ups=0.34, wpb=12152, bsz=509.2, num_updates=4250, lr=2.93367e-05, gnorm=5.591, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17331
2023-05-19 21:26:22 | INFO | train_inner | epoch 066:     47 / 65 loss=6.873, nll_loss=5.157, ppl=35.67, wps=3634.2, ups=0.3, wpb=12166.8, bsz=549.8, num_updates=4255, lr=2.93112e-05, gnorm=5.564, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=17348
2023-05-19 21:26:37 | INFO | train_inner | epoch 066:     52 / 65 loss=6.934, nll_loss=5.217, ppl=37.2, wps=4120.9, ups=0.34, wpb=12094.6, bsz=602.8, num_updates=4260, lr=2.92857e-05, gnorm=26.684, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=17363
2023-05-19 21:26:52 | INFO | train_inner | epoch 066:     57 / 65 loss=6.825, nll_loss=5.094, ppl=34.15, wps=4184.9, ups=0.34, wpb=12288.2, bsz=552.8, num_updates=4265, lr=2.92602e-05, gnorm=7.426, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=17377
2023-05-19 21:27:07 | INFO | train_inner | epoch 066:     62 / 65 loss=6.873, nll_loss=5.147, ppl=35.43, wps=4019.9, ups=0.31, wpb=12788.2, bsz=589.4, num_updates=4270, lr=2.92347e-05, gnorm=8.65, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=17393
2023-05-19 21:27:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:27:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:27:55 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.273 | nll_loss 4.413 | ppl 21.3 | bleu 3.96 | wps 902.5 | wpb 2785 | bsz 105.2 | num_updates 4273 | best_loss 6.248
2023-05-19 21:27:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 4273 updates
2023-05-19 21:27:55 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint66.pt
2023-05-19 21:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint66.pt
2023-05-19 21:28:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint66.pt (epoch 66 @ 4273 updates, score 6.273) (writing took 6.732142612338066 seconds)
2023-05-19 21:28:02 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2023-05-19 21:28:02 | INFO | train | epoch 066 | loss 6.918 | nll_loss 5.201 | ppl 36.78 | wps 3117.4 | ups 0.26 | wpb 12057.1 | bsz 559.9 | num_updates 4273 | lr 2.92194e-05 | gnorm 8.292 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21 | wall 17448
2023-05-19 21:28:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:28:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:28:02 | INFO | fairseq.trainer | begin training epoch 67
2023-05-19 21:28:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:28:09 | INFO | train_inner | epoch 067:      2 / 65 loss=6.897, nll_loss=5.18, ppl=36.24, wps=828.4, ups=0.08, wpb=10165.4, bsz=469.4, num_updates=4275, lr=2.92092e-05, gnorm=5.094, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17455
2023-05-19 21:28:24 | INFO | train_inner | epoch 067:      7 / 65 loss=7.031, nll_loss=5.32, ppl=39.95, wps=3863.4, ups=0.32, wpb=12000.2, bsz=606.4, num_updates=4280, lr=2.91837e-05, gnorm=8.006, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=17470
2023-05-19 21:28:40 | INFO | train_inner | epoch 067:     12 / 65 loss=6.892, nll_loss=5.181, ppl=36.28, wps=4075.1, ups=0.32, wpb=12664.6, bsz=624.2, num_updates=4285, lr=2.91582e-05, gnorm=4.174, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=17486
2023-05-19 21:28:56 | INFO | train_inner | epoch 067:     17 / 65 loss=6.983, nll_loss=5.279, ppl=38.83, wps=3794.5, ups=0.31, wpb=12302.8, bsz=561.4, num_updates=4290, lr=2.91327e-05, gnorm=4.804, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=17502
2023-05-19 21:29:12 | INFO | train_inner | epoch 067:     22 / 65 loss=6.961, nll_loss=5.255, ppl=38.18, wps=3819.7, ups=0.31, wpb=12235, bsz=607.8, num_updates=4295, lr=2.91071e-05, gnorm=14.269, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=17518
2023-05-19 21:29:28 | INFO | train_inner | epoch 067:     27 / 65 loss=6.783, nll_loss=5.053, ppl=33.19, wps=3859.1, ups=0.32, wpb=11997.4, bsz=535.8, num_updates=4300, lr=2.90816e-05, gnorm=4.153, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.9, wall=17533
2023-05-19 21:29:42 | INFO | train_inner | epoch 067:     32 / 65 loss=7.031, nll_loss=5.319, ppl=39.92, wps=4138.3, ups=0.34, wpb=12180, bsz=631.2, num_updates=4305, lr=2.90561e-05, gnorm=8.58, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=17548
2023-05-19 21:29:58 | INFO | train_inner | epoch 067:     37 / 65 loss=6.877, nll_loss=5.16, ppl=35.75, wps=3966.3, ups=0.33, wpb=12011.6, bsz=491.4, num_updates=4310, lr=2.90306e-05, gnorm=5.324, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17563
2023-05-19 21:30:14 | INFO | train_inner | epoch 067:     42 / 65 loss=6.92, nll_loss=5.207, ppl=36.95, wps=3619.5, ups=0.31, wpb=11823.4, bsz=513, num_updates=4315, lr=2.90051e-05, gnorm=2.825, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=17580
2023-05-19 21:30:29 | INFO | train_inner | epoch 067:     47 / 65 loss=6.849, nll_loss=5.137, ppl=35.2, wps=4105.9, ups=0.33, wpb=12349.6, bsz=541.8, num_updates=4320, lr=2.89796e-05, gnorm=9.822, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17595
2023-05-19 21:30:45 | INFO | train_inner | epoch 067:     52 / 65 loss=6.924, nll_loss=5.214, ppl=37.11, wps=4033, ups=0.32, wpb=12648.2, bsz=601.8, num_updates=4325, lr=2.89541e-05, gnorm=7.513, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=17610
2023-05-19 21:31:02 | INFO | train_inner | epoch 067:     57 / 65 loss=6.818, nll_loss=5.097, ppl=34.22, wps=3479.4, ups=0.29, wpb=12048.4, bsz=535.8, num_updates=4330, lr=2.89286e-05, gnorm=5.951, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=17628
2023-05-19 21:31:17 | INFO | train_inner | epoch 067:     62 / 65 loss=6.871, nll_loss=5.15, ppl=35.51, wps=4042.7, ups=0.33, wpb=12266, bsz=559.8, num_updates=4335, lr=2.89031e-05, gnorm=4.956, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17643
2023-05-19 21:31:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:31:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:32:06 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.245 | nll_loss 4.373 | ppl 20.72 | bleu 4.26 | wps 845.8 | wpb 2785 | bsz 105.2 | num_updates 4338 | best_loss 6.245
2023-05-19 21:32:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 4338 updates
2023-05-19 21:32:06 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint67.pt
2023-05-19 21:32:11 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint67.pt
2023-05-19 21:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint67.pt (epoch 67 @ 4338 updates, score 6.245) (writing took 14.338272254914045 seconds)
2023-05-19 21:32:20 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2023-05-19 21:32:20 | INFO | train | epoch 067 | loss 6.908 | nll_loss 5.194 | ppl 36.6 | wps 3036.9 | ups 0.25 | wpb 12058.4 | bsz 559.6 | num_updates 4338 | lr 2.88878e-05 | gnorm 6.767 | clip 100 | loss_scale 0.125 | train_wall 201 | gb_free 21.7 | wall 17706
2023-05-19 21:32:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:32:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:32:20 | INFO | fairseq.trainer | begin training epoch 68
2023-05-19 21:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:32:28 | INFO | train_inner | epoch 068:      2 / 65 loss=6.849, nll_loss=5.126, ppl=34.93, wps=744.9, ups=0.07, wpb=10518.2, bsz=455.2, num_updates=4340, lr=2.88776e-05, gnorm=6.986, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=17713
2023-05-19 21:32:44 | INFO | train_inner | epoch 068:      7 / 65 loss=6.913, nll_loss=5.196, ppl=36.64, wps=3826, ups=0.31, wpb=12356.6, bsz=621.6, num_updates=4345, lr=2.8852e-05, gnorm=4.061, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=17730
2023-05-19 21:33:01 | INFO | train_inner | epoch 068:     12 / 65 loss=6.86, nll_loss=5.137, ppl=35.19, wps=3499.2, ups=0.29, wpb=12213.4, bsz=556.6, num_updates=4350, lr=2.88265e-05, gnorm=11.205, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=17747
2023-05-19 21:33:17 | INFO | train_inner | epoch 068:     17 / 65 loss=7.095, nll_loss=5.39, ppl=41.93, wps=4032, ups=0.32, wpb=12775.8, bsz=667.8, num_updates=4355, lr=2.8801e-05, gnorm=5.098, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=17763
2023-05-19 21:33:34 | INFO | train_inner | epoch 068:     22 / 65 loss=6.888, nll_loss=5.172, ppl=36.06, wps=3712.4, ups=0.3, wpb=12451.4, bsz=629.6, num_updates=4360, lr=2.87755e-05, gnorm=5.337, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=17780
2023-05-19 21:33:50 | INFO | train_inner | epoch 068:     27 / 65 loss=6.797, nll_loss=5.07, ppl=33.6, wps=3636.8, ups=0.3, wpb=12057, bsz=490.4, num_updates=4365, lr=2.875e-05, gnorm=5.222, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=17796
2023-05-19 21:34:06 | INFO | train_inner | epoch 068:     32 / 65 loss=6.895, nll_loss=5.175, ppl=36.14, wps=3839.6, ups=0.32, wpb=12050.6, bsz=500, num_updates=4370, lr=2.87245e-05, gnorm=4.409, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=17812
2023-05-19 21:34:22 | INFO | train_inner | epoch 068:     37 / 65 loss=6.916, nll_loss=5.21, ppl=37.01, wps=3837.5, ups=0.31, wpb=12244.4, bsz=605.4, num_updates=4375, lr=2.8699e-05, gnorm=13.589, clip=100, loss_scale=0.125, train_wall=16, gb_free=21, wall=17828
2023-05-19 21:34:36 | INFO | train_inner | epoch 068:     42 / 65 loss=6.791, nll_loss=5.059, ppl=33.34, wps=4199.9, ups=0.35, wpb=11998.4, bsz=569.8, num_updates=4380, lr=2.86735e-05, gnorm=7.17, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=17842
2023-05-19 21:34:52 | INFO | train_inner | epoch 068:     47 / 65 loss=6.862, nll_loss=5.138, ppl=35.21, wps=3699.3, ups=0.31, wpb=11827.6, bsz=521.2, num_updates=4385, lr=2.8648e-05, gnorm=4.443, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=17858
2023-05-19 21:35:09 | INFO | train_inner | epoch 068:     52 / 65 loss=7.031, nll_loss=5.315, ppl=39.8, wps=3751.6, ups=0.31, wpb=12110.4, bsz=580.8, num_updates=4390, lr=2.86224e-05, gnorm=4.904, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=17874
2023-05-19 21:35:24 | INFO | train_inner | epoch 068:     57 / 65 loss=6.838, nll_loss=5.114, ppl=34.63, wps=3994.1, ups=0.33, wpb=12189.4, bsz=517.8, num_updates=4395, lr=2.85969e-05, gnorm=4.745, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=17890
2023-05-19 21:35:40 | INFO | train_inner | epoch 068:     62 / 65 loss=6.953, nll_loss=5.242, ppl=37.83, wps=3793.9, ups=0.31, wpb=12094.4, bsz=553.4, num_updates=4400, lr=2.85714e-05, gnorm=3.611, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=17905
2023-05-19 21:35:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:35:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:36:29 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.248 | nll_loss 4.385 | ppl 20.89 | bleu 4.28 | wps 867.8 | wpb 2785 | bsz 105.2 | num_updates 4403 | best_loss 6.245
2023-05-19 21:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 4403 updates
2023-05-19 21:36:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint68.pt
2023-05-19 21:36:34 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint68.pt
2023-05-19 21:36:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint68.pt (epoch 68 @ 4403 updates, score 6.248) (writing took 6.864604480564594 seconds)
2023-05-19 21:36:36 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2023-05-19 21:36:36 | INFO | train | epoch 068 | loss 6.903 | nll_loss 5.184 | ppl 36.36 | wps 3064.6 | ups 0.25 | wpb 12048.3 | bsz 559.7 | num_updates 4403 | lr 2.85561e-05 | gnorm 6.091 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.8 | wall 17961
2023-05-19 21:36:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:36:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:36:36 | INFO | fairseq.trainer | begin training epoch 69
2023-05-19 21:36:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:36:42 | INFO | train_inner | epoch 069:      2 / 65 loss=6.873, nll_loss=5.161, ppl=35.78, wps=832.2, ups=0.08, wpb=10401.6, bsz=467, num_updates=4405, lr=2.85459e-05, gnorm=4.446, clip=100, loss_scale=0.125, train_wall=14, gb_free=20.8, wall=17968
2023-05-19 21:36:58 | INFO | train_inner | epoch 069:      7 / 65 loss=6.852, nll_loss=5.131, ppl=35.04, wps=3981.1, ups=0.32, wpb=12324, bsz=519.8, num_updates=4410, lr=2.85204e-05, gnorm=6.282, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=17983
2023-05-19 21:37:14 | INFO | train_inner | epoch 069:     12 / 65 loss=6.861, nll_loss=5.145, ppl=35.39, wps=3750, ups=0.31, wpb=12089.2, bsz=597.4, num_updates=4415, lr=2.84949e-05, gnorm=3.113, clip=100, loss_scale=0.125, train_wall=16, gb_free=21, wall=18000
2023-05-19 21:37:30 | INFO | train_inner | epoch 069:     17 / 65 loss=6.827, nll_loss=5.096, ppl=34.21, wps=3739.9, ups=0.32, wpb=11853, bsz=523.8, num_updates=4420, lr=2.84694e-05, gnorm=4.32, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=18015
2023-05-19 21:37:45 | INFO | train_inner | epoch 069:     22 / 65 loss=6.97, nll_loss=5.255, ppl=38.18, wps=3861.3, ups=0.32, wpb=11904, bsz=566.6, num_updates=4425, lr=2.84439e-05, gnorm=6.666, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=18031
2023-05-19 21:38:00 | INFO | train_inner | epoch 069:     27 / 65 loss=6.904, nll_loss=5.187, ppl=36.43, wps=4263.7, ups=0.34, wpb=12536.8, bsz=626.6, num_updates=4430, lr=2.84184e-05, gnorm=7.699, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=18046
2023-05-19 21:38:15 | INFO | train_inner | epoch 069:     32 / 65 loss=6.915, nll_loss=5.195, ppl=36.64, wps=3896, ups=0.32, wpb=12155.2, bsz=569, num_updates=4435, lr=2.83929e-05, gnorm=4.579, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=18061
2023-05-19 21:38:31 | INFO | train_inner | epoch 069:     37 / 65 loss=6.866, nll_loss=5.148, ppl=35.45, wps=3926.9, ups=0.32, wpb=12347.6, bsz=584.4, num_updates=4440, lr=2.83673e-05, gnorm=6.741, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=18077
2023-05-19 21:38:48 | INFO | train_inner | epoch 069:     42 / 65 loss=6.943, nll_loss=5.23, ppl=37.53, wps=3658.7, ups=0.3, wpb=12348.8, bsz=606.2, num_updates=4445, lr=2.83418e-05, gnorm=4.814, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=18094
2023-05-19 21:39:03 | INFO | train_inner | epoch 069:     47 / 65 loss=6.789, nll_loss=5.065, ppl=33.47, wps=4047.1, ups=0.32, wpb=12477.4, bsz=526, num_updates=4450, lr=2.83163e-05, gnorm=9.274, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=18109
2023-05-19 21:39:18 | INFO | train_inner | epoch 069:     52 / 65 loss=6.849, nll_loss=5.132, ppl=35.05, wps=4117.7, ups=0.34, wpb=11993.4, bsz=521, num_updates=4455, lr=2.82908e-05, gnorm=7.943, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.9, wall=18124
2023-05-19 21:39:34 | INFO | train_inner | epoch 069:     57 / 65 loss=7.116, nll_loss=5.416, ppl=42.69, wps=3729.2, ups=0.31, wpb=12001.2, bsz=613, num_updates=4460, lr=2.82653e-05, gnorm=5.128, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=18140
2023-05-19 21:39:52 | INFO | train_inner | epoch 069:     62 / 65 loss=6.873, nll_loss=5.155, ppl=35.64, wps=3391.4, ups=0.28, wpb=12243.2, bsz=547.2, num_updates=4465, lr=2.82398e-05, gnorm=6.384, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=18158
2023-05-19 21:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:40:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:40:42 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.245 | nll_loss 4.385 | ppl 20.89 | bleu 4.17 | wps 883.7 | wpb 2785 | bsz 105.2 | num_updates 4468 | best_loss 6.245
2023-05-19 21:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 4468 updates
2023-05-19 21:40:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint69.pt
2023-05-19 21:40:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint69.pt
2023-05-19 21:41:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint69.pt (epoch 69 @ 4468 updates, score 6.245) (writing took 23.611761920154095 seconds)
2023-05-19 21:41:05 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2023-05-19 21:41:05 | INFO | train | epoch 069 | loss 6.9 | nll_loss 5.183 | ppl 36.34 | wps 2905.4 | ups 0.24 | wpb 12052.1 | bsz 559.4 | num_updates 4468 | lr 2.82245e-05 | gnorm 6.049 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.3 | wall 18231
2023-05-19 21:41:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:41:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:41:05 | INFO | fairseq.trainer | begin training epoch 70
2023-05-19 21:41:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:41:11 | INFO | train_inner | epoch 070:      2 / 65 loss=6.985, nll_loss=5.279, ppl=38.84, wps=639.6, ups=0.06, wpb=10139.4, bsz=490, num_updates=4470, lr=2.82143e-05, gnorm=9.942, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.4, wall=18237
2023-05-19 21:41:27 | INFO | train_inner | epoch 070:      7 / 65 loss=6.952, nll_loss=5.241, ppl=37.83, wps=3685.4, ups=0.31, wpb=11880, bsz=533.6, num_updates=4475, lr=2.81888e-05, gnorm=3.398, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=18253
2023-05-19 21:41:43 | INFO | train_inner | epoch 070:     12 / 65 loss=6.85, nll_loss=5.131, ppl=35.04, wps=3842.9, ups=0.32, wpb=12148.8, bsz=548.8, num_updates=4480, lr=2.81633e-05, gnorm=5.532, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=18269
2023-05-19 21:41:59 | INFO | train_inner | epoch 070:     17 / 65 loss=6.872, nll_loss=5.147, ppl=35.43, wps=4003, ups=0.32, wpb=12533.4, bsz=583.8, num_updates=4485, lr=2.81378e-05, gnorm=3.026, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=18285
2023-05-19 21:42:16 | INFO | train_inner | epoch 070:     22 / 65 loss=6.848, nll_loss=5.131, ppl=35.03, wps=3600.3, ups=0.3, wpb=12032.2, bsz=588.2, num_updates=4490, lr=2.81122e-05, gnorm=3.813, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=18301
2023-05-19 21:42:30 | INFO | train_inner | epoch 070:     27 / 65 loss=7.025, nll_loss=5.32, ppl=39.94, wps=4189.9, ups=0.35, wpb=12105.8, bsz=609.2, num_updates=4495, lr=2.80867e-05, gnorm=9.103, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.9, wall=18316
2023-05-19 21:42:45 | INFO | train_inner | epoch 070:     32 / 65 loss=6.906, nll_loss=5.191, ppl=36.53, wps=4121.8, ups=0.34, wpb=12183.6, bsz=559.2, num_updates=4500, lr=2.80612e-05, gnorm=4.375, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=18331
2023-05-19 21:43:04 | INFO | train_inner | epoch 070:     37 / 65 loss=6.868, nll_loss=5.159, ppl=35.72, wps=3262.1, ups=0.26, wpb=12326.4, bsz=546.6, num_updates=4505, lr=2.80357e-05, gnorm=4.377, clip=100, loss_scale=0.125, train_wall=19, gb_free=20.7, wall=18350
2023-05-19 21:43:21 | INFO | train_inner | epoch 070:     42 / 65 loss=6.823, nll_loss=5.097, ppl=34.22, wps=3444.7, ups=0.29, wpb=11677.6, bsz=489.8, num_updates=4510, lr=2.80102e-05, gnorm=4.32, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=18366
2023-05-19 21:43:36 | INFO | train_inner | epoch 070:     47 / 65 loss=6.895, nll_loss=5.182, ppl=36.31, wps=3950.3, ups=0.32, wpb=12422.8, bsz=619.8, num_updates=4515, lr=2.79847e-05, gnorm=10.663, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.4, wall=18382
2023-05-19 21:43:53 | INFO | train_inner | epoch 070:     52 / 65 loss=7.053, nll_loss=5.335, ppl=40.36, wps=3799, ups=0.3, wpb=12552.2, bsz=587, num_updates=4520, lr=2.79592e-05, gnorm=6.161, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=18399
2023-05-19 21:44:08 | INFO | train_inner | epoch 070:     57 / 65 loss=6.813, nll_loss=5.084, ppl=33.92, wps=4091.3, ups=0.33, wpb=12255.4, bsz=546.2, num_updates=4525, lr=2.79337e-05, gnorm=8.509, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=18414
2023-05-19 21:44:25 | INFO | train_inner | epoch 070:     62 / 65 loss=6.86, nll_loss=5.14, ppl=35.26, wps=3767.2, ups=0.3, wpb=12635.4, bsz=600, num_updates=4530, lr=2.79082e-05, gnorm=6.521, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.5, wall=18430
2023-05-19 21:44:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:44:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:45:16 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.239 | nll_loss 4.376 | ppl 20.76 | bleu 4.42 | wps 830.5 | wpb 2785 | bsz 105.2 | num_updates 4533 | best_loss 6.239
2023-05-19 21:45:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 4533 updates
2023-05-19 21:45:16 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint70.pt
2023-05-19 21:45:21 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint70.pt
2023-05-19 21:45:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint70.pt (epoch 70 @ 4533 updates, score 6.239) (writing took 24.593146163970232 seconds)
2023-05-19 21:45:40 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2023-05-19 21:45:40 | INFO | train | epoch 070 | loss 6.898 | nll_loss 5.181 | ppl 36.27 | wps 2849.2 | ups 0.24 | wpb 12060.8 | bsz 558.8 | num_updates 4533 | lr 2.78929e-05 | gnorm 5.969 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.5 | wall 18506
2023-05-19 21:45:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:45:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:45:41 | INFO | fairseq.trainer | begin training epoch 71
2023-05-19 21:45:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:45:47 | INFO | train_inner | epoch 071:      2 / 65 loss=6.878, nll_loss=5.159, ppl=35.74, wps=615.8, ups=0.06, wpb=10126.2, bsz=444.4, num_updates=4535, lr=2.78827e-05, gnorm=4.492, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=18513
2023-05-19 21:46:02 | INFO | train_inner | epoch 071:      7 / 65 loss=6.845, nll_loss=5.123, ppl=34.85, wps=3989.2, ups=0.32, wpb=12326, bsz=563.6, num_updates=4540, lr=2.78571e-05, gnorm=12.314, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=18528
2023-05-19 21:46:18 | INFO | train_inner | epoch 071:     12 / 65 loss=6.81, nll_loss=5.082, ppl=33.88, wps=3957.1, ups=0.32, wpb=12327.6, bsz=548.8, num_updates=4545, lr=2.78316e-05, gnorm=4.278, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=18544
2023-05-19 21:46:33 | INFO | train_inner | epoch 071:     17 / 65 loss=6.908, nll_loss=5.199, ppl=36.73, wps=4087.1, ups=0.34, wpb=12009.8, bsz=590.6, num_updates=4550, lr=2.78061e-05, gnorm=4.964, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=18558
2023-05-19 21:46:49 | INFO | train_inner | epoch 071:     22 / 65 loss=6.861, nll_loss=5.139, ppl=35.25, wps=3781.3, ups=0.31, wpb=12004.6, bsz=495.4, num_updates=4555, lr=2.77806e-05, gnorm=7.267, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=18574
2023-05-19 21:47:05 | INFO | train_inner | epoch 071:     27 / 65 loss=6.83, nll_loss=5.104, ppl=34.4, wps=3859.2, ups=0.31, wpb=12507.2, bsz=572.2, num_updates=4560, lr=2.77551e-05, gnorm=4.501, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=18590
2023-05-19 21:47:24 | INFO | train_inner | epoch 071:     32 / 65 loss=6.857, nll_loss=5.126, ppl=34.92, wps=3142.5, ups=0.26, wpb=12007, bsz=539.4, num_updates=4565, lr=2.77296e-05, gnorm=7.773, clip=100, loss_scale=0.125, train_wall=19, gb_free=21.5, wall=18610
2023-05-19 21:47:42 | INFO | train_inner | epoch 071:     37 / 65 loss=6.904, nll_loss=5.191, ppl=36.52, wps=3246.3, ups=0.27, wpb=11902, bsz=580.4, num_updates=4570, lr=2.77041e-05, gnorm=2.872, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=18628
2023-05-19 21:47:59 | INFO | train_inner | epoch 071:     42 / 65 loss=6.851, nll_loss=5.132, ppl=35.07, wps=3597.3, ups=0.3, wpb=12053.8, bsz=527.4, num_updates=4575, lr=2.76786e-05, gnorm=3.443, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=18645
2023-05-19 21:48:14 | INFO | train_inner | epoch 071:     47 / 65 loss=7.045, nll_loss=5.33, ppl=40.21, wps=4111.6, ups=0.33, wpb=12496.2, bsz=599.8, num_updates=4580, lr=2.76531e-05, gnorm=25.765, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=18660
2023-05-19 21:48:29 | INFO | train_inner | epoch 071:     52 / 65 loss=6.873, nll_loss=5.152, ppl=35.56, wps=3935.9, ups=0.33, wpb=11898.2, bsz=545, num_updates=4585, lr=2.76276e-05, gnorm=3.938, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=18675
2023-05-19 21:48:45 | INFO | train_inner | epoch 071:     57 / 65 loss=7.059, nll_loss=5.355, ppl=40.94, wps=4083.3, ups=0.33, wpb=12505.8, bsz=664.6, num_updates=4590, lr=2.7602e-05, gnorm=3.501, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=18690
2023-05-19 21:49:01 | INFO | train_inner | epoch 071:     62 / 65 loss=6.889, nll_loss=5.175, ppl=36.13, wps=3666.9, ups=0.3, wpb=12407.2, bsz=586, num_updates=4595, lr=2.75765e-05, gnorm=2.495, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=18707
2023-05-19 21:49:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:49:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:49:49 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 6.225 | nll_loss 4.358 | ppl 20.51 | bleu 4.14 | wps 891.6 | wpb 2785 | bsz 105.2 | num_updates 4598 | best_loss 6.225
2023-05-19 21:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 4598 updates
2023-05-19 21:49:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint71.pt
2023-05-19 21:49:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint71.pt
2023-05-19 21:50:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint71.pt (epoch 71 @ 4598 updates, score 6.225) (writing took 23.572336316108704 seconds)
2023-05-19 21:50:12 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2023-05-19 21:50:12 | INFO | train | epoch 071 | loss 6.89 | nll_loss 5.172 | ppl 36.05 | wps 2884.9 | ups 0.24 | wpb 12056.8 | bsz 559.6 | num_updates 4598 | lr 2.75612e-05 | gnorm 7.048 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.8 | wall 18778
2023-05-19 21:50:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:50:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:50:12 | INFO | fairseq.trainer | begin training epoch 72
2023-05-19 21:50:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:50:18 | INFO | train_inner | epoch 072:      2 / 65 loss=6.777, nll_loss=5.05, ppl=33.12, wps=660.8, ups=0.07, wpb=10159, bsz=417.8, num_updates=4600, lr=2.7551e-05, gnorm=8.014, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.7, wall=18784
2023-05-19 21:50:36 | INFO | train_inner | epoch 072:      7 / 65 loss=6.919, nll_loss=5.205, ppl=36.88, wps=3456.6, ups=0.28, wpb=12349.2, bsz=625.6, num_updates=4605, lr=2.75255e-05, gnorm=13.794, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=18802
2023-05-19 21:50:53 | INFO | train_inner | epoch 072:     12 / 65 loss=6.918, nll_loss=5.199, ppl=36.74, wps=3720, ups=0.3, wpb=12374.6, bsz=557, num_updates=4610, lr=2.75e-05, gnorm=5.381, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.1, wall=18819
2023-05-19 21:51:10 | INFO | train_inner | epoch 072:     17 / 65 loss=6.889, nll_loss=5.176, ppl=36.14, wps=3559.2, ups=0.29, wpb=12301.2, bsz=565.2, num_updates=4615, lr=2.74745e-05, gnorm=4.175, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=18836
2023-05-19 21:51:25 | INFO | train_inner | epoch 072:     22 / 65 loss=6.911, nll_loss=5.194, ppl=36.6, wps=3998.9, ups=0.33, wpb=11992.4, bsz=552.2, num_updates=4620, lr=2.7449e-05, gnorm=8.012, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=18851
2023-05-19 21:51:41 | INFO | train_inner | epoch 072:     27 / 65 loss=6.879, nll_loss=5.162, ppl=35.81, wps=3895, ups=0.31, wpb=12544.8, bsz=626, num_updates=4625, lr=2.74235e-05, gnorm=12.95, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=18867
2023-05-19 21:51:57 | INFO | train_inner | epoch 072:     32 / 65 loss=6.805, nll_loss=5.081, ppl=33.85, wps=3921.9, ups=0.32, wpb=12117.8, bsz=542.4, num_updates=4630, lr=2.7398e-05, gnorm=7.1, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.5, wall=18882
2023-05-19 21:52:13 | INFO | train_inner | epoch 072:     37 / 65 loss=6.798, nll_loss=5.071, ppl=33.61, wps=3680.7, ups=0.31, wpb=11854, bsz=504.6, num_updates=4635, lr=2.73724e-05, gnorm=3.857, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=18899
2023-05-19 21:52:29 | INFO | train_inner | epoch 072:     42 / 65 loss=6.839, nll_loss=5.113, ppl=34.61, wps=3750.4, ups=0.31, wpb=12252.6, bsz=566.6, num_updates=4640, lr=2.73469e-05, gnorm=5.812, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.1, wall=18915
2023-05-19 21:52:45 | INFO | train_inner | epoch 072:     47 / 65 loss=6.998, nll_loss=5.289, ppl=39.1, wps=3928.1, ups=0.31, wpb=12481.8, bsz=629.8, num_updates=4645, lr=2.73214e-05, gnorm=3.721, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=18931
2023-05-19 21:53:00 | INFO | train_inner | epoch 072:     52 / 65 loss=6.952, nll_loss=5.241, ppl=37.82, wps=3996.9, ups=0.33, wpb=12189.4, bsz=619, num_updates=4650, lr=2.72959e-05, gnorm=3.147, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=18946
2023-05-19 21:53:16 | INFO | train_inner | epoch 072:     57 / 65 loss=6.995, nll_loss=5.273, ppl=38.66, wps=3859.1, ups=0.32, wpb=12132.6, bsz=556.4, num_updates=4655, lr=2.72704e-05, gnorm=9.195, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=18962
2023-05-19 21:53:33 | INFO | train_inner | epoch 072:     62 / 65 loss=6.794, nll_loss=5.066, ppl=33.51, wps=3715.6, ups=0.3, wpb=12325, bsz=506.8, num_updates=4660, lr=2.72449e-05, gnorm=5.523, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=18978
2023-05-19 21:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:53:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:54:23 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 6.201 | nll_loss 4.327 | ppl 20.07 | bleu 4.24 | wps 842.3 | wpb 2785 | bsz 105.2 | num_updates 4663 | best_loss 6.201
2023-05-19 21:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 4663 updates
2023-05-19 21:54:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint72.pt
2023-05-19 21:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint72.pt
2023-05-19 21:54:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint72.pt (epoch 72 @ 4663 updates, score 6.201) (writing took 10.362386375665665 seconds)
2023-05-19 21:54:33 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2023-05-19 21:54:33 | INFO | train | epoch 072 | loss 6.885 | nll_loss 5.165 | ppl 35.88 | wps 2997.5 | ups 0.25 | wpb 12051.9 | bsz 559.4 | num_updates 4663 | lr 2.72296e-05 | gnorm 6.767 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21.8 | wall 19039
2023-05-19 21:54:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:54:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:54:33 | INFO | fairseq.trainer | begin training epoch 73
2023-05-19 21:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:54:40 | INFO | train_inner | epoch 073:      2 / 65 loss=6.772, nll_loss=5.035, ppl=32.79, wps=728.9, ups=0.07, wpb=9763.2, bsz=431.4, num_updates=4665, lr=2.72194e-05, gnorm=10.013, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.4, wall=19045
2023-05-19 21:54:56 | INFO | train_inner | epoch 073:      7 / 65 loss=6.876, nll_loss=5.146, ppl=35.42, wps=3783.9, ups=0.31, wpb=12116, bsz=566.8, num_updates=4670, lr=2.71939e-05, gnorm=6.644, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19061
2023-05-19 21:55:12 | INFO | train_inner | epoch 073:     12 / 65 loss=6.724, nll_loss=4.977, ppl=31.49, wps=3734.2, ups=0.31, wpb=12114.2, bsz=524.4, num_updates=4675, lr=2.71684e-05, gnorm=4.999, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=19077
2023-05-19 21:55:28 | INFO | train_inner | epoch 073:     17 / 65 loss=6.809, nll_loss=5.078, ppl=33.78, wps=3791.5, ups=0.32, wpb=11967.2, bsz=509.2, num_updates=4680, lr=2.71429e-05, gnorm=7.922, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19093
2023-05-19 21:55:43 | INFO | train_inner | epoch 073:     22 / 65 loss=7.047, nll_loss=5.334, ppl=40.34, wps=3930.4, ups=0.32, wpb=12422.8, bsz=600.4, num_updates=4685, lr=2.71173e-05, gnorm=6.594, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=19109
2023-05-19 21:56:00 | INFO | train_inner | epoch 073:     27 / 65 loss=6.869, nll_loss=5.152, ppl=35.56, wps=3616.9, ups=0.29, wpb=12280.4, bsz=584, num_updates=4690, lr=2.70918e-05, gnorm=13.193, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=19126
2023-05-19 21:56:18 | INFO | train_inner | epoch 073:     32 / 65 loss=6.839, nll_loss=5.118, ppl=34.73, wps=3334.1, ups=0.28, wpb=11875.4, bsz=519.2, num_updates=4695, lr=2.70663e-05, gnorm=7.73, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=19144
2023-05-19 21:56:34 | INFO | train_inner | epoch 073:     37 / 65 loss=6.933, nll_loss=5.215, ppl=37.15, wps=3909.6, ups=0.31, wpb=12465.2, bsz=616, num_updates=4700, lr=2.70408e-05, gnorm=5.23, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.9, wall=19160
2023-05-19 21:56:49 | INFO | train_inner | epoch 073:     42 / 65 loss=6.888, nll_loss=5.171, ppl=36.03, wps=4177.5, ups=0.34, wpb=12263.4, bsz=586.2, num_updates=4705, lr=2.70153e-05, gnorm=8.972, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=19174
2023-05-19 21:57:05 | INFO | train_inner | epoch 073:     47 / 65 loss=6.881, nll_loss=5.161, ppl=35.79, wps=3896.7, ups=0.31, wpb=12579.4, bsz=613, num_updates=4710, lr=2.69898e-05, gnorm=10.106, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=19191
2023-05-19 21:57:21 | INFO | train_inner | epoch 073:     52 / 65 loss=6.95, nll_loss=5.239, ppl=37.78, wps=3898.3, ups=0.32, wpb=12226.8, bsz=588, num_updates=4715, lr=2.69643e-05, gnorm=6.673, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=19206
2023-05-19 21:57:36 | INFO | train_inner | epoch 073:     57 / 65 loss=6.994, nll_loss=5.278, ppl=38.8, wps=3898.6, ups=0.32, wpb=12120, bsz=590, num_updates=4720, lr=2.69388e-05, gnorm=3.721, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=19222
2023-05-19 21:57:53 | INFO | train_inner | epoch 073:     62 / 65 loss=6.817, nll_loss=5.09, ppl=34.07, wps=3608.4, ups=0.3, wpb=12060.8, bsz=507.8, num_updates=4725, lr=2.69133e-05, gnorm=7.914, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=19239
2023-05-19 21:58:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 21:58:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:58:43 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 6.232 | nll_loss 4.363 | ppl 20.57 | bleu 4.53 | wps 842.5 | wpb 2785 | bsz 105.2 | num_updates 4728 | best_loss 6.201
2023-05-19 21:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 4728 updates
2023-05-19 21:58:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint73.pt
2023-05-19 21:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint73.pt
2023-05-19 21:58:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint73.pt (epoch 73 @ 4728 updates, score 6.232) (writing took 6.923234019428492 seconds)
2023-05-19 21:58:50 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2023-05-19 21:58:50 | INFO | train | epoch 073 | loss 6.879 | nll_loss 5.156 | ppl 35.66 | wps 3057.9 | ups 0.25 | wpb 12056.5 | bsz 559.8 | num_updates 4728 | lr 2.6898e-05 | gnorm 7.557 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.8 | wall 19295
2023-05-19 21:58:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 21:58:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 21:58:50 | INFO | fairseq.trainer | begin training epoch 74
2023-05-19 21:58:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 21:58:55 | INFO | train_inner | epoch 074:      2 / 65 loss=6.859, nll_loss=5.143, ppl=35.33, wps=863.4, ups=0.08, wpb=10809.2, bsz=541.2, num_updates=4730, lr=2.68878e-05, gnorm=4.229, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.1, wall=19301
2023-05-19 21:59:14 | INFO | train_inner | epoch 074:      7 / 65 loss=6.779, nll_loss=5.042, ppl=32.95, wps=3233.9, ups=0.28, wpb=11754.4, bsz=518.4, num_updates=4735, lr=2.68622e-05, gnorm=10.08, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.7, wall=19319
2023-05-19 21:59:29 | INFO | train_inner | epoch 074:     12 / 65 loss=7.048, nll_loss=5.334, ppl=40.32, wps=4057.3, ups=0.32, wpb=12695.8, bsz=643.8, num_updates=4740, lr=2.68367e-05, gnorm=4.186, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19335
2023-05-19 21:59:46 | INFO | train_inner | epoch 074:     17 / 65 loss=6.822, nll_loss=5.098, ppl=34.25, wps=3573.9, ups=0.29, wpb=12201.6, bsz=550, num_updates=4745, lr=2.68112e-05, gnorm=3.736, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=19352
2023-05-19 22:00:01 | INFO | train_inner | epoch 074:     22 / 65 loss=6.824, nll_loss=5.098, ppl=34.24, wps=4001, ups=0.33, wpb=11992.2, bsz=568.8, num_updates=4750, lr=2.67857e-05, gnorm=4.039, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=19367
2023-05-19 22:00:18 | INFO | train_inner | epoch 074:     27 / 65 loss=6.851, nll_loss=5.129, ppl=34.99, wps=3865.6, ups=0.31, wpb=12550.4, bsz=579.2, num_updates=4755, lr=2.67602e-05, gnorm=14.514, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19383
2023-05-19 22:00:34 | INFO | train_inner | epoch 074:     32 / 65 loss=6.897, nll_loss=5.186, ppl=36.4, wps=3674.9, ups=0.3, wpb=12198.6, bsz=566.8, num_updates=4760, lr=2.67347e-05, gnorm=3.585, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=19400
2023-05-19 22:00:49 | INFO | train_inner | epoch 074:     37 / 65 loss=6.88, nll_loss=5.165, ppl=35.87, wps=4142.3, ups=0.33, wpb=12599.8, bsz=580, num_updates=4765, lr=2.67092e-05, gnorm=6.173, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=19415
2023-05-19 22:01:05 | INFO | train_inner | epoch 074:     42 / 65 loss=6.829, nll_loss=5.109, ppl=34.51, wps=3779.5, ups=0.32, wpb=11736.2, bsz=503.2, num_updates=4770, lr=2.66837e-05, gnorm=4.516, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=19431
2023-05-19 22:01:20 | INFO | train_inner | epoch 074:     47 / 65 loss=6.876, nll_loss=5.155, ppl=35.63, wps=4157.8, ups=0.34, wpb=12248.2, bsz=537.4, num_updates=4775, lr=2.66582e-05, gnorm=6.435, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=19445
2023-05-19 22:01:34 | INFO | train_inner | epoch 074:     52 / 65 loss=7.002, nll_loss=5.297, ppl=39.32, wps=4030.7, ups=0.34, wpb=11900.6, bsz=552.6, num_updates=4780, lr=2.66327e-05, gnorm=3.88, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=19460
2023-05-19 22:01:51 | INFO | train_inner | epoch 074:     57 / 65 loss=6.93, nll_loss=5.222, ppl=37.32, wps=3511.8, ups=0.3, wpb=11887.8, bsz=576.4, num_updates=4785, lr=2.66071e-05, gnorm=22.13, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=19477
2023-05-19 22:02:06 | INFO | train_inner | epoch 074:     62 / 65 loss=6.819, nll_loss=5.093, ppl=34.12, wps=4071.2, ups=0.33, wpb=12337, bsz=562, num_updates=4790, lr=2.65816e-05, gnorm=4.446, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.4, wall=19492
2023-05-19 22:02:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:02:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:02:54 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 6.208 | nll_loss 4.34 | ppl 20.25 | bleu 4.42 | wps 899.6 | wpb 2785 | bsz 105.2 | num_updates 4793 | best_loss 6.201
2023-05-19 22:02:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 4793 updates
2023-05-19 22:02:54 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint74.pt
2023-05-19 22:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint74.pt
2023-05-19 22:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint74.pt (epoch 74 @ 4793 updates, score 6.208) (writing took 6.809157986193895 seconds)
2023-05-19 22:03:01 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2023-05-19 22:03:01 | INFO | train | epoch 074 | loss 6.885 | nll_loss 5.166 | ppl 35.91 | wps 3119.8 | ups 0.26 | wpb 12060.5 | bsz 559.6 | num_updates 4793 | lr 2.65663e-05 | gnorm 8.428 | clip 100 | loss_scale 0.25 | train_wall 204 | gb_free 21.6 | wall 19547
2023-05-19 22:03:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:03:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:03:01 | INFO | fairseq.trainer | begin training epoch 75
2023-05-19 22:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:03:08 | INFO | train_inner | epoch 075:      2 / 65 loss=6.931, nll_loss=5.212, ppl=37.07, wps=840.2, ups=0.08, wpb=10304, bsz=467.8, num_updates=4795, lr=2.65561e-05, gnorm=21.15, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=19553
2023-05-19 22:03:24 | INFO | train_inner | epoch 075:      7 / 65 loss=7.019, nll_loss=5.309, ppl=39.65, wps=3817.1, ups=0.31, wpb=12290, bsz=623.6, num_updates=4800, lr=2.65306e-05, gnorm=3.501, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=19570
2023-05-19 22:03:40 | INFO | train_inner | epoch 075:     12 / 65 loss=6.883, nll_loss=5.157, ppl=35.68, wps=3689.7, ups=0.3, wpb=12252.6, bsz=525.2, num_updates=4805, lr=2.65051e-05, gnorm=5.932, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=19586
2023-05-19 22:03:56 | INFO | train_inner | epoch 075:     17 / 65 loss=6.876, nll_loss=5.15, ppl=35.52, wps=3980.6, ups=0.32, wpb=12411, bsz=587, num_updates=4810, lr=2.64796e-05, gnorm=5.596, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19602
2023-05-19 22:04:11 | INFO | train_inner | epoch 075:     22 / 65 loss=6.814, nll_loss=5.081, ppl=33.86, wps=3944, ups=0.33, wpb=12034, bsz=535.2, num_updates=4815, lr=2.64541e-05, gnorm=8.18, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=19617
2023-05-19 22:04:27 | INFO | train_inner | epoch 075:     27 / 65 loss=6.823, nll_loss=5.103, ppl=34.36, wps=3893.6, ups=0.32, wpb=12115.6, bsz=573.6, num_updates=4820, lr=2.64286e-05, gnorm=5.309, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=19633
2023-05-19 22:04:44 | INFO | train_inner | epoch 075:     32 / 65 loss=6.901, nll_loss=5.193, ppl=36.57, wps=3613.1, ups=0.3, wpb=12103.6, bsz=582.6, num_updates=4825, lr=2.64031e-05, gnorm=5.598, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=19649
2023-05-19 22:05:00 | INFO | train_inner | epoch 075:     37 / 65 loss=6.81, nll_loss=5.085, ppl=33.93, wps=3635.3, ups=0.3, wpb=12270.6, bsz=543.2, num_updates=4830, lr=2.63776e-05, gnorm=4.104, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=19666
2023-05-19 22:05:16 | INFO | train_inner | epoch 075:     42 / 65 loss=6.855, nll_loss=5.132, ppl=35.08, wps=3797.7, ups=0.32, wpb=11893.8, bsz=523.8, num_updates=4835, lr=2.6352e-05, gnorm=5.265, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19682
2023-05-19 22:05:31 | INFO | train_inner | epoch 075:     47 / 65 loss=6.941, nll_loss=5.212, ppl=37.07, wps=4198.5, ups=0.33, wpb=12596.2, bsz=571, num_updates=4840, lr=2.63265e-05, gnorm=5.21, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=19697
2023-05-19 22:05:46 | INFO | train_inner | epoch 075:     52 / 65 loss=6.826, nll_loss=5.102, ppl=34.34, wps=4047.5, ups=0.34, wpb=12048.6, bsz=618.8, num_updates=4845, lr=2.6301e-05, gnorm=11.628, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=19712
2023-05-19 22:06:02 | INFO | train_inner | epoch 075:     57 / 65 loss=6.862, nll_loss=5.142, ppl=35.31, wps=3790.8, ups=0.31, wpb=12084.2, bsz=561.2, num_updates=4850, lr=2.62755e-05, gnorm=4.717, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=19728
2023-05-19 22:06:17 | INFO | train_inner | epoch 075:     62 / 65 loss=6.863, nll_loss=5.144, ppl=35.35, wps=4020.5, ups=0.33, wpb=12139.4, bsz=568.2, num_updates=4855, lr=2.625e-05, gnorm=6.651, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=19743
2023-05-19 22:06:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:06:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:07:04 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 6.213 | nll_loss 4.344 | ppl 20.3 | bleu 4.51 | wps 892.3 | wpb 2785 | bsz 105.2 | num_updates 4858 | best_loss 6.201
2023-05-19 22:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 4858 updates
2023-05-19 22:07:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint75.pt
2023-05-19 22:07:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint75.pt
2023-05-19 22:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint75.pt (epoch 75 @ 4858 updates, score 6.213) (writing took 6.666727416217327 seconds)
2023-05-19 22:07:11 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2023-05-19 22:07:11 | INFO | train | epoch 075 | loss 6.869 | nll_loss 5.147 | ppl 35.43 | wps 3137.3 | ups 0.26 | wpb 12047.9 | bsz 558.6 | num_updates 4858 | lr 2.62347e-05 | gnorm 5.855 | clip 100 | loss_scale 0.25 | train_wall 202 | gb_free 21.6 | wall 19796
2023-05-19 22:07:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:07:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:07:11 | INFO | fairseq.trainer | begin training epoch 76
2023-05-19 22:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:07:17 | INFO | train_inner | epoch 076:      2 / 65 loss=6.782, nll_loss=5.05, ppl=33.13, wps=851.8, ups=0.08, wpb=10131.8, bsz=460.6, num_updates=4860, lr=2.62245e-05, gnorm=5.007, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.3, wall=19802
2023-05-19 22:07:31 | INFO | train_inner | epoch 076:      7 / 65 loss=6.893, nll_loss=5.18, ppl=36.26, wps=4301.1, ups=0.34, wpb=12724.4, bsz=651, num_updates=4865, lr=2.6199e-05, gnorm=5.92, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=19817
2023-05-19 22:07:48 | INFO | train_inner | epoch 076:     12 / 65 loss=6.846, nll_loss=5.116, ppl=34.67, wps=3756.5, ups=0.31, wpb=12250.8, bsz=592.2, num_updates=4870, lr=2.61735e-05, gnorm=6.353, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.1, wall=19833
2023-05-19 22:08:04 | INFO | train_inner | epoch 076:     17 / 65 loss=6.797, nll_loss=5.069, ppl=33.58, wps=3633.1, ups=0.3, wpb=12199.6, bsz=536.2, num_updates=4875, lr=2.6148e-05, gnorm=6.004, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=19850
2023-05-19 22:08:20 | INFO | train_inner | epoch 076:     22 / 65 loss=6.846, nll_loss=5.122, ppl=34.82, wps=4003.8, ups=0.33, wpb=12233.2, bsz=559.6, num_updates=4880, lr=2.61224e-05, gnorm=3.253, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=19865
2023-05-19 22:08:37 | INFO | train_inner | epoch 076:     27 / 65 loss=6.949, nll_loss=5.242, ppl=37.85, wps=3718.6, ups=0.29, wpb=12810.8, bsz=673.6, num_updates=4885, lr=2.60969e-05, gnorm=4.513, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=19883
2023-05-19 22:08:55 | INFO | train_inner | epoch 076:     32 / 65 loss=6.796, nll_loss=5.07, ppl=33.59, wps=3310.9, ups=0.28, wpb=11995, bsz=480.2, num_updates=4890, lr=2.60714e-05, gnorm=4.445, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=19901
2023-05-19 22:09:10 | INFO | train_inner | epoch 076:     37 / 65 loss=6.868, nll_loss=5.155, ppl=35.63, wps=4187.9, ups=0.33, wpb=12542, bsz=602.2, num_updates=4895, lr=2.60459e-05, gnorm=7.857, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=19916
2023-05-19 22:09:26 | INFO | train_inner | epoch 076:     42 / 65 loss=6.836, nll_loss=5.109, ppl=34.5, wps=3656.4, ups=0.32, wpb=11601.8, bsz=494.8, num_updates=4900, lr=2.60204e-05, gnorm=6.413, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=19932
2023-05-19 22:09:42 | INFO | train_inner | epoch 076:     47 / 65 loss=7.084, nll_loss=5.363, ppl=41.16, wps=3590.9, ups=0.3, wpb=11855, bsz=584.8, num_updates=4905, lr=2.59949e-05, gnorm=3.364, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=19948
2023-05-19 22:09:58 | INFO | train_inner | epoch 076:     52 / 65 loss=6.764, nll_loss=5.02, ppl=32.45, wps=3796.7, ups=0.31, wpb=12065.8, bsz=509.4, num_updates=4910, lr=2.59694e-05, gnorm=5.178, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=19964
2023-05-19 22:10:14 | INFO | train_inner | epoch 076:     57 / 65 loss=6.893, nll_loss=5.166, ppl=35.89, wps=3951.6, ups=0.32, wpb=12387, bsz=558.4, num_updates=4915, lr=2.59439e-05, gnorm=12.878, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.6, wall=19980
2023-05-19 22:10:30 | INFO | train_inner | epoch 076:     62 / 65 loss=6.824, nll_loss=5.105, ppl=34.42, wps=3657.4, ups=0.3, wpb=12070.6, bsz=592.6, num_updates=4920, lr=2.59184e-05, gnorm=3.67, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=19996
2023-05-19 22:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:11:19 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 6.183 | nll_loss 4.312 | ppl 19.87 | bleu 4.58 | wps 886.1 | wpb 2785 | bsz 105.2 | num_updates 4923 | best_loss 6.183
2023-05-19 22:11:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 4923 updates
2023-05-19 22:11:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint76.pt
2023-05-19 22:11:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint76.pt
2023-05-19 22:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint76.pt (epoch 76 @ 4923 updates, score 6.183) (writing took 20.810832533985376 seconds)
2023-05-19 22:11:40 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2023-05-19 22:11:40 | INFO | train | epoch 076 | loss 6.861 | nll_loss 5.137 | ppl 35.19 | wps 2904.7 | ups 0.24 | wpb 12057.4 | bsz 559.4 | num_updates 4923 | lr 2.59031e-05 | gnorm 5.957 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.6 | wall 20066
2023-05-19 22:11:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:11:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:11:40 | INFO | fairseq.trainer | begin training epoch 77
2023-05-19 22:11:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:11:46 | INFO | train_inner | epoch 077:      2 / 65 loss=6.794, nll_loss=5.069, ppl=33.57, wps=685.3, ups=0.07, wpb=10305.2, bsz=465.6, num_updates=4925, lr=2.58929e-05, gnorm=7.847, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.7, wall=20071
2023-05-19 22:12:03 | INFO | train_inner | epoch 077:      7 / 65 loss=6.8, nll_loss=5.069, ppl=33.58, wps=3508.4, ups=0.29, wpb=11900, bsz=489.4, num_updates=4930, lr=2.58673e-05, gnorm=5.697, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=20088
2023-05-19 22:12:19 | INFO | train_inner | epoch 077:     12 / 65 loss=6.835, nll_loss=5.105, ppl=34.42, wps=3707.1, ups=0.31, wpb=11995.4, bsz=555.4, num_updates=4935, lr=2.58418e-05, gnorm=3.415, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=20105
2023-05-19 22:12:35 | INFO | train_inner | epoch 077:     17 / 65 loss=6.831, nll_loss=5.105, ppl=34.41, wps=3878, ups=0.31, wpb=12459, bsz=592.4, num_updates=4940, lr=2.58163e-05, gnorm=3.427, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=20121
2023-05-19 22:12:50 | INFO | train_inner | epoch 077:     22 / 65 loss=6.918, nll_loss=5.206, ppl=36.92, wps=3953.8, ups=0.32, wpb=12308.6, bsz=615.8, num_updates=4945, lr=2.57908e-05, gnorm=4.007, clip=100, loss_scale=0.25, train_wall=16, gb_free=21, wall=20136
2023-05-19 22:13:06 | INFO | train_inner | epoch 077:     27 / 65 loss=6.856, nll_loss=5.137, ppl=35.18, wps=3774.9, ups=0.31, wpb=12055.6, bsz=569.6, num_updates=4950, lr=2.57653e-05, gnorm=10.007, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=20152
2023-05-19 22:13:25 | INFO | train_inner | epoch 077:     32 / 65 loss=6.794, nll_loss=5.064, ppl=33.45, wps=3295.5, ups=0.28, wpb=11948, bsz=472.6, num_updates=4955, lr=2.57398e-05, gnorm=8.942, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.8, wall=20170
2023-05-19 22:13:39 | INFO | train_inner | epoch 077:     37 / 65 loss=6.786, nll_loss=5.054, ppl=33.22, wps=4508.9, ups=0.35, wpb=12720, bsz=621.8, num_updates=4960, lr=2.57143e-05, gnorm=7.536, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=20184
2023-05-19 22:13:54 | INFO | train_inner | epoch 077:     42 / 65 loss=6.813, nll_loss=5.085, ppl=33.95, wps=3902.4, ups=0.33, wpb=11935.8, bsz=552.4, num_updates=4965, lr=2.56888e-05, gnorm=8.656, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=20200
2023-05-19 22:14:11 | INFO | train_inner | epoch 077:     47 / 65 loss=6.853, nll_loss=5.13, ppl=35.03, wps=3508.8, ups=0.29, wpb=12062.6, bsz=557.2, num_updates=4970, lr=2.56633e-05, gnorm=5.921, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=20217
2023-05-19 22:14:27 | INFO | train_inner | epoch 077:     52 / 65 loss=6.926, nll_loss=5.202, ppl=36.82, wps=3691.2, ups=0.31, wpb=11997.6, bsz=534.8, num_updates=4975, lr=2.56378e-05, gnorm=4.719, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=20233
2023-05-19 22:14:43 | INFO | train_inner | epoch 077:     57 / 65 loss=7.1, nll_loss=5.386, ppl=41.82, wps=4162.3, ups=0.33, wpb=12676, bsz=687, num_updates=4980, lr=2.56122e-05, gnorm=8.578, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=20248
2023-05-19 22:14:59 | INFO | train_inner | epoch 077:     62 / 65 loss=6.797, nll_loss=5.059, ppl=33.33, wps=3775.3, ups=0.31, wpb=12073.8, bsz=509.6, num_updates=4985, lr=2.55867e-05, gnorm=7.107, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=20264
2023-05-19 22:15:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:15:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:15:45 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 6.181 | nll_loss 4.304 | ppl 19.75 | bleu 4.89 | wps 899.7 | wpb 2785 | bsz 105.2 | num_updates 4988 | best_loss 6.181
2023-05-19 22:15:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 4988 updates
2023-05-19 22:15:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint77.pt
2023-05-19 22:15:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint77.pt
2023-05-19 22:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint77.pt (epoch 77 @ 4988 updates, score 6.181) (writing took 9.633850753307343 seconds)
2023-05-19 22:15:59 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2023-05-19 22:15:59 | INFO | train | epoch 077 | loss 6.859 | nll_loss 5.134 | ppl 35.12 | wps 3033.2 | ups 0.25 | wpb 12051.7 | bsz 558.9 | num_updates 4988 | lr 2.55714e-05 | gnorm 6.479 | clip 100 | loss_scale 0.25 | train_wall 204 | gb_free 21.6 | wall 20324
2023-05-19 22:15:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:15:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:15:59 | INFO | fairseq.trainer | begin training epoch 78
2023-05-19 22:15:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:16:05 | INFO | train_inner | epoch 078:      2 / 65 loss=7.058, nll_loss=5.346, ppl=40.68, wps=791, ups=0.08, wpb=10488.2, bsz=527.2, num_updates=4990, lr=2.55612e-05, gnorm=5.144, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=20331
2023-05-19 22:16:22 | INFO | train_inner | epoch 078:      7 / 65 loss=6.812, nll_loss=5.083, ppl=33.89, wps=3621.9, ups=0.3, wpb=12129.4, bsz=506.4, num_updates=4995, lr=2.55357e-05, gnorm=6.017, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=20347
2023-05-19 22:16:40 | INFO | train_inner | epoch 078:     12 / 65 loss=6.809, nll_loss=5.08, ppl=33.83, wps=3348.9, ups=0.28, wpb=12087.8, bsz=555.8, num_updates=5000, lr=2.55102e-05, gnorm=5.109, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=20365
2023-05-19 22:16:55 | INFO | train_inner | epoch 078:     17 / 65 loss=6.875, nll_loss=5.148, ppl=35.46, wps=4018.3, ups=0.32, wpb=12681, bsz=632.6, num_updates=5005, lr=2.54847e-05, gnorm=6.583, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=20381
2023-05-19 22:17:12 | INFO | train_inner | epoch 078:     22 / 65 loss=6.838, nll_loss=5.116, ppl=34.68, wps=3723.4, ups=0.3, wpb=12330, bsz=563.8, num_updates=5010, lr=2.54592e-05, gnorm=5.68, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=20398
2023-05-19 22:17:28 | INFO | train_inner | epoch 078:     27 / 65 loss=6.946, nll_loss=5.232, ppl=37.59, wps=3801.2, ups=0.31, wpb=12461, bsz=581.4, num_updates=5015, lr=2.54337e-05, gnorm=4.31, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=20414
2023-05-19 22:17:44 | INFO | train_inner | epoch 078:     32 / 65 loss=6.785, nll_loss=5.053, ppl=33.19, wps=3825.8, ups=0.32, wpb=12096.6, bsz=509.4, num_updates=5020, lr=2.54082e-05, gnorm=11.205, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=20430
2023-05-19 22:18:01 | INFO | train_inner | epoch 078:     37 / 65 loss=6.79, nll_loss=5.06, ppl=33.36, wps=3579.4, ups=0.3, wpb=12113.6, bsz=531.6, num_updates=5025, lr=2.53827e-05, gnorm=7.337, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=20447
2023-05-19 22:18:18 | INFO | train_inner | epoch 078:     42 / 65 loss=6.898, nll_loss=5.183, ppl=36.32, wps=3562.1, ups=0.29, wpb=12083.6, bsz=571, num_updates=5030, lr=2.53571e-05, gnorm=6.855, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.4, wall=20464
2023-05-19 22:18:34 | INFO | train_inner | epoch 078:     47 / 65 loss=6.836, nll_loss=5.116, ppl=34.69, wps=3948.6, ups=0.32, wpb=12202.6, bsz=594.6, num_updates=5035, lr=2.53316e-05, gnorm=3.898, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=20479
2023-05-19 22:18:48 | INFO | train_inner | epoch 078:     52 / 65 loss=6.877, nll_loss=5.151, ppl=35.52, wps=4264.8, ups=0.34, wpb=12402.2, bsz=575.6, num_updates=5040, lr=2.53061e-05, gnorm=6.266, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=20494
2023-05-19 22:19:03 | INFO | train_inner | epoch 078:     57 / 65 loss=6.793, nll_loss=5.063, ppl=33.42, wps=4143.2, ups=0.34, wpb=12101.2, bsz=563, num_updates=5045, lr=2.52806e-05, gnorm=2.535, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=20508
2023-05-19 22:19:18 | INFO | train_inner | epoch 078:     62 / 65 loss=6.8, nll_loss=5.069, ppl=33.56, wps=3747.9, ups=0.32, wpb=11687.4, bsz=537.8, num_updates=5050, lr=2.52551e-05, gnorm=4.105, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=20524
2023-05-19 22:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:19:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:20:07 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 6.204 | nll_loss 4.331 | ppl 20.13 | bleu 4.65 | wps 865 | wpb 2785 | bsz 105.2 | num_updates 5053 | best_loss 6.181
2023-05-19 22:20:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 5053 updates
2023-05-19 22:20:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint78.pt
2023-05-19 22:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint78.pt
2023-05-19 22:20:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint78.pt (epoch 78 @ 5053 updates, score 6.204) (writing took 9.022774986922741 seconds)
2023-05-19 22:20:16 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2023-05-19 22:20:16 | INFO | train | epoch 078 | loss 6.855 | nll_loss 5.131 | ppl 35.04 | wps 3041 | ups 0.25 | wpb 12052.9 | bsz 558.9 | num_updates 5053 | lr 2.52398e-05 | gnorm 5.634 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.7 | wall 20582
2023-05-19 22:20:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:20:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:20:16 | INFO | fairseq.trainer | begin training epoch 79
2023-05-19 22:20:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:20:23 | INFO | train_inner | epoch 079:      2 / 65 loss=6.831, nll_loss=5.111, ppl=34.57, wps=802, ups=0.08, wpb=10327.8, bsz=502.8, num_updates=5055, lr=2.52296e-05, gnorm=4.015, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=20588
2023-05-19 22:20:38 | INFO | train_inner | epoch 079:      7 / 65 loss=6.865, nll_loss=5.141, ppl=35.29, wps=3972, ups=0.33, wpb=12079.6, bsz=591.8, num_updates=5060, lr=2.52041e-05, gnorm=3.737, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=20604
2023-05-19 22:20:54 | INFO | train_inner | epoch 079:     12 / 65 loss=6.777, nll_loss=5.047, ppl=33.07, wps=3837.7, ups=0.32, wpb=12051.4, bsz=568.4, num_updates=5065, lr=2.51786e-05, gnorm=7.894, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=20619
2023-05-19 22:21:11 | INFO | train_inner | epoch 079:     17 / 65 loss=6.87, nll_loss=5.148, ppl=35.46, wps=3609.8, ups=0.29, wpb=12266.8, bsz=561.2, num_updates=5070, lr=2.51531e-05, gnorm=6.161, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.6, wall=20636
2023-05-19 22:21:26 | INFO | train_inner | epoch 079:     22 / 65 loss=6.903, nll_loss=5.179, ppl=36.24, wps=3881.1, ups=0.33, wpb=11788.6, bsz=536.2, num_updates=5075, lr=2.51276e-05, gnorm=3.675, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=20652
2023-05-19 22:21:42 | INFO | train_inner | epoch 079:     27 / 65 loss=6.793, nll_loss=5.064, ppl=33.44, wps=3715.3, ups=0.3, wpb=12391.2, bsz=572.4, num_updates=5080, lr=2.5102e-05, gnorm=5.521, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=20668
2023-05-19 22:22:00 | INFO | train_inner | epoch 079:     32 / 65 loss=6.865, nll_loss=5.153, ppl=35.58, wps=3456.3, ups=0.29, wpb=11936.4, bsz=541.4, num_updates=5085, lr=2.50765e-05, gnorm=7.46, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=20685
2023-05-19 22:22:16 | INFO | train_inner | epoch 079:     37 / 65 loss=6.816, nll_loss=5.096, ppl=34.2, wps=3905.8, ups=0.31, wpb=12693.2, bsz=572, num_updates=5090, lr=2.5051e-05, gnorm=9.341, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=20702
2023-05-19 22:22:32 | INFO | train_inner | epoch 079:     42 / 65 loss=6.855, nll_loss=5.139, ppl=35.24, wps=3784.2, ups=0.32, wpb=11868.4, bsz=528.8, num_updates=5095, lr=2.50255e-05, gnorm=5.591, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=20717
2023-05-19 22:22:47 | INFO | train_inner | epoch 079:     47 / 65 loss=6.877, nll_loss=5.158, ppl=35.71, wps=4070.6, ups=0.34, wpb=12107.8, bsz=582.8, num_updates=5100, lr=2.5e-05, gnorm=5.325, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.4, wall=20732
2023-05-19 22:23:02 | INFO | train_inner | epoch 079:     52 / 65 loss=6.84, nll_loss=5.114, ppl=34.63, wps=3972, ups=0.32, wpb=12280.2, bsz=570.6, num_updates=5105, lr=2.49745e-05, gnorm=6.314, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=20748
2023-05-19 22:23:18 | INFO | train_inner | epoch 079:     57 / 65 loss=6.989, nll_loss=5.27, ppl=38.59, wps=3822.8, ups=0.3, wpb=12611.8, bsz=632, num_updates=5110, lr=2.4949e-05, gnorm=5.261, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=20764
2023-05-19 22:23:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-19 22:23:36 | INFO | train_inner | epoch 079:     63 / 65 loss=6.772, nll_loss=5.036, ppl=32.8, wps=3415.4, ups=0.28, wpb=12042.8, bsz=527, num_updates=5115, lr=2.49235e-05, gnorm=6.611, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.2, wall=20782
2023-05-19 22:23:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:23:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:24:25 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 6.212 | nll_loss 4.344 | ppl 20.3 | bleu 4.6 | wps 825.2 | wpb 2785 | bsz 105.2 | num_updates 5117 | best_loss 6.181
2023-05-19 22:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 5117 updates
2023-05-19 22:24:25 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint79.pt
2023-05-19 22:24:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint79.pt
2023-05-19 22:24:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint79.pt (epoch 79 @ 5117 updates, score 6.212) (writing took 6.881346046924591 seconds)
2023-05-19 22:24:31 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2023-05-19 22:24:31 | INFO | train | epoch 079 | loss 6.86 | nll_loss 5.137 | ppl 35.19 | wps 3022.9 | ups 0.25 | wpb 12048.4 | bsz 560.5 | num_updates 5117 | lr 2.49133e-05 | gnorm 6.114 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.9 | wall 20837
2023-05-19 22:24:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:24:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:24:31 | INFO | fairseq.trainer | begin training epoch 80
2023-05-19 22:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:24:40 | INFO | train_inner | epoch 080:      3 / 65 loss=6.932, nll_loss=5.212, ppl=37.07, wps=817.2, ups=0.08, wpb=10514.6, bsz=496.2, num_updates=5120, lr=2.4898e-05, gnorm=5.388, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.8, wall=20846
2023-05-19 22:24:58 | INFO | train_inner | epoch 080:      8 / 65 loss=6.869, nll_loss=5.153, ppl=35.59, wps=3530, ups=0.29, wpb=12079.6, bsz=562, num_updates=5125, lr=2.48724e-05, gnorm=17.878, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=20863
2023-05-19 22:25:15 | INFO | train_inner | epoch 080:     13 / 65 loss=6.881, nll_loss=5.161, ppl=35.78, wps=3549.2, ups=0.29, wpb=12447, bsz=631.4, num_updates=5130, lr=2.48469e-05, gnorm=5.873, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.5, wall=20881
2023-05-19 22:25:30 | INFO | train_inner | epoch 080:     18 / 65 loss=6.907, nll_loss=5.196, ppl=36.65, wps=4012.1, ups=0.33, wpb=12245.4, bsz=587.8, num_updates=5135, lr=2.48214e-05, gnorm=10.989, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=20896
2023-05-19 22:25:46 | INFO | train_inner | epoch 080:     23 / 65 loss=7.02, nll_loss=5.314, ppl=39.77, wps=3989.5, ups=0.32, wpb=12299.4, bsz=628.2, num_updates=5140, lr=2.47959e-05, gnorm=5.095, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=20912
2023-05-19 22:26:03 | INFO | train_inner | epoch 080:     28 / 65 loss=6.77, nll_loss=5.044, ppl=32.98, wps=3615.8, ups=0.3, wpb=12152.8, bsz=543.2, num_updates=5145, lr=2.47704e-05, gnorm=2.733, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=20928
2023-05-19 22:26:18 | INFO | train_inner | epoch 080:     33 / 65 loss=6.776, nll_loss=5.046, ppl=33.04, wps=3946.9, ups=0.33, wpb=11888.4, bsz=520.8, num_updates=5150, lr=2.47449e-05, gnorm=4.42, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=20943
2023-05-19 22:26:35 | INFO | train_inner | epoch 080:     38 / 65 loss=6.779, nll_loss=5.047, ppl=33.07, wps=3615.1, ups=0.29, wpb=12326.2, bsz=567.6, num_updates=5155, lr=2.47194e-05, gnorm=4.988, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=20960
2023-05-19 22:26:49 | INFO | train_inner | epoch 080:     43 / 65 loss=6.741, nll_loss=5.005, ppl=32.11, wps=4314.8, ups=0.35, wpb=12309, bsz=556.8, num_updates=5160, lr=2.46939e-05, gnorm=5.474, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=20975
2023-05-19 22:27:04 | INFO | train_inner | epoch 080:     48 / 65 loss=7.06, nll_loss=5.349, ppl=40.77, wps=3977.2, ups=0.33, wpb=12104.8, bsz=606.6, num_updates=5165, lr=2.46684e-05, gnorm=4.46, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=20990
2023-05-19 22:27:19 | INFO | train_inner | epoch 080:     53 / 65 loss=6.79, nll_loss=5.058, ppl=33.32, wps=4094.4, ups=0.34, wpb=12102, bsz=496.8, num_updates=5170, lr=2.46429e-05, gnorm=3.131, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=21005
2023-05-19 22:27:35 | INFO | train_inner | epoch 080:     58 / 65 loss=6.855, nll_loss=5.135, ppl=35.14, wps=3768, ups=0.31, wpb=12088.6, bsz=569.8, num_updates=5175, lr=2.46173e-05, gnorm=5.623, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=21021
2023-05-19 22:27:50 | INFO | train_inner | epoch 080:     63 / 65 loss=6.866, nll_loss=5.147, ppl=35.44, wps=3976.7, ups=0.33, wpb=12012.2, bsz=528.4, num_updates=5180, lr=2.45918e-05, gnorm=4.303, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=21036
2023-05-19 22:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:27:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:28:36 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 6.196 | nll_loss 4.33 | ppl 20.11 | bleu 4.61 | wps 861.2 | wpb 2785 | bsz 105.2 | num_updates 5182 | best_loss 6.181
2023-05-19 22:28:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5182 updates
2023-05-19 22:28:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint80.pt
2023-05-19 22:28:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint80.pt
2023-05-19 22:28:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint80.pt (epoch 80 @ 5182 updates, score 6.196) (writing took 6.888465374708176 seconds)
2023-05-19 22:28:43 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2023-05-19 22:28:43 | INFO | train | epoch 080 | loss 6.853 | nll_loss 5.131 | ppl 35.05 | wps 3115.2 | ups 0.26 | wpb 12052.9 | bsz 559.5 | num_updates 5182 | lr 2.45816e-05 | gnorm 6.283 | clip 100 | loss_scale 0.25 | train_wall 203 | gb_free 21.3 | wall 21089
2023-05-19 22:28:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:28:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:28:43 | INFO | fairseq.trainer | begin training epoch 81
2023-05-19 22:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:28:52 | INFO | train_inner | epoch 081:      3 / 65 loss=6.812, nll_loss=5.093, ppl=34.13, wps=872.6, ups=0.08, wpb=10844.4, bsz=534, num_updates=5185, lr=2.45663e-05, gnorm=8.272, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.5, wall=21098
2023-05-19 22:29:07 | INFO | train_inner | epoch 081:      8 / 65 loss=6.733, nll_loss=5.005, ppl=32.1, wps=4184.7, ups=0.34, wpb=12343.8, bsz=574.4, num_updates=5190, lr=2.45408e-05, gnorm=2.692, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=21113
2023-05-19 22:29:23 | INFO | train_inner | epoch 081:     13 / 65 loss=6.771, nll_loss=5.036, ppl=32.81, wps=3529.3, ups=0.3, wpb=11650.6, bsz=470, num_updates=5195, lr=2.45153e-05, gnorm=3.84, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=21129
2023-05-19 22:29:41 | INFO | train_inner | epoch 081:     18 / 65 loss=6.815, nll_loss=5.089, ppl=34.05, wps=3499.1, ups=0.28, wpb=12287.6, bsz=547.6, num_updates=5200, lr=2.44898e-05, gnorm=7.816, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.8, wall=21147
2023-05-19 22:29:56 | INFO | train_inner | epoch 081:     23 / 65 loss=6.934, nll_loss=5.206, ppl=36.91, wps=4091.5, ups=0.33, wpb=12279.8, bsz=615.2, num_updates=5205, lr=2.44643e-05, gnorm=7.288, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=21162
2023-05-19 22:30:13 | INFO | train_inner | epoch 081:     28 / 65 loss=6.829, nll_loss=5.099, ppl=34.28, wps=3568.2, ups=0.29, wpb=12145.2, bsz=538.6, num_updates=5210, lr=2.44388e-05, gnorm=4.499, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=21179
2023-05-19 22:30:29 | INFO | train_inner | epoch 081:     33 / 65 loss=6.851, nll_loss=5.122, ppl=34.84, wps=3747.6, ups=0.31, wpb=11959.2, bsz=533.4, num_updates=5215, lr=2.44133e-05, gnorm=11.285, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=21195
2023-05-19 22:30:44 | INFO | train_inner | epoch 081:     38 / 65 loss=7.152, nll_loss=5.452, ppl=43.76, wps=4087.5, ups=0.33, wpb=12506, bsz=757.6, num_updates=5220, lr=2.43878e-05, gnorm=4.22, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=21210
2023-05-19 22:31:00 | INFO | train_inner | epoch 081:     43 / 65 loss=6.772, nll_loss=5.04, ppl=32.9, wps=3924.7, ups=0.32, wpb=12383.6, bsz=479.4, num_updates=5225, lr=2.43622e-05, gnorm=6.331, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=21226
2023-05-19 22:31:16 | INFO | train_inner | epoch 081:     48 / 65 loss=6.861, nll_loss=5.143, ppl=35.33, wps=3820.9, ups=0.32, wpb=11863.6, bsz=534.6, num_updates=5230, lr=2.43367e-05, gnorm=2.869, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=21241
2023-05-19 22:31:30 | INFO | train_inner | epoch 081:     53 / 65 loss=6.927, nll_loss=5.219, ppl=37.25, wps=4253.3, ups=0.34, wpb=12549.6, bsz=643.8, num_updates=5235, lr=2.43112e-05, gnorm=10.115, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=21256
2023-05-19 22:31:46 | INFO | train_inner | epoch 081:     58 / 65 loss=6.765, nll_loss=5.038, ppl=32.85, wps=3819.5, ups=0.32, wpb=11889.8, bsz=513.2, num_updates=5240, lr=2.42857e-05, gnorm=5.515, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=21272
2023-05-19 22:32:01 | INFO | train_inner | epoch 081:     63 / 65 loss=6.819, nll_loss=5.1, ppl=34.3, wps=4080.3, ups=0.33, wpb=12321, bsz=536.2, num_updates=5245, lr=2.42602e-05, gnorm=6.232, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=21287
2023-05-19 22:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:32:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:32:48 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 6.194 | nll_loss 4.325 | ppl 20.04 | bleu 4.99 | wps 901.2 | wpb 2785 | bsz 105.2 | num_updates 5247 | best_loss 6.181
2023-05-19 22:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 5247 updates
2023-05-19 22:32:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint81.pt
2023-05-19 22:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint81.pt
2023-05-19 22:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint81.pt (epoch 81 @ 5247 updates, score 6.194) (writing took 6.892768412828445 seconds)
2023-05-19 22:32:55 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2023-05-19 22:32:55 | INFO | train | epoch 081 | loss 6.852 | nll_loss 5.129 | ppl 35 | wps 3115 | ups 0.26 | wpb 12059.9 | bsz 560.7 | num_updates 5247 | lr 2.425e-05 | gnorm 6.077 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.2 | wall 21340
2023-05-19 22:32:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:32:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:32:55 | INFO | fairseq.trainer | begin training epoch 82
2023-05-19 22:32:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:33:03 | INFO | train_inner | epoch 082:      3 / 65 loss=6.774, nll_loss=5.042, ppl=32.94, wps=845.2, ups=0.08, wpb=10451.2, bsz=493.6, num_updates=5250, lr=2.42347e-05, gnorm=6.173, clip=100, loss_scale=0.25, train_wall=15, gb_free=22, wall=21349
2023-05-19 22:33:20 | INFO | train_inner | epoch 082:      8 / 65 loss=6.945, nll_loss=5.228, ppl=37.48, wps=3601.2, ups=0.3, wpb=12184.4, bsz=590.8, num_updates=5255, lr=2.42092e-05, gnorm=3.943, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.5, wall=21366
2023-05-19 22:33:37 | INFO | train_inner | epoch 082:     13 / 65 loss=6.81, nll_loss=5.077, ppl=33.76, wps=3595.5, ups=0.29, wpb=12297.2, bsz=513.8, num_updates=5260, lr=2.41837e-05, gnorm=4.81, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.6, wall=21383
2023-05-19 22:33:52 | INFO | train_inner | epoch 082:     18 / 65 loss=7.02, nll_loss=5.304, ppl=39.52, wps=4125.7, ups=0.34, wpb=12183.2, bsz=612.2, num_updates=5265, lr=2.41582e-05, gnorm=3.895, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=21397
2023-05-19 22:34:06 | INFO | train_inner | epoch 082:     23 / 65 loss=6.785, nll_loss=5.054, ppl=33.23, wps=4086.4, ups=0.34, wpb=11908.6, bsz=539.8, num_updates=5270, lr=2.41327e-05, gnorm=4.24, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=21412
2023-05-19 22:34:22 | INFO | train_inner | epoch 082:     28 / 65 loss=6.874, nll_loss=5.156, ppl=35.65, wps=3814.7, ups=0.31, wpb=12396.8, bsz=563.6, num_updates=5275, lr=2.41071e-05, gnorm=4.047, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=21428
2023-05-19 22:34:39 | INFO | train_inner | epoch 082:     33 / 65 loss=6.795, nll_loss=5.067, ppl=33.52, wps=3678, ups=0.31, wpb=11902.6, bsz=525.6, num_updates=5280, lr=2.40816e-05, gnorm=5.919, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=21444
2023-05-19 22:34:56 | INFO | train_inner | epoch 082:     38 / 65 loss=6.827, nll_loss=5.106, ppl=34.43, wps=3513, ups=0.29, wpb=12015.6, bsz=508, num_updates=5285, lr=2.40561e-05, gnorm=8.277, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.7, wall=21461
2023-05-19 22:35:11 | INFO | train_inner | epoch 082:     43 / 65 loss=6.847, nll_loss=5.133, ppl=35.09, wps=3892.2, ups=0.32, wpb=12019.8, bsz=573, num_updates=5290, lr=2.40306e-05, gnorm=4.049, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.9, wall=21477
2023-05-19 22:35:30 | INFO | train_inner | epoch 082:     48 / 65 loss=6.846, nll_loss=5.127, ppl=34.94, wps=3313.3, ups=0.27, wpb=12456.2, bsz=616.4, num_updates=5295, lr=2.40051e-05, gnorm=3.721, clip=100, loss_scale=0.25, train_wall=19, gb_free=21.5, wall=21496
2023-05-19 22:35:45 | INFO | train_inner | epoch 082:     53 / 65 loss=6.825, nll_loss=5.102, ppl=34.35, wps=3998.6, ups=0.33, wpb=12240.8, bsz=566.8, num_updates=5300, lr=2.39796e-05, gnorm=5.8, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=21511
2023-05-19 22:36:01 | INFO | train_inner | epoch 082:     58 / 65 loss=6.869, nll_loss=5.149, ppl=35.49, wps=3980.8, ups=0.33, wpb=12210.6, bsz=602.6, num_updates=5305, lr=2.39541e-05, gnorm=6.103, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=21526
2023-05-19 22:36:16 | INFO | train_inner | epoch 082:     63 / 65 loss=6.897, nll_loss=5.182, ppl=36.31, wps=3976.1, ups=0.32, wpb=12468, bsz=606, num_updates=5310, lr=2.39286e-05, gnorm=2.532, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=21542
2023-05-19 22:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:36:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:37:01 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 6.221 | nll_loss 4.355 | ppl 20.46 | bleu 4.96 | wps 883.8 | wpb 2785 | bsz 105.2 | num_updates 5312 | best_loss 6.181
2023-05-19 22:37:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 5312 updates
2023-05-19 22:37:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint82.pt
2023-05-19 22:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint82.pt
2023-05-19 22:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint82.pt (epoch 82 @ 5312 updates, score 6.221) (writing took 6.933834183961153 seconds)
2023-05-19 22:37:08 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2023-05-19 22:37:08 | INFO | train | epoch 082 | loss 6.853 | nll_loss 5.131 | ppl 35.03 | wps 3089.8 | ups 0.26 | wpb 12044.4 | bsz 557.8 | num_updates 5312 | lr 2.39184e-05 | gnorm 5.076 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.7 | wall 21594
2023-05-19 22:37:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:37:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:37:08 | INFO | fairseq.trainer | begin training epoch 83
2023-05-19 22:37:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:37:18 | INFO | train_inner | epoch 083:      3 / 65 loss=6.755, nll_loss=5.024, ppl=32.53, wps=811.8, ups=0.08, wpb=10015.2, bsz=381.4, num_updates=5315, lr=2.39031e-05, gnorm=7.642, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=21604
2023-05-19 22:37:33 | INFO | train_inner | epoch 083:      8 / 65 loss=6.936, nll_loss=5.217, ppl=37.19, wps=3974.2, ups=0.32, wpb=12241.2, bsz=605, num_updates=5320, lr=2.38776e-05, gnorm=6.382, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=21619
2023-05-19 22:37:50 | INFO | train_inner | epoch 083:     13 / 65 loss=6.772, nll_loss=5.041, ppl=32.93, wps=3557.9, ups=0.29, wpb=12125.6, bsz=528.6, num_updates=5325, lr=2.3852e-05, gnorm=4.601, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.2, wall=21636
2023-05-19 22:38:06 | INFO | train_inner | epoch 083:     18 / 65 loss=7.007, nll_loss=5.295, ppl=39.25, wps=3965.3, ups=0.33, wpb=12114.8, bsz=648.4, num_updates=5330, lr=2.38265e-05, gnorm=4.644, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=21651
2023-05-19 22:38:23 | INFO | train_inner | epoch 083:     23 / 65 loss=6.931, nll_loss=5.219, ppl=37.24, wps=3571.3, ups=0.3, wpb=12060.6, bsz=551.6, num_updates=5335, lr=2.3801e-05, gnorm=3.838, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=21668
2023-05-19 22:38:38 | INFO | train_inner | epoch 083:     28 / 65 loss=6.794, nll_loss=5.07, ppl=33.58, wps=4106.8, ups=0.33, wpb=12332.8, bsz=601.4, num_updates=5340, lr=2.37755e-05, gnorm=4.27, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=21683
2023-05-19 22:38:53 | INFO | train_inner | epoch 083:     33 / 65 loss=6.804, nll_loss=5.08, ppl=33.82, wps=3940.3, ups=0.33, wpb=12114.2, bsz=505.4, num_updates=5345, lr=2.375e-05, gnorm=6.016, clip=100, loss_scale=0.25, train_wall=15, gb_free=21, wall=21699
2023-05-19 22:39:09 | INFO | train_inner | epoch 083:     38 / 65 loss=6.833, nll_loss=5.115, ppl=34.66, wps=3928.6, ups=0.32, wpb=12437.4, bsz=566.8, num_updates=5350, lr=2.37245e-05, gnorm=4.26, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=21715
2023-05-19 22:39:24 | INFO | train_inner | epoch 083:     43 / 65 loss=6.85, nll_loss=5.129, ppl=35, wps=3937.3, ups=0.33, wpb=11879.8, bsz=600, num_updates=5355, lr=2.3699e-05, gnorm=2.67, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=21730
2023-05-19 22:39:40 | INFO | train_inner | epoch 083:     48 / 65 loss=6.832, nll_loss=5.119, ppl=34.75, wps=3810.1, ups=0.31, wpb=12155, bsz=579.4, num_updates=5360, lr=2.36735e-05, gnorm=5.447, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=21746
2023-05-19 22:39:56 | INFO | train_inner | epoch 083:     53 / 65 loss=6.889, nll_loss=5.182, ppl=36.3, wps=3804.5, ups=0.31, wpb=12358, bsz=584.4, num_updates=5365, lr=2.3648e-05, gnorm=4.094, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=21762
2023-05-19 22:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-19 22:40:15 | INFO | train_inner | epoch 083:     59 / 65 loss=6.805, nll_loss=5.083, ppl=33.88, wps=3252.8, ups=0.26, wpb=12368.8, bsz=573.4, num_updates=5370, lr=2.36224e-05, gnorm=3.197, clip=100, loss_scale=0.125, train_wall=19, gb_free=21.7, wall=21781
2023-05-19 22:40:31 | INFO | train_inner | epoch 083:     64 / 65 loss=6.866, nll_loss=5.136, ppl=35.16, wps=3901.7, ups=0.32, wpb=12234.8, bsz=513.2, num_updates=5375, lr=2.35969e-05, gnorm=3.01, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=21797
2023-05-19 22:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:40:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:41:15 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 6.215 | nll_loss 4.349 | ppl 20.38 | bleu 4.88 | wps 832.5 | wpb 2785 | bsz 105.2 | num_updates 5376 | best_loss 6.181
2023-05-19 22:41:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 5376 updates
2023-05-19 22:41:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint83.pt
2023-05-19 22:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint83.pt
2023-05-19 22:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint83.pt (epoch 83 @ 5376 updates, score 6.215) (writing took 6.887452103197575 seconds)
2023-05-19 22:41:22 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2023-05-19 22:41:22 | INFO | train | epoch 083 | loss 6.855 | nll_loss 5.135 | ppl 35.13 | wps 3036.4 | ups 0.25 | wpb 12051 | bsz 559.5 | num_updates 5376 | lr 2.35918e-05 | gnorm 4.344 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.9 | wall 21848
2023-05-19 22:41:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:41:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:41:22 | INFO | fairseq.trainer | begin training epoch 84
2023-05-19 22:41:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:41:35 | INFO | train_inner | epoch 084:      4 / 65 loss=6.794, nll_loss=5.067, ppl=33.52, wps=810.5, ups=0.08, wpb=10440.8, bsz=448.6, num_updates=5380, lr=2.35714e-05, gnorm=12.172, clip=100, loss_scale=0.125, train_wall=15, gb_free=21, wall=21861
2023-05-19 22:41:52 | INFO | train_inner | epoch 084:      9 / 65 loss=6.819, nll_loss=5.099, ppl=34.28, wps=3618.3, ups=0.3, wpb=12161.2, bsz=628, num_updates=5385, lr=2.35459e-05, gnorm=3.596, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.5, wall=21878
2023-05-19 22:42:08 | INFO | train_inner | epoch 084:     14 / 65 loss=6.958, nll_loss=5.242, ppl=37.83, wps=3826.7, ups=0.32, wpb=12016.4, bsz=549.6, num_updates=5390, lr=2.35204e-05, gnorm=5.572, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=21894
2023-05-19 22:42:23 | INFO | train_inner | epoch 084:     19 / 65 loss=6.794, nll_loss=5.072, ppl=33.63, wps=3965.9, ups=0.32, wpb=12407.2, bsz=594.2, num_updates=5395, lr=2.34949e-05, gnorm=4.308, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=21909
2023-05-19 22:42:40 | INFO | train_inner | epoch 084:     24 / 65 loss=6.853, nll_loss=5.135, ppl=35.13, wps=3552.8, ups=0.29, wpb=12104.4, bsz=530.4, num_updates=5400, lr=2.34694e-05, gnorm=11.401, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.6, wall=21926
2023-05-19 22:42:57 | INFO | train_inner | epoch 084:     29 / 65 loss=6.788, nll_loss=5.063, ppl=33.42, wps=3779, ups=0.3, wpb=12391.8, bsz=569.2, num_updates=5405, lr=2.34439e-05, gnorm=3.709, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=21943
2023-05-19 22:43:12 | INFO | train_inner | epoch 084:     34 / 65 loss=7.025, nll_loss=5.314, ppl=39.77, wps=3908.6, ups=0.32, wpb=12123.4, bsz=583.6, num_updates=5410, lr=2.34184e-05, gnorm=3.808, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=21958
2023-05-19 22:43:29 | INFO | train_inner | epoch 084:     39 / 65 loss=6.783, nll_loss=5.057, ppl=33.28, wps=3719.5, ups=0.31, wpb=12077.8, bsz=524.6, num_updates=5415, lr=2.33929e-05, gnorm=6.25, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=21974
2023-05-19 22:43:44 | INFO | train_inner | epoch 084:     44 / 65 loss=6.911, nll_loss=5.2, ppl=36.76, wps=3928.6, ups=0.32, wpb=12343.4, bsz=577.6, num_updates=5420, lr=2.33673e-05, gnorm=6.65, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=21990
2023-05-19 22:44:00 | INFO | train_inner | epoch 084:     49 / 65 loss=6.82, nll_loss=5.099, ppl=34.27, wps=3953.2, ups=0.32, wpb=12372.6, bsz=605.4, num_updates=5425, lr=2.33418e-05, gnorm=7.712, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=22006
2023-05-19 22:44:18 | INFO | train_inner | epoch 084:     54 / 65 loss=6.772, nll_loss=5.035, ppl=32.8, wps=3273.5, ups=0.28, wpb=11770.4, bsz=507.8, num_updates=5430, lr=2.33163e-05, gnorm=4.567, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.9, wall=22024
2023-05-19 22:44:34 | INFO | train_inner | epoch 084:     59 / 65 loss=6.761, nll_loss=5.028, ppl=32.62, wps=3822.6, ups=0.31, wpb=12309, bsz=535.4, num_updates=5435, lr=2.32908e-05, gnorm=6.757, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22040
2023-05-19 22:44:50 | INFO | train_inner | epoch 084:     64 / 65 loss=6.873, nll_loss=5.147, ppl=35.43, wps=3928.3, ups=0.32, wpb=12410.2, bsz=627.8, num_updates=5440, lr=2.32653e-05, gnorm=8.562, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=22056
2023-05-19 22:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:44:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:45:32 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 6.148 | nll_loss 4.272 | ppl 19.32 | bleu 4.75 | wps 894 | wpb 2785 | bsz 105.2 | num_updates 5441 | best_loss 6.148
2023-05-19 22:45:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 5441 updates
2023-05-19 22:45:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint84.pt
2023-05-19 22:45:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint84.pt
2023-05-19 22:46:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint84.pt (epoch 84 @ 5441 updates, score 6.148) (writing took 28.956154618412256 seconds)
2023-05-19 22:46:01 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2023-05-19 22:46:01 | INFO | train | epoch 084 | loss 6.842 | nll_loss 5.119 | ppl 34.76 | wps 2812.3 | ups 0.23 | wpb 12058.6 | bsz 559.8 | num_updates 5441 | lr 2.32602e-05 | gnorm 6.624 | clip 100 | loss_scale 0.125 | train_wall 209 | gb_free 21.7 | wall 22126
2023-05-19 22:46:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:46:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:46:01 | INFO | fairseq.trainer | begin training epoch 85
2023-05-19 22:46:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:46:14 | INFO | train_inner | epoch 085:      4 / 65 loss=6.799, nll_loss=5.079, ppl=33.8, wps=621.2, ups=0.06, wpb=10399.4, bsz=475, num_updates=5445, lr=2.32398e-05, gnorm=5.674, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=22139
2023-05-19 22:46:30 | INFO | train_inner | epoch 085:      9 / 65 loss=7.007, nll_loss=5.293, ppl=39.22, wps=3779.3, ups=0.3, wpb=12521.8, bsz=635, num_updates=5450, lr=2.32143e-05, gnorm=5.74, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=22156
2023-05-19 22:46:47 | INFO | train_inner | epoch 085:     14 / 65 loss=6.95, nll_loss=5.234, ppl=37.63, wps=3657.5, ups=0.3, wpb=12305.2, bsz=612.8, num_updates=5455, lr=2.31888e-05, gnorm=5.38, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=22173
2023-05-19 22:47:02 | INFO | train_inner | epoch 085:     19 / 65 loss=6.759, nll_loss=5.028, ppl=32.63, wps=4105, ups=0.32, wpb=12722.4, bsz=630, num_updates=5460, lr=2.31633e-05, gnorm=5.587, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=22188
2023-05-19 22:47:19 | INFO | train_inner | epoch 085:     24 / 65 loss=6.766, nll_loss=5.039, ppl=32.88, wps=3586.3, ups=0.3, wpb=11986.2, bsz=494.6, num_updates=5465, lr=2.31378e-05, gnorm=3.553, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=22205
2023-05-19 22:47:35 | INFO | train_inner | epoch 085:     29 / 65 loss=6.902, nll_loss=5.19, ppl=36.49, wps=3947.6, ups=0.33, wpb=12124.8, bsz=591.2, num_updates=5470, lr=2.31122e-05, gnorm=9.361, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=22220
2023-05-19 22:47:52 | INFO | train_inner | epoch 085:     34 / 65 loss=6.801, nll_loss=5.076, ppl=33.74, wps=3492.6, ups=0.29, wpb=12060.6, bsz=571.6, num_updates=5475, lr=2.30867e-05, gnorm=6.813, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=22238
2023-05-19 22:48:08 | INFO | train_inner | epoch 085:     39 / 65 loss=6.857, nll_loss=5.137, ppl=35.19, wps=3779.8, ups=0.31, wpb=12065.8, bsz=539.8, num_updates=5480, lr=2.30612e-05, gnorm=10.025, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.8, wall=22253
2023-05-19 22:48:23 | INFO | train_inner | epoch 085:     44 / 65 loss=6.805, nll_loss=5.077, ppl=33.75, wps=3954.6, ups=0.32, wpb=12190.6, bsz=575, num_updates=5485, lr=2.30357e-05, gnorm=7.642, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=22269
2023-05-19 22:48:40 | INFO | train_inner | epoch 085:     49 / 65 loss=6.752, nll_loss=5.012, ppl=32.27, wps=3628.4, ups=0.3, wpb=12121.8, bsz=526.8, num_updates=5490, lr=2.30102e-05, gnorm=26.846, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=22286
2023-05-19 22:48:56 | INFO | train_inner | epoch 085:     54 / 65 loss=6.799, nll_loss=5.07, ppl=33.59, wps=3772.8, ups=0.31, wpb=12291.2, bsz=548.8, num_updates=5495, lr=2.29847e-05, gnorm=4.369, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=22302
2023-05-19 22:49:13 | INFO | train_inner | epoch 085:     59 / 65 loss=6.798, nll_loss=5.067, ppl=33.51, wps=3597.3, ups=0.3, wpb=11846.4, bsz=551.4, num_updates=5500, lr=2.29592e-05, gnorm=3.93, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22318
2023-05-19 22:49:27 | INFO | train_inner | epoch 085:     64 / 65 loss=6.786, nll_loss=5.047, ppl=33.07, wps=4266.5, ups=0.35, wpb=12031.2, bsz=513, num_updates=5505, lr=2.29337e-05, gnorm=5.754, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=22332
2023-05-19 22:49:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:49:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:50:08 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 6.16 | nll_loss 4.28 | ppl 19.43 | bleu 5.25 | wps 905 | wpb 2785 | bsz 105.2 | num_updates 5506 | best_loss 6.148
2023-05-19 22:50:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 5506 updates
2023-05-19 22:50:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint85.pt
2023-05-19 22:50:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint85.pt
2023-05-19 22:50:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint85.pt (epoch 85 @ 5506 updates, score 6.16) (writing took 7.086420498788357 seconds)
2023-05-19 22:50:15 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2023-05-19 22:50:15 | INFO | train | epoch 085 | loss 6.832 | nll_loss 5.106 | ppl 34.45 | wps 3079.6 | ups 0.26 | wpb 12060.5 | bsz 560.4 | num_updates 5506 | lr 2.29286e-05 | gnorm 7.67 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.7 | wall 22381
2023-05-19 22:50:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:50:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:50:15 | INFO | fairseq.trainer | begin training epoch 86
2023-05-19 22:50:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:50:28 | INFO | train_inner | epoch 086:      4 / 65 loss=6.879, nll_loss=5.16, ppl=35.76, wps=853.3, ups=0.08, wpb=10465.2, bsz=487.4, num_updates=5510, lr=2.29082e-05, gnorm=7.688, clip=100, loss_scale=0.125, train_wall=14, gb_free=20.4, wall=22394
2023-05-19 22:50:43 | INFO | train_inner | epoch 086:      9 / 65 loss=6.761, nll_loss=5.023, ppl=32.51, wps=3886.7, ups=0.32, wpb=12009.2, bsz=479.8, num_updates=5515, lr=2.28827e-05, gnorm=4.121, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=22409
2023-05-19 22:51:00 | INFO | train_inner | epoch 086:     14 / 65 loss=6.838, nll_loss=5.113, ppl=34.6, wps=3677.8, ups=0.31, wpb=11829.2, bsz=507.8, num_updates=5520, lr=2.28571e-05, gnorm=2.717, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=22425
2023-05-19 22:51:15 | INFO | train_inner | epoch 086:     19 / 65 loss=6.845, nll_loss=5.122, ppl=34.82, wps=4033.8, ups=0.33, wpb=12136.8, bsz=564.2, num_updates=5525, lr=2.28316e-05, gnorm=4.732, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=22440
2023-05-19 22:51:32 | INFO | train_inner | epoch 086:     24 / 65 loss=6.82, nll_loss=5.09, ppl=34.06, wps=3417.1, ups=0.29, wpb=11877, bsz=582.6, num_updates=5530, lr=2.28061e-05, gnorm=4.606, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=22458
2023-05-19 22:51:50 | INFO | train_inner | epoch 086:     29 / 65 loss=6.739, nll_loss=5.001, ppl=32.03, wps=3588.3, ups=0.28, wpb=12593.8, bsz=518.4, num_updates=5535, lr=2.27806e-05, gnorm=4.828, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.8, wall=22475
2023-05-19 22:52:07 | INFO | train_inner | epoch 086:     34 / 65 loss=6.843, nll_loss=5.121, ppl=34.8, wps=3667, ups=0.29, wpb=12489.6, bsz=638, num_updates=5540, lr=2.27551e-05, gnorm=2.898, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=22492
2023-05-19 22:52:23 | INFO | train_inner | epoch 086:     39 / 65 loss=6.788, nll_loss=5.061, ppl=33.38, wps=3696, ups=0.31, wpb=11869.8, bsz=526, num_updates=5545, lr=2.27296e-05, gnorm=15.006, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=22508
2023-05-19 22:52:38 | INFO | train_inner | epoch 086:     44 / 65 loss=6.797, nll_loss=5.068, ppl=33.54, wps=4101.9, ups=0.34, wpb=12241.8, bsz=568, num_updates=5550, lr=2.27041e-05, gnorm=5.918, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=22523
2023-05-19 22:52:54 | INFO | train_inner | epoch 086:     49 / 65 loss=6.789, nll_loss=5.063, ppl=33.42, wps=3696.2, ups=0.3, wpb=12308.6, bsz=540.8, num_updates=5555, lr=2.26786e-05, gnorm=7.083, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=22540
2023-05-19 22:53:09 | INFO | train_inner | epoch 086:     54 / 65 loss=6.965, nll_loss=5.252, ppl=38.1, wps=3971, ups=0.33, wpb=12010.4, bsz=621.2, num_updates=5560, lr=2.26531e-05, gnorm=4.385, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=22555
2023-05-19 22:53:26 | INFO | train_inner | epoch 086:     59 / 65 loss=7.024, nll_loss=5.315, ppl=39.8, wps=3839.6, ups=0.3, wpb=12711, bsz=659.8, num_updates=5565, lr=2.26276e-05, gnorm=4.897, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=22572
2023-05-19 22:53:41 | INFO | train_inner | epoch 086:     64 / 65 loss=6.804, nll_loss=5.085, ppl=33.93, wps=4092.7, ups=0.33, wpb=12220.6, bsz=588.6, num_updates=5570, lr=2.2602e-05, gnorm=6.461, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=22587
2023-05-19 22:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:53:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:54:23 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 6.195 | nll_loss 4.329 | ppl 20.1 | bleu 4.97 | wps 874.6 | wpb 2785 | bsz 105.2 | num_updates 5571 | best_loss 6.148
2023-05-19 22:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 5571 updates
2023-05-19 22:54:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint86.pt
2023-05-19 22:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint86.pt
2023-05-19 22:54:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint86.pt (epoch 86 @ 5571 updates, score 6.195) (writing took 6.809596814215183 seconds)
2023-05-19 22:54:30 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2023-05-19 22:54:30 | INFO | train | epoch 086 | loss 6.837 | nll_loss 5.112 | ppl 34.59 | wps 3074.8 | ups 0.25 | wpb 12059.5 | bsz 559.8 | num_updates 5571 | lr 2.25969e-05 | gnorm 5.809 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.8 | wall 22636
2023-05-19 22:54:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:54:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:54:30 | INFO | fairseq.trainer | begin training epoch 87
2023-05-19 22:54:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:54:44 | INFO | train_inner | epoch 087:      4 / 65 loss=6.73, nll_loss=4.999, ppl=31.98, wps=799.2, ups=0.08, wpb=10107.2, bsz=404.8, num_updates=5575, lr=2.25765e-05, gnorm=4.73, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=22650
2023-05-19 22:54:59 | INFO | train_inner | epoch 087:      9 / 65 loss=6.795, nll_loss=5.072, ppl=33.64, wps=4099.6, ups=0.33, wpb=12373.4, bsz=602.4, num_updates=5580, lr=2.2551e-05, gnorm=3.632, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=22665
2023-05-19 22:55:15 | INFO | train_inner | epoch 087:     14 / 65 loss=6.92, nll_loss=5.201, ppl=36.79, wps=3920.6, ups=0.32, wpb=12259.8, bsz=611.6, num_updates=5585, lr=2.25255e-05, gnorm=5.59, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=22681
2023-05-19 22:55:33 | INFO | train_inner | epoch 087:     19 / 65 loss=6.75, nll_loss=5.014, ppl=32.31, wps=3356.8, ups=0.28, wpb=12063.6, bsz=485.6, num_updates=5590, lr=2.25e-05, gnorm=3.278, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=22698
2023-05-19 22:55:49 | INFO | train_inner | epoch 087:     24 / 65 loss=7.067, nll_loss=5.357, ppl=40.99, wps=3933.6, ups=0.31, wpb=12597.2, bsz=671.6, num_updates=5595, lr=2.24745e-05, gnorm=3.458, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=22714
2023-05-19 22:56:06 | INFO | train_inner | epoch 087:     29 / 65 loss=6.76, nll_loss=5.029, ppl=32.66, wps=3544.6, ups=0.3, wpb=11937.6, bsz=512.2, num_updates=5600, lr=2.2449e-05, gnorm=9.252, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.1, wall=22731
2023-05-19 22:56:22 | INFO | train_inner | epoch 087:     34 / 65 loss=6.77, nll_loss=5.042, ppl=32.95, wps=3707.2, ups=0.3, wpb=12175.6, bsz=508, num_updates=5605, lr=2.24235e-05, gnorm=6.185, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=22748
2023-05-19 22:56:38 | INFO | train_inner | epoch 087:     39 / 65 loss=6.974, nll_loss=5.267, ppl=38.5, wps=3680.4, ups=0.31, wpb=11754, bsz=581.6, num_updates=5610, lr=2.2398e-05, gnorm=6.439, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=22764
2023-05-19 22:56:54 | INFO | train_inner | epoch 087:     44 / 65 loss=6.885, nll_loss=5.168, ppl=35.94, wps=3908.5, ups=0.31, wpb=12415, bsz=603.4, num_updates=5615, lr=2.23724e-05, gnorm=13.086, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=22780
2023-05-19 22:57:10 | INFO | train_inner | epoch 087:     49 / 65 loss=6.776, nll_loss=5.044, ppl=32.98, wps=3600.1, ups=0.3, wpb=11925.2, bsz=528.4, num_updates=5620, lr=2.23469e-05, gnorm=5.63, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=22796
2023-05-19 22:57:27 | INFO | train_inner | epoch 087:     54 / 65 loss=6.754, nll_loss=5.015, ppl=32.34, wps=3899.3, ups=0.31, wpb=12611, bsz=634.2, num_updates=5625, lr=2.23214e-05, gnorm=5.811, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=22812
2023-05-19 22:57:41 | INFO | train_inner | epoch 087:     59 / 65 loss=6.783, nll_loss=5.055, ppl=33.25, wps=4110.8, ups=0.34, wpb=12212.2, bsz=545.8, num_updates=5630, lr=2.22959e-05, gnorm=7.063, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=22827
2023-05-19 22:57:55 | INFO | train_inner | epoch 087:     64 / 65 loss=6.841, nll_loss=5.122, ppl=34.82, wps=4510.5, ups=0.37, wpb=12292.4, bsz=579.8, num_updates=5635, lr=2.22704e-05, gnorm=4.606, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=22841
2023-05-19 22:57:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 22:57:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:58:36 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 6.218 | nll_loss 4.349 | ppl 20.38 | bleu 4.81 | wps 918.2 | wpb 2785 | bsz 105.2 | num_updates 5636 | best_loss 6.148
2023-05-19 22:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 5636 updates
2023-05-19 22:58:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint87.pt
2023-05-19 22:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint87.pt
2023-05-19 22:58:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint87.pt (epoch 87 @ 5636 updates, score 6.218) (writing took 6.772240597754717 seconds)
2023-05-19 22:58:43 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2023-05-19 22:58:43 | INFO | train | epoch 087 | loss 6.833 | nll_loss 5.108 | ppl 34.5 | wps 3099.6 | ups 0.26 | wpb 12054 | bsz 559.4 | num_updates 5636 | lr 2.22653e-05 | gnorm 6.082 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.8 | wall 22889
2023-05-19 22:58:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 22:58:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 22:58:43 | INFO | fairseq.trainer | begin training epoch 88
2023-05-19 22:58:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 22:58:57 | INFO | train_inner | epoch 088:      4 / 65 loss=6.948, nll_loss=5.232, ppl=37.58, wps=831.9, ups=0.08, wpb=10331.6, bsz=513.8, num_updates=5640, lr=2.22449e-05, gnorm=3.681, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22903
2023-05-19 22:59:13 | INFO | train_inner | epoch 088:      9 / 65 loss=6.789, nll_loss=5.064, ppl=33.45, wps=3967.7, ups=0.33, wpb=12205.2, bsz=548, num_updates=5645, lr=2.22194e-05, gnorm=3.752, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.9, wall=22918
2023-05-19 22:59:29 | INFO | train_inner | epoch 088:     14 / 65 loss=6.83, nll_loss=5.108, ppl=34.49, wps=3795.5, ups=0.31, wpb=12228.2, bsz=514.2, num_updates=5650, lr=2.21939e-05, gnorm=2.785, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22934
2023-05-19 22:59:44 | INFO | train_inner | epoch 088:     19 / 65 loss=7.012, nll_loss=5.301, ppl=39.42, wps=3926.1, ups=0.33, wpb=12056.2, bsz=598, num_updates=5655, lr=2.21684e-05, gnorm=5.835, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=22950
2023-05-19 23:00:00 | INFO | train_inner | epoch 088:     24 / 65 loss=6.797, nll_loss=5.072, ppl=33.63, wps=3855.3, ups=0.32, wpb=12158.4, bsz=558.6, num_updates=5660, lr=2.21429e-05, gnorm=9.976, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22966
2023-05-19 23:00:15 | INFO | train_inner | epoch 088:     29 / 65 loss=6.881, nll_loss=5.176, ppl=36.15, wps=4011.5, ups=0.32, wpb=12599.6, bsz=627.2, num_updates=5665, lr=2.21173e-05, gnorm=8.518, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=22981
2023-05-19 23:00:31 | INFO | train_inner | epoch 088:     34 / 65 loss=6.823, nll_loss=5.105, ppl=34.41, wps=3837.8, ups=0.32, wpb=12056, bsz=569, num_updates=5670, lr=2.20918e-05, gnorm=6.849, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=22997
2023-05-19 23:00:46 | INFO | train_inner | epoch 088:     39 / 65 loss=6.786, nll_loss=5.065, ppl=33.48, wps=4155.5, ups=0.33, wpb=12424.6, bsz=597.4, num_updates=5675, lr=2.20663e-05, gnorm=4.075, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=23012
2023-05-19 23:01:04 | INFO | train_inner | epoch 088:     44 / 65 loss=6.814, nll_loss=5.095, ppl=34.18, wps=3436.1, ups=0.29, wpb=12006, bsz=502.6, num_updates=5680, lr=2.20408e-05, gnorm=8.235, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=23029
2023-05-19 23:01:20 | INFO | train_inner | epoch 088:     49 / 65 loss=6.82, nll_loss=5.091, ppl=34.08, wps=3613.9, ups=0.3, wpb=12080.2, bsz=544.8, num_updates=5685, lr=2.20153e-05, gnorm=7.546, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=23046
2023-05-19 23:01:37 | INFO | train_inner | epoch 088:     54 / 65 loss=6.929, nll_loss=5.216, ppl=37.17, wps=3761.6, ups=0.31, wpb=12267.4, bsz=604.2, num_updates=5690, lr=2.19898e-05, gnorm=2.364, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=23062
2023-05-19 23:01:53 | INFO | train_inner | epoch 088:     59 / 65 loss=6.789, nll_loss=5.068, ppl=33.54, wps=3701.7, ups=0.31, wpb=11993.2, bsz=519.4, num_updates=5695, lr=2.19643e-05, gnorm=8.331, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=23079
2023-05-19 23:02:07 | INFO | train_inner | epoch 088:     64 / 65 loss=6.804, nll_loss=5.085, ppl=33.95, wps=4371.9, ups=0.35, wpb=12341.6, bsz=591.4, num_updates=5700, lr=2.19388e-05, gnorm=8.059, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=23093
2023-05-19 23:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:02:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:02:53 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 6.207 | nll_loss 4.346 | ppl 20.34 | bleu 4.75 | wps 801.8 | wpb 2785 | bsz 105.2 | num_updates 5701 | best_loss 6.148
2023-05-19 23:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 5701 updates
2023-05-19 23:02:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint88.pt
2023-05-19 23:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint88.pt
2023-05-19 23:03:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint88.pt (epoch 88 @ 5701 updates, score 6.207) (writing took 6.991488516330719 seconds)
2023-05-19 23:03:00 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2023-05-19 23:03:00 | INFO | train | epoch 088 | loss 6.847 | nll_loss 5.128 | ppl 34.97 | wps 3050.3 | ups 0.25 | wpb 12053.5 | bsz 559.6 | num_updates 5701 | lr 2.19337e-05 | gnorm 6.267 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.8 | wall 23146
2023-05-19 23:03:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:03:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:03:00 | INFO | fairseq.trainer | begin training epoch 89
2023-05-19 23:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:03:14 | INFO | train_inner | epoch 089:      4 / 65 loss=6.711, nll_loss=4.974, ppl=31.43, wps=742.9, ups=0.07, wpb=10029.8, bsz=422.8, num_updates=5705, lr=2.19133e-05, gnorm=12.655, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.6, wall=23160
2023-05-19 23:03:31 | INFO | train_inner | epoch 089:      9 / 65 loss=7.022, nll_loss=5.315, ppl=39.81, wps=3715.5, ups=0.3, wpb=12200, bsz=612.6, num_updates=5710, lr=2.18878e-05, gnorm=6.027, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=23177
2023-05-19 23:03:47 | INFO | train_inner | epoch 089:     14 / 65 loss=6.806, nll_loss=5.087, ppl=33.98, wps=3682.6, ups=0.3, wpb=12117.6, bsz=520, num_updates=5715, lr=2.18622e-05, gnorm=3.621, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=23193
2023-05-19 23:04:03 | INFO | train_inner | epoch 089:     19 / 65 loss=6.84, nll_loss=5.121, ppl=34.81, wps=4000.8, ups=0.32, wpb=12521, bsz=568.2, num_updates=5720, lr=2.18367e-05, gnorm=10.054, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=23209
2023-05-19 23:04:18 | INFO | train_inner | epoch 089:     24 / 65 loss=6.796, nll_loss=5.072, ppl=33.65, wps=4067.5, ups=0.34, wpb=11904.8, bsz=566.6, num_updates=5725, lr=2.18112e-05, gnorm=3.198, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=23223
2023-05-19 23:04:33 | INFO | train_inner | epoch 089:     29 / 65 loss=6.938, nll_loss=5.23, ppl=37.53, wps=3960, ups=0.32, wpb=12244, bsz=588.2, num_updates=5730, lr=2.17857e-05, gnorm=16.132, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=23239
2023-05-19 23:04:48 | INFO | train_inner | epoch 089:     34 / 65 loss=6.778, nll_loss=5.052, ppl=33.16, wps=3967.5, ups=0.33, wpb=12088.2, bsz=568.8, num_updates=5735, lr=2.17602e-05, gnorm=5.904, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=23254
2023-05-19 23:05:05 | INFO | train_inner | epoch 089:     39 / 65 loss=6.755, nll_loss=5.021, ppl=32.47, wps=3784.6, ups=0.31, wpb=12373, bsz=552.2, num_updates=5740, lr=2.17347e-05, gnorm=3.979, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=23270
2023-05-19 23:05:21 | INFO | train_inner | epoch 089:     44 / 65 loss=6.841, nll_loss=5.119, ppl=34.76, wps=3667.9, ups=0.31, wpb=11921.6, bsz=516.4, num_updates=5745, lr=2.17092e-05, gnorm=3.876, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=23287
2023-05-19 23:05:37 | INFO | train_inner | epoch 089:     49 / 65 loss=6.899, nll_loss=5.181, ppl=36.28, wps=3703.1, ups=0.3, wpb=12251.6, bsz=598.8, num_updates=5750, lr=2.16837e-05, gnorm=4.079, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=23303
2023-05-19 23:05:55 | INFO | train_inner | epoch 089:     54 / 65 loss=6.788, nll_loss=5.061, ppl=33.38, wps=3485.7, ups=0.29, wpb=11903, bsz=500.6, num_updates=5755, lr=2.16582e-05, gnorm=3.603, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=23320
2023-05-19 23:06:09 | INFO | train_inner | epoch 089:     59 / 65 loss=6.734, nll_loss=5.001, ppl=32.03, wps=4317.6, ups=0.34, wpb=12614, bsz=595.8, num_updates=5760, lr=2.16327e-05, gnorm=4.124, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.4, wall=23335
2023-05-19 23:06:24 | INFO | train_inner | epoch 089:     64 / 65 loss=6.826, nll_loss=5.107, ppl=34.46, wps=4063.8, ups=0.33, wpb=12483, bsz=643.6, num_updates=5765, lr=2.16071e-05, gnorm=6.672, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=23350
2023-05-19 23:06:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:06:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:07:07 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 6.156 | nll_loss 4.276 | ppl 19.37 | bleu 5.07 | wps 904.4 | wpb 2785 | bsz 105.2 | num_updates 5766 | best_loss 6.148
2023-05-19 23:07:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 5766 updates
2023-05-19 23:07:07 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint89.pt
2023-05-19 23:07:12 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint89.pt
2023-05-19 23:07:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint89.pt (epoch 89 @ 5766 updates, score 6.156) (writing took 6.950695022940636 seconds)
2023-05-19 23:07:14 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2023-05-19 23:07:14 | INFO | train | epoch 089 | loss 6.828 | nll_loss 5.106 | ppl 34.43 | wps 3086.7 | ups 0.26 | wpb 12053.4 | bsz 559.4 | num_updates 5766 | lr 2.1602e-05 | gnorm 6.611 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.5 | wall 23399
2023-05-19 23:07:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:07:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:07:14 | INFO | fairseq.trainer | begin training epoch 90
2023-05-19 23:07:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:07:27 | INFO | train_inner | epoch 090:      4 / 65 loss=6.808, nll_loss=5.082, ppl=33.88, wps=845, ups=0.08, wpb=10480.6, bsz=481.2, num_updates=5770, lr=2.15816e-05, gnorm=7.25, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=23412
2023-05-19 23:07:43 | INFO | train_inner | epoch 090:      9 / 65 loss=6.86, nll_loss=5.137, ppl=35.18, wps=3592.2, ups=0.3, wpb=11971.2, bsz=521.8, num_updates=5775, lr=2.15561e-05, gnorm=8.589, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=23429
2023-05-19 23:07:59 | INFO | train_inner | epoch 090:     14 / 65 loss=6.813, nll_loss=5.084, ppl=33.91, wps=3797.2, ups=0.31, wpb=12347.4, bsz=573, num_updates=5780, lr=2.15306e-05, gnorm=6.316, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=23445
2023-05-19 23:08:16 | INFO | train_inner | epoch 090:     19 / 65 loss=6.856, nll_loss=5.131, ppl=35.05, wps=3707.2, ups=0.31, wpb=11914.8, bsz=557.8, num_updates=5785, lr=2.15051e-05, gnorm=9.921, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.9, wall=23461
2023-05-19 23:08:31 | INFO | train_inner | epoch 090:     24 / 65 loss=6.929, nll_loss=5.208, ppl=36.96, wps=3972.6, ups=0.32, wpb=12332, bsz=583.6, num_updates=5790, lr=2.14796e-05, gnorm=3.726, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=23477
2023-05-19 23:08:48 | INFO | train_inner | epoch 090:     29 / 65 loss=6.782, nll_loss=5.052, ppl=33.17, wps=3538.5, ups=0.29, wpb=11997, bsz=513, num_updates=5795, lr=2.14541e-05, gnorm=4.932, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.2, wall=23494
2023-05-19 23:09:04 | INFO | train_inner | epoch 090:     34 / 65 loss=6.762, nll_loss=5.028, ppl=32.62, wps=3842.9, ups=0.31, wpb=12220.4, bsz=536, num_updates=5800, lr=2.14286e-05, gnorm=9.328, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=23510
2023-05-19 23:09:19 | INFO | train_inner | epoch 090:     39 / 65 loss=6.768, nll_loss=5.033, ppl=32.73, wps=4012.5, ups=0.33, wpb=12237.2, bsz=597.6, num_updates=5805, lr=2.14031e-05, gnorm=4.618, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.6, wall=23525
2023-05-19 23:09:35 | INFO | train_inner | epoch 090:     44 / 65 loss=6.816, nll_loss=5.088, ppl=34.01, wps=3819.3, ups=0.31, wpb=12309.8, bsz=582.8, num_updates=5810, lr=2.13776e-05, gnorm=9.623, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=23541
2023-05-19 23:09:50 | INFO | train_inner | epoch 090:     49 / 65 loss=6.792, nll_loss=5.057, ppl=33.3, wps=4148.4, ups=0.34, wpb=12054.2, bsz=542, num_updates=5815, lr=2.1352e-05, gnorm=6.467, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=23556
2023-05-19 23:10:04 | INFO | train_inner | epoch 090:     54 / 65 loss=6.753, nll_loss=5.016, ppl=32.36, wps=4286.5, ups=0.35, wpb=12414.8, bsz=566.2, num_updates=5820, lr=2.13265e-05, gnorm=5.332, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=23570
2023-05-19 23:10:21 | INFO | train_inner | epoch 090:     59 / 65 loss=6.727, nll_loss=4.989, ppl=31.76, wps=3652.3, ups=0.3, wpb=12285.8, bsz=592.4, num_updates=5825, lr=2.1301e-05, gnorm=5.573, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=23587
2023-05-19 23:10:37 | INFO | train_inner | epoch 090:     64 / 65 loss=7.017, nll_loss=5.311, ppl=39.7, wps=3757.2, ups=0.31, wpb=12058, bsz=645.4, num_updates=5830, lr=2.12755e-05, gnorm=6.174, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=23603
2023-05-19 23:10:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:10:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:11:22 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 6.128 | nll_loss 4.248 | ppl 19 | bleu 5 | wps 823.4 | wpb 2785 | bsz 105.2 | num_updates 5831 | best_loss 6.128
2023-05-19 23:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 5831 updates
2023-05-19 23:11:22 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint90.pt
2023-05-19 23:11:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint90.pt
2023-05-19 23:11:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint90.pt (epoch 90 @ 5831 updates, score 6.128) (writing took 15.55718918144703 seconds)
2023-05-19 23:11:38 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2023-05-19 23:11:38 | INFO | train | epoch 090 | loss 6.821 | nll_loss 5.093 | ppl 34.13 | wps 2966.3 | ups 0.25 | wpb 12049.1 | bsz 559.8 | num_updates 5831 | lr 2.12704e-05 | gnorm 6.645 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.5 | wall 23663
2023-05-19 23:11:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:11:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:11:38 | INFO | fairseq.trainer | begin training epoch 91
2023-05-19 23:11:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:11:50 | INFO | train_inner | epoch 091:      4 / 65 loss=6.859, nll_loss=5.148, ppl=35.45, wps=751.1, ups=0.07, wpb=10916.2, bsz=542.8, num_updates=5835, lr=2.125e-05, gnorm=5.707, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=23676
2023-05-19 23:12:07 | INFO | train_inner | epoch 091:      9 / 65 loss=6.719, nll_loss=4.986, ppl=31.69, wps=3668.6, ups=0.29, wpb=12454.4, bsz=595.6, num_updates=5840, lr=2.12245e-05, gnorm=6.264, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=23693
2023-05-19 23:12:22 | INFO | train_inner | epoch 091:     14 / 65 loss=6.658, nll_loss=4.905, ppl=29.96, wps=3969.4, ups=0.33, wpb=11969.6, bsz=522.2, num_updates=5845, lr=2.1199e-05, gnorm=4.505, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.9, wall=23708
2023-05-19 23:12:38 | INFO | train_inner | epoch 091:     19 / 65 loss=6.816, nll_loss=5.092, ppl=34.11, wps=3943.6, ups=0.32, wpb=12491.8, bsz=609, num_updates=5850, lr=2.11735e-05, gnorm=4.398, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=23723
2023-05-19 23:12:55 | INFO | train_inner | epoch 091:     24 / 65 loss=6.805, nll_loss=5.075, ppl=33.7, wps=3467.9, ups=0.3, wpb=11736, bsz=501.4, num_updates=5855, lr=2.1148e-05, gnorm=5.643, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.4, wall=23740
2023-05-19 23:13:10 | INFO | train_inner | epoch 091:     29 / 65 loss=6.853, nll_loss=5.131, ppl=35.03, wps=3888, ups=0.32, wpb=12207.8, bsz=610.2, num_updates=5860, lr=2.11224e-05, gnorm=4.85, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=23756
2023-05-19 23:13:27 | INFO | train_inner | epoch 091:     34 / 65 loss=6.774, nll_loss=5.042, ppl=32.95, wps=3761, ups=0.3, wpb=12334, bsz=586.8, num_updates=5865, lr=2.10969e-05, gnorm=4.074, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=23772
2023-05-19 23:13:44 | INFO | train_inner | epoch 091:     39 / 65 loss=6.846, nll_loss=5.121, ppl=34.8, wps=3413.4, ups=0.28, wpb=12082.8, bsz=584.2, num_updates=5870, lr=2.10714e-05, gnorm=5.535, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=23790
2023-05-19 23:13:59 | INFO | train_inner | epoch 091:     44 / 65 loss=6.809, nll_loss=5.076, ppl=33.73, wps=4036, ups=0.33, wpb=12096, bsz=498.4, num_updates=5875, lr=2.10459e-05, gnorm=14.208, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.7, wall=23805
2023-05-19 23:14:16 | INFO | train_inner | epoch 091:     49 / 65 loss=6.975, nll_loss=5.26, ppl=38.32, wps=3662.8, ups=0.31, wpb=11987.8, bsz=559.4, num_updates=5880, lr=2.10204e-05, gnorm=4.784, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.8, wall=23822
2023-05-19 23:14:31 | INFO | train_inner | epoch 091:     54 / 65 loss=6.918, nll_loss=5.197, ppl=36.68, wps=4006.8, ups=0.33, wpb=12300.4, bsz=556.2, num_updates=5885, lr=2.09949e-05, gnorm=7.36, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.6, wall=23837
2023-05-19 23:14:48 | INFO | train_inner | epoch 091:     59 / 65 loss=6.738, nll_loss=5.001, ppl=32.03, wps=3369.8, ups=0.29, wpb=11690, bsz=517.6, num_updates=5890, lr=2.09694e-05, gnorm=6.084, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=23854
2023-05-19 23:15:03 | INFO | train_inner | epoch 091:     64 / 65 loss=6.794, nll_loss=5.065, ppl=33.47, wps=4193.8, ups=0.34, wpb=12460.2, bsz=570.4, num_updates=5895, lr=2.09439e-05, gnorm=5.445, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=23869
2023-05-19 23:15:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:15:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:15:48 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 6.134 | nll_loss 4.247 | ppl 18.99 | bleu 5.05 | wps 824 | wpb 2785 | bsz 105.2 | num_updates 5896 | best_loss 6.128
2023-05-19 23:15:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 5896 updates
2023-05-19 23:15:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint91.pt
2023-05-19 23:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint91.pt
2023-05-19 23:15:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint91.pt (epoch 91 @ 5896 updates, score 6.134) (writing took 9.403788819909096 seconds)
2023-05-19 23:15:57 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2023-05-19 23:15:57 | INFO | train | epoch 091 | loss 6.813 | nll_loss 5.085 | ppl 33.95 | wps 3018.4 | ups 0.25 | wpb 12061.5 | bsz 559.6 | num_updates 5896 | lr 2.09388e-05 | gnorm 6.098 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.9 | wall 23923
2023-05-19 23:15:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:15:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:15:57 | INFO | fairseq.trainer | begin training epoch 92
2023-05-19 23:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:16:10 | INFO | train_inner | epoch 092:      4 / 65 loss=6.862, nll_loss=5.137, ppl=35.18, wps=792.3, ups=0.08, wpb=10531.4, bsz=506.8, num_updates=5900, lr=2.09184e-05, gnorm=6.405, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=23936
2023-05-19 23:16:25 | INFO | train_inner | epoch 092:      9 / 65 loss=6.764, nll_loss=5.029, ppl=32.66, wps=3986.1, ups=0.33, wpb=12190.8, bsz=563, num_updates=5905, lr=2.08929e-05, gnorm=2.548, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=23951
2023-05-19 23:16:41 | INFO | train_inner | epoch 092:     14 / 65 loss=7.057, nll_loss=5.344, ppl=40.61, wps=3950.7, ups=0.32, wpb=12409.2, bsz=662, num_updates=5910, lr=2.08673e-05, gnorm=5.788, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=23967
2023-05-19 23:16:58 | INFO | train_inner | epoch 092:     19 / 65 loss=6.814, nll_loss=5.091, ppl=34.08, wps=3685.5, ups=0.3, wpb=12490.6, bsz=611, num_updates=5915, lr=2.08418e-05, gnorm=3.586, clip=100, loss_scale=0.25, train_wall=17, gb_free=21, wall=23983
2023-05-19 23:17:13 | INFO | train_inner | epoch 092:     24 / 65 loss=6.781, nll_loss=5.057, ppl=33.3, wps=3873.9, ups=0.32, wpb=11999, bsz=515, num_updates=5920, lr=2.08163e-05, gnorm=3.926, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=23999
2023-05-19 23:17:29 | INFO | train_inner | epoch 092:     29 / 65 loss=6.739, nll_loss=5.006, ppl=32.12, wps=3843.4, ups=0.32, wpb=12059, bsz=498.4, num_updates=5925, lr=2.07908e-05, gnorm=6.864, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24015
2023-05-19 23:17:45 | INFO | train_inner | epoch 092:     34 / 65 loss=6.693, nll_loss=4.948, ppl=30.87, wps=3816.9, ups=0.31, wpb=12242.4, bsz=561.2, num_updates=5930, lr=2.07653e-05, gnorm=9.267, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24031
2023-05-19 23:18:02 | INFO | train_inner | epoch 092:     39 / 65 loss=6.85, nll_loss=5.126, ppl=34.93, wps=3642.9, ups=0.3, wpb=12101.8, bsz=548.6, num_updates=5935, lr=2.07398e-05, gnorm=3.319, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.5, wall=24047
2023-05-19 23:18:19 | INFO | train_inner | epoch 092:     44 / 65 loss=6.772, nll_loss=5.04, ppl=32.89, wps=3543.7, ups=0.29, wpb=12071, bsz=534.6, num_updates=5940, lr=2.07143e-05, gnorm=2.666, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=24064
2023-05-19 23:18:34 | INFO | train_inner | epoch 092:     49 / 65 loss=6.718, nll_loss=4.982, ppl=31.6, wps=4105.8, ups=0.33, wpb=12367.8, bsz=558.6, num_updates=5945, lr=2.06888e-05, gnorm=5.81, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=24079
2023-05-19 23:18:51 | INFO | train_inner | epoch 092:     54 / 65 loss=6.817, nll_loss=5.093, ppl=34.14, wps=3561, ups=0.29, wpb=12287.8, bsz=592.4, num_updates=5950, lr=2.06633e-05, gnorm=7.155, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.9, wall=24097
2023-05-19 23:19:06 | INFO | train_inner | epoch 092:     59 / 65 loss=6.799, nll_loss=5.071, ppl=33.61, wps=3867.3, ups=0.33, wpb=11868.8, bsz=591, num_updates=5955, lr=2.06378e-05, gnorm=4.266, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=24112
2023-05-19 23:19:21 | INFO | train_inner | epoch 092:     64 / 65 loss=6.807, nll_loss=5.086, ppl=33.95, wps=4117.3, ups=0.33, wpb=12290.4, bsz=559.4, num_updates=5960, lr=2.06122e-05, gnorm=6.385, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.7, wall=24127
2023-05-19 23:19:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:19:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:20:03 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 6.134 | nll_loss 4.253 | ppl 19.06 | bleu 5.23 | wps 881.5 | wpb 2785 | bsz 105.2 | num_updates 5961 | best_loss 6.128
2023-05-19 23:20:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 5961 updates
2023-05-19 23:20:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint92.pt
2023-05-19 23:20:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint92.pt
2023-05-19 23:20:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint92.pt (epoch 92 @ 5961 updates, score 6.134) (writing took 9.751319870352745 seconds)
2023-05-19 23:20:13 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2023-05-19 23:20:13 | INFO | train | epoch 092 | loss 6.803 | nll_loss 5.075 | ppl 33.7 | wps 3063.8 | ups 0.25 | wpb 12057.9 | bsz 559.8 | num_updates 5961 | lr 2.06071e-05 | gnorm 5.081 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.4 | wall 24179
2023-05-19 23:20:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:20:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:20:13 | INFO | fairseq.trainer | begin training epoch 93
2023-05-19 23:20:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:20:27 | INFO | train_inner | epoch 093:      4 / 65 loss=6.716, nll_loss=4.978, ppl=31.51, wps=760.5, ups=0.08, wpb=10044.2, bsz=427, num_updates=5965, lr=2.05867e-05, gnorm=7.437, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=24193
2023-05-19 23:20:42 | INFO | train_inner | epoch 093:      9 / 65 loss=6.829, nll_loss=5.109, ppl=34.5, wps=4090.2, ups=0.33, wpb=12466.8, bsz=646.8, num_updates=5970, lr=2.05612e-05, gnorm=3.864, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=24208
2023-05-19 23:20:58 | INFO | train_inner | epoch 093:     14 / 65 loss=6.75, nll_loss=5.022, ppl=32.49, wps=3808.6, ups=0.32, wpb=11888.6, bsz=543.2, num_updates=5975, lr=2.05357e-05, gnorm=4.345, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=24224
2023-05-19 23:21:13 | INFO | train_inner | epoch 093:     19 / 65 loss=6.799, nll_loss=5.072, ppl=33.65, wps=4000.5, ups=0.33, wpb=12226.8, bsz=581.8, num_updates=5980, lr=2.05102e-05, gnorm=3.62, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=24239
2023-05-19 23:21:29 | INFO | train_inner | epoch 093:     24 / 65 loss=6.793, nll_loss=5.061, ppl=33.38, wps=3829.4, ups=0.32, wpb=12143.6, bsz=525, num_updates=5985, lr=2.04847e-05, gnorm=3.241, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=24255
2023-05-19 23:21:44 | INFO | train_inner | epoch 093:     29 / 65 loss=6.703, nll_loss=4.957, ppl=31.06, wps=4074.9, ups=0.35, wpb=11796.4, bsz=474.4, num_updates=5990, lr=2.04592e-05, gnorm=7.406, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=24269
2023-05-19 23:22:00 | INFO | train_inner | epoch 093:     34 / 65 loss=6.928, nll_loss=5.208, ppl=36.96, wps=3667.1, ups=0.3, wpb=12341.4, bsz=625.2, num_updates=5995, lr=2.04337e-05, gnorm=7.442, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=24286
2023-05-19 23:22:17 | INFO | train_inner | epoch 093:     39 / 65 loss=7.152, nll_loss=5.453, ppl=43.79, wps=3830.5, ups=0.3, wpb=12592.6, bsz=691, num_updates=6000, lr=2.04082e-05, gnorm=4.847, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=24303
2023-05-19 23:22:32 | INFO | train_inner | epoch 093:     44 / 65 loss=6.69, nll_loss=4.945, ppl=30.81, wps=4324.7, ups=0.34, wpb=12619, bsz=552.4, num_updates=6005, lr=2.03827e-05, gnorm=3.208, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=24317
2023-05-19 23:22:47 | INFO | train_inner | epoch 093:     49 / 65 loss=6.698, nll_loss=4.955, ppl=31.01, wps=3762.5, ups=0.32, wpb=11867.4, bsz=502, num_updates=6010, lr=2.03571e-05, gnorm=3.805, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=24333
2023-05-19 23:23:04 | INFO | train_inner | epoch 093:     54 / 65 loss=6.733, nll_loss=4.994, ppl=31.88, wps=3708.9, ups=0.3, wpb=12198.8, bsz=550, num_updates=6015, lr=2.03316e-05, gnorm=5.52, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=24349
2023-05-19 23:23:20 | INFO | train_inner | epoch 093:     59 / 65 loss=6.848, nll_loss=5.134, ppl=35.13, wps=3800.6, ups=0.3, wpb=12607.2, bsz=617.4, num_updates=6020, lr=2.03061e-05, gnorm=11.304, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.5, wall=24366
2023-05-19 23:23:37 | INFO | train_inner | epoch 093:     64 / 65 loss=6.794, nll_loss=5.065, ppl=33.48, wps=3518.2, ups=0.3, wpb=11850, bsz=538.4, num_updates=6025, lr=2.02806e-05, gnorm=12.646, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.4, wall=24383
2023-05-19 23:23:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:23:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:24:23 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 6.118 | nll_loss 4.234 | ppl 18.81 | bleu 4.79 | wps 823.8 | wpb 2785 | bsz 105.2 | num_updates 6026 | best_loss 6.118
2023-05-19 23:24:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 6026 updates
2023-05-19 23:24:23 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint93.pt
2023-05-19 23:24:27 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint93.pt
2023-05-19 23:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint93.pt (epoch 93 @ 6026 updates, score 6.118) (writing took 27.816465009003878 seconds)
2023-05-19 23:24:50 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2023-05-19 23:24:50 | INFO | train | epoch 093 | loss 6.805 | nll_loss 5.076 | ppl 33.74 | wps 2826.4 | ups 0.23 | wpb 12052.4 | bsz 559.3 | num_updates 6026 | lr 2.02755e-05 | gnorm 5.983 | clip 100 | loss_scale 0.25 | train_wall 206 | gb_free 21.4 | wall 24456
2023-05-19 23:24:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:24:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:24:50 | INFO | fairseq.trainer | begin training epoch 94
2023-05-19 23:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:25:02 | INFO | train_inner | epoch 094:      4 / 65 loss=6.815, nll_loss=5.077, ppl=33.75, wps=597.5, ups=0.06, wpb=10158.6, bsz=479.4, num_updates=6030, lr=2.02551e-05, gnorm=8.427, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=24468
2023-05-19 23:25:18 | INFO | train_inner | epoch 094:      9 / 65 loss=6.795, nll_loss=5.066, ppl=33.49, wps=4031.5, ups=0.32, wpb=12592.4, bsz=578.6, num_updates=6035, lr=2.02296e-05, gnorm=5.302, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=24484
2023-05-19 23:25:33 | INFO | train_inner | epoch 094:     14 / 65 loss=6.749, nll_loss=5.017, ppl=32.38, wps=4046.9, ups=0.33, wpb=12148.2, bsz=524.8, num_updates=6040, lr=2.02041e-05, gnorm=6.805, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=24499
2023-05-19 23:25:49 | INFO | train_inner | epoch 094:     19 / 65 loss=6.784, nll_loss=5.053, ppl=33.19, wps=3664.8, ups=0.31, wpb=11825.8, bsz=534.2, num_updates=6045, lr=2.01786e-05, gnorm=6.135, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=24515
2023-05-19 23:26:07 | INFO | train_inner | epoch 094:     24 / 65 loss=6.88, nll_loss=5.161, ppl=35.78, wps=3397.8, ups=0.28, wpb=12009.4, bsz=527.4, num_updates=6050, lr=2.01531e-05, gnorm=4.861, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.5, wall=24532
2023-05-19 23:26:23 | INFO | train_inner | epoch 094:     29 / 65 loss=6.728, nll_loss=4.985, ppl=31.66, wps=3752.2, ups=0.31, wpb=12119.8, bsz=498.2, num_updates=6055, lr=2.01276e-05, gnorm=9.355, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24549
2023-05-19 23:26:38 | INFO | train_inner | epoch 094:     34 / 65 loss=6.813, nll_loss=5.087, ppl=34, wps=4158.1, ups=0.34, wpb=12291.4, bsz=575.2, num_updates=6060, lr=2.0102e-05, gnorm=4.473, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=24563
2023-05-19 23:26:53 | INFO | train_inner | epoch 094:     39 / 65 loss=6.957, nll_loss=5.23, ppl=37.53, wps=3936.1, ups=0.32, wpb=12148.6, bsz=629.8, num_updates=6065, lr=2.00765e-05, gnorm=5.898, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.5, wall=24579
2023-05-19 23:27:09 | INFO | train_inner | epoch 094:     44 / 65 loss=6.736, nll_loss=5.005, ppl=32.12, wps=3889.1, ups=0.32, wpb=12209.2, bsz=535.6, num_updates=6070, lr=2.0051e-05, gnorm=8.845, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24594
2023-05-19 23:27:25 | INFO | train_inner | epoch 094:     49 / 65 loss=6.844, nll_loss=5.125, ppl=34.9, wps=3734.8, ups=0.31, wpb=11984.8, bsz=586.6, num_updates=6075, lr=2.00255e-05, gnorm=7.858, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24610
2023-05-19 23:27:42 | INFO | train_inner | epoch 094:     54 / 65 loss=6.78, nll_loss=5.055, ppl=33.25, wps=3596.4, ups=0.29, wpb=12230.4, bsz=583.8, num_updates=6080, lr=2e-05, gnorm=6.156, clip=100, loss_scale=0.25, train_wall=17, gb_free=21, wall=24627
2023-05-19 23:27:57 | INFO | train_inner | epoch 094:     59 / 65 loss=6.776, nll_loss=5.05, ppl=33.12, wps=4213.6, ups=0.33, wpb=12632, bsz=596.8, num_updates=6085, lr=1.99745e-05, gnorm=3.426, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=24642
2023-05-19 23:28:13 | INFO | train_inner | epoch 094:     64 / 65 loss=6.786, nll_loss=5.056, ppl=33.27, wps=3850.8, ups=0.31, wpb=12255.2, bsz=604.4, num_updates=6090, lr=1.9949e-05, gnorm=3.583, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=24658
2023-05-19 23:28:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:28:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:28:56 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 6.175 | nll_loss 4.299 | ppl 19.68 | bleu 5.34 | wps 887.3 | wpb 2785 | bsz 105.2 | num_updates 6091 | best_loss 6.118
2023-05-19 23:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 6091 updates
2023-05-19 23:28:56 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint94.pt
2023-05-19 23:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint94.pt
2023-05-19 23:29:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint94.pt (epoch 94 @ 6091 updates, score 6.175) (writing took 6.969354573637247 seconds)
2023-05-19 23:29:03 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2023-05-19 23:29:03 | INFO | train | epoch 094 | loss 6.803 | nll_loss 5.074 | ppl 33.69 | wps 3103 | ups 0.26 | wpb 12057.4 | bsz 558.8 | num_updates 6091 | lr 1.99439e-05 | gnorm 6.228 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.1 | wall 24709
2023-05-19 23:29:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:29:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:29:03 | INFO | fairseq.trainer | begin training epoch 95
2023-05-19 23:29:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:29:16 | INFO | train_inner | epoch 095:      4 / 65 loss=6.76, nll_loss=5.029, ppl=32.64, wps=842.7, ups=0.08, wpb=10753.2, bsz=463.6, num_updates=6095, lr=1.99235e-05, gnorm=5.266, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=24722
2023-05-19 23:29:32 | INFO | train_inner | epoch 095:      9 / 65 loss=6.844, nll_loss=5.129, ppl=34.99, wps=3966, ups=0.32, wpb=12574.2, bsz=597.2, num_updates=6100, lr=1.9898e-05, gnorm=4.462, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=24738
2023-05-19 23:29:49 | INFO | train_inner | epoch 095:     14 / 65 loss=6.755, nll_loss=5.028, ppl=32.63, wps=3637.9, ups=0.3, wpb=11979.2, bsz=522.4, num_updates=6105, lr=1.98724e-05, gnorm=5.038, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=24754
2023-05-19 23:30:06 | INFO | train_inner | epoch 095:     19 / 65 loss=7.002, nll_loss=5.287, ppl=39.04, wps=3530.8, ups=0.3, wpb=11865.6, bsz=574.4, num_updates=6110, lr=1.98469e-05, gnorm=3.461, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.6, wall=24771
2023-05-19 23:30:21 | INFO | train_inner | epoch 095:     24 / 65 loss=6.763, nll_loss=5.033, ppl=32.74, wps=3824.8, ups=0.32, wpb=12003, bsz=557.2, num_updates=6115, lr=1.98214e-05, gnorm=2.615, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=24787
2023-05-19 23:30:37 | INFO | train_inner | epoch 095:     29 / 65 loss=6.817, nll_loss=5.094, ppl=34.15, wps=3921, ups=0.31, wpb=12484, bsz=678.2, num_updates=6120, lr=1.97959e-05, gnorm=4.086, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.3, wall=24803
2023-05-19 23:30:53 | INFO | train_inner | epoch 095:     34 / 65 loss=6.726, nll_loss=4.986, ppl=31.68, wps=3684.7, ups=0.31, wpb=11865.8, bsz=530.2, num_updates=6125, lr=1.97704e-05, gnorm=4.457, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=24819
2023-05-19 23:31:10 | INFO | train_inner | epoch 095:     39 / 65 loss=6.813, nll_loss=5.09, ppl=34.07, wps=3676.3, ups=0.3, wpb=12364, bsz=570, num_updates=6130, lr=1.97449e-05, gnorm=5.987, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=24836
2023-05-19 23:31:25 | INFO | train_inner | epoch 095:     44 / 65 loss=6.746, nll_loss=5.012, ppl=32.27, wps=4034.1, ups=0.33, wpb=12351.2, bsz=515.6, num_updates=6135, lr=1.97194e-05, gnorm=2.7, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=24851
2023-05-19 23:31:43 | INFO | train_inner | epoch 095:     49 / 65 loss=6.839, nll_loss=5.119, ppl=34.75, wps=3352.3, ups=0.28, wpb=11961.2, bsz=555, num_updates=6140, lr=1.96939e-05, gnorm=5.521, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.5, wall=24869
2023-05-19 23:31:58 | INFO | train_inner | epoch 095:     54 / 65 loss=6.766, nll_loss=5.037, ppl=32.84, wps=4066, ups=0.33, wpb=12321, bsz=561.6, num_updates=6145, lr=1.96684e-05, gnorm=4.369, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=24884
2023-05-19 23:32:14 | INFO | train_inner | epoch 095:     59 / 65 loss=6.707, nll_loss=4.969, ppl=31.32, wps=3830.8, ups=0.32, wpb=12033.4, bsz=520.8, num_updates=6150, lr=1.96429e-05, gnorm=18.59, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=24900
2023-05-19 23:32:28 | INFO | train_inner | epoch 095:     64 / 65 loss=6.894, nll_loss=5.176, ppl=36.16, wps=4386.5, ups=0.36, wpb=12196.2, bsz=633.4, num_updates=6155, lr=1.96173e-05, gnorm=5.949, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.4, wall=24914
2023-05-19 23:32:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:32:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:33:14 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 6.121 | nll_loss 4.244 | ppl 18.94 | bleu 4.85 | wps 822.2 | wpb 2785 | bsz 105.2 | num_updates 6156 | best_loss 6.118
2023-05-19 23:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 6156 updates
2023-05-19 23:33:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint95.pt
2023-05-19 23:33:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint95.pt
2023-05-19 23:33:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint95.pt (epoch 95 @ 6156 updates, score 6.121) (writing took 6.6868135295808315 seconds)
2023-05-19 23:33:21 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2023-05-19 23:33:21 | INFO | train | epoch 095 | loss 6.803 | nll_loss 5.077 | ppl 33.74 | wps 3040.5 | ups 0.25 | wpb 12049.4 | bsz 559.6 | num_updates 6156 | lr 1.96122e-05 | gnorm 5.659 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.8 | wall 24966
2023-05-19 23:33:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:33:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:33:21 | INFO | fairseq.trainer | begin training epoch 96
2023-05-19 23:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:33:33 | INFO | train_inner | epoch 096:      4 / 65 loss=6.67, nll_loss=4.929, ppl=30.46, wps=779.9, ups=0.08, wpb=10203.4, bsz=400.4, num_updates=6160, lr=1.95918e-05, gnorm=4.587, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=24979
2023-05-19 23:33:51 | INFO | train_inner | epoch 096:      9 / 65 loss=6.746, nll_loss=5.017, ppl=32.38, wps=3356.4, ups=0.29, wpb=11774.4, bsz=502.6, num_updates=6165, lr=1.95663e-05, gnorm=5.157, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.7, wall=24997
2023-05-19 23:34:09 | INFO | train_inner | epoch 096:     14 / 65 loss=6.79, nll_loss=5.062, ppl=33.41, wps=3322, ups=0.28, wpb=12043.2, bsz=589.8, num_updates=6170, lr=1.95408e-05, gnorm=3.412, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=25015
2023-05-19 23:34:25 | INFO | train_inner | epoch 096:     19 / 65 loss=6.956, nll_loss=5.23, ppl=37.52, wps=3868.2, ups=0.31, wpb=12593.8, bsz=613.2, num_updates=6175, lr=1.95153e-05, gnorm=3.009, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=25031
2023-05-19 23:34:41 | INFO | train_inner | epoch 096:     24 / 65 loss=6.867, nll_loss=5.145, ppl=35.38, wps=4077.5, ups=0.32, wpb=12580.4, bsz=666.8, num_updates=6180, lr=1.94898e-05, gnorm=3.56, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=25047
2023-05-19 23:34:55 | INFO | train_inner | epoch 096:     29 / 65 loss=6.809, nll_loss=5.081, ppl=33.84, wps=4327.6, ups=0.35, wpb=12513.6, bsz=618.2, num_updates=6185, lr=1.94643e-05, gnorm=6.099, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=25061
2023-05-19 23:35:10 | INFO | train_inner | epoch 096:     34 / 65 loss=6.723, nll_loss=4.989, ppl=31.76, wps=3935, ups=0.33, wpb=11775.8, bsz=549.4, num_updates=6190, lr=1.94388e-05, gnorm=8.027, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=25076
2023-05-19 23:35:25 | INFO | train_inner | epoch 096:     39 / 65 loss=6.89, nll_loss=5.167, ppl=35.94, wps=4256.1, ups=0.35, wpb=12329.4, bsz=593.8, num_updates=6195, lr=1.94133e-05, gnorm=2.933, clip=100, loss_scale=0.25, train_wall=14, gb_free=20.6, wall=25090
2023-05-19 23:35:41 | INFO | train_inner | epoch 096:     44 / 65 loss=6.773, nll_loss=5.042, ppl=32.95, wps=3741.5, ups=0.31, wpb=12254.2, bsz=552.4, num_updates=6200, lr=1.93878e-05, gnorm=3.463, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=25107
2023-05-19 23:35:59 | INFO | train_inner | epoch 096:     49 / 65 loss=6.773, nll_loss=5.044, ppl=33, wps=3585.3, ups=0.28, wpb=12665.8, bsz=595.6, num_updates=6205, lr=1.93622e-05, gnorm=8.12, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.1, wall=25124
2023-05-19 23:36:15 | INFO | train_inner | epoch 096:     54 / 65 loss=6.712, nll_loss=4.974, ppl=31.43, wps=3612.9, ups=0.3, wpb=11987.8, bsz=491.4, num_updates=6210, lr=1.93367e-05, gnorm=3.681, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=25141
2023-05-19 23:36:31 | INFO | train_inner | epoch 096:     59 / 65 loss=6.779, nll_loss=5.05, ppl=33.13, wps=3692.9, ups=0.31, wpb=11954.2, bsz=542.4, num_updates=6215, lr=1.93112e-05, gnorm=2.563, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=25157
2023-05-19 23:36:47 | INFO | train_inner | epoch 096:     64 / 65 loss=6.765, nll_loss=5.033, ppl=32.74, wps=4023.9, ups=0.33, wpb=12098, bsz=559.6, num_updates=6220, lr=1.92857e-05, gnorm=4.256, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=25172
2023-05-19 23:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:36:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:37:31 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 6.107 | nll_loss 4.217 | ppl 18.6 | bleu 4.98 | wps 835.5 | wpb 2785 | bsz 105.2 | num_updates 6221 | best_loss 6.107
2023-05-19 23:37:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 6221 updates
2023-05-19 23:37:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint96.pt
2023-05-19 23:37:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint96.pt
2023-05-19 23:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint96.pt (epoch 96 @ 6221 updates, score 6.107) (writing took 27.99041148647666 seconds)
2023-05-19 23:37:59 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2023-05-19 23:37:59 | INFO | train | epoch 096 | loss 6.791 | nll_loss 5.061 | ppl 33.39 | wps 2816.5 | ups 0.23 | wpb 12056.1 | bsz 560.1 | num_updates 6221 | lr 1.92806e-05 | gnorm 4.608 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.9 | wall 25244
2023-05-19 23:37:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:37:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:37:59 | INFO | fairseq.trainer | begin training epoch 97
2023-05-19 23:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-19 23:38:14 | INFO | train_inner | epoch 097:      5 / 65 loss=6.771, nll_loss=5.041, ppl=32.93, wps=612.2, ups=0.06, wpb=10704.6, bsz=520.2, num_updates=6225, lr=1.92602e-05, gnorm=9.675, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=25260
2023-05-19 23:38:30 | INFO | train_inner | epoch 097:     10 / 65 loss=6.73, nll_loss=4.992, ppl=31.82, wps=3649.6, ups=0.31, wpb=11745.2, bsz=502.4, num_updates=6230, lr=1.92347e-05, gnorm=4.318, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=25276
2023-05-19 23:38:46 | INFO | train_inner | epoch 097:     15 / 65 loss=6.739, nll_loss=5.003, ppl=32.06, wps=3805.9, ups=0.3, wpb=12487.4, bsz=602.8, num_updates=6235, lr=1.92092e-05, gnorm=5.13, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=25292
2023-05-19 23:39:02 | INFO | train_inner | epoch 097:     20 / 65 loss=7.04, nll_loss=5.322, ppl=39.99, wps=4056.1, ups=0.33, wpb=12384.6, bsz=648, num_updates=6240, lr=1.91837e-05, gnorm=5.166, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=25307
2023-05-19 23:39:19 | INFO | train_inner | epoch 097:     25 / 65 loss=6.775, nll_loss=5.037, ppl=32.83, wps=3617.6, ups=0.3, wpb=12146.2, bsz=552.6, num_updates=6245, lr=1.91582e-05, gnorm=5.746, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=25324
2023-05-19 23:39:35 | INFO | train_inner | epoch 097:     30 / 65 loss=6.688, nll_loss=4.938, ppl=30.66, wps=3704.9, ups=0.31, wpb=11848.4, bsz=527.6, num_updates=6250, lr=1.91327e-05, gnorm=8.961, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=25340
2023-05-19 23:39:51 | INFO | train_inner | epoch 097:     35 / 65 loss=6.788, nll_loss=5.052, ppl=33.17, wps=3634.7, ups=0.31, wpb=11733.8, bsz=544.2, num_updates=6255, lr=1.91071e-05, gnorm=5.359, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=25356
2023-05-19 23:40:06 | INFO | train_inner | epoch 097:     40 / 65 loss=6.775, nll_loss=5.042, ppl=32.94, wps=4077.5, ups=0.33, wpb=12416.4, bsz=590.2, num_updates=6260, lr=1.90816e-05, gnorm=3.382, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=25372
2023-05-19 23:40:22 | INFO | train_inner | epoch 097:     45 / 65 loss=6.753, nll_loss=5.019, ppl=32.42, wps=3925.9, ups=0.32, wpb=12299.2, bsz=560, num_updates=6265, lr=1.90561e-05, gnorm=6.157, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=25387
2023-05-19 23:40:37 | INFO | train_inner | epoch 097:     50 / 65 loss=6.814, nll_loss=5.086, ppl=33.97, wps=3904.7, ups=0.33, wpb=11839.2, bsz=549.6, num_updates=6270, lr=1.90306e-05, gnorm=6.718, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=25402
2023-05-19 23:40:55 | INFO | train_inner | epoch 097:     55 / 65 loss=6.778, nll_loss=5.05, ppl=33.12, wps=3393.4, ups=0.27, wpb=12504.8, bsz=578.6, num_updates=6275, lr=1.90051e-05, gnorm=5.028, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.1, wall=25421
2023-05-19 23:41:11 | INFO | train_inner | epoch 097:     60 / 65 loss=6.803, nll_loss=5.085, ppl=33.93, wps=3960.8, ups=0.31, wpb=12589.2, bsz=590.8, num_updates=6280, lr=1.89796e-05, gnorm=3.476, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=25437
2023-05-19 23:41:24 | INFO | train_inner | epoch 097:     65 / 65 loss=6.717, nll_loss=4.978, ppl=31.52, wps=3895.1, ups=0.38, wpb=10219.6, bsz=421.6, num_updates=6285, lr=1.89541e-05, gnorm=9.746, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.5, wall=25450
2023-05-19 23:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:41:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:42:05 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 6.121 | nll_loss 4.237 | ppl 18.85 | bleu 4.92 | wps 868.8 | wpb 2785 | bsz 105.2 | num_updates 6285 | best_loss 6.107
2023-05-19 23:42:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 6285 updates
2023-05-19 23:42:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint97.pt
2023-05-19 23:42:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint97.pt
2023-05-19 23:42:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint97.pt (epoch 97 @ 6285 updates, score 6.121) (writing took 6.823273904621601 seconds)
2023-05-19 23:42:12 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2023-05-19 23:42:12 | INFO | train | epoch 097 | loss 6.785 | nll_loss 5.052 | ppl 33.17 | wps 3042.8 | ups 0.25 | wpb 12052.5 | bsz 558.9 | num_updates 6285 | lr 1.89541e-05 | gnorm 5.952 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.5 | wall 25498
2023-05-19 23:42:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:42:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:42:12 | INFO | fairseq.trainer | begin training epoch 98
2023-05-19 23:42:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:42:28 | INFO | train_inner | epoch 098:      5 / 65 loss=6.786, nll_loss=5.053, ppl=33.21, wps=916.8, ups=0.08, wpb=11700.8, bsz=546.4, num_updates=6290, lr=1.89286e-05, gnorm=3.565, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=25514
2023-05-19 23:42:45 | INFO | train_inner | epoch 098:     10 / 65 loss=6.784, nll_loss=5.059, ppl=33.34, wps=3727.7, ups=0.3, wpb=12372.8, bsz=613.2, num_updates=6295, lr=1.89031e-05, gnorm=7.303, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=25530
2023-05-19 23:43:00 | INFO | train_inner | epoch 098:     15 / 65 loss=6.731, nll_loss=4.994, ppl=31.87, wps=4088.5, ups=0.32, wpb=12647.6, bsz=644, num_updates=6300, lr=1.88776e-05, gnorm=6.591, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=25546
2023-05-19 23:43:17 | INFO | train_inner | epoch 098:     20 / 65 loss=6.729, nll_loss=4.986, ppl=31.7, wps=3566.8, ups=0.3, wpb=11933.6, bsz=501.2, num_updates=6305, lr=1.8852e-05, gnorm=4.206, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=25562
2023-05-19 23:43:33 | INFO | train_inner | epoch 098:     25 / 65 loss=6.885, nll_loss=5.169, ppl=35.97, wps=3777.9, ups=0.3, wpb=12575.6, bsz=611, num_updates=6310, lr=1.88265e-05, gnorm=3.81, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=25579
2023-05-19 23:43:49 | INFO | train_inner | epoch 098:     30 / 65 loss=6.676, nll_loss=4.926, ppl=30.4, wps=3839.4, ups=0.32, wpb=12073.2, bsz=529.2, num_updates=6315, lr=1.8801e-05, gnorm=2.609, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=25595
2023-05-19 23:44:04 | INFO | train_inner | epoch 098:     35 / 65 loss=6.744, nll_loss=5.013, ppl=32.29, wps=4127.6, ups=0.33, wpb=12349, bsz=589.8, num_updates=6320, lr=1.87755e-05, gnorm=6.566, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=25610
2023-05-19 23:44:20 | INFO | train_inner | epoch 098:     40 / 65 loss=6.69, nll_loss=4.946, ppl=30.83, wps=3786.3, ups=0.32, wpb=11755, bsz=498.2, num_updates=6325, lr=1.875e-05, gnorm=5.142, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=25625
2023-05-19 23:44:35 | INFO | train_inner | epoch 098:     45 / 65 loss=7.082, nll_loss=5.372, ppl=41.41, wps=4224.5, ups=0.33, wpb=12625.4, bsz=658.6, num_updates=6330, lr=1.87245e-05, gnorm=13.259, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=25640
2023-05-19 23:44:52 | INFO | train_inner | epoch 098:     50 / 65 loss=6.741, nll_loss=5.01, ppl=32.22, wps=3514.4, ups=0.29, wpb=12055.4, bsz=518.2, num_updates=6335, lr=1.8699e-05, gnorm=4.546, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.2, wall=25657
2023-05-19 23:45:08 | INFO | train_inner | epoch 098:     55 / 65 loss=6.787, nll_loss=5.056, ppl=33.28, wps=3717.6, ups=0.32, wpb=11759, bsz=531.2, num_updates=6340, lr=1.86735e-05, gnorm=3.27, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=25673
2023-05-19 23:45:22 | INFO | train_inner | epoch 098:     60 / 65 loss=6.762, nll_loss=5.028, ppl=32.63, wps=4137.5, ups=0.34, wpb=12275.8, bsz=546, num_updates=6345, lr=1.8648e-05, gnorm=5.053, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.9, wall=25688
2023-05-19 23:45:37 | INFO | train_inner | epoch 098:     65 / 65 loss=6.798, nll_loss=5.069, ppl=33.56, wps=3620.1, ups=0.34, wpb=10603.4, bsz=484.2, num_updates=6350, lr=1.86224e-05, gnorm=5.369, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=25703
2023-05-19 23:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:45:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:46:19 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 6.125 | nll_loss 4.239 | ppl 18.88 | bleu 5.26 | wps 850.5 | wpb 2785 | bsz 105.2 | num_updates 6350 | best_loss 6.107
2023-05-19 23:46:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 6350 updates
2023-05-19 23:46:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint98.pt
2023-05-19 23:46:24 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint98.pt
2023-05-19 23:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint98.pt (epoch 98 @ 6350 updates, score 6.125) (writing took 9.73868703842163 seconds)
2023-05-19 23:46:29 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2023-05-19 23:46:29 | INFO | train | epoch 098 | loss 6.785 | nll_loss 5.054 | ppl 33.22 | wps 3057.3 | ups 0.25 | wpb 12055.9 | bsz 559.3 | num_updates 6350 | lr 1.86224e-05 | gnorm 5.484 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.7 | wall 25754
2023-05-19 23:46:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:46:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:46:29 | INFO | fairseq.trainer | begin training epoch 99
2023-05-19 23:46:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:46:44 | INFO | train_inner | epoch 099:      5 / 65 loss=6.649, nll_loss=4.895, ppl=29.75, wps=885.9, ups=0.07, wpb=11934, bsz=486.6, num_updates=6355, lr=1.85969e-05, gnorm=7.016, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=25770
2023-05-19 23:47:01 | INFO | train_inner | epoch 099:     10 / 65 loss=6.718, nll_loss=4.974, ppl=31.43, wps=3698.6, ups=0.3, wpb=12142.2, bsz=526.4, num_updates=6360, lr=1.85714e-05, gnorm=39.222, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=25787
2023-05-19 23:47:17 | INFO | train_inner | epoch 099:     15 / 65 loss=6.924, nll_loss=5.212, ppl=37.07, wps=3845.6, ups=0.3, wpb=12650.4, bsz=668.6, num_updates=6365, lr=1.85459e-05, gnorm=3.731, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=25803
2023-05-19 23:47:32 | INFO | train_inner | epoch 099:     20 / 65 loss=6.748, nll_loss=5.015, ppl=32.33, wps=4310, ups=0.34, wpb=12603.4, bsz=565.2, num_updates=6370, lr=1.85204e-05, gnorm=8.407, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=25818
2023-05-19 23:47:48 | INFO | train_inner | epoch 099:     25 / 65 loss=6.79, nll_loss=5.059, ppl=33.33, wps=3917.8, ups=0.32, wpb=12396, bsz=570.6, num_updates=6375, lr=1.84949e-05, gnorm=10.495, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=25833
2023-05-19 23:48:05 | INFO | train_inner | epoch 099:     30 / 65 loss=6.686, nll_loss=4.944, ppl=30.77, wps=3558.8, ups=0.29, wpb=12110, bsz=546, num_updates=6380, lr=1.84694e-05, gnorm=17.485, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=25850
2023-05-19 23:48:21 | INFO | train_inner | epoch 099:     35 / 65 loss=6.767, nll_loss=5.041, ppl=32.92, wps=3790.5, ups=0.32, wpb=12019.6, bsz=549, num_updates=6385, lr=1.84439e-05, gnorm=4.32, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=25866
2023-05-19 23:48:37 | INFO | train_inner | epoch 099:     40 / 65 loss=6.724, nll_loss=4.984, ppl=31.65, wps=3488, ups=0.3, wpb=11762.8, bsz=481.8, num_updates=6390, lr=1.84184e-05, gnorm=7.486, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=25883
2023-05-19 23:48:53 | INFO | train_inner | epoch 099:     45 / 65 loss=6.688, nll_loss=4.942, ppl=30.74, wps=3925.8, ups=0.32, wpb=12113.4, bsz=574, num_updates=6395, lr=1.83929e-05, gnorm=5.507, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.6, wall=25899
2023-05-19 23:49:08 | INFO | train_inner | epoch 099:     50 / 65 loss=6.936, nll_loss=5.22, ppl=37.27, wps=4114.2, ups=0.33, wpb=12523.2, bsz=667.8, num_updates=6400, lr=1.83673e-05, gnorm=7.704, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=25914
2023-05-19 23:49:24 | INFO | train_inner | epoch 099:     55 / 65 loss=6.948, nll_loss=5.228, ppl=37.49, wps=3868.2, ups=0.31, wpb=12388.6, bsz=611.4, num_updates=6405, lr=1.83418e-05, gnorm=4.713, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=25930
2023-05-19 23:49:39 | INFO | train_inner | epoch 099:     60 / 65 loss=6.752, nll_loss=5.021, ppl=32.46, wps=3917.1, ups=0.33, wpb=11898.8, bsz=527.6, num_updates=6410, lr=1.83163e-05, gnorm=5.034, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=25945
2023-05-19 23:49:52 | INFO | train_inner | epoch 099:     65 / 65 loss=6.894, nll_loss=5.175, ppl=36.13, wps=3877.7, ups=0.38, wpb=10160.6, bsz=493, num_updates=6415, lr=1.82908e-05, gnorm=8.158, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.4, wall=25958
2023-05-19 23:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:49:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:50:34 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 6.117 | nll_loss 4.234 | ppl 18.82 | bleu 5.21 | wps 850.1 | wpb 2785 | bsz 105.2 | num_updates 6415 | best_loss 6.107
2023-05-19 23:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 6415 updates
2023-05-19 23:50:34 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint99.pt
2023-05-19 23:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint99.pt
2023-05-19 23:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint99.pt (epoch 99 @ 6415 updates, score 6.117) (writing took 6.719585094600916 seconds)
2023-05-19 23:50:41 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2023-05-19 23:50:41 | INFO | train | epoch 099 | loss 6.787 | nll_loss 5.055 | ppl 33.23 | wps 3104.5 | ups 0.26 | wpb 12054.1 | bsz 559.1 | num_updates 6415 | lr 1.82908e-05 | gnorm 9.945 | clip 100 | loss_scale 0.125 | train_wall 203 | gb_free 21.4 | wall 26007
2023-05-19 23:50:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:50:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:50:41 | INFO | fairseq.trainer | begin training epoch 100
2023-05-19 23:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:50:58 | INFO | train_inner | epoch 100:      5 / 65 loss=6.951, nll_loss=5.224, ppl=37.37, wps=915.3, ups=0.08, wpb=12035, bsz=597.2, num_updates=6420, lr=1.82653e-05, gnorm=5.293, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=26024
2023-05-19 23:51:13 | INFO | train_inner | epoch 100:     10 / 65 loss=6.729, nll_loss=4.994, ppl=31.86, wps=4078.6, ups=0.33, wpb=12467.8, bsz=545.8, num_updates=6425, lr=1.82398e-05, gnorm=16.667, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=26039
2023-05-19 23:51:30 | INFO | train_inner | epoch 100:     15 / 65 loss=6.752, nll_loss=5.021, ppl=32.47, wps=3738.4, ups=0.31, wpb=12191.4, bsz=575, num_updates=6430, lr=1.82143e-05, gnorm=7.002, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=26055
2023-05-19 23:51:45 | INFO | train_inner | epoch 100:     20 / 65 loss=6.758, nll_loss=5.025, ppl=32.57, wps=4170.4, ups=0.33, wpb=12551.6, bsz=608.4, num_updates=6435, lr=1.81888e-05, gnorm=5.241, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=26070
2023-05-19 23:52:02 | INFO | train_inner | epoch 100:     25 / 65 loss=6.716, nll_loss=4.974, ppl=31.42, wps=3522.9, ups=0.29, wpb=11960.4, bsz=562, num_updates=6440, lr=1.81633e-05, gnorm=6.361, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=26087
2023-05-19 23:52:17 | INFO | train_inner | epoch 100:     30 / 65 loss=6.785, nll_loss=5.053, ppl=33.19, wps=3954.7, ups=0.33, wpb=12074.8, bsz=548.4, num_updates=6445, lr=1.81378e-05, gnorm=3.484, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=26103
2023-05-19 23:52:34 | INFO | train_inner | epoch 100:     35 / 65 loss=6.758, nll_loss=5.027, ppl=32.61, wps=3675.2, ups=0.3, wpb=12248, bsz=526.6, num_updates=6450, lr=1.81122e-05, gnorm=11.658, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=26119
2023-05-19 23:52:52 | INFO | train_inner | epoch 100:     40 / 65 loss=6.693, nll_loss=4.956, ppl=31.04, wps=3365.4, ups=0.28, wpb=12122, bsz=517.8, num_updates=6455, lr=1.80867e-05, gnorm=4.888, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.8, wall=26137
2023-05-19 23:53:09 | INFO | train_inner | epoch 100:     45 / 65 loss=6.847, nll_loss=5.126, ppl=34.91, wps=3470.4, ups=0.3, wpb=11726.8, bsz=554.2, num_updates=6460, lr=1.80612e-05, gnorm=6.146, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=26154
2023-05-19 23:53:26 | INFO | train_inner | epoch 100:     50 / 65 loss=6.7, nll_loss=4.965, ppl=31.23, wps=3540.8, ups=0.29, wpb=12255.2, bsz=541.6, num_updates=6465, lr=1.80357e-05, gnorm=4.417, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=26172
2023-05-19 23:53:42 | INFO | train_inner | epoch 100:     55 / 65 loss=6.866, nll_loss=5.146, ppl=35.4, wps=3758.1, ups=0.31, wpb=12291.6, bsz=568, num_updates=6470, lr=1.80102e-05, gnorm=13.515, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=26188
2023-05-19 23:53:57 | INFO | train_inner | epoch 100:     60 / 65 loss=6.838, nll_loss=5.119, ppl=34.76, wps=4101.4, ups=0.34, wpb=12181.2, bsz=636.8, num_updates=6475, lr=1.79847e-05, gnorm=7.319, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=26203
2023-05-19 23:54:10 | INFO | train_inner | epoch 100:     65 / 65 loss=6.792, nll_loss=5.065, ppl=33.47, wps=4087.6, ups=0.39, wpb=10578.4, bsz=476.6, num_updates=6480, lr=1.79592e-05, gnorm=11.866, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.7, wall=26216
2023-05-19 23:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:54:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:54:49 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 6.102 | nll_loss 4.216 | ppl 18.58 | bleu 5.3 | wps 908.6 | wpb 2785 | bsz 105.2 | num_updates 6480 | best_loss 6.102
2023-05-19 23:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 6480 updates
2023-05-19 23:54:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint100.pt
2023-05-19 23:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint100.pt
2023-05-19 23:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint100.pt (epoch 100 @ 6480 updates, score 6.102) (writing took 30.471835665404797 seconds)
2023-05-19 23:55:20 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2023-05-19 23:55:20 | INFO | train | epoch 100 | loss 6.783 | nll_loss 5.053 | ppl 33.19 | wps 2808.9 | ups 0.23 | wpb 12052.6 | bsz 558.3 | num_updates 6480 | lr 1.79592e-05 | gnorm 7.989 | clip 100 | loss_scale 0.125 | train_wall 208 | gb_free 21.7 | wall 26286
2023-05-19 23:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:55:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:55:20 | INFO | fairseq.trainer | begin training epoch 101
2023-05-19 23:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:55:36 | INFO | train_inner | epoch 101:      5 / 65 loss=6.798, nll_loss=5.073, ppl=33.65, wps=731.7, ups=0.06, wpb=12573.4, bsz=612, num_updates=6485, lr=1.79337e-05, gnorm=11.078, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=26302
2023-05-19 23:55:52 | INFO | train_inner | epoch 101:     10 / 65 loss=6.785, nll_loss=5.054, ppl=33.21, wps=3737.8, ups=0.31, wpb=12249.8, bsz=571.2, num_updates=6490, lr=1.79082e-05, gnorm=4.415, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=26318
2023-05-19 23:56:09 | INFO | train_inner | epoch 101:     15 / 65 loss=6.762, nll_loss=5.031, ppl=32.69, wps=3801.9, ups=0.31, wpb=12385.4, bsz=605.2, num_updates=6495, lr=1.78827e-05, gnorm=2.746, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=26334
2023-05-19 23:56:24 | INFO | train_inner | epoch 101:     20 / 65 loss=6.696, nll_loss=4.957, ppl=31.07, wps=3905.9, ups=0.32, wpb=12043, bsz=560.2, num_updates=6500, lr=1.78571e-05, gnorm=6.358, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=26350
2023-05-19 23:56:39 | INFO | train_inner | epoch 101:     25 / 65 loss=6.779, nll_loss=5.047, ppl=33.06, wps=4084.1, ups=0.33, wpb=12404.8, bsz=550.8, num_updates=6505, lr=1.78316e-05, gnorm=4.816, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=26365
2023-05-19 23:56:56 | INFO | train_inner | epoch 101:     30 / 65 loss=6.737, nll_loss=5.001, ppl=32.03, wps=3421.6, ups=0.3, wpb=11463.2, bsz=509.8, num_updates=6510, lr=1.78061e-05, gnorm=4.489, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=26382
2023-05-19 23:57:12 | INFO | train_inner | epoch 101:     35 / 65 loss=6.773, nll_loss=5.039, ppl=32.89, wps=3835.3, ups=0.31, wpb=12251.6, bsz=566, num_updates=6515, lr=1.77806e-05, gnorm=7.033, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=26398
2023-05-19 23:57:29 | INFO | train_inner | epoch 101:     40 / 65 loss=6.819, nll_loss=5.082, ppl=33.88, wps=3639.3, ups=0.3, wpb=12221, bsz=524.2, num_updates=6520, lr=1.77551e-05, gnorm=3.003, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=26414
2023-05-19 23:57:45 | INFO | train_inner | epoch 101:     45 / 65 loss=6.75, nll_loss=5.016, ppl=32.36, wps=3809, ups=0.31, wpb=12101.4, bsz=517.6, num_updates=6525, lr=1.77296e-05, gnorm=7.354, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=26430
2023-05-19 23:58:02 | INFO | train_inner | epoch 101:     50 / 65 loss=6.762, nll_loss=5.023, ppl=32.51, wps=3563.2, ups=0.29, wpb=12171, bsz=550.2, num_updates=6530, lr=1.77041e-05, gnorm=5.798, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.9, wall=26447
2023-05-19 23:58:17 | INFO | train_inner | epoch 101:     55 / 65 loss=6.919, nll_loss=5.195, ppl=36.64, wps=4173.2, ups=0.33, wpb=12772.6, bsz=709.4, num_updates=6535, lr=1.76786e-05, gnorm=4.97, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=26463
2023-05-19 23:58:33 | INFO | train_inner | epoch 101:     60 / 65 loss=6.724, nll_loss=4.984, ppl=31.64, wps=3692.2, ups=0.32, wpb=11539.6, bsz=447.8, num_updates=6540, lr=1.76531e-05, gnorm=3.916, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.1, wall=26478
2023-05-19 23:58:47 | INFO | train_inner | epoch 101:     65 / 65 loss=6.863, nll_loss=5.148, ppl=35.45, wps=3766.6, ups=0.36, wpb=10535.2, bsz=529.2, num_updates=6545, lr=1.76276e-05, gnorm=5.19, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=26492
2023-05-19 23:58:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-19 23:58:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:59:28 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 6.123 | nll_loss 4.241 | ppl 18.91 | bleu 5.07 | wps 852.5 | wpb 2785 | bsz 105.2 | num_updates 6545 | best_loss 6.102
2023-05-19 23:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 6545 updates
2023-05-19 23:59:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint101.pt
2023-05-19 23:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint101.pt
2023-05-19 23:59:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint101.pt (epoch 101 @ 6545 updates, score 6.123) (writing took 7.259358864277601 seconds)
2023-05-19 23:59:38 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2023-05-19 23:59:38 | INFO | train | epoch 101 | loss 6.782 | nll_loss 5.05 | ppl 33.13 | wps 3035.9 | ups 0.25 | wpb 12054.8 | bsz 558 | num_updates 6545 | lr 1.76276e-05 | gnorm 5.474 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.7 | wall 26544
2023-05-19 23:59:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-19 23:59:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-19 23:59:38 | INFO | fairseq.trainer | begin training epoch 102
2023-05-19 23:59:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-19 23:59:53 | INFO | train_inner | epoch 102:      5 / 65 loss=6.749, nll_loss=5.017, ppl=32.37, wps=900, ups=0.08, wpb=11961.4, bsz=594.8, num_updates=6550, lr=1.7602e-05, gnorm=3.272, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=26559
2023-05-20 00:00:10 | INFO | train_inner | epoch 102:     10 / 65 loss=6.787, nll_loss=5.059, ppl=33.35, wps=3677.5, ups=0.3, wpb=12292.6, bsz=596.2, num_updates=6555, lr=1.75765e-05, gnorm=22.506, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=26575
2023-05-20 00:00:26 | INFO | train_inner | epoch 102:     15 / 65 loss=6.77, nll_loss=5.042, ppl=32.94, wps=3748.4, ups=0.31, wpb=12129.2, bsz=549.2, num_updates=6560, lr=1.7551e-05, gnorm=2.779, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=26592
2023-05-20 00:00:43 | INFO | train_inner | epoch 102:     20 / 65 loss=6.692, nll_loss=4.953, ppl=30.98, wps=3609.8, ups=0.3, wpb=12126, bsz=526.2, num_updates=6565, lr=1.75255e-05, gnorm=5.432, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=26608
2023-05-20 00:00:58 | INFO | train_inner | epoch 102:     25 / 65 loss=6.978, nll_loss=5.258, ppl=38.26, wps=4429.9, ups=0.34, wpb=13126.6, bsz=708, num_updates=6570, lr=1.75e-05, gnorm=4.396, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=26623
2023-05-20 00:01:14 | INFO | train_inner | epoch 102:     30 / 65 loss=6.86, nll_loss=5.133, ppl=35.08, wps=3839.8, ups=0.31, wpb=12485.8, bsz=606, num_updates=6575, lr=1.74745e-05, gnorm=6.241, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=26640
2023-05-20 00:01:30 | INFO | train_inner | epoch 102:     35 / 65 loss=6.696, nll_loss=4.951, ppl=30.93, wps=3723.5, ups=0.31, wpb=11928.8, bsz=475.4, num_updates=6580, lr=1.7449e-05, gnorm=2.51, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=26656
2023-05-20 00:01:45 | INFO | train_inner | epoch 102:     40 / 65 loss=6.754, nll_loss=5.019, ppl=32.43, wps=4036.3, ups=0.32, wpb=12453, bsz=589.2, num_updates=6585, lr=1.74235e-05, gnorm=8.34, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=26671
2023-05-20 00:02:02 | INFO | train_inner | epoch 102:     45 / 65 loss=6.776, nll_loss=5.048, ppl=33.09, wps=3504.6, ups=0.3, wpb=11767, bsz=536.8, num_updates=6590, lr=1.7398e-05, gnorm=4.582, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=26688
2023-05-20 00:02:17 | INFO | train_inner | epoch 102:     50 / 65 loss=6.752, nll_loss=5.017, ppl=32.39, wps=3917.5, ups=0.32, wpb=12079.4, bsz=541.2, num_updates=6595, lr=1.73724e-05, gnorm=2.86, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=26703
2023-05-20 00:02:34 | INFO | train_inner | epoch 102:     55 / 65 loss=6.761, nll_loss=5.025, ppl=32.55, wps=3548.3, ups=0.29, wpb=12079.6, bsz=588.8, num_updates=6600, lr=1.73469e-05, gnorm=2.915, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=26720
2023-05-20 00:02:51 | INFO | train_inner | epoch 102:     60 / 65 loss=6.681, nll_loss=4.936, ppl=30.62, wps=3629.8, ups=0.31, wpb=11828.8, bsz=467.2, num_updates=6605, lr=1.73214e-05, gnorm=5.367, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=26737
2023-05-20 00:03:06 | INFO | train_inner | epoch 102:     65 / 65 loss=6.79, nll_loss=5.065, ppl=33.49, wps=3323.2, ups=0.32, wpb=10461.2, bsz=485.8, num_updates=6610, lr=1.72959e-05, gnorm=6.472, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=26752
2023-05-20 00:03:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:03:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:03:47 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 6.144 | nll_loss 4.265 | ppl 19.22 | bleu 5.22 | wps 871.9 | wpb 2785 | bsz 105.2 | num_updates 6610 | best_loss 6.102
2023-05-20 00:03:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 6610 updates
2023-05-20 00:03:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint102.pt
2023-05-20 00:03:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint102.pt
2023-05-20 00:03:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint102.pt (epoch 102 @ 6610 updates, score 6.144) (writing took 8.973078653216362 seconds)
2023-05-20 00:03:56 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2023-05-20 00:03:56 | INFO | train | epoch 102 | loss 6.774 | nll_loss 5.042 | ppl 32.94 | wps 3031.8 | ups 0.25 | wpb 12055.3 | bsz 558.8 | num_updates 6610 | lr 1.72959e-05 | gnorm 5.975 | clip 100 | loss_scale 0.125 | train_wall 208 | gb_free 21.6 | wall 26802
2023-05-20 00:03:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:03:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:03:56 | INFO | fairseq.trainer | begin training epoch 103
2023-05-20 00:03:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:04:12 | INFO | train_inner | epoch 103:      5 / 65 loss=6.741, nll_loss=5.007, ppl=32.16, wps=928.8, ups=0.08, wpb=12153.6, bsz=537.8, num_updates=6615, lr=1.72704e-05, gnorm=4.159, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=26818
2023-05-20 00:04:28 | INFO | train_inner | epoch 103:     10 / 65 loss=6.762, nll_loss=5.032, ppl=32.72, wps=3771.1, ups=0.31, wpb=12210.4, bsz=555.4, num_updates=6620, lr=1.72449e-05, gnorm=13.168, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=26834
2023-05-20 00:04:44 | INFO | train_inner | epoch 103:     15 / 65 loss=6.761, nll_loss=5.032, ppl=32.72, wps=3670.9, ups=0.31, wpb=12028.6, bsz=554.8, num_updates=6625, lr=1.72194e-05, gnorm=6.834, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=26850
2023-05-20 00:05:01 | INFO | train_inner | epoch 103:     20 / 65 loss=6.819, nll_loss=5.084, ppl=33.93, wps=3816, ups=0.31, wpb=12247.8, bsz=576.8, num_updates=6630, lr=1.71939e-05, gnorm=3.66, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=26866
2023-05-20 00:05:18 | INFO | train_inner | epoch 103:     25 / 65 loss=6.807, nll_loss=5.079, ppl=33.81, wps=3422, ups=0.28, wpb=12032.2, bsz=515.2, num_updates=6635, lr=1.71684e-05, gnorm=4.767, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=26884
2023-05-20 00:05:33 | INFO | train_inner | epoch 103:     30 / 65 loss=6.867, nll_loss=5.143, ppl=35.35, wps=4111.7, ups=0.33, wpb=12312.8, bsz=627.6, num_updates=6640, lr=1.71429e-05, gnorm=2.775, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=26899
2023-05-20 00:05:49 | INFO | train_inner | epoch 103:     35 / 65 loss=6.781, nll_loss=5.054, ppl=33.23, wps=3820.9, ups=0.31, wpb=12352.8, bsz=550.4, num_updates=6645, lr=1.71173e-05, gnorm=6.142, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=26915
2023-05-20 00:06:05 | INFO | train_inner | epoch 103:     40 / 65 loss=6.703, nll_loss=4.971, ppl=31.37, wps=3822.1, ups=0.32, wpb=11890, bsz=515.6, num_updates=6650, lr=1.70918e-05, gnorm=11.267, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=26931
2023-05-20 00:06:21 | INFO | train_inner | epoch 103:     45 / 65 loss=6.988, nll_loss=5.277, ppl=38.76, wps=3860.6, ups=0.31, wpb=12393, bsz=646.8, num_updates=6655, lr=1.70663e-05, gnorm=4.578, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=26947
2023-05-20 00:06:36 | INFO | train_inner | epoch 103:     50 / 65 loss=6.748, nll_loss=5.019, ppl=32.42, wps=3991.2, ups=0.33, wpb=12164.6, bsz=562, num_updates=6660, lr=1.70408e-05, gnorm=6.268, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=26962
2023-05-20 00:06:52 | INFO | train_inner | epoch 103:     55 / 65 loss=6.777, nll_loss=5.05, ppl=33.13, wps=3812, ups=0.32, wpb=12062, bsz=512.8, num_updates=6665, lr=1.70153e-05, gnorm=9.581, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=26978
2023-05-20 00:07:08 | INFO | train_inner | epoch 103:     60 / 65 loss=6.768, nll_loss=5.042, ppl=32.94, wps=3819.5, ups=0.31, wpb=12377, bsz=568.4, num_updates=6670, lr=1.69898e-05, gnorm=8.84, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=26994
2023-05-20 00:07:23 | INFO | train_inner | epoch 103:     65 / 65 loss=6.817, nll_loss=5.093, ppl=34.13, wps=3558, ups=0.34, wpb=10557.2, bsz=549, num_updates=6675, lr=1.69643e-05, gnorm=4.214, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=27009
2023-05-20 00:07:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:07:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:08:02 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 6.172 | nll_loss 4.297 | ppl 19.66 | bleu 5.39 | wps 920.2 | wpb 2785 | bsz 105.2 | num_updates 6675 | best_loss 6.102
2023-05-20 00:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 6675 updates
2023-05-20 00:08:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint103.pt
2023-05-20 00:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint103.pt
2023-05-20 00:08:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint103.pt (epoch 103 @ 6675 updates, score 6.172) (writing took 9.531762801110744 seconds)
2023-05-20 00:08:11 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2023-05-20 00:08:11 | INFO | train | epoch 103 | loss 6.796 | nll_loss 5.068 | ppl 33.55 | wps 3074 | ups 0.25 | wpb 12060.2 | bsz 559.4 | num_updates 6675 | lr 1.69643e-05 | gnorm 6.635 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.7 | wall 27057
2023-05-20 00:08:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:08:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:08:11 | INFO | fairseq.trainer | begin training epoch 104
2023-05-20 00:08:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:08:28 | INFO | train_inner | epoch 104:      5 / 65 loss=6.765, nll_loss=5.042, ppl=32.94, wps=964.6, ups=0.08, wpb=12495.6, bsz=606, num_updates=6680, lr=1.69388e-05, gnorm=7.504, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27073
2023-05-20 00:08:43 | INFO | train_inner | epoch 104:     10 / 65 loss=6.771, nll_loss=5.043, ppl=32.98, wps=4152.7, ups=0.34, wpb=12354, bsz=581.8, num_updates=6685, lr=1.69133e-05, gnorm=2.805, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=27088
2023-05-20 00:08:58 | INFO | train_inner | epoch 104:     15 / 65 loss=6.767, nll_loss=5.039, ppl=32.89, wps=3962.8, ups=0.33, wpb=12124.6, bsz=586.6, num_updates=6690, lr=1.68878e-05, gnorm=7.817, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=27104
2023-05-20 00:09:15 | INFO | train_inner | epoch 104:     20 / 65 loss=6.761, nll_loss=5.029, ppl=32.66, wps=3429, ups=0.29, wpb=11791.6, bsz=476.4, num_updates=6695, lr=1.68622e-05, gnorm=5.436, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=27121
2023-05-20 00:09:32 | INFO | train_inner | epoch 104:     25 / 65 loss=6.841, nll_loss=5.116, ppl=34.69, wps=3502.6, ups=0.29, wpb=12118, bsz=586.8, num_updates=6700, lr=1.68367e-05, gnorm=3.647, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.4, wall=27138
2023-05-20 00:09:48 | INFO | train_inner | epoch 104:     30 / 65 loss=6.723, nll_loss=4.988, ppl=31.74, wps=3935.8, ups=0.32, wpb=12148, bsz=529, num_updates=6705, lr=1.68112e-05, gnorm=4.973, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=27154
2023-05-20 00:10:04 | INFO | train_inner | epoch 104:     35 / 65 loss=6.816, nll_loss=5.097, ppl=34.22, wps=4002.8, ups=0.32, wpb=12664.8, bsz=633.6, num_updates=6710, lr=1.67857e-05, gnorm=5.33, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=27169
2023-05-20 00:10:21 | INFO | train_inner | epoch 104:     40 / 65 loss=6.698, nll_loss=4.956, ppl=31.05, wps=3535.3, ups=0.29, wpb=12276.4, bsz=546.2, num_updates=6715, lr=1.67602e-05, gnorm=3.886, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=27187
2023-05-20 00:10:37 | INFO | train_inner | epoch 104:     45 / 65 loss=6.717, nll_loss=4.986, ppl=31.68, wps=3857.7, ups=0.32, wpb=12127.4, bsz=535, num_updates=6720, lr=1.67347e-05, gnorm=3.177, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27203
2023-05-20 00:10:53 | INFO | train_inner | epoch 104:     50 / 65 loss=6.794, nll_loss=5.073, ppl=33.65, wps=3912, ups=0.31, wpb=12496.2, bsz=573.6, num_updates=6725, lr=1.67092e-05, gnorm=2.731, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=27218
2023-05-20 00:11:07 | INFO | train_inner | epoch 104:     55 / 65 loss=6.86, nll_loss=5.135, ppl=35.15, wps=4089.6, ups=0.34, wpb=12009.8, bsz=565.6, num_updates=6730, lr=1.66837e-05, gnorm=4.615, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=27233
2023-05-20 00:11:24 | INFO | train_inner | epoch 104:     60 / 65 loss=6.794, nll_loss=5.067, ppl=33.53, wps=3542.1, ups=0.3, wpb=11937.6, bsz=554.2, num_updates=6735, lr=1.66582e-05, gnorm=10.743, clip=100, loss_scale=0.125, train_wall=17, gb_free=21, wall=27250
2023-05-20 00:11:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 00:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:11:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:12:17 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 6.137 | nll_loss 4.255 | ppl 19.09 | bleu 5.28 | wps 907.3 | wpb 2785 | bsz 105.2 | num_updates 6739 | best_loss 6.102
2023-05-20 00:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 6739 updates
2023-05-20 00:12:17 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint104.pt
2023-05-20 00:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint104.pt
2023-05-20 00:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint104.pt (epoch 104 @ 6739 updates, score 6.137) (writing took 10.674217071384192 seconds)
2023-05-20 00:12:28 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2023-05-20 00:12:28 | INFO | train | epoch 104 | loss 6.774 | nll_loss 5.046 | ppl 33.03 | wps 3008 | ups 0.25 | wpb 12055.4 | bsz 556.5 | num_updates 6739 | lr 1.66378e-05 | gnorm 5.529 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.4 | wall 27314
2023-05-20 00:12:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:12:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:12:28 | INFO | fairseq.trainer | begin training epoch 105
2023-05-20 00:12:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:12:30 | INFO | train_inner | epoch 105:      1 / 65 loss=6.976, nll_loss=5.258, ppl=38.27, wps=789.3, ups=0.08, wpb=10452.6, bsz=524.2, num_updates=6740, lr=1.66327e-05, gnorm=8.605, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27316
2023-05-20 00:12:46 | INFO | train_inner | epoch 105:      6 / 65 loss=6.805, nll_loss=5.078, ppl=33.78, wps=3875.5, ups=0.32, wpb=12035, bsz=577.2, num_updates=6745, lr=1.66071e-05, gnorm=9.237, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=27332
2023-05-20 00:13:02 | INFO | train_inner | epoch 105:     11 / 65 loss=6.722, nll_loss=4.99, ppl=31.77, wps=3882.3, ups=0.32, wpb=12222, bsz=536.4, num_updates=6750, lr=1.65816e-05, gnorm=4.993, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27347
2023-05-20 00:13:17 | INFO | train_inner | epoch 105:     16 / 65 loss=6.711, nll_loss=4.975, ppl=31.46, wps=3747.2, ups=0.32, wpb=11744.4, bsz=543.6, num_updates=6755, lr=1.65561e-05, gnorm=5.165, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=27363
2023-05-20 00:13:33 | INFO | train_inner | epoch 105:     21 / 65 loss=6.914, nll_loss=5.194, ppl=36.62, wps=4018.9, ups=0.32, wpb=12595.6, bsz=643, num_updates=6760, lr=1.65306e-05, gnorm=27.331, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=27379
2023-05-20 00:13:50 | INFO | train_inner | epoch 105:     26 / 65 loss=6.908, nll_loss=5.198, ppl=36.7, wps=3551.6, ups=0.29, wpb=12315.4, bsz=613.2, num_updates=6765, lr=1.65051e-05, gnorm=3.125, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=27396
2023-05-20 00:14:07 | INFO | train_inner | epoch 105:     31 / 65 loss=6.773, nll_loss=5.043, ppl=32.96, wps=3472, ups=0.29, wpb=11775.4, bsz=512.8, num_updates=6770, lr=1.64796e-05, gnorm=7.259, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=27413
2023-05-20 00:14:23 | INFO | train_inner | epoch 105:     36 / 65 loss=6.797, nll_loss=5.068, ppl=33.55, wps=3920, ups=0.32, wpb=12266.8, bsz=554.2, num_updates=6775, lr=1.64541e-05, gnorm=4.521, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=27429
2023-05-20 00:14:41 | INFO | train_inner | epoch 105:     41 / 65 loss=6.688, nll_loss=4.946, ppl=30.83, wps=3541.2, ups=0.29, wpb=12377, bsz=530.6, num_updates=6780, lr=1.64286e-05, gnorm=2.689, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=27446
2023-05-20 00:14:56 | INFO | train_inner | epoch 105:     46 / 65 loss=6.694, nll_loss=4.954, ppl=31, wps=4064.1, ups=0.33, wpb=12233.4, bsz=578.8, num_updates=6785, lr=1.64031e-05, gnorm=4.454, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.1, wall=27461
2023-05-20 00:15:10 | INFO | train_inner | epoch 105:     51 / 65 loss=6.734, nll_loss=5.002, ppl=32.03, wps=4190.4, ups=0.34, wpb=12325, bsz=535.6, num_updates=6790, lr=1.63776e-05, gnorm=6.317, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=27476
2023-05-20 00:15:26 | INFO | train_inner | epoch 105:     56 / 65 loss=6.73, nll_loss=5.003, ppl=32.06, wps=3949.5, ups=0.32, wpb=12216.8, bsz=600.8, num_updates=6795, lr=1.6352e-05, gnorm=2.435, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=27491
2023-05-20 00:15:42 | INFO | train_inner | epoch 105:     61 / 65 loss=6.718, nll_loss=4.979, ppl=31.53, wps=3684.7, ups=0.31, wpb=11726.6, bsz=489, num_updates=6800, lr=1.63265e-05, gnorm=5.42, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27507
2023-05-20 00:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:15:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:16:33 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 6.129 | nll_loss 4.243 | ppl 18.94 | bleu 5.47 | wps 888.8 | wpb 2785 | bsz 105.2 | num_updates 6804 | best_loss 6.102
2023-05-20 00:16:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 6804 updates
2023-05-20 00:16:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint105.pt
2023-05-20 00:16:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint105.pt
2023-05-20 00:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint105.pt (epoch 105 @ 6804 updates, score 6.129) (writing took 10.779741529375315 seconds)
2023-05-20 00:16:44 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2023-05-20 00:16:44 | INFO | train | epoch 105 | loss 6.779 | nll_loss 5.05 | ppl 33.13 | wps 3063.3 | ups 0.25 | wpb 12046.5 | bsz 557.2 | num_updates 6804 | lr 1.63061e-05 | gnorm 6.677 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 20.6 | wall 27569
2023-05-20 00:16:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:16:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:16:44 | INFO | fairseq.trainer | begin training epoch 106
2023-05-20 00:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:16:48 | INFO | train_inner | epoch 106:      1 / 65 loss=6.715, nll_loss=4.978, ppl=31.52, wps=798.8, ups=0.08, wpb=10555.2, bsz=450, num_updates=6805, lr=1.6301e-05, gnorm=4.365, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.1, wall=27573
2023-05-20 00:17:06 | INFO | train_inner | epoch 106:      6 / 65 loss=6.654, nll_loss=4.903, ppl=29.91, wps=3478.2, ups=0.28, wpb=12504.8, bsz=560.6, num_updates=6810, lr=1.62755e-05, gnorm=8.004, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=27591
2023-05-20 00:17:22 | INFO | train_inner | epoch 106:     11 / 65 loss=6.699, nll_loss=4.958, ppl=31.08, wps=3762, ups=0.31, wpb=12039.8, bsz=501.2, num_updates=6815, lr=1.625e-05, gnorm=9.693, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27607
2023-05-20 00:17:37 | INFO | train_inner | epoch 106:     16 / 65 loss=6.84, nll_loss=5.121, ppl=34.79, wps=3914.5, ups=0.32, wpb=12216, bsz=574, num_updates=6820, lr=1.62245e-05, gnorm=4.911, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27623
2023-05-20 00:17:53 | INFO | train_inner | epoch 106:     21 / 65 loss=6.699, nll_loss=4.963, ppl=31.19, wps=3830.9, ups=0.32, wpb=11875.6, bsz=478.6, num_updates=6825, lr=1.6199e-05, gnorm=12.081, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=27639
2023-05-20 00:18:08 | INFO | train_inner | epoch 106:     26 / 65 loss=6.928, nll_loss=5.206, ppl=36.92, wps=4105.8, ups=0.33, wpb=12407.2, bsz=633.4, num_updates=6830, lr=1.61735e-05, gnorm=5.025, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=27654
2023-05-20 00:18:23 | INFO | train_inner | epoch 106:     31 / 65 loss=6.816, nll_loss=5.099, ppl=34.27, wps=4035.7, ups=0.33, wpb=12308.4, bsz=644, num_updates=6835, lr=1.6148e-05, gnorm=5.406, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=27669
2023-05-20 00:18:40 | INFO | train_inner | epoch 106:     36 / 65 loss=6.776, nll_loss=5.052, ppl=33.17, wps=3504.4, ups=0.29, wpb=12093.8, bsz=555, num_updates=6840, lr=1.61224e-05, gnorm=7.615, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=27686
2023-05-20 00:18:56 | INFO | train_inner | epoch 106:     41 / 65 loss=6.701, nll_loss=4.965, ppl=31.24, wps=3970.6, ups=0.33, wpb=11988.4, bsz=575.2, num_updates=6845, lr=1.60969e-05, gnorm=4.275, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=27701
2023-05-20 00:19:12 | INFO | train_inner | epoch 106:     46 / 65 loss=6.758, nll_loss=5.029, ppl=32.65, wps=3709, ups=0.31, wpb=12103.6, bsz=546.6, num_updates=6850, lr=1.60714e-05, gnorm=3.449, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27718
2023-05-20 00:19:28 | INFO | train_inner | epoch 106:     51 / 65 loss=6.859, nll_loss=5.134, ppl=35.12, wps=3918.8, ups=0.31, wpb=12593.2, bsz=651.4, num_updates=6855, lr=1.60459e-05, gnorm=6.175, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27734
2023-05-20 00:19:44 | INFO | train_inner | epoch 106:     56 / 65 loss=6.683, nll_loss=4.939, ppl=30.67, wps=3710.9, ups=0.32, wpb=11733.2, bsz=492.2, num_updates=6860, lr=1.60204e-05, gnorm=3.087, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=27749
2023-05-20 00:20:00 | INFO | train_inner | epoch 106:     61 / 65 loss=6.888, nll_loss=5.172, ppl=36.06, wps=3740.9, ups=0.31, wpb=12191, bsz=618.4, num_updates=6865, lr=1.59949e-05, gnorm=5.617, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=27766
2023-05-20 00:20:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:20:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:20:49 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 6.114 | nll_loss 4.23 | ppl 18.76 | bleu 5.32 | wps 904.7 | wpb 2785 | bsz 105.2 | num_updates 6869 | best_loss 6.102
2023-05-20 00:20:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 6869 updates
2023-05-20 00:20:49 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint106.pt
2023-05-20 00:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint106.pt
2023-05-20 00:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint106.pt (epoch 106 @ 6869 updates, score 6.114) (writing took 9.636361744254827 seconds)
2023-05-20 00:20:59 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2023-05-20 00:20:59 | INFO | train | epoch 106 | loss 6.773 | nll_loss 5.042 | ppl 32.95 | wps 3069.5 | ups 0.25 | wpb 12053.2 | bsz 559.5 | num_updates 6869 | lr 1.59745e-05 | gnorm 6.184 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.6 | wall 27825
2023-05-20 00:20:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:20:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:20:59 | INFO | fairseq.trainer | begin training epoch 107
2023-05-20 00:20:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:21:02 | INFO | train_inner | epoch 107:      1 / 65 loss=6.701, nll_loss=4.958, ppl=31.07, wps=857.9, ups=0.08, wpb=10664.8, bsz=443.8, num_updates=6870, lr=1.59694e-05, gnorm=4.659, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.1, wall=27828
2023-05-20 00:21:17 | INFO | train_inner | epoch 107:      6 / 65 loss=6.729, nll_loss=4.992, ppl=31.82, wps=4052.6, ups=0.34, wpb=12091.4, bsz=545, num_updates=6875, lr=1.59439e-05, gnorm=4.812, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=27843
2023-05-20 00:21:34 | INFO | train_inner | epoch 107:     11 / 65 loss=6.765, nll_loss=5.039, ppl=32.87, wps=3627.9, ups=0.3, wpb=12067.6, bsz=583, num_updates=6880, lr=1.59184e-05, gnorm=12.794, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=27859
2023-05-20 00:21:50 | INFO | train_inner | epoch 107:     16 / 65 loss=6.799, nll_loss=5.071, ppl=33.62, wps=3621.2, ups=0.3, wpb=12103.8, bsz=589.8, num_updates=6885, lr=1.58929e-05, gnorm=6.144, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=27876
2023-05-20 00:22:07 | INFO | train_inner | epoch 107:     21 / 65 loss=6.695, nll_loss=4.958, ppl=31.08, wps=3596, ups=0.29, wpb=12239.6, bsz=491.8, num_updates=6890, lr=1.58673e-05, gnorm=32.895, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=27893
2023-05-20 00:22:23 | INFO | train_inner | epoch 107:     26 / 65 loss=6.741, nll_loss=5.002, ppl=32.03, wps=3917.8, ups=0.32, wpb=12345, bsz=581.2, num_updates=6895, lr=1.58418e-05, gnorm=4.537, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27909
2023-05-20 00:22:39 | INFO | train_inner | epoch 107:     31 / 65 loss=6.934, nll_loss=5.213, ppl=37.1, wps=3948, ups=0.31, wpb=12667.4, bsz=647, num_updates=6900, lr=1.58163e-05, gnorm=25.085, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=27925
2023-05-20 00:22:55 | INFO | train_inner | epoch 107:     36 / 65 loss=6.832, nll_loss=5.107, ppl=34.46, wps=3978, ups=0.33, wpb=12195, bsz=613.4, num_updates=6905, lr=1.57908e-05, gnorm=4.293, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=27940
2023-05-20 00:23:11 | INFO | train_inner | epoch 107:     41 / 65 loss=6.717, nll_loss=4.977, ppl=31.5, wps=3561.4, ups=0.3, wpb=11782.2, bsz=514.2, num_updates=6910, lr=1.57653e-05, gnorm=3.556, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=27957
2023-05-20 00:23:28 | INFO | train_inner | epoch 107:     46 / 65 loss=6.861, nll_loss=5.136, ppl=35.16, wps=3562, ups=0.3, wpb=11997.6, bsz=552.6, num_updates=6915, lr=1.57398e-05, gnorm=5.521, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.8, wall=27974
2023-05-20 00:23:44 | INFO | train_inner | epoch 107:     51 / 65 loss=6.686, nll_loss=4.947, ppl=30.85, wps=3801.7, ups=0.32, wpb=12007.2, bsz=524.6, num_updates=6920, lr=1.57143e-05, gnorm=6.182, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=27989
2023-05-20 00:24:00 | INFO | train_inner | epoch 107:     56 / 65 loss=6.773, nll_loss=5.05, ppl=33.12, wps=3857.5, ups=0.3, wpb=12757.6, bsz=654.6, num_updates=6925, lr=1.56888e-05, gnorm=3.184, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.2, wall=28006
2023-05-20 00:24:16 | INFO | train_inner | epoch 107:     61 / 65 loss=6.666, nll_loss=4.92, ppl=30.27, wps=3838.1, ups=0.32, wpb=11913.6, bsz=512.4, num_updates=6930, lr=1.56633e-05, gnorm=3.145, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=28022
2023-05-20 00:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:24:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:25:08 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 6.102 | nll_loss 4.216 | ppl 18.58 | bleu 5.53 | wps 857.2 | wpb 2785 | bsz 105.2 | num_updates 6934 | best_loss 6.102
2023-05-20 00:25:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 6934 updates
2023-05-20 00:25:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint107.pt
2023-05-20 00:25:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint107.pt
2023-05-20 00:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint107.pt (epoch 107 @ 6934 updates, score 6.102) (writing took 25.732218828052282 seconds)
2023-05-20 00:25:34 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2023-05-20 00:25:34 | INFO | train | epoch 107 | loss 6.765 | nll_loss 5.033 | ppl 32.73 | wps 2844.5 | ups 0.24 | wpb 12051.7 | bsz 558.6 | num_updates 6934 | lr 1.56429e-05 | gnorm 9.106 | clip 100 | loss_scale 0.125 | train_wall 208 | gb_free 21.8 | wall 28100
2023-05-20 00:25:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:25:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:25:34 | INFO | fairseq.trainer | begin training epoch 108
2023-05-20 00:25:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:25:37 | INFO | train_inner | epoch 108:      1 / 65 loss=6.733, nll_loss=5.003, ppl=32.06, wps=647.7, ups=0.06, wpb=10532.2, bsz=453.4, num_updates=6935, lr=1.56378e-05, gnorm=6.284, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=28103
2023-05-20 00:25:53 | INFO | train_inner | epoch 108:      6 / 65 loss=6.771, nll_loss=5.031, ppl=32.7, wps=3769.8, ups=0.32, wpb=11875.2, bsz=564, num_updates=6940, lr=1.56122e-05, gnorm=7.654, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=28119
2023-05-20 00:26:09 | INFO | train_inner | epoch 108:     11 / 65 loss=6.665, nll_loss=4.918, ppl=30.23, wps=3718.9, ups=0.32, wpb=11797, bsz=480.2, num_updates=6945, lr=1.55867e-05, gnorm=8.145, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=28134
2023-05-20 00:26:24 | INFO | train_inner | epoch 108:     16 / 65 loss=6.719, nll_loss=4.983, ppl=31.62, wps=4051.3, ups=0.33, wpb=12295.2, bsz=578.2, num_updates=6950, lr=1.55612e-05, gnorm=3.126, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=28150
2023-05-20 00:26:39 | INFO | train_inner | epoch 108:     21 / 65 loss=6.797, nll_loss=5.067, ppl=33.52, wps=4174.5, ups=0.34, wpb=12248.2, bsz=548.2, num_updates=6955, lr=1.55357e-05, gnorm=4.347, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28164
2023-05-20 00:26:54 | INFO | train_inner | epoch 108:     26 / 65 loss=6.693, nll_loss=4.951, ppl=30.93, wps=3977.8, ups=0.33, wpb=12143.8, bsz=542, num_updates=6960, lr=1.55102e-05, gnorm=3.077, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28180
2023-05-20 00:27:10 | INFO | train_inner | epoch 108:     31 / 65 loss=6.707, nll_loss=4.963, ppl=31.18, wps=3868.6, ups=0.32, wpb=12215.8, bsz=557.2, num_updates=6965, lr=1.54847e-05, gnorm=4.469, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=28195
2023-05-20 00:27:25 | INFO | train_inner | epoch 108:     36 / 65 loss=6.742, nll_loss=5.011, ppl=32.24, wps=3891.8, ups=0.32, wpb=12297.2, bsz=600.4, num_updates=6970, lr=1.54592e-05, gnorm=5.538, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=28211
2023-05-20 00:27:42 | INFO | train_inner | epoch 108:     41 / 65 loss=6.824, nll_loss=5.101, ppl=34.31, wps=3559.4, ups=0.29, wpb=12132, bsz=598.6, num_updates=6975, lr=1.54337e-05, gnorm=4.517, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=28228
2023-05-20 00:27:59 | INFO | train_inner | epoch 108:     46 / 65 loss=6.767, nll_loss=5.042, ppl=32.95, wps=3699.1, ups=0.31, wpb=12027.2, bsz=538.8, num_updates=6980, lr=1.54082e-05, gnorm=5.764, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=28244
2023-05-20 00:28:16 | INFO | train_inner | epoch 108:     51 / 65 loss=6.732, nll_loss=4.998, ppl=31.96, wps=3575.4, ups=0.29, wpb=12185.6, bsz=520.8, num_updates=6985, lr=1.53827e-05, gnorm=6.186, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.2, wall=28262
2023-05-20 00:28:32 | INFO | train_inner | epoch 108:     56 / 65 loss=6.734, nll_loss=5.001, ppl=32.03, wps=3715.8, ups=0.31, wpb=12094, bsz=536.8, num_updates=6990, lr=1.53571e-05, gnorm=8.312, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=28278
2023-05-20 00:28:49 | INFO | train_inner | epoch 108:     61 / 65 loss=6.852, nll_loss=5.137, ppl=35.19, wps=3725.8, ups=0.3, wpb=12433.8, bsz=620.6, num_updates=6995, lr=1.53316e-05, gnorm=5.784, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=28294
2023-05-20 00:28:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:28:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:29:41 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 6.129 | nll_loss 4.245 | ppl 18.96 | bleu 5.25 | wps 830.4 | wpb 2785 | bsz 105.2 | num_updates 6999 | best_loss 6.102
2023-05-20 00:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 6999 updates
2023-05-20 00:29:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint108.pt
2023-05-20 00:29:46 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint108.pt
2023-05-20 00:29:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint108.pt (epoch 108 @ 6999 updates, score 6.129) (writing took 8.534842565655708 seconds)
2023-05-20 00:29:50 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2023-05-20 00:29:50 | INFO | train | epoch 108 | loss 6.77 | nll_loss 5.038 | ppl 32.85 | wps 3064.9 | ups 0.25 | wpb 12051.6 | bsz 559 | num_updates 6999 | lr 1.53112e-05 | gnorm 5.553 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.8 | wall 28356
2023-05-20 00:29:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:29:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:29:50 | INFO | fairseq.trainer | begin training epoch 109
2023-05-20 00:29:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:29:52 | INFO | train_inner | epoch 109:      1 / 65 loss=7.063, nll_loss=5.351, ppl=40.82, wps=851.7, ups=0.08, wpb=10849, bsz=604.8, num_updates=7000, lr=1.53061e-05, gnorm=4.946, clip=100, loss_scale=0.125, train_wall=12, gb_free=21.8, wall=28358
2023-05-20 00:30:08 | INFO | train_inner | epoch 109:      6 / 65 loss=6.702, nll_loss=4.961, ppl=31.16, wps=3862.2, ups=0.32, wpb=12258, bsz=530, num_updates=7005, lr=1.52806e-05, gnorm=3.371, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=28374
2023-05-20 00:30:24 | INFO | train_inner | epoch 109:     11 / 65 loss=6.81, nll_loss=5.084, ppl=33.92, wps=3916.7, ups=0.32, wpb=12214.8, bsz=631.2, num_updates=7010, lr=1.52551e-05, gnorm=2.478, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=28390
2023-05-20 00:30:40 | INFO | train_inner | epoch 109:     16 / 65 loss=6.708, nll_loss=4.97, ppl=31.34, wps=3905.3, ups=0.31, wpb=12681.2, bsz=592, num_updates=7015, lr=1.52296e-05, gnorm=5.106, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=28406
2023-05-20 00:30:55 | INFO | train_inner | epoch 109:     21 / 65 loss=6.726, nll_loss=4.982, ppl=31.61, wps=4055.2, ups=0.33, wpb=12451, bsz=606.4, num_updates=7020, lr=1.52041e-05, gnorm=3.464, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=28421
2023-05-20 00:31:14 | INFO | train_inner | epoch 109:     26 / 65 loss=6.762, nll_loss=5.033, ppl=32.74, wps=3376.2, ups=0.27, wpb=12358.4, bsz=606, num_updates=7025, lr=1.51786e-05, gnorm=4.075, clip=100, loss_scale=0.125, train_wall=18, gb_free=20.4, wall=28440
2023-05-20 00:31:29 | INFO | train_inner | epoch 109:     31 / 65 loss=6.725, nll_loss=4.989, ppl=31.75, wps=3946.4, ups=0.32, wpb=12290.6, bsz=531.2, num_updates=7030, lr=1.51531e-05, gnorm=3.029, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=28455
2023-05-20 00:31:48 | INFO | train_inner | epoch 109:     36 / 65 loss=6.744, nll_loss=5.005, ppl=32.12, wps=3250.8, ups=0.27, wpb=11827.2, bsz=503.4, num_updates=7035, lr=1.51276e-05, gnorm=6.246, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=28473
2023-05-20 00:32:03 | INFO | train_inner | epoch 109:     41 / 65 loss=6.767, nll_loss=5.032, ppl=32.71, wps=3896.6, ups=0.33, wpb=11948.4, bsz=545.4, num_updates=7040, lr=1.5102e-05, gnorm=7.392, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=28489
2023-05-20 00:32:20 | INFO | train_inner | epoch 109:     46 / 65 loss=6.827, nll_loss=5.101, ppl=34.33, wps=3290.2, ups=0.28, wpb=11591.4, bsz=556.6, num_updates=7045, lr=1.50765e-05, gnorm=5.613, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=28506
2023-05-20 00:32:35 | INFO | train_inner | epoch 109:     51 / 65 loss=6.728, nll_loss=4.993, ppl=31.84, wps=4095.3, ups=0.33, wpb=12276, bsz=545.6, num_updates=7050, lr=1.5051e-05, gnorm=6.629, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=28521
2023-05-20 00:32:52 | INFO | train_inner | epoch 109:     56 / 65 loss=6.75, nll_loss=5.022, ppl=32.48, wps=3818.6, ups=0.31, wpb=12281.2, bsz=553, num_updates=7055, lr=1.50255e-05, gnorm=5.553, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=28537
2023-05-20 00:33:06 | INFO | train_inner | epoch 109:     61 / 65 loss=6.916, nll_loss=5.194, ppl=36.6, wps=4262.8, ups=0.34, wpb=12522.4, bsz=650.6, num_updates=7060, lr=1.5e-05, gnorm=2.932, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28552
2023-05-20 00:33:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:33:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:33:57 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 6.072 | nll_loss 4.183 | ppl 18.17 | bleu 5.14 | wps 890.7 | wpb 2785 | bsz 105.2 | num_updates 7064 | best_loss 6.072
2023-05-20 00:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 7064 updates
2023-05-20 00:33:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint109.pt
2023-05-20 00:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint109.pt
2023-05-20 00:34:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint109.pt (epoch 109 @ 7064 updates, score 6.072) (writing took 28.968252323567867 seconds)
2023-05-20 00:34:26 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2023-05-20 00:34:26 | INFO | train | epoch 109 | loss 6.762 | nll_loss 5.029 | ppl 32.64 | wps 2833.6 | ups 0.24 | wpb 12056.8 | bsz 560.1 | num_updates 7064 | lr 1.49796e-05 | gnorm 4.915 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.5 | wall 28632
2023-05-20 00:34:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:34:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:34:26 | INFO | fairseq.trainer | begin training epoch 110
2023-05-20 00:34:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:34:30 | INFO | train_inner | epoch 110:      1 / 65 loss=6.704, nll_loss=4.968, ppl=31.3, wps=598.5, ups=0.06, wpb=9985.2, bsz=405.6, num_updates=7065, lr=1.49745e-05, gnorm=8.875, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.5, wall=28635
2023-05-20 00:34:45 | INFO | train_inner | epoch 110:      6 / 65 loss=6.711, nll_loss=4.973, ppl=31.41, wps=3785, ups=0.32, wpb=11894.2, bsz=559, num_updates=7070, lr=1.4949e-05, gnorm=9.146, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=28651
2023-05-20 00:35:01 | INFO | train_inner | epoch 110:     11 / 65 loss=6.671, nll_loss=4.922, ppl=30.31, wps=3618.7, ups=0.31, wpb=11526.8, bsz=474, num_updates=7075, lr=1.49235e-05, gnorm=4.214, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=28667
2023-05-20 00:35:17 | INFO | train_inner | epoch 110:     16 / 65 loss=6.897, nll_loss=5.171, ppl=36.02, wps=4020.3, ups=0.32, wpb=12472.2, bsz=646.4, num_updates=7080, lr=1.4898e-05, gnorm=15.814, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28683
2023-05-20 00:35:32 | INFO | train_inner | epoch 110:     21 / 65 loss=6.726, nll_loss=4.99, ppl=31.77, wps=4042.9, ups=0.33, wpb=12152.4, bsz=536.8, num_updates=7085, lr=1.48724e-05, gnorm=2.61, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=28698
2023-05-20 00:35:47 | INFO | train_inner | epoch 110:     26 / 65 loss=6.688, nll_loss=4.945, ppl=30.8, wps=4159.7, ups=0.34, wpb=12246.8, bsz=509.2, num_updates=7090, lr=1.48469e-05, gnorm=3.935, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=28712
2023-05-20 00:36:04 | INFO | train_inner | epoch 110:     31 / 65 loss=6.705, nll_loss=4.968, ppl=31.29, wps=3402.9, ups=0.28, wpb=12084.2, bsz=546.8, num_updates=7095, lr=1.48214e-05, gnorm=3.419, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=28730
2023-05-20 00:36:21 | INFO | train_inner | epoch 110:     36 / 65 loss=6.939, nll_loss=5.221, ppl=37.3, wps=3559.2, ups=0.3, wpb=11924.4, bsz=592.6, num_updates=7100, lr=1.47959e-05, gnorm=3.216, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=28747
2023-05-20 00:36:37 | INFO | train_inner | epoch 110:     41 / 65 loss=6.75, nll_loss=5.018, ppl=32.39, wps=3971, ups=0.32, wpb=12501, bsz=572.8, num_updates=7105, lr=1.47704e-05, gnorm=3.198, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=28763
2023-05-20 00:36:51 | INFO | train_inner | epoch 110:     46 / 65 loss=6.671, nll_loss=4.928, ppl=30.43, wps=4567.6, ups=0.35, wpb=13002.6, bsz=649.6, num_updates=7110, lr=1.47449e-05, gnorm=4.114, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=28777
2023-05-20 00:37:06 | INFO | train_inner | epoch 110:     51 / 65 loss=6.779, nll_loss=5.052, ppl=33.18, wps=4175.8, ups=0.34, wpb=12388.8, bsz=569, num_updates=7115, lr=1.47194e-05, gnorm=7.885, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=28792
2023-05-20 00:37:21 | INFO | train_inner | epoch 110:     56 / 65 loss=6.738, nll_loss=5.006, ppl=32.13, wps=3895.5, ups=0.32, wpb=12060, bsz=575, num_updates=7120, lr=1.46939e-05, gnorm=3.94, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28807
2023-05-20 00:37:39 | INFO | train_inner | epoch 110:     61 / 65 loss=6.78, nll_loss=5.052, ppl=33.18, wps=3482.5, ups=0.28, wpb=12236.6, bsz=573.6, num_updates=7125, lr=1.46684e-05, gnorm=14.055, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=28825
2023-05-20 00:37:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:37:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:38:29 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 6.095 | nll_loss 4.202 | ppl 18.4 | bleu 5.12 | wps 897.1 | wpb 2785 | bsz 105.2 | num_updates 7129 | best_loss 6.072
2023-05-20 00:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 7129 updates
2023-05-20 00:38:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint110.pt
2023-05-20 00:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint110.pt
2023-05-20 00:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint110.pt (epoch 110 @ 7129 updates, score 6.095) (writing took 6.676454737782478 seconds)
2023-05-20 00:38:35 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2023-05-20 00:38:35 | INFO | train | epoch 110 | loss 6.754 | nll_loss 5.02 | ppl 32.45 | wps 3148.4 | ups 0.26 | wpb 12055.4 | bsz 559.6 | num_updates 7129 | lr 1.4648e-05 | gnorm 6.646 | clip 100 | loss_scale 0.125 | train_wall 202 | gb_free 21.7 | wall 28881
2023-05-20 00:38:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:38:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:38:35 | INFO | fairseq.trainer | begin training epoch 111
2023-05-20 00:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:38:38 | INFO | train_inner | epoch 111:      1 / 65 loss=6.779, nll_loss=5.05, ppl=33.14, wps=887.5, ups=0.09, wpb=10403.8, bsz=476.4, num_updates=7130, lr=1.46429e-05, gnorm=11.452, clip=100, loss_scale=0.125, train_wall=12, gb_free=20.6, wall=28883
2023-05-20 00:38:52 | INFO | train_inner | epoch 111:      6 / 65 loss=6.704, nll_loss=4.963, ppl=31.19, wps=4122.2, ups=0.34, wpb=12191, bsz=552.6, num_updates=7135, lr=1.46173e-05, gnorm=2.601, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28898
2023-05-20 00:39:08 | INFO | train_inner | epoch 111:     11 / 65 loss=6.835, nll_loss=5.104, ppl=34.38, wps=3906.3, ups=0.32, wpb=12193.4, bsz=592.8, num_updates=7140, lr=1.45918e-05, gnorm=4.815, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=28914
2023-05-20 00:39:25 | INFO | train_inner | epoch 111:     16 / 65 loss=6.739, nll_loss=5.003, ppl=32.06, wps=3673.7, ups=0.3, wpb=12216.4, bsz=555.2, num_updates=7145, lr=1.45663e-05, gnorm=3.125, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.6, wall=28931
2023-05-20 00:39:40 | INFO | train_inner | epoch 111:     21 / 65 loss=6.675, nll_loss=4.93, ppl=30.49, wps=3969.6, ups=0.32, wpb=12241.2, bsz=575, num_updates=7150, lr=1.45408e-05, gnorm=7.544, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=28946
2023-05-20 00:39:57 | INFO | train_inner | epoch 111:     26 / 65 loss=6.747, nll_loss=5.013, ppl=32.28, wps=3638.7, ups=0.3, wpb=12082.4, bsz=570.8, num_updates=7155, lr=1.45153e-05, gnorm=3.297, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=28963
2023-05-20 00:40:13 | INFO | train_inner | epoch 111:     31 / 65 loss=6.701, nll_loss=4.955, ppl=31.02, wps=3768.6, ups=0.3, wpb=12494.4, bsz=550.2, num_updates=7160, lr=1.44898e-05, gnorm=4.758, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=28979
2023-05-20 00:40:30 | INFO | train_inner | epoch 111:     36 / 65 loss=6.715, nll_loss=4.97, ppl=31.33, wps=3568.6, ups=0.3, wpb=11780.6, bsz=485.2, num_updates=7165, lr=1.44643e-05, gnorm=13.083, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=28996
2023-05-20 00:40:46 | INFO | train_inner | epoch 111:     41 / 65 loss=6.807, nll_loss=5.076, ppl=33.74, wps=3834.9, ups=0.32, wpb=12126, bsz=542.2, num_updates=7170, lr=1.44388e-05, gnorm=20.835, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=29012
2023-05-20 00:41:02 | INFO | train_inner | epoch 111:     46 / 65 loss=6.772, nll_loss=5.046, ppl=33.04, wps=3834.2, ups=0.32, wpb=12163.8, bsz=552.4, num_updates=7175, lr=1.44133e-05, gnorm=4.76, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.2, wall=29027
2023-05-20 00:41:19 | INFO | train_inner | epoch 111:     51 / 65 loss=6.763, nll_loss=5.032, ppl=32.72, wps=3421.6, ups=0.29, wpb=11947.8, bsz=577.8, num_updates=7180, lr=1.43878e-05, gnorm=6.599, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.4, wall=29045
2023-05-20 00:41:35 | INFO | train_inner | epoch 111:     56 / 65 loss=6.766, nll_loss=5.039, ppl=32.87, wps=3897.3, ups=0.32, wpb=12231.2, bsz=566.6, num_updates=7185, lr=1.43622e-05, gnorm=4.539, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=29061
2023-05-20 00:41:50 | INFO | train_inner | epoch 111:     61 / 65 loss=6.905, nll_loss=5.178, ppl=36.21, wps=3909.6, ups=0.32, wpb=12211.4, bsz=658.6, num_updates=7190, lr=1.43367e-05, gnorm=6.641, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=29076
2023-05-20 00:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:42:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:42:39 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 6.111 | nll_loss 4.225 | ppl 18.7 | bleu 5.47 | wps 951.5 | wpb 2785 | bsz 105.2 | num_updates 7194 | best_loss 6.072
2023-05-20 00:42:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 7194 updates
2023-05-20 00:42:39 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint111.pt
2023-05-20 00:42:43 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint111.pt
2023-05-20 00:42:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint111.pt (epoch 111 @ 7194 updates, score 6.111) (writing took 5.899184200912714 seconds)
2023-05-20 00:42:45 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2023-05-20 00:42:45 | INFO | train | epoch 111 | loss 6.762 | nll_loss 5.027 | ppl 32.61 | wps 3131.6 | ups 0.26 | wpb 12048.4 | bsz 559 | num_updates 7194 | lr 1.43163e-05 | gnorm 6.759 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.4 | wall 29131
2023-05-20 00:42:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:42:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:42:45 | INFO | fairseq.trainer | begin training epoch 112
2023-05-20 00:42:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:42:49 | INFO | train_inner | epoch 112:      1 / 65 loss=6.828, nll_loss=5.104, ppl=34.39, wps=921.2, ups=0.09, wpb=10741.6, bsz=554.4, num_updates=7195, lr=1.43112e-05, gnorm=3.955, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=29135
2023-05-20 00:43:05 | INFO | train_inner | epoch 112:      6 / 65 loss=6.798, nll_loss=5.072, ppl=33.64, wps=3688, ups=0.3, wpb=12132.4, bsz=579.6, num_updates=7200, lr=1.42857e-05, gnorm=4.868, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=29151
2023-05-20 00:43:20 | INFO | train_inner | epoch 112:     11 / 65 loss=6.67, nll_loss=4.924, ppl=30.37, wps=4152.8, ups=0.33, wpb=12567.2, bsz=563.8, num_updates=7205, lr=1.42602e-05, gnorm=3.527, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=29166
2023-05-20 00:43:36 | INFO | train_inner | epoch 112:     16 / 65 loss=6.946, nll_loss=5.225, ppl=37.39, wps=3907.9, ups=0.32, wpb=12133.2, bsz=572, num_updates=7210, lr=1.42347e-05, gnorm=4.802, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=29182
2023-05-20 00:43:51 | INFO | train_inner | epoch 112:     21 / 65 loss=6.751, nll_loss=5.017, ppl=32.39, wps=3863.7, ups=0.33, wpb=11685.4, bsz=547.8, num_updates=7215, lr=1.42092e-05, gnorm=10.141, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=29197
2023-05-20 00:44:07 | INFO | train_inner | epoch 112:     26 / 65 loss=6.735, nll_loss=5.003, ppl=32.07, wps=3735.5, ups=0.31, wpb=12032.2, bsz=532.2, num_updates=7220, lr=1.41837e-05, gnorm=7.279, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=29213
2023-05-20 00:44:23 | INFO | train_inner | epoch 112:     31 / 65 loss=6.726, nll_loss=4.99, ppl=31.78, wps=3801.9, ups=0.31, wpb=12102.4, bsz=544, num_updates=7225, lr=1.41582e-05, gnorm=7.154, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=29229
2023-05-20 00:44:39 | INFO | train_inner | epoch 112:     36 / 65 loss=6.87, nll_loss=5.147, ppl=35.43, wps=3726.9, ups=0.3, wpb=12264.2, bsz=631, num_updates=7230, lr=1.41327e-05, gnorm=5.496, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=29245
2023-05-20 00:44:55 | INFO | train_inner | epoch 112:     41 / 65 loss=6.695, nll_loss=4.95, ppl=30.92, wps=3677.8, ups=0.31, wpb=11787, bsz=483.2, num_updates=7235, lr=1.41071e-05, gnorm=8.43, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=29261
2023-05-20 00:45:13 | INFO | train_inner | epoch 112:     46 / 65 loss=6.661, nll_loss=4.916, ppl=30.18, wps=3561.7, ups=0.29, wpb=12177.2, bsz=492.4, num_updates=7240, lr=1.40816e-05, gnorm=4.539, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=29278
2023-05-20 00:45:27 | INFO | train_inner | epoch 112:     51 / 65 loss=6.682, nll_loss=4.94, ppl=30.69, wps=4273.1, ups=0.34, wpb=12549.6, bsz=645.4, num_updates=7245, lr=1.40561e-05, gnorm=3.338, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=29293
2023-05-20 00:45:42 | INFO | train_inner | epoch 112:     56 / 65 loss=6.758, nll_loss=5.026, ppl=32.58, wps=4228.3, ups=0.33, wpb=12622.4, bsz=590.8, num_updates=7250, lr=1.40306e-05, gnorm=3.351, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=29308
2023-05-20 00:45:59 | INFO | train_inner | epoch 112:     61 / 65 loss=6.743, nll_loss=5.005, ppl=32.12, wps=3755, ups=0.3, wpb=12326.2, bsz=538.2, num_updates=7255, lr=1.40051e-05, gnorm=4.482, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=29324
2023-05-20 00:46:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:46:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:46:53 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 6.154 | nll_loss 4.274 | ppl 19.35 | bleu 5.24 | wps 849.3 | wpb 2785 | bsz 105.2 | num_updates 7259 | best_loss 6.072
2023-05-20 00:46:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 7259 updates
2023-05-20 00:46:53 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint112.pt
2023-05-20 00:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint112.pt
2023-05-20 00:46:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint112.pt (epoch 112 @ 7259 updates, score 6.154) (writing took 5.740659989416599 seconds)
2023-05-20 00:46:59 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2023-05-20 00:46:59 | INFO | train | epoch 112 | loss 6.758 | nll_loss 5.024 | ppl 32.53 | wps 3096.1 | ups 0.26 | wpb 12061.6 | bsz 559.8 | num_updates 7259 | lr 1.39847e-05 | gnorm 5.558 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.9 | wall 29384
2023-05-20 00:46:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:46:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:46:59 | INFO | fairseq.trainer | begin training epoch 113
2023-05-20 00:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:47:02 | INFO | train_inner | epoch 113:      1 / 65 loss=6.761, nll_loss=5.026, ppl=32.58, wps=807.1, ups=0.08, wpb=10211.6, bsz=483.2, num_updates=7260, lr=1.39796e-05, gnorm=4.845, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=29388
2023-05-20 00:47:17 | INFO | train_inner | epoch 113:      6 / 65 loss=6.791, nll_loss=5.063, ppl=33.42, wps=3985.3, ups=0.33, wpb=12261.4, bsz=542.6, num_updates=7265, lr=1.39541e-05, gnorm=4.123, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=29403
2023-05-20 00:47:34 | INFO | train_inner | epoch 113:     11 / 65 loss=6.921, nll_loss=5.202, ppl=36.82, wps=3682.7, ups=0.31, wpb=12017.8, bsz=554.8, num_updates=7270, lr=1.39286e-05, gnorm=4.304, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29419
2023-05-20 00:47:47 | INFO | train_inner | epoch 113:     16 / 65 loss=6.759, nll_loss=5.03, ppl=32.68, wps=4395.6, ups=0.36, wpb=12166.8, bsz=569.4, num_updates=7275, lr=1.39031e-05, gnorm=8.758, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=29433
2023-05-20 00:48:04 | INFO | train_inner | epoch 113:     21 / 65 loss=6.775, nll_loss=5.052, ppl=33.18, wps=3741.5, ups=0.3, wpb=12331.6, bsz=554.2, num_updates=7280, lr=1.38776e-05, gnorm=6.144, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29450
2023-05-20 00:48:20 | INFO | train_inner | epoch 113:     26 / 65 loss=6.732, nll_loss=5, ppl=31.99, wps=3774.2, ups=0.3, wpb=12472.6, bsz=526.6, num_updates=7285, lr=1.3852e-05, gnorm=8.695, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.2, wall=29466
2023-05-20 00:48:36 | INFO | train_inner | epoch 113:     31 / 65 loss=6.722, nll_loss=4.992, ppl=31.82, wps=4016.2, ups=0.33, wpb=12278.4, bsz=576.2, num_updates=7290, lr=1.38265e-05, gnorm=7.615, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=29481
2023-05-20 00:48:52 | INFO | train_inner | epoch 113:     36 / 65 loss=6.755, nll_loss=5.027, ppl=32.61, wps=3786.6, ups=0.31, wpb=12034.6, bsz=593.4, num_updates=7295, lr=1.3801e-05, gnorm=6.284, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=29497
2023-05-20 00:49:08 | INFO | train_inner | epoch 113:     41 / 65 loss=6.757, nll_loss=5.032, ppl=32.73, wps=3698.5, ups=0.3, wpb=12234.6, bsz=639.2, num_updates=7300, lr=1.37755e-05, gnorm=6.466, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=29514
2023-05-20 00:49:24 | INFO | train_inner | epoch 113:     46 / 65 loss=6.765, nll_loss=5.037, ppl=32.83, wps=3940.5, ups=0.32, wpb=12204.6, bsz=559.2, num_updates=7305, lr=1.375e-05, gnorm=3.617, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=29529
2023-05-20 00:49:39 | INFO | train_inner | epoch 113:     51 / 65 loss=6.978, nll_loss=5.266, ppl=38.48, wps=3852.4, ups=0.32, wpb=12215, bsz=610.8, num_updates=7310, lr=1.37245e-05, gnorm=2.164, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=29545
2023-05-20 00:49:55 | INFO | train_inner | epoch 113:     56 / 65 loss=6.768, nll_loss=5.043, ppl=32.97, wps=3774.3, ups=0.31, wpb=12000.8, bsz=550.2, num_updates=7315, lr=1.3699e-05, gnorm=5.03, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=29561
2023-05-20 00:50:10 | INFO | train_inner | epoch 113:     61 / 65 loss=6.673, nll_loss=4.937, ppl=30.63, wps=4206.8, ups=0.34, wpb=12353.6, bsz=574.8, num_updates=7320, lr=1.36735e-05, gnorm=7.456, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=29576
2023-05-20 00:50:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:50:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:51:04 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 6.145 | nll_loss 4.27 | ppl 19.29 | bleu 5.27 | wps 855.7 | wpb 2785 | bsz 105.2 | num_updates 7324 | best_loss 6.072
2023-05-20 00:51:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 7324 updates
2023-05-20 00:51:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint113.pt
2023-05-20 00:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint113.pt
2023-05-20 00:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint113.pt (epoch 113 @ 7324 updates, score 6.145) (writing took 5.8196286633610725 seconds)
2023-05-20 00:51:10 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2023-05-20 00:51:10 | INFO | train | epoch 113 | loss 6.78 | nll_loss 5.054 | ppl 33.21 | wps 3112.9 | ups 0.26 | wpb 12054.6 | bsz 558.9 | num_updates 7324 | lr 1.36531e-05 | gnorm 5.796 | clip 100 | loss_scale 0.25 | train_wall 204 | gb_free 21.5 | wall 29636
2023-05-20 00:51:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:51:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:51:10 | INFO | fairseq.trainer | begin training epoch 114
2023-05-20 00:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:51:14 | INFO | train_inner | epoch 114:      1 / 65 loss=6.747, nll_loss=5.018, ppl=32.4, wps=805.9, ups=0.08, wpb=10247.4, bsz=408.2, num_updates=7325, lr=1.3648e-05, gnorm=4.72, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=29639
2023-05-20 00:51:29 | INFO | train_inner | epoch 114:      6 / 65 loss=6.786, nll_loss=5.066, ppl=33.5, wps=3869.1, ups=0.32, wpb=12167, bsz=587.8, num_updates=7330, lr=1.36224e-05, gnorm=3.963, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=29655
2023-05-20 00:51:45 | INFO | train_inner | epoch 114:     11 / 65 loss=6.669, nll_loss=4.927, ppl=30.42, wps=3823.9, ups=0.32, wpb=12047.4, bsz=504.8, num_updates=7335, lr=1.35969e-05, gnorm=4.173, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29671
2023-05-20 00:52:01 | INFO | train_inner | epoch 114:     16 / 65 loss=6.715, nll_loss=4.973, ppl=31.41, wps=3713.3, ups=0.31, wpb=12004.2, bsz=572.6, num_updates=7340, lr=1.35714e-05, gnorm=5.152, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29687
2023-05-20 00:52:18 | INFO | train_inner | epoch 114:     21 / 65 loss=6.708, nll_loss=4.97, ppl=31.34, wps=3727.6, ups=0.31, wpb=12152.4, bsz=535.2, num_updates=7345, lr=1.35459e-05, gnorm=8.078, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=29703
2023-05-20 00:52:37 | INFO | train_inner | epoch 114:     26 / 65 loss=6.834, nll_loss=5.107, ppl=34.47, wps=3172.6, ups=0.26, wpb=12088.6, bsz=561.8, num_updates=7350, lr=1.35204e-05, gnorm=4.094, clip=100, loss_scale=0.25, train_wall=19, gb_free=21.6, wall=29722
2023-05-20 00:52:53 | INFO | train_inner | epoch 114:     31 / 65 loss=6.765, nll_loss=5.036, ppl=32.81, wps=3758.9, ups=0.31, wpb=11993.8, bsz=554.8, num_updates=7355, lr=1.34949e-05, gnorm=3.964, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=29738
2023-05-20 00:53:09 | INFO | train_inner | epoch 114:     36 / 65 loss=6.776, nll_loss=5.047, ppl=33.05, wps=3752.1, ups=0.31, wpb=12215.8, bsz=572.6, num_updates=7360, lr=1.34694e-05, gnorm=6.114, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=29755
2023-05-20 00:53:25 | INFO | train_inner | epoch 114:     41 / 65 loss=6.837, nll_loss=5.111, ppl=34.57, wps=3833.1, ups=0.32, wpb=12154.6, bsz=536.6, num_updates=7365, lr=1.34439e-05, gnorm=6.451, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=29770
2023-05-20 00:53:42 | INFO | train_inner | epoch 114:     46 / 65 loss=6.667, nll_loss=4.925, ppl=30.39, wps=3617.8, ups=0.3, wpb=12154.2, bsz=488, num_updates=7370, lr=1.34184e-05, gnorm=7.724, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=29787
2023-05-20 00:53:57 | INFO | train_inner | epoch 114:     51 / 65 loss=6.766, nll_loss=5.042, ppl=32.94, wps=4133.6, ups=0.33, wpb=12568.2, bsz=546, num_updates=7375, lr=1.33929e-05, gnorm=5.244, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=29802
2023-05-20 00:54:12 | INFO | train_inner | epoch 114:     56 / 65 loss=6.935, nll_loss=5.219, ppl=37.25, wps=4018.7, ups=0.32, wpb=12478.2, bsz=682.8, num_updates=7380, lr=1.33673e-05, gnorm=5.42, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=29818
2023-05-20 00:54:28 | INFO | train_inner | epoch 114:     61 / 65 loss=6.713, nll_loss=4.984, ppl=31.64, wps=3864.5, ups=0.31, wpb=12339.8, bsz=644.4, num_updates=7385, lr=1.33418e-05, gnorm=4.569, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=29834
2023-05-20 00:54:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:54:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:55:19 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 6.103 | nll_loss 4.217 | ppl 18.6 | bleu 5.29 | wps 891.6 | wpb 2785 | bsz 105.2 | num_updates 7389 | best_loss 6.072
2023-05-20 00:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 7389 updates
2023-05-20 00:55:19 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint114.pt
2023-05-20 00:55:23 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint114.pt
2023-05-20 00:55:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint114.pt (epoch 114 @ 7389 updates, score 6.103) (writing took 5.869329657405615 seconds)
2023-05-20 00:55:25 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2023-05-20 00:55:25 | INFO | train | epoch 114 | loss 6.763 | nll_loss 5.033 | ppl 32.74 | wps 3080.5 | ups 0.26 | wpb 12052.3 | bsz 559.4 | num_updates 7389 | lr 1.33214e-05 | gnorm 5.237 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21.8 | wall 29890
2023-05-20 00:55:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:55:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:55:25 | INFO | fairseq.trainer | begin training epoch 115
2023-05-20 00:55:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:55:27 | INFO | train_inner | epoch 115:      1 / 65 loss=6.794, nll_loss=5.072, ppl=33.64, wps=872.5, ups=0.08, wpb=10305.6, bsz=532.6, num_updates=7390, lr=1.33163e-05, gnorm=3.166, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=29893
2023-05-20 00:55:43 | INFO | train_inner | epoch 115:      6 / 65 loss=6.877, nll_loss=5.153, ppl=35.57, wps=3882.5, ups=0.33, wpb=11865, bsz=592.8, num_updates=7395, lr=1.32908e-05, gnorm=7.11, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.8, wall=29908
2023-05-20 00:55:58 | INFO | train_inner | epoch 115:     11 / 65 loss=6.771, nll_loss=5.044, ppl=32.98, wps=3970.5, ups=0.32, wpb=12379.8, bsz=587, num_updates=7400, lr=1.32653e-05, gnorm=5.459, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=29924
2023-05-20 00:56:14 | INFO | train_inner | epoch 115:     16 / 65 loss=6.707, nll_loss=4.967, ppl=31.27, wps=3760.8, ups=0.31, wpb=12133.6, bsz=543.6, num_updates=7405, lr=1.32398e-05, gnorm=4.587, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=29940
2023-05-20 00:56:30 | INFO | train_inner | epoch 115:     21 / 65 loss=6.718, nll_loss=4.979, ppl=31.54, wps=3877.4, ups=0.31, wpb=12452.4, bsz=538.8, num_updates=7410, lr=1.32143e-05, gnorm=4.987, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29956
2023-05-20 00:56:46 | INFO | train_inner | epoch 115:     26 / 65 loss=6.676, nll_loss=4.934, ppl=30.58, wps=3924.6, ups=0.32, wpb=12332, bsz=557.2, num_updates=7415, lr=1.31888e-05, gnorm=7.051, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=29972
2023-05-20 00:57:02 | INFO | train_inner | epoch 115:     31 / 65 loss=6.672, nll_loss=4.93, ppl=30.49, wps=3819.9, ups=0.32, wpb=11927.2, bsz=551.4, num_updates=7420, lr=1.31633e-05, gnorm=3.587, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=29987
2023-05-20 00:57:18 | INFO | train_inner | epoch 115:     36 / 65 loss=6.678, nll_loss=4.932, ppl=30.52, wps=3773.2, ups=0.32, wpb=11959.8, bsz=486, num_updates=7425, lr=1.31378e-05, gnorm=3.042, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=30003
2023-05-20 00:57:35 | INFO | train_inner | epoch 115:     41 / 65 loss=6.731, nll_loss=4.997, ppl=31.93, wps=3681.3, ups=0.29, wpb=12561.4, bsz=568, num_updates=7430, lr=1.31122e-05, gnorm=2.912, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.1, wall=30020
2023-05-20 00:57:52 | INFO | train_inner | epoch 115:     46 / 65 loss=6.722, nll_loss=4.987, ppl=31.72, wps=3545.8, ups=0.3, wpb=12007.4, bsz=576.2, num_updates=7435, lr=1.30867e-05, gnorm=4.407, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.3, wall=30037
2023-05-20 00:58:09 | INFO | train_inner | epoch 115:     51 / 65 loss=6.68, nll_loss=4.941, ppl=30.72, wps=3480.9, ups=0.29, wpb=11842, bsz=538.2, num_updates=7440, lr=1.30612e-05, gnorm=3.431, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=30054
2023-05-20 00:58:24 | INFO | train_inner | epoch 115:     56 / 65 loss=6.727, nll_loss=4.988, ppl=31.74, wps=3864.2, ups=0.32, wpb=11987.4, bsz=539.6, num_updates=7445, lr=1.30357e-05, gnorm=6.201, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=30070
2023-05-20 00:58:39 | INFO | train_inner | epoch 115:     61 / 65 loss=6.932, nll_loss=5.209, ppl=36.99, wps=4210.8, ups=0.34, wpb=12529.2, bsz=638.4, num_updates=7450, lr=1.30102e-05, gnorm=8.662, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=30085
2023-05-20 00:58:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 00:58:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:59:29 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 6.085 | nll_loss 4.192 | ppl 18.27 | bleu 5.27 | wps 896.7 | wpb 2785 | bsz 105.2 | num_updates 7454 | best_loss 6.072
2023-05-20 00:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 7454 updates
2023-05-20 00:59:29 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint115.pt
2023-05-20 00:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint115.pt
2023-05-20 00:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint115.pt (epoch 115 @ 7454 updates, score 6.085) (writing took 6.312220890074968 seconds)
2023-05-20 00:59:35 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2023-05-20 00:59:35 | INFO | train | epoch 115 | loss 6.745 | nll_loss 5.01 | ppl 32.23 | wps 3124.5 | ups 0.26 | wpb 12058.6 | bsz 559.8 | num_updates 7454 | lr 1.29898e-05 | gnorm 5.478 | clip 100 | loss_scale 0.25 | train_wall 204 | gb_free 21.7 | wall 30141
2023-05-20 00:59:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 00:59:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 00:59:35 | INFO | fairseq.trainer | begin training epoch 116
2023-05-20 00:59:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 00:59:38 | INFO | train_inner | epoch 116:      1 / 65 loss=6.742, nll_loss=5.012, ppl=32.26, wps=903.1, ups=0.08, wpb=10742, bsz=517.4, num_updates=7455, lr=1.29847e-05, gnorm=15.118, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.2, wall=30144
2023-05-20 00:59:54 | INFO | train_inner | epoch 116:      6 / 65 loss=6.679, nll_loss=4.935, ppl=30.59, wps=3928.3, ups=0.32, wpb=12193.8, bsz=555.8, num_updates=7460, lr=1.29592e-05, gnorm=7.305, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=30160
2023-05-20 01:00:10 | INFO | train_inner | epoch 116:     11 / 65 loss=6.884, nll_loss=5.156, ppl=35.64, wps=3843.7, ups=0.31, wpb=12515, bsz=594.4, num_updates=7465, lr=1.29337e-05, gnorm=3.229, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=30176
2023-05-20 01:00:25 | INFO | train_inner | epoch 116:     16 / 65 loss=6.656, nll_loss=4.91, ppl=30.07, wps=4117, ups=0.34, wpb=12240, bsz=584.4, num_updates=7470, lr=1.29082e-05, gnorm=8.715, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=30191
2023-05-20 01:00:41 | INFO | train_inner | epoch 116:     21 / 65 loss=6.712, nll_loss=4.981, ppl=31.58, wps=3742.4, ups=0.31, wpb=12153.4, bsz=556.2, num_updates=7475, lr=1.28827e-05, gnorm=11.566, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=30207
2023-05-20 01:00:58 | INFO | train_inner | epoch 116:     26 / 65 loss=6.745, nll_loss=5.016, ppl=32.35, wps=3781.5, ups=0.31, wpb=12313.4, bsz=584, num_updates=7480, lr=1.28571e-05, gnorm=4.424, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=30223
2023-05-20 01:01:13 | INFO | train_inner | epoch 116:     31 / 65 loss=6.873, nll_loss=5.141, ppl=35.28, wps=3985.8, ups=0.32, wpb=12368.8, bsz=571.4, num_updates=7485, lr=1.28316e-05, gnorm=5.106, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=30239
2023-05-20 01:01:29 | INFO | train_inner | epoch 116:     36 / 65 loss=6.698, nll_loss=4.963, ppl=31.2, wps=3837.7, ups=0.31, wpb=12200.4, bsz=543, num_updates=7490, lr=1.28061e-05, gnorm=6.407, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=30255
2023-05-20 01:01:46 | INFO | train_inner | epoch 116:     41 / 65 loss=6.645, nll_loss=4.899, ppl=29.84, wps=3445.1, ups=0.29, wpb=12008.4, bsz=523.8, num_updates=7495, lr=1.27806e-05, gnorm=4.112, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=30272
2023-05-20 01:02:02 | INFO | train_inner | epoch 116:     46 / 65 loss=6.82, nll_loss=5.093, ppl=34.12, wps=3865, ups=0.32, wpb=12020.8, bsz=570.8, num_updates=7500, lr=1.27551e-05, gnorm=7.005, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=30288
2023-05-20 01:02:18 | INFO | train_inner | epoch 116:     51 / 65 loss=6.686, nll_loss=4.946, ppl=30.82, wps=3657.9, ups=0.3, wpb=12072.6, bsz=551, num_updates=7505, lr=1.27296e-05, gnorm=3.032, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=30304
2023-05-20 01:02:36 | INFO | train_inner | epoch 116:     56 / 65 loss=6.757, nll_loss=5.017, ppl=32.38, wps=3448.8, ups=0.29, wpb=12035.6, bsz=556.2, num_updates=7510, lr=1.27041e-05, gnorm=6.945, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=30322
2023-05-20 01:02:51 | INFO | train_inner | epoch 116:     61 / 65 loss=6.832, nll_loss=5.111, ppl=34.56, wps=4081.2, ups=0.33, wpb=12510.4, bsz=637.2, num_updates=7515, lr=1.26786e-05, gnorm=3.843, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=30337
2023-05-20 01:03:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:03:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:03:42 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 6.067 | nll_loss 4.172 | ppl 18.03 | bleu 5.47 | wps 882.7 | wpb 2785 | bsz 105.2 | num_updates 7519 | best_loss 6.067
2023-05-20 01:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 7519 updates
2023-05-20 01:03:42 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint116.pt
2023-05-20 01:03:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint116.pt
2023-05-20 01:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint116.pt (epoch 116 @ 7519 updates, score 6.067) (writing took 8.928167063742876 seconds)
2023-05-20 01:03:51 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2023-05-20 01:03:51 | INFO | train | epoch 116 | loss 6.747 | nll_loss 5.012 | ppl 32.27 | wps 3065.9 | ups 0.25 | wpb 12062.8 | bsz 560.1 | num_updates 7519 | lr 1.26582e-05 | gnorm 6.481 | clip 100 | loss_scale 0.25 | train_wall 206 | gb_free 21.6 | wall 30397
2023-05-20 01:03:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:03:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:03:51 | INFO | fairseq.trainer | begin training epoch 117
2023-05-20 01:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:03:54 | INFO | train_inner | epoch 117:      1 / 65 loss=6.738, nll_loss=5.007, ppl=32.16, wps=808.8, ups=0.08, wpb=10183.2, bsz=452.8, num_updates=7520, lr=1.26531e-05, gnorm=7.145, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.4, wall=30400
2023-05-20 01:04:10 | INFO | train_inner | epoch 117:      6 / 65 loss=6.789, nll_loss=5.061, ppl=33.38, wps=3964.2, ups=0.32, wpb=12333.8, bsz=637.2, num_updates=7525, lr=1.26276e-05, gnorm=7.533, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=30415
2023-05-20 01:04:26 | INFO | train_inner | epoch 117:     11 / 65 loss=6.759, nll_loss=5.024, ppl=32.54, wps=3830.6, ups=0.31, wpb=12346.4, bsz=588.6, num_updates=7530, lr=1.2602e-05, gnorm=7.007, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=30432
2023-05-20 01:04:41 | INFO | train_inner | epoch 117:     16 / 65 loss=6.64, nll_loss=4.889, ppl=29.63, wps=3929.5, ups=0.32, wpb=12181.2, bsz=552.4, num_updates=7535, lr=1.25765e-05, gnorm=4.981, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=30447
2023-05-20 01:04:57 | INFO | train_inner | epoch 117:     21 / 65 loss=6.697, nll_loss=4.956, ppl=31.04, wps=3960.9, ups=0.32, wpb=12237.2, bsz=592.2, num_updates=7540, lr=1.2551e-05, gnorm=3.079, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.2, wall=30463
2023-05-20 01:05:14 | INFO | train_inner | epoch 117:     26 / 65 loss=6.677, nll_loss=4.933, ppl=30.56, wps=3623.1, ups=0.29, wpb=12359.6, bsz=515.6, num_updates=7545, lr=1.25255e-05, gnorm=10.54, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=30480
2023-05-20 01:05:30 | INFO | train_inner | epoch 117:     31 / 65 loss=6.754, nll_loss=5.023, ppl=32.51, wps=3867.7, ups=0.31, wpb=12345.8, bsz=553.8, num_updates=7550, lr=1.25e-05, gnorm=6.625, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=30496
2023-05-20 01:05:47 | INFO | train_inner | epoch 117:     36 / 65 loss=6.787, nll_loss=5.057, ppl=33.29, wps=3359.8, ups=0.28, wpb=11791, bsz=529, num_updates=7555, lr=1.24745e-05, gnorm=3.511, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.7, wall=30513
2023-05-20 01:06:04 | INFO | train_inner | epoch 117:     41 / 65 loss=6.761, nll_loss=5.03, ppl=32.67, wps=3627, ups=0.3, wpb=11923.8, bsz=545.6, num_updates=7560, lr=1.2449e-05, gnorm=5.472, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=30530
2023-05-20 01:06:21 | INFO | train_inner | epoch 117:     46 / 65 loss=6.761, nll_loss=5.018, ppl=32.4, wps=3518, ups=0.29, wpb=12281.6, bsz=517, num_updates=7565, lr=1.24235e-05, gnorm=7.168, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.3, wall=30547
2023-05-20 01:06:38 | INFO | train_inner | epoch 117:     51 / 65 loss=6.86, nll_loss=5.127, ppl=34.94, wps=3773, ups=0.3, wpb=12383.8, bsz=616.8, num_updates=7570, lr=1.2398e-05, gnorm=2.221, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=30563
2023-05-20 01:06:54 | INFO | train_inner | epoch 117:     56 / 65 loss=6.734, nll_loss=5.001, ppl=32.01, wps=3856.4, ups=0.31, wpb=12345.8, bsz=610.4, num_updates=7575, lr=1.23724e-05, gnorm=29.348, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=30579
2023-05-20 01:07:09 | INFO | train_inner | epoch 117:     61 / 65 loss=6.676, nll_loss=4.933, ppl=30.54, wps=3914.3, ups=0.33, wpb=12025.6, bsz=562.2, num_updates=7580, lr=1.23469e-05, gnorm=4.92, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=30595
2023-05-20 01:07:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:07:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:08:00 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 6.066 | nll_loss 4.171 | ppl 18.02 | bleu 5.61 | wps 876.1 | wpb 2785 | bsz 105.2 | num_updates 7584 | best_loss 6.066
2023-05-20 01:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 7584 updates
2023-05-20 01:08:00 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint117.pt
2023-05-20 01:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint117.pt
2023-05-20 01:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint117.pt (epoch 117 @ 7584 updates, score 6.066) (writing took 9.029715273529291 seconds)
2023-05-20 01:08:10 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2023-05-20 01:08:10 | INFO | train | epoch 117 | loss 6.741 | nll_loss 5.004 | ppl 32.09 | wps 3033.4 | ups 0.25 | wpb 12056.5 | bsz 560.3 | num_updates 7584 | lr 1.23265e-05 | gnorm 7.442 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 20.5 | wall 30655
2023-05-20 01:08:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:08:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:08:10 | INFO | fairseq.trainer | begin training epoch 118
2023-05-20 01:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:08:12 | INFO | train_inner | epoch 118:      1 / 65 loss=6.729, nll_loss=4.989, ppl=31.76, wps=794.2, ups=0.08, wpb=10010.6, bsz=456.8, num_updates=7585, lr=1.23214e-05, gnorm=4.302, clip=100, loss_scale=0.25, train_wall=13, gb_free=20.8, wall=30658
2023-05-20 01:08:28 | INFO | train_inner | epoch 118:      6 / 65 loss=6.685, nll_loss=4.94, ppl=30.7, wps=3946.5, ups=0.32, wpb=12244.4, bsz=525.6, num_updates=7590, lr=1.22959e-05, gnorm=4.446, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.1, wall=30673
2023-05-20 01:08:44 | INFO | train_inner | epoch 118:     11 / 65 loss=6.714, nll_loss=4.977, ppl=31.49, wps=3879.3, ups=0.31, wpb=12555.6, bsz=560.8, num_updates=7595, lr=1.22704e-05, gnorm=4.757, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=30690
2023-05-20 01:09:01 | INFO | train_inner | epoch 118:     16 / 65 loss=6.77, nll_loss=5.042, ppl=32.94, wps=3690.5, ups=0.3, wpb=12450.2, bsz=585.6, num_updates=7600, lr=1.22449e-05, gnorm=4.418, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.5, wall=30706
2023-05-20 01:09:17 | INFO | train_inner | epoch 118:     21 / 65 loss=6.626, nll_loss=4.876, ppl=29.36, wps=3707.4, ups=0.31, wpb=11786.2, bsz=524.6, num_updates=7605, lr=1.22194e-05, gnorm=4.051, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=30722
2023-05-20 01:09:32 | INFO | train_inner | epoch 118:     26 / 65 loss=6.7, nll_loss=4.957, ppl=31.07, wps=3765, ups=0.32, wpb=11919, bsz=509, num_updates=7610, lr=1.21939e-05, gnorm=4.797, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.7, wall=30738
2023-05-20 01:09:48 | INFO | train_inner | epoch 118:     31 / 65 loss=6.714, nll_loss=4.977, ppl=31.49, wps=3909.9, ups=0.33, wpb=11964.8, bsz=535.4, num_updates=7615, lr=1.21684e-05, gnorm=4.718, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.9, wall=30753
2023-05-20 01:10:05 | INFO | train_inner | epoch 118:     36 / 65 loss=6.698, nll_loss=4.959, ppl=31.1, wps=3509.5, ups=0.29, wpb=11925.2, bsz=527, num_updates=7620, lr=1.21429e-05, gnorm=6.952, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=30770
2023-05-20 01:10:20 | INFO | train_inner | epoch 118:     41 / 65 loss=6.973, nll_loss=5.251, ppl=38.08, wps=4134.3, ups=0.33, wpb=12465.4, bsz=654.6, num_updates=7625, lr=1.21173e-05, gnorm=6.637, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.1, wall=30785
2023-05-20 01:10:35 | INFO | train_inner | epoch 118:     46 / 65 loss=6.738, nll_loss=5.002, ppl=32.05, wps=3929.8, ups=0.32, wpb=12199.8, bsz=593.6, num_updates=7630, lr=1.20918e-05, gnorm=9.117, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=30801
2023-05-20 01:10:52 | INFO | train_inner | epoch 118:     51 / 65 loss=6.746, nll_loss=5.007, ppl=32.14, wps=3666.8, ups=0.31, wpb=11949.6, bsz=524.6, num_updates=7635, lr=1.20663e-05, gnorm=5.63, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=30817
2023-05-20 01:11:08 | INFO | train_inner | epoch 118:     56 / 65 loss=6.888, nll_loss=5.169, ppl=35.97, wps=3759, ups=0.3, wpb=12505.8, bsz=665.6, num_updates=7640, lr=1.20408e-05, gnorm=3.095, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=30834
2023-05-20 01:11:23 | INFO | train_inner | epoch 118:     61 / 65 loss=6.701, nll_loss=4.965, ppl=31.22, wps=4303.3, ups=0.35, wpb=12428.8, bsz=593.8, num_updates=7645, lr=1.20153e-05, gnorm=4.048, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=30848
2023-05-20 01:11:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:11:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:12:11 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 6.083 | nll_loss 4.19 | ppl 18.26 | bleu 5.4 | wps 925.6 | wpb 2785 | bsz 105.2 | num_updates 7649 | best_loss 6.066
2023-05-20 01:12:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 7649 updates
2023-05-20 01:12:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint118.pt
2023-05-20 01:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint118.pt
2023-05-20 01:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint118.pt (epoch 118 @ 7649 updates, score 6.083) (writing took 9.329605672508478 seconds)
2023-05-20 01:12:21 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2023-05-20 01:12:21 | INFO | train | epoch 118 | loss 6.74 | nll_loss 5.003 | ppl 32.07 | wps 3119 | ups 0.26 | wpb 12053.5 | bsz 557.3 | num_updates 7649 | lr 1.19949e-05 | gnorm 5.215 | clip 100 | loss_scale 0.25 | train_wall 203 | gb_free 21.5 | wall 30906
2023-05-20 01:12:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:12:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:12:21 | INFO | fairseq.trainer | begin training epoch 119
2023-05-20 01:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:12:24 | INFO | train_inner | epoch 119:      1 / 65 loss=6.643, nll_loss=4.894, ppl=29.73, wps=870.2, ups=0.08, wpb=10613, bsz=461.6, num_updates=7650, lr=1.19898e-05, gnorm=5.751, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.4, wall=30909
2023-05-20 01:12:40 | INFO | train_inner | epoch 119:      6 / 65 loss=6.669, nll_loss=4.923, ppl=30.34, wps=3638.6, ups=0.31, wpb=11736, bsz=498.6, num_updates=7655, lr=1.19643e-05, gnorm=10.4, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=30925
2023-05-20 01:12:56 | INFO | train_inner | epoch 119:     11 / 65 loss=6.645, nll_loss=4.898, ppl=29.81, wps=3693.6, ups=0.3, wpb=12261.2, bsz=552.2, num_updates=7660, lr=1.19388e-05, gnorm=6.963, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.4, wall=30942
2023-05-20 01:13:14 | INFO | train_inner | epoch 119:     16 / 65 loss=6.728, nll_loss=4.992, ppl=31.81, wps=3577.3, ups=0.29, wpb=12296.6, bsz=566, num_updates=7665, lr=1.19133e-05, gnorm=4.937, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=30959
2023-05-20 01:13:29 | INFO | train_inner | epoch 119:     21 / 65 loss=6.708, nll_loss=4.967, ppl=31.27, wps=3857.2, ups=0.32, wpb=11906.8, bsz=568.4, num_updates=7670, lr=1.18878e-05, gnorm=5.907, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=30975
2023-05-20 01:13:44 | INFO | train_inner | epoch 119:     26 / 65 loss=6.765, nll_loss=5.032, ppl=32.73, wps=4166, ups=0.34, wpb=12166.8, bsz=565.8, num_updates=7675, lr=1.18622e-05, gnorm=4.859, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=30989
2023-05-20 01:14:01 | INFO | train_inner | epoch 119:     31 / 65 loss=6.822, nll_loss=5.09, ppl=34.07, wps=3509.1, ups=0.29, wpb=11954.2, bsz=536.4, num_updates=7680, lr=1.18367e-05, gnorm=5.878, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=31006
2023-05-20 01:14:17 | INFO | train_inner | epoch 119:     36 / 65 loss=6.715, nll_loss=4.977, ppl=31.49, wps=3430.6, ups=0.3, wpb=11537.8, bsz=516.8, num_updates=7685, lr=1.18112e-05, gnorm=6.048, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=31023
2023-05-20 01:14:32 | INFO | train_inner | epoch 119:     41 / 65 loss=6.697, nll_loss=4.962, ppl=31.16, wps=4058.9, ups=0.33, wpb=12170.8, bsz=547.4, num_updates=7690, lr=1.17857e-05, gnorm=4.687, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31038
2023-05-20 01:14:48 | INFO | train_inner | epoch 119:     46 / 65 loss=6.723, nll_loss=4.991, ppl=31.81, wps=3834.5, ups=0.31, wpb=12331.8, bsz=571.2, num_updates=7695, lr=1.17602e-05, gnorm=4.146, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=31054
2023-05-20 01:15:06 | INFO | train_inner | epoch 119:     51 / 65 loss=6.946, nll_loss=5.221, ppl=37.29, wps=3684.1, ups=0.29, wpb=12617.6, bsz=666, num_updates=7700, lr=1.17347e-05, gnorm=7.234, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=31071
2023-05-20 01:15:21 | INFO | train_inner | epoch 119:     56 / 65 loss=6.722, nll_loss=4.984, ppl=31.64, wps=4017.5, ups=0.32, wpb=12456.8, bsz=589.2, num_updates=7705, lr=1.17092e-05, gnorm=7.862, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31087
2023-05-20 01:15:37 | INFO | train_inner | epoch 119:     61 / 65 loss=6.768, nll_loss=5.04, ppl=32.91, wps=3963.2, ups=0.32, wpb=12408.4, bsz=593.8, num_updates=7710, lr=1.16837e-05, gnorm=4.649, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=31103
2023-05-20 01:15:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:15:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:16:28 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 6.079 | nll_loss 4.188 | ppl 18.23 | bleu 5.06 | wps 901 | wpb 2785 | bsz 105.2 | num_updates 7714 | best_loss 6.066
2023-05-20 01:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 7714 updates
2023-05-20 01:16:28 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint119.pt
2023-05-20 01:16:33 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint119.pt
2023-05-20 01:16:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint119.pt (epoch 119 @ 7714 updates, score 6.079) (writing took 7.1763657964766026 seconds)
2023-05-20 01:16:35 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2023-05-20 01:16:35 | INFO | train | epoch 119 | loss 6.74 | nll_loss 5.004 | ppl 32.09 | wps 3083.6 | ups 0.26 | wpb 12052.3 | bsz 558.4 | num_updates 7714 | lr 1.16633e-05 | gnorm 6.641 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.7 | wall 31161
2023-05-20 01:16:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:16:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:16:35 | INFO | fairseq.trainer | begin training epoch 120
2023-05-20 01:16:35 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:16:38 | INFO | train_inner | epoch 120:      1 / 65 loss=6.715, nll_loss=4.977, ppl=31.49, wps=887.6, ups=0.08, wpb=10853, bsz=523, num_updates=7715, lr=1.16582e-05, gnorm=12.395, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=31164
2023-05-20 01:16:54 | INFO | train_inner | epoch 120:      6 / 65 loss=6.74, nll_loss=5.005, ppl=32.1, wps=3887, ups=0.32, wpb=12294.8, bsz=548.8, num_updates=7720, lr=1.16327e-05, gnorm=5.764, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=31179
2023-05-20 01:17:09 | INFO | train_inner | epoch 120:     11 / 65 loss=6.62, nll_loss=4.869, ppl=29.22, wps=3967, ups=0.32, wpb=12257.4, bsz=564.6, num_updates=7725, lr=1.16071e-05, gnorm=14.065, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31195
2023-05-20 01:17:26 | INFO | train_inner | epoch 120:     16 / 65 loss=6.648, nll_loss=4.909, ppl=30.05, wps=3502.3, ups=0.3, wpb=11756, bsz=532.8, num_updates=7730, lr=1.15816e-05, gnorm=8.624, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=31212
2023-05-20 01:17:43 | INFO | train_inner | epoch 120:     21 / 65 loss=6.917, nll_loss=5.195, ppl=36.63, wps=3542.1, ups=0.3, wpb=11875.2, bsz=601.2, num_updates=7735, lr=1.15561e-05, gnorm=6.713, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=31228
2023-05-20 01:17:58 | INFO | train_inner | epoch 120:     26 / 65 loss=6.677, nll_loss=4.939, ppl=30.68, wps=4059.8, ups=0.33, wpb=12295.2, bsz=551.8, num_updates=7740, lr=1.15306e-05, gnorm=3.339, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=31244
2023-05-20 01:18:17 | INFO | train_inner | epoch 120:     31 / 65 loss=6.69, nll_loss=4.952, ppl=30.95, wps=3249.1, ups=0.27, wpb=12154.2, bsz=527.4, num_updates=7745, lr=1.15051e-05, gnorm=2.928, clip=100, loss_scale=0.25, train_wall=19, gb_free=21.6, wall=31262
2023-05-20 01:18:32 | INFO | train_inner | epoch 120:     36 / 65 loss=6.738, nll_loss=5.003, ppl=32.08, wps=3959.7, ups=0.32, wpb=12246.6, bsz=541.2, num_updates=7750, lr=1.14796e-05, gnorm=7.593, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.1, wall=31278
2023-05-20 01:18:47 | INFO | train_inner | epoch 120:     41 / 65 loss=6.818, nll_loss=5.09, ppl=34.07, wps=3975.3, ups=0.33, wpb=12112, bsz=576.8, num_updates=7755, lr=1.14541e-05, gnorm=4.669, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31293
2023-05-20 01:19:05 | INFO | train_inner | epoch 120:     46 / 65 loss=6.734, nll_loss=4.998, ppl=31.96, wps=3412.9, ups=0.28, wpb=12216.2, bsz=536.4, num_updates=7760, lr=1.14286e-05, gnorm=6.057, clip=100, loss_scale=0.25, train_wall=18, gb_free=20.7, wall=31311
2023-05-20 01:19:21 | INFO | train_inner | epoch 120:     51 / 65 loss=6.673, nll_loss=4.928, ppl=30.45, wps=4015.6, ups=0.32, wpb=12629, bsz=637.2, num_updates=7765, lr=1.14031e-05, gnorm=6.759, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=31327
2023-05-20 01:19:38 | INFO | train_inner | epoch 120:     56 / 65 loss=6.697, nll_loss=4.958, ppl=31.07, wps=3558, ups=0.3, wpb=11880.6, bsz=523.4, num_updates=7770, lr=1.13776e-05, gnorm=3.092, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.6, wall=31343
2023-05-20 01:19:52 | INFO | train_inner | epoch 120:     61 / 65 loss=6.676, nll_loss=4.933, ppl=30.54, wps=4259.2, ups=0.35, wpb=12131.8, bsz=520.2, num_updates=7775, lr=1.1352e-05, gnorm=3.488, clip=100, loss_scale=0.5, train_wall=14, gb_free=21.4, wall=31358
2023-05-20 01:20:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:20:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:20:43 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 6.069 | nll_loss 4.175 | ppl 18.06 | bleu 5.34 | wps 866.4 | wpb 2785 | bsz 105.2 | num_updates 7779 | best_loss 6.066
2023-05-20 01:20:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7779 updates
2023-05-20 01:20:43 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint120.pt
2023-05-20 01:20:47 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint120.pt
2023-05-20 01:20:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint120.pt (epoch 120 @ 7779 updates, score 6.069) (writing took 7.073099967092276 seconds)
2023-05-20 01:20:50 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2023-05-20 01:20:50 | INFO | train | epoch 120 | loss 6.738 | nll_loss 5.003 | ppl 32.06 | wps 3075.6 | ups 0.26 | wpb 12057.9 | bsz 559.4 | num_updates 7779 | lr 1.13316e-05 | gnorm 6.146 | clip 100 | loss_scale 0.5 | train_wall 206 | gb_free 21.5 | wall 31415
2023-05-20 01:20:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:20:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:20:50 | INFO | fairseq.trainer | begin training epoch 121
2023-05-20 01:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:20:52 | INFO | train_inner | epoch 121:      1 / 65 loss=6.984, nll_loss=5.268, ppl=38.53, wps=874.9, ups=0.08, wpb=10592.6, bsz=568, num_updates=7780, lr=1.13265e-05, gnorm=7.171, clip=100, loss_scale=0.5, train_wall=12, gb_free=21.4, wall=31418
2023-05-20 01:21:09 | INFO | train_inner | epoch 121:      6 / 65 loss=6.75, nll_loss=5.022, ppl=32.49, wps=3654.7, ups=0.31, wpb=11946.4, bsz=554.2, num_updates=7785, lr=1.1301e-05, gnorm=9.572, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.9, wall=31434
2023-05-20 01:21:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 01:21:29 | INFO | train_inner | epoch 121:     12 / 65 loss=6.694, nll_loss=4.954, ppl=30.99, wps=3014.4, ups=0.25, wpb=12144.6, bsz=547, num_updates=7790, lr=1.12755e-05, gnorm=4.893, clip=100, loss_scale=0.25, train_wall=20, gb_free=21.5, wall=31455
2023-05-20 01:21:45 | INFO | train_inner | epoch 121:     17 / 65 loss=6.751, nll_loss=5.023, ppl=32.51, wps=3910, ups=0.32, wpb=12287.8, bsz=594.8, num_updates=7795, lr=1.125e-05, gnorm=10.939, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=31470
2023-05-20 01:22:00 | INFO | train_inner | epoch 121:     22 / 65 loss=6.71, nll_loss=4.976, ppl=31.48, wps=4012.7, ups=0.33, wpb=12213.6, bsz=568.2, num_updates=7800, lr=1.12245e-05, gnorm=10.231, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31486
2023-05-20 01:22:15 | INFO | train_inner | epoch 121:     27 / 65 loss=6.869, nll_loss=5.15, ppl=35.5, wps=4253.7, ups=0.34, wpb=12686.2, bsz=691.2, num_updates=7805, lr=1.1199e-05, gnorm=10.288, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31500
2023-05-20 01:22:32 | INFO | train_inner | epoch 121:     32 / 65 loss=6.781, nll_loss=5.053, ppl=33.21, wps=3449.8, ups=0.29, wpb=11822.6, bsz=548, num_updates=7810, lr=1.11735e-05, gnorm=11.135, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=31518
2023-05-20 01:22:48 | INFO | train_inner | epoch 121:     37 / 65 loss=6.7, nll_loss=4.966, ppl=31.25, wps=3704.4, ups=0.3, wpb=12328.8, bsz=566, num_updates=7815, lr=1.1148e-05, gnorm=10.749, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=31534
2023-05-20 01:23:05 | INFO | train_inner | epoch 121:     42 / 65 loss=6.658, nll_loss=4.913, ppl=30.13, wps=3751.2, ups=0.31, wpb=12204.2, bsz=512.8, num_updates=7820, lr=1.11224e-05, gnorm=7.352, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=31550
2023-05-20 01:23:21 | INFO | train_inner | epoch 121:     47 / 65 loss=6.708, nll_loss=4.975, ppl=31.46, wps=3734.8, ups=0.3, wpb=12277.6, bsz=576.6, num_updates=7825, lr=1.10969e-05, gnorm=9.605, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=31567
2023-05-20 01:23:38 | INFO | train_inner | epoch 121:     52 / 65 loss=6.791, nll_loss=5.067, ppl=33.53, wps=3595.9, ups=0.3, wpb=12061.8, bsz=592.4, num_updates=7830, lr=1.10714e-05, gnorm=8.115, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=31584
2023-05-20 01:23:54 | INFO | train_inner | epoch 121:     57 / 65 loss=6.867, nll_loss=5.133, ppl=35.1, wps=3789.6, ups=0.3, wpb=12483.8, bsz=563.4, num_updates=7835, lr=1.10459e-05, gnorm=6.179, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=31600
2023-05-20 01:24:10 | INFO | train_inner | epoch 121:     62 / 65 loss=6.637, nll_loss=4.881, ppl=29.47, wps=3977.6, ups=0.33, wpb=12049.6, bsz=499.8, num_updates=7840, lr=1.10204e-05, gnorm=7.007, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.2, wall=31615
2023-05-20 01:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:24:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:24:58 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 6.071 | nll_loss 4.175 | ppl 18.06 | bleu 5.26 | wps 885.9 | wpb 2785 | bsz 105.2 | num_updates 7843 | best_loss 6.066
2023-05-20 01:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 7843 updates
2023-05-20 01:24:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint121.pt
2023-05-20 01:25:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint121.pt
2023-05-20 01:25:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint121.pt (epoch 121 @ 7843 updates, score 6.071) (writing took 6.97400576248765 seconds)
2023-05-20 01:25:04 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2023-05-20 01:25:04 | INFO | train | epoch 121 | loss 6.74 | nll_loss 5.006 | ppl 32.14 | wps 3026 | ups 0.25 | wpb 12052.5 | bsz 560.3 | num_updates 7843 | lr 1.10051e-05 | gnorm 8.648 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.6 | wall 31670
2023-05-20 01:25:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:25:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:25:05 | INFO | fairseq.trainer | begin training epoch 122
2023-05-20 01:25:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:25:11 | INFO | train_inner | epoch 122:      2 / 65 loss=6.634, nll_loss=4.884, ppl=29.52, wps=814.8, ups=0.08, wpb=10073.8, bsz=429, num_updates=7845, lr=1.09949e-05, gnorm=6.63, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=31677
2023-05-20 01:25:27 | INFO | train_inner | epoch 122:      7 / 65 loss=6.683, nll_loss=4.94, ppl=30.7, wps=3773.9, ups=0.32, wpb=11833.4, bsz=551.4, num_updates=7850, lr=1.09694e-05, gnorm=5.411, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=31693
2023-05-20 01:25:43 | INFO | train_inner | epoch 122:     12 / 65 loss=6.924, nll_loss=5.207, ppl=36.93, wps=3869.1, ups=0.32, wpb=12274.8, bsz=677.6, num_updates=7855, lr=1.09439e-05, gnorm=5.947, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=31709
2023-05-20 01:26:00 | INFO | train_inner | epoch 122:     17 / 65 loss=6.78, nll_loss=5.055, ppl=33.25, wps=3833.2, ups=0.3, wpb=12753.6, bsz=585.2, num_updates=7860, lr=1.09184e-05, gnorm=15.973, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=31725
2023-05-20 01:26:19 | INFO | train_inner | epoch 122:     22 / 65 loss=6.726, nll_loss=4.99, ppl=31.77, wps=3086.5, ups=0.26, wpb=11956.2, bsz=520.4, num_updates=7865, lr=1.08929e-05, gnorm=4.644, clip=100, loss_scale=0.25, train_wall=19, gb_free=21.6, wall=31745
2023-05-20 01:26:35 | INFO | train_inner | epoch 122:     27 / 65 loss=6.698, nll_loss=4.955, ppl=31.01, wps=3878, ups=0.32, wpb=12153.6, bsz=540, num_updates=7870, lr=1.08673e-05, gnorm=5.077, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=31760
2023-05-20 01:26:50 | INFO | train_inner | epoch 122:     32 / 65 loss=6.74, nll_loss=5.007, ppl=32.16, wps=4116.5, ups=0.33, wpb=12389.4, bsz=583.2, num_updates=7875, lr=1.08418e-05, gnorm=3.705, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=31775
2023-05-20 01:27:06 | INFO | train_inner | epoch 122:     37 / 65 loss=6.801, nll_loss=5.073, ppl=33.67, wps=3854.4, ups=0.31, wpb=12329.4, bsz=573.2, num_updates=7880, lr=1.08163e-05, gnorm=8.122, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=31791
2023-05-20 01:27:21 | INFO | train_inner | epoch 122:     42 / 65 loss=6.626, nll_loss=4.874, ppl=29.32, wps=4092.6, ups=0.33, wpb=12291.6, bsz=579.2, num_updates=7885, lr=1.07908e-05, gnorm=3.082, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=31806
2023-05-20 01:27:36 | INFO | train_inner | epoch 122:     47 / 65 loss=6.672, nll_loss=4.93, ppl=30.49, wps=3959, ups=0.32, wpb=12252.2, bsz=542.4, num_updates=7890, lr=1.07653e-05, gnorm=2.932, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=31822
2023-05-20 01:27:53 | INFO | train_inner | epoch 122:     52 / 65 loss=6.713, nll_loss=4.972, ppl=31.38, wps=3524, ups=0.29, wpb=12135.2, bsz=522, num_updates=7895, lr=1.07398e-05, gnorm=3.962, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=31839
2023-05-20 01:28:08 | INFO | train_inner | epoch 122:     57 / 65 loss=6.714, nll_loss=4.967, ppl=31.28, wps=4092.9, ups=0.34, wpb=12052.8, bsz=564.4, num_updates=7900, lr=1.07143e-05, gnorm=5.23, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=31854
2023-05-20 01:28:23 | INFO | train_inner | epoch 122:     62 / 65 loss=6.764, nll_loss=5.031, ppl=32.69, wps=4115, ups=0.34, wpb=12267.8, bsz=614.2, num_updates=7905, lr=1.06888e-05, gnorm=4.117, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.9, wall=31869
2023-05-20 01:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:28:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:29:10 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 6.075 | nll_loss 4.182 | ppl 18.15 | bleu 5.49 | wps 922 | wpb 2785 | bsz 105.2 | num_updates 7908 | best_loss 6.066
2023-05-20 01:29:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 7908 updates
2023-05-20 01:29:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint122.pt
2023-05-20 01:29:15 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint122.pt
2023-05-20 01:29:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint122.pt (epoch 122 @ 7908 updates, score 6.075) (writing took 6.973039600998163 seconds)
2023-05-20 01:29:17 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2023-05-20 01:29:17 | INFO | train | epoch 122 | loss 6.732 | nll_loss 4.994 | ppl 31.87 | wps 3099.2 | ups 0.26 | wpb 12052.2 | bsz 558.2 | num_updates 7908 | lr 1.06735e-05 | gnorm 5.896 | clip 100 | loss_scale 0.25 | train_wall 206 | gb_free 21.2 | wall 31923
2023-05-20 01:29:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:29:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:29:17 | INFO | fairseq.trainer | begin training epoch 123
2023-05-20 01:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:29:24 | INFO | train_inner | epoch 123:      2 / 65 loss=6.642, nll_loss=4.892, ppl=29.7, wps=837.3, ups=0.08, wpb=10154.6, bsz=426.6, num_updates=7910, lr=1.06633e-05, gnorm=7.383, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.6, wall=31929
2023-05-20 01:29:39 | INFO | train_inner | epoch 123:      7 / 65 loss=6.697, nll_loss=4.961, ppl=31.14, wps=4209, ups=0.34, wpb=12522, bsz=579.2, num_updates=7915, lr=1.06378e-05, gnorm=4.166, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=31944
2023-05-20 01:29:53 | INFO | train_inner | epoch 123:     12 / 65 loss=6.658, nll_loss=4.914, ppl=30.16, wps=4356.6, ups=0.34, wpb=12800.2, bsz=634.4, num_updates=7920, lr=1.06122e-05, gnorm=4.203, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=31959
2023-05-20 01:30:09 | INFO | train_inner | epoch 123:     17 / 65 loss=6.762, nll_loss=5.031, ppl=32.7, wps=3772.2, ups=0.31, wpb=11994.6, bsz=511.4, num_updates=7925, lr=1.05867e-05, gnorm=2.93, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=31975
2023-05-20 01:30:26 | INFO | train_inner | epoch 123:     22 / 65 loss=6.737, nll_loss=5.001, ppl=32.02, wps=3709.8, ups=0.3, wpb=12288, bsz=565.4, num_updates=7930, lr=1.05612e-05, gnorm=7.494, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=31991
2023-05-20 01:30:41 | INFO | train_inner | epoch 123:     27 / 65 loss=6.808, nll_loss=5.08, ppl=33.83, wps=3885.4, ups=0.32, wpb=12084.8, bsz=551, num_updates=7935, lr=1.05357e-05, gnorm=4.223, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=32007
2023-05-20 01:30:59 | INFO | train_inner | epoch 123:     32 / 65 loss=6.753, nll_loss=5.02, ppl=32.45, wps=3557.7, ups=0.29, wpb=12310.4, bsz=582.4, num_updates=7940, lr=1.05102e-05, gnorm=4.052, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=32024
2023-05-20 01:31:14 | INFO | train_inner | epoch 123:     37 / 65 loss=6.816, nll_loss=5.091, ppl=34.07, wps=4057.1, ups=0.33, wpb=12159, bsz=569.2, num_updates=7945, lr=1.04847e-05, gnorm=5.42, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=32039
2023-05-20 01:31:30 | INFO | train_inner | epoch 123:     42 / 65 loss=6.672, nll_loss=4.932, ppl=30.52, wps=3651.8, ups=0.3, wpb=12050.2, bsz=543, num_updates=7950, lr=1.04592e-05, gnorm=9.694, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=32056
2023-05-20 01:31:46 | INFO | train_inner | epoch 123:     47 / 65 loss=6.748, nll_loss=5.016, ppl=32.36, wps=3695.2, ups=0.3, wpb=12179.4, bsz=583.4, num_updates=7955, lr=1.04337e-05, gnorm=5.251, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=32072
2023-05-20 01:32:03 | INFO | train_inner | epoch 123:     52 / 65 loss=6.684, nll_loss=4.941, ppl=30.73, wps=3567.1, ups=0.3, wpb=12036.6, bsz=519, num_updates=7960, lr=1.04082e-05, gnorm=4.423, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=32089
2023-05-20 01:32:19 | INFO | train_inner | epoch 123:     57 / 65 loss=6.658, nll_loss=4.917, ppl=30.22, wps=3842.4, ups=0.31, wpb=12277.8, bsz=567.4, num_updates=7965, lr=1.03827e-05, gnorm=8.112, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=32105
2023-05-20 01:32:34 | INFO | train_inner | epoch 123:     62 / 65 loss=6.913, nll_loss=5.192, ppl=36.55, wps=4036.1, ups=0.34, wpb=11981.6, bsz=635.6, num_updates=7970, lr=1.03571e-05, gnorm=12.483, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=32120
2023-05-20 01:32:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:32:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:33:24 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 6.056 | nll_loss 4.159 | ppl 17.87 | bleu 5.02 | wps 872.9 | wpb 2785 | bsz 105.2 | num_updates 7973 | best_loss 6.056
2023-05-20 01:33:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 7973 updates
2023-05-20 01:33:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint123.pt
2023-05-20 01:33:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint123.pt
2023-05-20 01:33:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint123.pt (epoch 123 @ 7973 updates, score 6.056) (writing took 10.119416043162346 seconds)
2023-05-20 01:33:34 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2023-05-20 01:33:34 | INFO | train | epoch 123 | loss 6.736 | nll_loss 5.001 | ppl 32.02 | wps 3055.5 | ups 0.25 | wpb 12061.6 | bsz 559.7 | num_updates 7973 | lr 1.03418e-05 | gnorm 6.233 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.7 | wall 32180
2023-05-20 01:33:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:33:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:33:34 | INFO | fairseq.trainer | begin training epoch 124
2023-05-20 01:33:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:33:40 | INFO | train_inner | epoch 124:      2 / 65 loss=6.729, nll_loss=4.99, ppl=31.79, wps=783.5, ups=0.08, wpb=10318.4, bsz=465.6, num_updates=7975, lr=1.03316e-05, gnorm=7.478, clip=100, loss_scale=0.25, train_wall=15, gb_free=21, wall=32186
2023-05-20 01:33:56 | INFO | train_inner | epoch 124:      7 / 65 loss=6.659, nll_loss=4.915, ppl=30.16, wps=3876.7, ups=0.31, wpb=12360.4, bsz=552.8, num_updates=7980, lr=1.03061e-05, gnorm=5.849, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32202
2023-05-20 01:34:12 | INFO | train_inner | epoch 124:     12 / 65 loss=6.693, nll_loss=4.952, ppl=30.94, wps=3834.6, ups=0.32, wpb=11983.4, bsz=538.8, num_updates=7985, lr=1.02806e-05, gnorm=7.266, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32217
2023-05-20 01:34:27 | INFO | train_inner | epoch 124:     17 / 65 loss=6.659, nll_loss=4.914, ppl=30.14, wps=3936.1, ups=0.32, wpb=12144.4, bsz=507.2, num_updates=7990, lr=1.02551e-05, gnorm=6.539, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=32233
2023-05-20 01:34:45 | INFO | train_inner | epoch 124:     22 / 65 loss=6.848, nll_loss=5.119, ppl=34.74, wps=3467.2, ups=0.28, wpb=12359.4, bsz=594.4, num_updates=7995, lr=1.02296e-05, gnorm=4.683, clip=100, loss_scale=0.25, train_wall=18, gb_free=20.9, wall=32251
2023-05-20 01:35:00 | INFO | train_inner | epoch 124:     27 / 65 loss=6.739, nll_loss=5.006, ppl=32.13, wps=4232.8, ups=0.34, wpb=12626.2, bsz=599.4, num_updates=8000, lr=1.02041e-05, gnorm=9.114, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=32266
2023-05-20 01:35:15 | INFO | train_inner | epoch 124:     32 / 65 loss=6.674, nll_loss=4.932, ppl=30.53, wps=3911.4, ups=0.32, wpb=12047.6, bsz=550, num_updates=8005, lr=1.01786e-05, gnorm=9.84, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=32281
2023-05-20 01:35:31 | INFO | train_inner | epoch 124:     37 / 65 loss=6.782, nll_loss=5.057, ppl=33.29, wps=3971.9, ups=0.31, wpb=12625.8, bsz=620.8, num_updates=8010, lr=1.01531e-05, gnorm=6.972, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=32297
2023-05-20 01:35:49 | INFO | train_inner | epoch 124:     42 / 65 loss=6.774, nll_loss=5.045, ppl=33.01, wps=3481.7, ups=0.29, wpb=12145.4, bsz=592.4, num_updates=8015, lr=1.01276e-05, gnorm=8.161, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=32314
2023-05-20 01:36:04 | INFO | train_inner | epoch 124:     47 / 65 loss=6.7, nll_loss=4.958, ppl=31.08, wps=3938.7, ups=0.32, wpb=12255.4, bsz=593.6, num_updates=8020, lr=1.0102e-05, gnorm=9.77, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32330
2023-05-20 01:36:19 | INFO | train_inner | epoch 124:     52 / 65 loss=6.922, nll_loss=5.196, ppl=36.67, wps=4047, ups=0.33, wpb=12132.8, bsz=620.4, num_updates=8025, lr=1.00765e-05, gnorm=8.256, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=32345
2023-05-20 01:36:36 | INFO | train_inner | epoch 124:     57 / 65 loss=6.74, nll_loss=5.006, ppl=32.14, wps=3412.1, ups=0.29, wpb=11606.2, bsz=501, num_updates=8030, lr=1.0051e-05, gnorm=6.638, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=32362
2023-05-20 01:36:52 | INFO | train_inner | epoch 124:     62 / 65 loss=6.636, nll_loss=4.885, ppl=29.55, wps=3784.1, ups=0.32, wpb=11903.8, bsz=564, num_updates=8035, lr=1.00255e-05, gnorm=10.522, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=32378
2023-05-20 01:37:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:37:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:37:40 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 6.076 | nll_loss 4.181 | ppl 18.14 | bleu 5.69 | wps 904.2 | wpb 2785 | bsz 105.2 | num_updates 8038 | best_loss 6.056
2023-05-20 01:37:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 8038 updates
2023-05-20 01:37:40 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint124.pt
2023-05-20 01:37:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint124.pt
2023-05-20 01:37:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint124.pt (epoch 124 @ 8038 updates, score 6.076) (writing took 6.90255294367671 seconds)
2023-05-20 01:37:48 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2023-05-20 01:37:48 | INFO | train | epoch 124 | loss 6.733 | nll_loss 4.996 | ppl 31.91 | wps 3079.8 | ups 0.26 | wpb 12052.5 | bsz 560 | num_updates 8038 | lr 1.00102e-05 | gnorm 7.705 | clip 100 | loss_scale 0.25 | train_wall 206 | gb_free 21.6 | wall 32434
2023-05-20 01:37:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:37:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:37:49 | INFO | fairseq.trainer | begin training epoch 125
2023-05-20 01:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:37:55 | INFO | train_inner | epoch 125:      2 / 65 loss=6.676, nll_loss=4.935, ppl=30.6, wps=809.9, ups=0.08, wpb=10180.6, bsz=441, num_updates=8040, lr=1e-05, gnorm=7.389, clip=100, loss_scale=0.25, train_wall=14, gb_free=20.4, wall=32440
2023-05-20 01:38:10 | INFO | train_inner | epoch 125:      7 / 65 loss=6.79, nll_loss=5.056, ppl=33.26, wps=3896.2, ups=0.32, wpb=12319, bsz=586.2, num_updates=8045, lr=9.97449e-06, gnorm=15.58, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=32456
2023-05-20 01:38:25 | INFO | train_inner | epoch 125:     12 / 65 loss=6.75, nll_loss=5.013, ppl=32.28, wps=4089.7, ups=0.34, wpb=12171, bsz=545.8, num_updates=8050, lr=9.94898e-06, gnorm=5.666, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.1, wall=32471
2023-05-20 01:38:43 | INFO | train_inner | epoch 125:     17 / 65 loss=6.75, nll_loss=5.007, ppl=32.16, wps=3457.9, ups=0.29, wpb=11966.8, bsz=535.6, num_updates=8055, lr=9.92347e-06, gnorm=9.481, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=32488
2023-05-20 01:39:00 | INFO | train_inner | epoch 125:     22 / 65 loss=6.685, nll_loss=4.944, ppl=30.78, wps=3402.7, ups=0.28, wpb=11950.8, bsz=544.4, num_updates=8060, lr=9.89796e-06, gnorm=4.103, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.8, wall=32506
2023-05-20 01:39:18 | INFO | train_inner | epoch 125:     27 / 65 loss=6.722, nll_loss=4.986, ppl=31.7, wps=3390, ups=0.28, wpb=12247.4, bsz=577.2, num_updates=8065, lr=9.87245e-06, gnorm=6.045, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.5, wall=32524
2023-05-20 01:39:34 | INFO | train_inner | epoch 125:     32 / 65 loss=6.884, nll_loss=5.154, ppl=35.61, wps=3845.2, ups=0.32, wpb=11994, bsz=584, num_updates=8070, lr=9.84694e-06, gnorm=4.209, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.6, wall=32540
2023-05-20 01:39:50 | INFO | train_inner | epoch 125:     37 / 65 loss=6.764, nll_loss=5.032, ppl=32.72, wps=3875.3, ups=0.32, wpb=12059.8, bsz=564.2, num_updates=8075, lr=9.82143e-06, gnorm=5.479, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32555
2023-05-20 01:40:05 | INFO | train_inner | epoch 125:     42 / 65 loss=6.683, nll_loss=4.947, ppl=30.84, wps=4198.8, ups=0.33, wpb=12598, bsz=593.6, num_updates=8080, lr=9.79592e-06, gnorm=7.69, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=32570
2023-05-20 01:40:22 | INFO | train_inner | epoch 125:     47 / 65 loss=6.685, nll_loss=4.945, ppl=30.8, wps=3366.5, ups=0.28, wpb=12010.8, bsz=502, num_updates=8085, lr=9.77041e-06, gnorm=6.355, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=32588
2023-05-20 01:40:37 | INFO | train_inner | epoch 125:     52 / 65 loss=6.732, nll_loss=4.998, ppl=31.96, wps=4238.3, ups=0.34, wpb=12519.8, bsz=603, num_updates=8090, lr=9.7449e-06, gnorm=3.139, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=32603
2023-05-20 01:40:53 | INFO | train_inner | epoch 125:     57 / 65 loss=6.688, nll_loss=4.949, ppl=30.9, wps=3741.8, ups=0.31, wpb=12158.8, bsz=536.2, num_updates=8095, lr=9.71939e-06, gnorm=5.414, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32619
2023-05-20 01:41:09 | INFO | train_inner | epoch 125:     62 / 65 loss=6.643, nll_loss=4.898, ppl=29.82, wps=3978.6, ups=0.32, wpb=12278.6, bsz=558, num_updates=8100, lr=9.69388e-06, gnorm=9.395, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.9, wall=32635
2023-05-20 01:41:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:41:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:41:56 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 6.065 | nll_loss 4.171 | ppl 18.01 | bleu 5.66 | wps 892.3 | wpb 2785 | bsz 105.2 | num_updates 8103 | best_loss 6.056
2023-05-20 01:41:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 8103 updates
2023-05-20 01:41:56 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint125.pt
2023-05-20 01:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint125.pt
2023-05-20 01:42:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint125.pt (epoch 125 @ 8103 updates, score 6.065) (writing took 7.027600459754467 seconds)
2023-05-20 01:42:03 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2023-05-20 01:42:03 | INFO | train | epoch 125 | loss 6.725 | nll_loss 4.987 | ppl 31.71 | wps 3074.1 | ups 0.26 | wpb 12048.7 | bsz 556.7 | num_updates 8103 | lr 9.67857e-06 | gnorm 6.981 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.6 | wall 32689
2023-05-20 01:42:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:42:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:42:03 | INFO | fairseq.trainer | begin training epoch 126
2023-05-20 01:42:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:42:09 | INFO | train_inner | epoch 126:      2 / 65 loss=6.68, nll_loss=4.939, ppl=30.67, wps=910.9, ups=0.08, wpb=10954.2, bsz=554, num_updates=8105, lr=9.66837e-06, gnorm=8.388, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=32695
2023-05-20 01:42:26 | INFO | train_inner | epoch 126:      7 / 65 loss=6.622, nll_loss=4.873, ppl=29.31, wps=3629.7, ups=0.3, wpb=12118.4, bsz=513.8, num_updates=8110, lr=9.64286e-06, gnorm=10.56, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=32711
2023-05-20 01:42:41 | INFO | train_inner | epoch 126:     12 / 65 loss=6.702, nll_loss=4.965, ppl=31.24, wps=3922.3, ups=0.32, wpb=12190.8, bsz=576.4, num_updates=8115, lr=9.61735e-06, gnorm=4.729, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=32727
2023-05-20 01:42:57 | INFO | train_inner | epoch 126:     17 / 65 loss=6.681, nll_loss=4.938, ppl=30.65, wps=3838.5, ups=0.33, wpb=11798.2, bsz=552, num_updates=8120, lr=9.59184e-06, gnorm=3.379, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=32742
2023-05-20 01:43:12 | INFO | train_inner | epoch 126:     22 / 65 loss=6.736, nll_loss=5, ppl=32, wps=4023.7, ups=0.32, wpb=12466, bsz=636.6, num_updates=8125, lr=9.56633e-06, gnorm=4.88, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=32758
2023-05-20 01:43:29 | INFO | train_inner | epoch 126:     27 / 65 loss=6.667, nll_loss=4.923, ppl=30.33, wps=3505.3, ups=0.29, wpb=11966.4, bsz=471.8, num_updates=8130, lr=9.54082e-06, gnorm=8.732, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.9, wall=32775
2023-05-20 01:43:45 | INFO | train_inner | epoch 126:     32 / 65 loss=6.766, nll_loss=5.04, ppl=32.9, wps=3849.5, ups=0.32, wpb=12115.2, bsz=620, num_updates=8135, lr=9.51531e-06, gnorm=4.743, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32791
2023-05-20 01:44:01 | INFO | train_inner | epoch 126:     37 / 65 loss=6.691, nll_loss=4.952, ppl=30.95, wps=3858.2, ups=0.32, wpb=12201, bsz=512.8, num_updates=8140, lr=9.4898e-06, gnorm=5.129, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=32806
2023-05-20 01:44:18 | INFO | train_inner | epoch 126:     42 / 65 loss=6.805, nll_loss=5.07, ppl=33.59, wps=3496.7, ups=0.29, wpb=12242.8, bsz=559.4, num_updates=8145, lr=9.46429e-06, gnorm=3.126, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=32824
2023-05-20 01:44:34 | INFO | train_inner | epoch 126:     47 / 65 loss=7.01, nll_loss=5.295, ppl=39.27, wps=4047.2, ups=0.33, wpb=12435, bsz=670.6, num_updates=8150, lr=9.43878e-06, gnorm=5.029, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=32839
2023-05-20 01:44:50 | INFO | train_inner | epoch 126:     52 / 65 loss=6.739, nll_loss=5.005, ppl=32.12, wps=3599.6, ups=0.3, wpb=11985.2, bsz=566, num_updates=8155, lr=9.41327e-06, gnorm=3.568, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=32856
2023-05-20 01:45:07 | INFO | train_inner | epoch 126:     57 / 65 loss=6.661, nll_loss=4.919, ppl=30.26, wps=3650.5, ups=0.3, wpb=12234, bsz=548.4, num_updates=8160, lr=9.38776e-06, gnorm=6.325, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=32873
2023-05-20 01:45:22 | INFO | train_inner | epoch 126:     62 / 65 loss=6.642, nll_loss=4.895, ppl=29.76, wps=3999.8, ups=0.32, wpb=12352.6, bsz=537, num_updates=8165, lr=9.36224e-06, gnorm=5.984, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=32888
2023-05-20 01:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:45:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:46:14 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 6.054 | nll_loss 4.159 | ppl 17.87 | bleu 5.5 | wps 835.2 | wpb 2785 | bsz 105.2 | num_updates 8168 | best_loss 6.054
2023-05-20 01:46:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 8168 updates
2023-05-20 01:46:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint126.pt
2023-05-20 01:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint126.pt
2023-05-20 01:46:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint126.pt (epoch 126 @ 8168 updates, score 6.054) (writing took 12.250908978283405 seconds)
2023-05-20 01:46:26 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2023-05-20 01:46:26 | INFO | train | epoch 126 | loss 6.728 | nll_loss 4.991 | ppl 31.8 | wps 2979.5 | ups 0.25 | wpb 12051.7 | bsz 559.1 | num_updates 8168 | lr 9.34694e-06 | gnorm 5.853 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21.6 | wall 32952
2023-05-20 01:46:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:46:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:46:26 | INFO | fairseq.trainer | begin training epoch 127
2023-05-20 01:46:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:46:32 | INFO | train_inner | epoch 127:      2 / 65 loss=6.683, nll_loss=4.944, ppl=30.78, wps=721.3, ups=0.07, wpb=10055.4, bsz=446.2, num_updates=8170, lr=9.33673e-06, gnorm=9.28, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=32958
2023-05-20 01:46:48 | INFO | train_inner | epoch 127:      7 / 65 loss=6.692, nll_loss=4.951, ppl=30.93, wps=3823.2, ups=0.31, wpb=12287.4, bsz=556, num_updates=8175, lr=9.31122e-06, gnorm=3.284, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=32974
2023-05-20 01:47:04 | INFO | train_inner | epoch 127:     12 / 65 loss=6.679, nll_loss=4.937, ppl=30.64, wps=3738, ups=0.32, wpb=11767, bsz=496.6, num_updates=8180, lr=9.28571e-06, gnorm=18.552, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.9, wall=32990
2023-05-20 01:47:20 | INFO | train_inner | epoch 127:     17 / 65 loss=6.676, nll_loss=4.94, ppl=30.7, wps=3862.3, ups=0.31, wpb=12282, bsz=541.2, num_updates=8185, lr=9.2602e-06, gnorm=8.429, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=33006
2023-05-20 01:47:35 | INFO | train_inner | epoch 127:     22 / 65 loss=6.708, nll_loss=4.972, ppl=31.37, wps=4040.3, ups=0.33, wpb=12361.6, bsz=549.6, num_updates=8190, lr=9.23469e-06, gnorm=5.397, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.5, wall=33021
2023-05-20 01:47:53 | INFO | train_inner | epoch 127:     27 / 65 loss=6.933, nll_loss=5.213, ppl=37.1, wps=3449.8, ups=0.28, wpb=12213.4, bsz=622.4, num_updates=8195, lr=9.20918e-06, gnorm=6.896, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=33039
2023-05-20 01:48:10 | INFO | train_inner | epoch 127:     32 / 65 loss=6.725, nll_loss=4.987, ppl=31.71, wps=3690.1, ups=0.3, wpb=12368.6, bsz=664.4, num_updates=8200, lr=9.18367e-06, gnorm=5.692, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=33055
2023-05-20 01:48:26 | INFO | train_inner | epoch 127:     37 / 65 loss=6.757, nll_loss=5.019, ppl=32.41, wps=3793.6, ups=0.31, wpb=12325.8, bsz=558.4, num_updates=8205, lr=9.15816e-06, gnorm=15.493, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=33072
2023-05-20 01:48:41 | INFO | train_inner | epoch 127:     42 / 65 loss=6.691, nll_loss=4.95, ppl=30.91, wps=3931.1, ups=0.32, wpb=12322, bsz=611.8, num_updates=8210, lr=9.13265e-06, gnorm=3.653, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=33087
2023-05-20 01:48:58 | INFO | train_inner | epoch 127:     47 / 65 loss=6.724, nll_loss=4.983, ppl=31.63, wps=3804.8, ups=0.31, wpb=12233.8, bsz=568.4, num_updates=8215, lr=9.10714e-06, gnorm=15.417, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=33103
2023-05-20 01:49:14 | INFO | train_inner | epoch 127:     52 / 65 loss=6.735, nll_loss=5.001, ppl=32.02, wps=3656.6, ups=0.3, wpb=12098.4, bsz=560.4, num_updates=8220, lr=9.08163e-06, gnorm=8.985, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=33120
2023-05-20 01:49:31 | INFO | train_inner | epoch 127:     57 / 65 loss=6.694, nll_loss=4.952, ppl=30.94, wps=3504.6, ups=0.3, wpb=11784.6, bsz=471.4, num_updates=8225, lr=9.05612e-06, gnorm=6.87, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.7, wall=33137
2023-05-20 01:49:47 | INFO | train_inner | epoch 127:     62 / 65 loss=6.757, nll_loss=5.031, ppl=32.7, wps=3889.2, ups=0.32, wpb=12305.2, bsz=613.8, num_updates=8230, lr=9.03061e-06, gnorm=3.468, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=33152
2023-05-20 01:49:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:49:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:50:36 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 6.055 | nll_loss 4.155 | ppl 17.82 | bleu 5.45 | wps 878.5 | wpb 2785 | bsz 105.2 | num_updates 8233 | best_loss 6.054
2023-05-20 01:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 8233 updates
2023-05-20 01:50:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint127.pt
2023-05-20 01:50:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint127.pt
2023-05-20 01:50:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint127.pt (epoch 127 @ 8233 updates, score 6.055) (writing took 11.424753095954657 seconds)
2023-05-20 01:50:47 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2023-05-20 01:50:47 | INFO | train | epoch 127 | loss 6.726 | nll_loss 4.99 | ppl 31.77 | wps 2997.8 | ups 0.25 | wpb 12047.9 | bsz 559.8 | num_updates 8233 | lr 9.01531e-06 | gnorm 8.235 | clip 100 | loss_scale 0.25 | train_wall 209 | gb_free 21.6 | wall 33213
2023-05-20 01:50:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:50:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:50:47 | INFO | fairseq.trainer | begin training epoch 128
2023-05-20 01:50:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:50:54 | INFO | train_inner | epoch 128:      2 / 65 loss=6.651, nll_loss=4.901, ppl=29.89, wps=779.9, ups=0.07, wpb=10450.8, bsz=484.8, num_updates=8235, lr=9.0051e-06, gnorm=5.331, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=33219
2023-05-20 01:51:09 | INFO | train_inner | epoch 128:      7 / 65 loss=6.752, nll_loss=5.016, ppl=32.35, wps=3974.2, ups=0.32, wpb=12513.4, bsz=596.4, num_updates=8240, lr=8.97959e-06, gnorm=10.643, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=33235
2023-05-20 01:51:25 | INFO | train_inner | epoch 128:     12 / 65 loss=6.637, nll_loss=4.886, ppl=29.57, wps=3812.8, ups=0.32, wpb=11836, bsz=511.6, num_updates=8245, lr=8.95408e-06, gnorm=4.856, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=33251
2023-05-20 01:51:42 | INFO | train_inner | epoch 128:     17 / 65 loss=6.808, nll_loss=5.079, ppl=33.8, wps=3482.9, ups=0.29, wpb=12144, bsz=586.8, num_updates=8250, lr=8.92857e-06, gnorm=6.285, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.4, wall=33268
2023-05-20 01:51:58 | INFO | train_inner | epoch 128:     22 / 65 loss=6.665, nll_loss=4.912, ppl=30.11, wps=3696.4, ups=0.31, wpb=11823.6, bsz=537.6, num_updates=8255, lr=8.90306e-06, gnorm=4.355, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=33284
2023-05-20 01:52:14 | INFO | train_inner | epoch 128:     27 / 65 loss=6.852, nll_loss=5.124, ppl=34.88, wps=3748.5, ups=0.31, wpb=11955.4, bsz=585.4, num_updates=8260, lr=8.87755e-06, gnorm=4.721, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=33300
2023-05-20 01:52:31 | INFO | train_inner | epoch 128:     32 / 65 loss=6.675, nll_loss=4.93, ppl=30.48, wps=3611.2, ups=0.31, wpb=11725.6, bsz=487.4, num_updates=8265, lr=8.85204e-06, gnorm=5.151, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=33316
2023-05-20 01:52:46 | INFO | train_inner | epoch 128:     37 / 65 loss=6.815, nll_loss=5.091, ppl=34.08, wps=4052.6, ups=0.33, wpb=12352, bsz=648, num_updates=8270, lr=8.82653e-06, gnorm=4.971, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=33332
2023-05-20 01:53:01 | INFO | train_inner | epoch 128:     42 / 65 loss=6.659, nll_loss=4.915, ppl=30.18, wps=3930.6, ups=0.32, wpb=12250.2, bsz=574.4, num_updates=8275, lr=8.80102e-06, gnorm=4.032, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=33347
2023-05-20 01:53:18 | INFO | train_inner | epoch 128:     47 / 65 loss=6.674, nll_loss=4.935, ppl=30.6, wps=3696.6, ups=0.29, wpb=12563.2, bsz=554.8, num_updates=8280, lr=8.77551e-06, gnorm=4.893, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=33364
2023-05-20 01:53:35 | INFO | train_inner | epoch 128:     52 / 65 loss=6.657, nll_loss=4.91, ppl=30.07, wps=3618.3, ups=0.3, wpb=12068.2, bsz=526.2, num_updates=8285, lr=8.75e-06, gnorm=16.5, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=33381
2023-05-20 01:53:53 | INFO | train_inner | epoch 128:     57 / 65 loss=6.696, nll_loss=4.955, ppl=31.02, wps=3434.5, ups=0.27, wpb=12575.2, bsz=546.8, num_updates=8290, lr=8.72449e-06, gnorm=4.268, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=33399
2023-05-20 01:54:09 | INFO | train_inner | epoch 128:     62 / 65 loss=6.695, nll_loss=4.953, ppl=30.96, wps=3853.4, ups=0.31, wpb=12255, bsz=547.6, num_updates=8295, lr=8.69898e-06, gnorm=2.806, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=33415
2023-05-20 01:54:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:54:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:54:57 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 6.058 | nll_loss 4.159 | ppl 17.87 | bleu 5.47 | wps 902.2 | wpb 2785 | bsz 105.2 | num_updates 8298 | best_loss 6.054
2023-05-20 01:54:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 8298 updates
2023-05-20 01:54:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint128.pt
2023-05-20 01:55:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint128.pt
2023-05-20 01:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint128.pt (epoch 128 @ 8298 updates, score 6.058) (writing took 6.85377236828208 seconds)
2023-05-20 01:55:04 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2023-05-20 01:55:04 | INFO | train | epoch 128 | loss 6.728 | nll_loss 4.989 | ppl 31.76 | wps 3055.1 | ups 0.25 | wpb 12051.7 | bsz 558.7 | num_updates 8298 | lr 8.68367e-06 | gnorm 6.138 | clip 100 | loss_scale 0.25 | train_wall 209 | gb_free 21.9 | wall 33469
2023-05-20 01:55:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:55:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:55:04 | INFO | fairseq.trainer | begin training epoch 129
2023-05-20 01:55:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:55:10 | INFO | train_inner | epoch 129:      2 / 65 loss=6.864, nll_loss=5.128, ppl=34.97, wps=879.7, ups=0.08, wpb=10608.2, bsz=548.6, num_updates=8300, lr=8.67347e-06, gnorm=7.667, clip=100, loss_scale=0.5, train_wall=13, gb_free=21.7, wall=33475
2023-05-20 01:55:26 | INFO | train_inner | epoch 129:      7 / 65 loss=6.675, nll_loss=4.935, ppl=30.6, wps=3644.1, ups=0.3, wpb=12198.2, bsz=526.4, num_updates=8305, lr=8.64796e-06, gnorm=6.181, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=33492
2023-05-20 01:55:43 | INFO | train_inner | epoch 129:     12 / 65 loss=6.812, nll_loss=5.088, ppl=34.02, wps=3880.9, ups=0.31, wpb=12533, bsz=681.4, num_updates=8310, lr=8.62245e-06, gnorm=6.065, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.3, wall=33508
2023-05-20 01:55:59 | INFO | train_inner | epoch 129:     17 / 65 loss=6.705, nll_loss=4.965, ppl=31.23, wps=3543.3, ups=0.29, wpb=12014.6, bsz=506.2, num_updates=8315, lr=8.59694e-06, gnorm=6.205, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=33525
2023-05-20 01:56:15 | INFO | train_inner | epoch 129:     22 / 65 loss=6.845, nll_loss=5.108, ppl=34.48, wps=4135.3, ups=0.33, wpb=12453.2, bsz=618.6, num_updates=8320, lr=8.57143e-06, gnorm=6.65, clip=100, loss_scale=0.5, train_wall=15, gb_free=21, wall=33540
2023-05-20 01:56:31 | INFO | train_inner | epoch 129:     27 / 65 loss=6.71, nll_loss=4.968, ppl=31.31, wps=3784.5, ups=0.31, wpb=12283.6, bsz=577.2, num_updates=8325, lr=8.54592e-06, gnorm=3.891, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=33557
2023-05-20 01:56:47 | INFO | train_inner | epoch 129:     32 / 65 loss=6.764, nll_loss=5.031, ppl=32.68, wps=3875.8, ups=0.31, wpb=12585.6, bsz=635.4, num_updates=8330, lr=8.52041e-06, gnorm=11.488, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.3, wall=33573
2023-05-20 01:57:02 | INFO | train_inner | epoch 129:     37 / 65 loss=6.821, nll_loss=5.091, ppl=34.07, wps=4251.9, ups=0.34, wpb=12528, bsz=635.2, num_updates=8335, lr=8.4949e-06, gnorm=3.695, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.4, wall=33587
2023-05-20 01:57:18 | INFO | train_inner | epoch 129:     42 / 65 loss=6.736, nll_loss=5.001, ppl=32.02, wps=3639.6, ups=0.31, wpb=11824.8, bsz=529.4, num_updates=8340, lr=8.46939e-06, gnorm=4.184, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=33604
2023-05-20 01:57:33 | INFO | train_inner | epoch 129:     47 / 65 loss=6.653, nll_loss=4.907, ppl=30.01, wps=3842.8, ups=0.33, wpb=11586, bsz=493.2, num_updates=8345, lr=8.44388e-06, gnorm=7.041, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=33619
2023-05-20 01:57:50 | INFO | train_inner | epoch 129:     52 / 65 loss=6.674, nll_loss=4.929, ppl=30.46, wps=3591.4, ups=0.3, wpb=12003, bsz=501.8, num_updates=8350, lr=8.41837e-06, gnorm=9.171, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.4, wall=33636
2023-05-20 01:58:05 | INFO | train_inner | epoch 129:     57 / 65 loss=6.717, nll_loss=4.985, ppl=31.66, wps=3999.9, ups=0.32, wpb=12435.6, bsz=594, num_updates=8355, lr=8.39286e-06, gnorm=3.548, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.2, wall=33651
2023-05-20 01:58:22 | INFO | train_inner | epoch 129:     62 / 65 loss=6.656, nll_loss=4.916, ppl=30.19, wps=3530, ups=0.3, wpb=11909.4, bsz=520.6, num_updates=8360, lr=8.36735e-06, gnorm=2.613, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.3, wall=33668
2023-05-20 01:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 01:58:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:59:10 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 6.064 | nll_loss 4.171 | ppl 18.02 | bleu 5.59 | wps 900 | wpb 2785 | bsz 105.2 | num_updates 8363 | best_loss 6.054
2023-05-20 01:59:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 8363 updates
2023-05-20 01:59:10 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint129.pt
2023-05-20 01:59:14 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint129.pt
2023-05-20 01:59:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint129.pt (epoch 129 @ 8363 updates, score 6.064) (writing took 6.760192058980465 seconds)
2023-05-20 01:59:17 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2023-05-20 01:59:17 | INFO | train | epoch 129 | loss 6.724 | nll_loss 4.986 | ppl 31.7 | wps 3099 | ups 0.26 | wpb 12052.5 | bsz 560 | num_updates 8363 | lr 8.35204e-06 | gnorm 6.597 | clip 100 | loss_scale 0.5 | train_wall 206 | gb_free 21.8 | wall 33722
2023-05-20 01:59:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 01:59:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 01:59:17 | INFO | fairseq.trainer | begin training epoch 130
2023-05-20 01:59:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 01:59:23 | INFO | train_inner | epoch 130:      2 / 65 loss=6.696, nll_loss=4.959, ppl=31.09, wps=848.7, ups=0.08, wpb=10279.4, bsz=500.6, num_updates=8365, lr=8.34184e-06, gnorm=13.59, clip=100, loss_scale=0.5, train_wall=14, gb_free=21.6, wall=33728
2023-05-20 01:59:39 | INFO | train_inner | epoch 130:      7 / 65 loss=6.853, nll_loss=5.124, ppl=34.88, wps=3886, ups=0.32, wpb=12314, bsz=600, num_updates=8370, lr=8.31633e-06, gnorm=6.806, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=33744
2023-05-20 01:59:54 | INFO | train_inner | epoch 130:     12 / 65 loss=6.656, nll_loss=4.911, ppl=30.09, wps=3847.5, ups=0.31, wpb=12236.8, bsz=532.8, num_updates=8375, lr=8.29082e-06, gnorm=4.286, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.7, wall=33760
2023-05-20 02:00:09 | INFO | train_inner | epoch 130:     17 / 65 loss=6.646, nll_loss=4.9, ppl=29.85, wps=4128, ups=0.34, wpb=12271.6, bsz=566.4, num_updates=8380, lr=8.26531e-06, gnorm=4.076, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.4, wall=33775
2023-05-20 02:00:25 | INFO | train_inner | epoch 130:     22 / 65 loss=6.685, nll_loss=4.944, ppl=30.77, wps=3903.1, ups=0.32, wpb=12091.6, bsz=521, num_updates=8385, lr=8.2398e-06, gnorm=4.669, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.5, wall=33791
2023-05-20 02:00:40 | INFO | train_inner | epoch 130:     27 / 65 loss=6.712, nll_loss=4.974, ppl=31.42, wps=4067.8, ups=0.33, wpb=12276.6, bsz=591.2, num_updates=8390, lr=8.21429e-06, gnorm=6.038, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.6, wall=33806
2023-05-20 02:00:56 | INFO | train_inner | epoch 130:     32 / 65 loss=6.761, nll_loss=5.028, ppl=32.64, wps=3916.1, ups=0.31, wpb=12455.2, bsz=568.6, num_updates=8395, lr=8.18878e-06, gnorm=4.197, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.4, wall=33822
2023-05-20 02:01:11 | INFO | train_inner | epoch 130:     37 / 65 loss=6.661, nll_loss=4.918, ppl=30.24, wps=4034.9, ups=0.32, wpb=12508.8, bsz=616.4, num_updates=8400, lr=8.16327e-06, gnorm=4.867, clip=100, loss_scale=0.5, train_wall=15, gb_free=20.8, wall=33837
2023-05-20 02:01:28 | INFO | train_inner | epoch 130:     42 / 65 loss=6.756, nll_loss=5.02, ppl=32.45, wps=3581.7, ups=0.3, wpb=11799.2, bsz=515.8, num_updates=8405, lr=8.13776e-06, gnorm=6.506, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.7, wall=33854
2023-05-20 02:01:45 | INFO | train_inner | epoch 130:     47 / 65 loss=6.727, nll_loss=4.987, ppl=31.71, wps=3428.6, ups=0.29, wpb=11962.4, bsz=520.2, num_updates=8410, lr=8.11224e-06, gnorm=4.006, clip=100, loss_scale=0.5, train_wall=17, gb_free=21, wall=33871
2023-05-20 02:02:01 | INFO | train_inner | epoch 130:     52 / 65 loss=6.703, nll_loss=4.969, ppl=31.32, wps=3929.3, ups=0.32, wpb=12308.6, bsz=572, num_updates=8415, lr=8.08673e-06, gnorm=6.437, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.9, wall=33887
2023-05-20 02:02:18 | INFO | train_inner | epoch 130:     57 / 65 loss=6.745, nll_loss=5.014, ppl=32.32, wps=3598, ups=0.29, wpb=12236.2, bsz=608.4, num_updates=8420, lr=8.06122e-06, gnorm=5.607, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.8, wall=33904
2023-05-20 02:02:35 | INFO | train_inner | epoch 130:     62 / 65 loss=6.746, nll_loss=5.013, ppl=32.29, wps=3447.8, ups=0.29, wpb=11928.4, bsz=537.2, num_updates=8425, lr=8.03571e-06, gnorm=5.148, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.8, wall=33921
2023-05-20 02:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:02:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:03:26 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 6.064 | nll_loss 4.171 | ppl 18.01 | bleu 5.82 | wps 846.1 | wpb 2785 | bsz 105.2 | num_updates 8428 | best_loss 6.054
2023-05-20 02:03:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 8428 updates
2023-05-20 02:03:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint130.pt
2023-05-20 02:03:30 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint130.pt
2023-05-20 02:03:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint130.pt (epoch 130 @ 8428 updates, score 6.064) (writing took 6.717506345361471 seconds)
2023-05-20 02:03:32 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2023-05-20 02:03:32 | INFO | train | epoch 130 | loss 6.718 | nll_loss 4.981 | ppl 31.57 | wps 3058.7 | ups 0.25 | wpb 12040.3 | bsz 556.7 | num_updates 8428 | lr 8.02041e-06 | gnorm 5.411 | clip 100 | loss_scale 0.5 | train_wall 206 | gb_free 21.6 | wall 33978
2023-05-20 02:03:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:03:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:03:32 | INFO | fairseq.trainer | begin training epoch 131
2023-05-20 02:03:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:03:39 | INFO | train_inner | epoch 131:      2 / 65 loss=6.615, nll_loss=4.868, ppl=29.2, wps=775.4, ups=0.08, wpb=9871, bsz=445.2, num_updates=8430, lr=8.0102e-06, gnorm=7.493, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.8, wall=33985
2023-05-20 02:03:55 | INFO | train_inner | epoch 131:      7 / 65 loss=6.723, nll_loss=4.991, ppl=31.81, wps=3766.5, ups=0.31, wpb=12161.4, bsz=587.6, num_updates=8435, lr=7.98469e-06, gnorm=6.288, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=34001
2023-05-20 02:04:10 | INFO | train_inner | epoch 131:     12 / 65 loss=6.683, nll_loss=4.943, ppl=30.77, wps=3956.3, ups=0.32, wpb=12191, bsz=527.4, num_updates=8440, lr=7.95918e-06, gnorm=6.034, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.2, wall=34016
2023-05-20 02:04:26 | INFO | train_inner | epoch 131:     17 / 65 loss=6.757, nll_loss=5.022, ppl=32.49, wps=3783.1, ups=0.31, wpb=12099.2, bsz=565.6, num_updates=8445, lr=7.93367e-06, gnorm=6.594, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=34032
2023-05-20 02:04:42 | INFO | train_inner | epoch 131:     22 / 65 loss=6.898, nll_loss=5.167, ppl=35.93, wps=3934.1, ups=0.32, wpb=12174.6, bsz=578.2, num_updates=8450, lr=7.90816e-06, gnorm=4.661, clip=100, loss_scale=0.5, train_wall=15, gb_free=20.3, wall=34048
2023-05-20 02:04:57 | INFO | train_inner | epoch 131:     27 / 65 loss=6.81, nll_loss=5.081, ppl=33.86, wps=4060, ups=0.32, wpb=12584, bsz=646.8, num_updates=8455, lr=7.88265e-06, gnorm=9.867, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=34063
2023-05-20 02:05:14 | INFO | train_inner | epoch 131:     32 / 65 loss=6.751, nll_loss=5.017, ppl=32.37, wps=3580.9, ups=0.29, wpb=12156, bsz=573.8, num_updates=8460, lr=7.85714e-06, gnorm=3.619, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.7, wall=34080
2023-05-20 02:05:29 | INFO | train_inner | epoch 131:     37 / 65 loss=6.746, nll_loss=5.014, ppl=32.32, wps=4189.5, ups=0.34, wpb=12405.2, bsz=603.4, num_updates=8465, lr=7.83163e-06, gnorm=6.327, clip=100, loss_scale=0.5, train_wall=15, gb_free=21, wall=34095
2023-05-20 02:05:46 | INFO | train_inner | epoch 131:     42 / 65 loss=6.616, nll_loss=4.865, ppl=29.14, wps=3683.8, ups=0.3, wpb=12256.6, bsz=533.4, num_updates=8470, lr=7.80612e-06, gnorm=8.083, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.3, wall=34112
2023-05-20 02:06:01 | INFO | train_inner | epoch 131:     47 / 65 loss=6.724, nll_loss=4.987, ppl=31.71, wps=4089.3, ups=0.32, wpb=12635.2, bsz=639.8, num_updates=8475, lr=7.78061e-06, gnorm=2.778, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.6, wall=34127
2023-05-20 02:06:18 | INFO | train_inner | epoch 131:     52 / 65 loss=6.653, nll_loss=4.913, ppl=30.12, wps=3562.9, ups=0.3, wpb=11928.4, bsz=531, num_updates=8480, lr=7.7551e-06, gnorm=9.066, clip=100, loss_scale=0.5, train_wall=17, gb_free=20.5, wall=34144
2023-05-20 02:06:33 | INFO | train_inner | epoch 131:     57 / 65 loss=6.635, nll_loss=4.888, ppl=29.62, wps=4062.1, ups=0.34, wpb=12066.8, bsz=489.8, num_updates=8485, lr=7.72959e-06, gnorm=3.694, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.5, wall=34159
2023-05-20 02:06:50 | INFO | train_inner | epoch 131:     62 / 65 loss=6.732, nll_loss=4.996, ppl=31.91, wps=3545.1, ups=0.3, wpb=11834.2, bsz=519, num_updates=8490, lr=7.70408e-06, gnorm=8.44, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.6, wall=34175
2023-05-20 02:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:06:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:07:40 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 6.067 | nll_loss 4.173 | ppl 18.03 | bleu 5.51 | wps 837.8 | wpb 2785 | bsz 105.2 | num_updates 8493 | best_loss 6.054
2023-05-20 02:07:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 8493 updates
2023-05-20 02:07:40 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint131.pt
2023-05-20 02:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint131.pt
2023-05-20 02:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint131.pt (epoch 131 @ 8493 updates, score 6.067) (writing took 6.868328042328358 seconds)
2023-05-20 02:07:46 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2023-05-20 02:07:46 | INFO | train | epoch 131 | loss 6.724 | nll_loss 4.987 | ppl 31.71 | wps 3083.5 | ups 0.26 | wpb 12052.7 | bsz 559.3 | num_updates 8493 | lr 7.68878e-06 | gnorm 6.319 | clip 100 | loss_scale 0.5 | train_wall 204 | gb_free 21.7 | wall 34232
2023-05-20 02:07:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:07:46 | INFO | fairseq.trainer | begin training epoch 132
2023-05-20 02:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:07:53 | INFO | train_inner | epoch 132:      2 / 65 loss=6.789, nll_loss=5.058, ppl=33.3, wps=846.9, ups=0.08, wpb=10801.2, bsz=512.2, num_updates=8495, lr=7.67857e-06, gnorm=12.212, clip=100, loss_scale=0.5, train_wall=14, gb_free=20.7, wall=34239
2023-05-20 02:08:09 | INFO | train_inner | epoch 132:      7 / 65 loss=6.687, nll_loss=4.948, ppl=30.86, wps=3874.9, ups=0.32, wpb=12100, bsz=504, num_updates=8500, lr=7.65306e-06, gnorm=7.745, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.5, wall=34255
2023-05-20 02:08:25 | INFO | train_inner | epoch 132:     12 / 65 loss=6.714, nll_loss=4.976, ppl=31.47, wps=3769, ups=0.32, wpb=11947.8, bsz=567.4, num_updates=8505, lr=7.62755e-06, gnorm=3.402, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=34271
2023-05-20 02:08:41 | INFO | train_inner | epoch 132:     17 / 65 loss=6.702, nll_loss=4.962, ppl=31.17, wps=3683.3, ups=0.31, wpb=12015.6, bsz=541.6, num_updates=8510, lr=7.60204e-06, gnorm=3.505, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=34287
2023-05-20 02:08:57 | INFO | train_inner | epoch 132:     22 / 65 loss=6.653, nll_loss=4.91, ppl=30.07, wps=3785.5, ups=0.31, wpb=12084.8, bsz=563.2, num_updates=8515, lr=7.57653e-06, gnorm=5.958, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.8, wall=34303
2023-05-20 02:09:12 | INFO | train_inner | epoch 132:     27 / 65 loss=6.748, nll_loss=5.019, ppl=32.42, wps=4011.8, ups=0.33, wpb=12157.2, bsz=602, num_updates=8520, lr=7.55102e-06, gnorm=4.08, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=34318
2023-05-20 02:09:28 | INFO | train_inner | epoch 132:     32 / 65 loss=6.629, nll_loss=4.878, ppl=29.4, wps=3849.8, ups=0.32, wpb=12082.4, bsz=488.8, num_updates=8525, lr=7.52551e-06, gnorm=6.652, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=34334
2023-05-20 02:09:45 | INFO | train_inner | epoch 132:     37 / 65 loss=6.625, nll_loss=4.873, ppl=29.3, wps=3641.2, ups=0.29, wpb=12454.2, bsz=583.4, num_updates=8530, lr=7.5e-06, gnorm=6.107, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.6, wall=34351
2023-05-20 02:10:02 | INFO | train_inner | epoch 132:     42 / 65 loss=6.867, nll_loss=5.141, ppl=35.28, wps=3537.6, ups=0.29, wpb=12298.6, bsz=602, num_updates=8535, lr=7.47449e-06, gnorm=6.734, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=34368
2023-05-20 02:10:20 | INFO | train_inner | epoch 132:     47 / 65 loss=6.704, nll_loss=4.968, ppl=31.3, wps=3574.3, ups=0.29, wpb=12288.2, bsz=556, num_updates=8540, lr=7.44898e-06, gnorm=3.311, clip=100, loss_scale=0.5, train_wall=17, gb_free=20.9, wall=34385
2023-05-20 02:10:36 | INFO | train_inner | epoch 132:     52 / 65 loss=6.815, nll_loss=5.086, ppl=33.96, wps=3875.4, ups=0.31, wpb=12534.2, bsz=630.8, num_updates=8545, lr=7.42347e-06, gnorm=7.345, clip=100, loss_scale=0.5, train_wall=16, gb_free=20.7, wall=34402
2023-05-20 02:10:52 | INFO | train_inner | epoch 132:     57 / 65 loss=6.765, nll_loss=5.035, ppl=32.79, wps=3572.2, ups=0.31, wpb=11652.8, bsz=575.2, num_updates=8550, lr=7.39796e-06, gnorm=3.93, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.5, wall=34418
2023-05-20 02:11:08 | INFO | train_inner | epoch 132:     62 / 65 loss=6.698, nll_loss=4.962, ppl=31.17, wps=3823.1, ups=0.31, wpb=12298.2, bsz=521, num_updates=8555, lr=7.37245e-06, gnorm=3.511, clip=100, loss_scale=0.5, train_wall=16, gb_free=20.4, wall=34434
2023-05-20 02:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:11:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:11:57 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 6.071 | nll_loss 4.178 | ppl 18.1 | bleu 5.71 | wps 866.5 | wpb 2785 | bsz 105.2 | num_updates 8558 | best_loss 6.054
2023-05-20 02:11:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 8558 updates
2023-05-20 02:11:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint132.pt
2023-05-20 02:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint132.pt
2023-05-20 02:12:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint132.pt (epoch 132 @ 8558 updates, score 6.071) (writing took 7.049916937947273 seconds)
2023-05-20 02:12:04 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2023-05-20 02:12:04 | INFO | train | epoch 132 | loss 6.723 | nll_loss 4.986 | ppl 31.69 | wps 3041.8 | ups 0.25 | wpb 12050 | bsz 558.8 | num_updates 8558 | lr 7.35714e-06 | gnorm 5.639 | clip 100 | loss_scale 0.5 | train_wall 209 | gb_free 21.5 | wall 34490
2023-05-20 02:12:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:12:04 | INFO | fairseq.trainer | begin training epoch 133
2023-05-20 02:12:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:12:09 | INFO | train_inner | epoch 133:      2 / 65 loss=6.771, nll_loss=5.04, ppl=32.9, wps=865.4, ups=0.08, wpb=10605.8, bsz=541.6, num_updates=8560, lr=7.34694e-06, gnorm=8.654, clip=100, loss_scale=0.5, train_wall=13, gb_free=21.5, wall=34495
2023-05-20 02:12:27 | INFO | train_inner | epoch 133:      7 / 65 loss=6.688, nll_loss=4.944, ppl=30.79, wps=3457, ups=0.29, wpb=11921.8, bsz=527.8, num_updates=8565, lr=7.32143e-06, gnorm=7.017, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.3, wall=34512
2023-05-20 02:12:44 | INFO | train_inner | epoch 133:     12 / 65 loss=6.692, nll_loss=4.953, ppl=30.98, wps=3438.3, ups=0.29, wpb=11719, bsz=565.8, num_updates=8570, lr=7.29592e-06, gnorm=5.727, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=34529
2023-05-20 02:13:00 | INFO | train_inner | epoch 133:     17 / 65 loss=6.827, nll_loss=5.101, ppl=34.31, wps=3904.8, ups=0.31, wpb=12515.6, bsz=601.2, num_updates=8575, lr=7.27041e-06, gnorm=5.699, clip=100, loss_scale=0.5, train_wall=16, gb_free=20.5, wall=34546
2023-05-20 02:13:15 | INFO | train_inner | epoch 133:     22 / 65 loss=6.687, nll_loss=4.95, ppl=30.9, wps=4253.4, ups=0.34, wpb=12514, bsz=575.6, num_updates=8580, lr=7.2449e-06, gnorm=10.149, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.4, wall=34560
2023-05-20 02:13:30 | INFO | train_inner | epoch 133:     27 / 65 loss=6.751, nll_loss=5.021, ppl=32.47, wps=3960.9, ups=0.32, wpb=12413.6, bsz=588, num_updates=8585, lr=7.21939e-06, gnorm=3.477, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.4, wall=34576
2023-05-20 02:13:48 | INFO | train_inner | epoch 133:     32 / 65 loss=6.712, nll_loss=4.973, ppl=31.4, wps=3459.8, ups=0.28, wpb=12196.8, bsz=552.2, num_updates=8590, lr=7.19388e-06, gnorm=4.858, clip=100, loss_scale=0.5, train_wall=18, gb_free=21.4, wall=34594
2023-05-20 02:14:05 | INFO | train_inner | epoch 133:     37 / 65 loss=6.733, nll_loss=4.999, ppl=31.97, wps=3599.6, ups=0.29, wpb=12328.8, bsz=568.8, num_updates=8595, lr=7.16837e-06, gnorm=5.77, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.4, wall=34611
2023-05-20 02:14:20 | INFO | train_inner | epoch 133:     42 / 65 loss=6.666, nll_loss=4.919, ppl=30.26, wps=4115.3, ups=0.34, wpb=12115, bsz=538.2, num_updates=8600, lr=7.14286e-06, gnorm=4.91, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=34625
2023-05-20 02:14:36 | INFO | train_inner | epoch 133:     47 / 65 loss=6.93, nll_loss=5.209, ppl=36.98, wps=3658.2, ups=0.3, wpb=12099.8, bsz=640.6, num_updates=8605, lr=7.11735e-06, gnorm=7.832, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.7, wall=34642
2023-05-20 02:14:52 | INFO | train_inner | epoch 133:     52 / 65 loss=6.637, nll_loss=4.889, ppl=29.62, wps=3653.7, ups=0.31, wpb=11708.2, bsz=484.8, num_updates=8610, lr=7.09184e-06, gnorm=6.814, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=34658
2023-05-20 02:15:07 | INFO | train_inner | epoch 133:     57 / 65 loss=6.627, nll_loss=4.878, ppl=29.41, wps=4174.9, ups=0.34, wpb=12361.4, bsz=536.4, num_updates=8615, lr=7.06633e-06, gnorm=4.368, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.6, wall=34673
2023-05-20 02:15:22 | INFO | train_inner | epoch 133:     62 / 65 loss=6.687, nll_loss=4.946, ppl=30.82, wps=3939.8, ups=0.33, wpb=12008.6, bsz=547.4, num_updates=8620, lr=7.04082e-06, gnorm=5.733, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.3, wall=34688
2023-05-20 02:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:15:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:16:11 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 6.072 | nll_loss 4.179 | ppl 18.12 | bleu 5.64 | wps 864.1 | wpb 2785 | bsz 105.2 | num_updates 8623 | best_loss 6.054
2023-05-20 02:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 8623 updates
2023-05-20 02:16:11 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint133.pt
2023-05-20 02:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint133.pt
2023-05-20 02:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint133.pt (epoch 133 @ 8623 updates, score 6.072) (writing took 6.94440022110939 seconds)
2023-05-20 02:16:18 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2023-05-20 02:16:18 | INFO | train | epoch 133 | loss 6.723 | nll_loss 4.985 | ppl 31.67 | wps 3084.8 | ups 0.26 | wpb 12060.8 | bsz 559.7 | num_updates 8623 | lr 7.02551e-06 | gnorm 6.196 | clip 100 | loss_scale 0.5 | train_wall 205 | gb_free 21.5 | wall 34744
2023-05-20 02:16:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:16:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:16:18 | INFO | fairseq.trainer | begin training epoch 134
2023-05-20 02:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:16:24 | INFO | train_inner | epoch 134:      2 / 65 loss=6.711, nll_loss=4.975, ppl=31.46, wps=868.5, ups=0.08, wpb=10725.4, bsz=489, num_updates=8625, lr=7.01531e-06, gnorm=4.8, clip=100, loss_scale=0.5, train_wall=13, gb_free=20.5, wall=34750
2023-05-20 02:16:40 | INFO | train_inner | epoch 134:      7 / 65 loss=6.717, nll_loss=4.977, ppl=31.5, wps=3946.1, ups=0.32, wpb=12235.6, bsz=560.2, num_updates=8630, lr=6.9898e-06, gnorm=3.546, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=34765
2023-05-20 02:16:56 | INFO | train_inner | epoch 134:     12 / 65 loss=6.698, nll_loss=4.96, ppl=31.12, wps=3605.9, ups=0.3, wpb=12220.2, bsz=539.6, num_updates=8635, lr=6.96429e-06, gnorm=4.096, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=34782
2023-05-20 02:17:12 | INFO | train_inner | epoch 134:     17 / 65 loss=6.657, nll_loss=4.915, ppl=30.16, wps=4061, ups=0.33, wpb=12317.4, bsz=554.6, num_updates=8640, lr=6.93878e-06, gnorm=3.772, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.8, wall=34797
2023-05-20 02:17:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 02:17:33 | INFO | train_inner | epoch 134:     23 / 65 loss=6.654, nll_loss=4.905, ppl=29.97, wps=2928.2, ups=0.24, wpb=12318.8, bsz=569.6, num_updates=8645, lr=6.91327e-06, gnorm=11.1, clip=100, loss_scale=0.25, train_wall=21, gb_free=21.8, wall=34818
2023-05-20 02:17:49 | INFO | train_inner | epoch 134:     28 / 65 loss=6.96, nll_loss=5.245, ppl=37.92, wps=3993.4, ups=0.31, wpb=12701.4, bsz=716.6, num_updates=8650, lr=6.88776e-06, gnorm=4.492, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=34834
2023-05-20 02:18:06 | INFO | train_inner | epoch 134:     33 / 65 loss=6.799, nll_loss=5.073, ppl=33.66, wps=3558.4, ups=0.29, wpb=12130.6, bsz=560.4, num_updates=8655, lr=6.86224e-06, gnorm=12.137, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=34851
2023-05-20 02:18:22 | INFO | train_inner | epoch 134:     38 / 65 loss=6.694, nll_loss=4.959, ppl=31.11, wps=3655.7, ups=0.3, wpb=12236.6, bsz=601.4, num_updates=8660, lr=6.83673e-06, gnorm=4.651, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.4, wall=34868
2023-05-20 02:18:38 | INFO | train_inner | epoch 134:     43 / 65 loss=6.664, nll_loss=4.92, ppl=30.27, wps=3699.5, ups=0.31, wpb=11869.6, bsz=536, num_updates=8665, lr=6.81122e-06, gnorm=7.271, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=34884
2023-05-20 02:18:55 | INFO | train_inner | epoch 134:     48 / 65 loss=6.638, nll_loss=4.888, ppl=29.6, wps=3687.7, ups=0.31, wpb=11883.2, bsz=461.8, num_updates=8670, lr=6.78571e-06, gnorm=4.547, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=34900
2023-05-20 02:19:10 | INFO | train_inner | epoch 134:     53 / 65 loss=6.647, nll_loss=4.902, ppl=29.89, wps=3859.8, ups=0.32, wpb=11879.2, bsz=505.8, num_updates=8675, lr=6.7602e-06, gnorm=4.217, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=34916
2023-05-20 02:19:25 | INFO | train_inner | epoch 134:     58 / 65 loss=6.726, nll_loss=4.989, ppl=31.77, wps=4159.3, ups=0.34, wpb=12253.4, bsz=612.4, num_updates=8680, lr=6.73469e-06, gnorm=2.538, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=34930
2023-05-20 02:19:42 | INFO | train_inner | epoch 134:     63 / 65 loss=6.853, nll_loss=5.13, ppl=35.02, wps=3476, ups=0.29, wpb=11993.2, bsz=583.4, num_updates=8685, lr=6.70918e-06, gnorm=3.867, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=34948
2023-05-20 02:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:19:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:20:26 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 6.083 | nll_loss 4.189 | ppl 18.24 | bleu 5.55 | wps 918.5 | wpb 2785 | bsz 105.2 | num_updates 8687 | best_loss 6.054
2023-05-20 02:20:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 8687 updates
2023-05-20 02:20:26 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint134.pt
2023-05-20 02:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint134.pt
2023-05-20 02:20:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint134.pt (epoch 134 @ 8687 updates, score 6.083) (writing took 6.800498183816671 seconds)
2023-05-20 02:20:33 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2023-05-20 02:20:33 | INFO | train | epoch 134 | loss 6.725 | nll_loss 4.987 | ppl 31.71 | wps 3029.6 | ups 0.25 | wpb 12048.8 | bsz 559.2 | num_updates 8687 | lr 6.69898e-06 | gnorm 5.706 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21 | wall 34998
2023-05-20 02:20:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:20:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:20:33 | INFO | fairseq.trainer | begin training epoch 135
2023-05-20 02:20:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:20:44 | INFO | train_inner | epoch 135:      3 / 65 loss=6.631, nll_loss=4.881, ppl=29.47, wps=846.8, ups=0.08, wpb=10490.8, bsz=474.2, num_updates=8690, lr=6.68367e-06, gnorm=9.763, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35010
2023-05-20 02:21:01 | INFO | train_inner | epoch 135:      8 / 65 loss=6.785, nll_loss=5.056, ppl=33.26, wps=3512.9, ups=0.29, wpb=12043.2, bsz=583.6, num_updates=8695, lr=6.65816e-06, gnorm=3.652, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=35027
2023-05-20 02:21:17 | INFO | train_inner | epoch 135:     13 / 65 loss=6.664, nll_loss=4.922, ppl=30.33, wps=3934.4, ups=0.31, wpb=12495.4, bsz=572.6, num_updates=8700, lr=6.63265e-06, gnorm=2.584, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=35043
2023-05-20 02:21:33 | INFO | train_inner | epoch 135:     18 / 65 loss=6.759, nll_loss=5.033, ppl=32.75, wps=3783.8, ups=0.31, wpb=12345.6, bsz=620.6, num_updates=8705, lr=6.60714e-06, gnorm=3.19, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=35059
2023-05-20 02:21:49 | INFO | train_inner | epoch 135:     23 / 65 loss=6.86, nll_loss=5.135, ppl=35.13, wps=3950.3, ups=0.33, wpb=12140, bsz=585.8, num_updates=8710, lr=6.58163e-06, gnorm=2.28, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=35074
2023-05-20 02:22:05 | INFO | train_inner | epoch 135:     28 / 65 loss=6.696, nll_loss=4.957, ppl=31.06, wps=3851.6, ups=0.31, wpb=12337, bsz=552.4, num_updates=8715, lr=6.55612e-06, gnorm=6.866, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=35090
2023-05-20 02:22:21 | INFO | train_inner | epoch 135:     33 / 65 loss=6.73, nll_loss=4.998, ppl=31.97, wps=3784.7, ups=0.31, wpb=12136.4, bsz=548.8, num_updates=8720, lr=6.53061e-06, gnorm=7.183, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=35106
2023-05-20 02:22:37 | INFO | train_inner | epoch 135:     38 / 65 loss=6.726, nll_loss=4.987, ppl=31.72, wps=3703.4, ups=0.31, wpb=12040.2, bsz=541.2, num_updates=8725, lr=6.5051e-06, gnorm=9.074, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=35123
2023-05-20 02:22:53 | INFO | train_inner | epoch 135:     43 / 65 loss=6.858, nll_loss=5.127, ppl=34.95, wps=3793.5, ups=0.31, wpb=12094.4, bsz=526.6, num_updates=8730, lr=6.47959e-06, gnorm=5.902, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=35139
2023-05-20 02:23:08 | INFO | train_inner | epoch 135:     48 / 65 loss=6.73, nll_loss=4.996, ppl=31.92, wps=3942.9, ups=0.32, wpb=12335.6, bsz=614.6, num_updates=8735, lr=6.45408e-06, gnorm=4.119, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=35154
2023-05-20 02:23:24 | INFO | train_inner | epoch 135:     53 / 65 loss=6.694, nll_loss=4.956, ppl=31.03, wps=3934.3, ups=0.32, wpb=12261.6, bsz=553.6, num_updates=8740, lr=6.42857e-06, gnorm=8.36, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.6, wall=35170
2023-05-20 02:23:40 | INFO | train_inner | epoch 135:     58 / 65 loss=6.671, nll_loss=4.921, ppl=30.29, wps=3907.6, ups=0.32, wpb=12356.2, bsz=587.6, num_updates=8745, lr=6.40306e-06, gnorm=5.215, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35186
2023-05-20 02:23:58 | INFO | train_inner | epoch 135:     63 / 65 loss=6.659, nll_loss=4.918, ppl=30.23, wps=3193, ups=0.27, wpb=11817, bsz=497.8, num_updates=8750, lr=6.37755e-06, gnorm=11.207, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=35204
2023-05-20 02:24:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:24:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:24:46 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 6.079 | nll_loss 4.189 | ppl 18.23 | bleu 5.43 | wps 834.9 | wpb 2785 | bsz 105.2 | num_updates 8752 | best_loss 6.054
2023-05-20 02:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 8752 updates
2023-05-20 02:24:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint135.pt
2023-05-20 02:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint135.pt
2023-05-20 02:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint135.pt (epoch 135 @ 8752 updates, score 6.079) (writing took 6.800230674445629 seconds)
2023-05-20 02:24:52 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2023-05-20 02:24:52 | INFO | train | epoch 135 | loss 6.731 | nll_loss 4.995 | ppl 31.89 | wps 3016.9 | ups 0.25 | wpb 12056.2 | bsz 560.2 | num_updates 8752 | lr 6.36735e-06 | gnorm 6.627 | clip 100 | loss_scale 0.25 | train_wall 210 | gb_free 21.4 | wall 35258
2023-05-20 02:24:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:24:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:24:52 | INFO | fairseq.trainer | begin training epoch 136
2023-05-20 02:24:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:25:03 | INFO | train_inner | epoch 136:      3 / 65 loss=6.805, nll_loss=5.08, ppl=33.82, wps=812.7, ups=0.08, wpb=10560.8, bsz=521, num_updates=8755, lr=6.35204e-06, gnorm=13.579, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=35269
2023-05-20 02:25:19 | INFO | train_inner | epoch 136:      8 / 65 loss=6.909, nll_loss=5.184, ppl=36.36, wps=3691.9, ups=0.31, wpb=11947.6, bsz=596.4, num_updates=8760, lr=6.32653e-06, gnorm=4.972, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=35285
2023-05-20 02:25:34 | INFO | train_inner | epoch 136:     13 / 65 loss=6.705, nll_loss=4.973, ppl=31.42, wps=4155.1, ups=0.33, wpb=12443.4, bsz=654.6, num_updates=8765, lr=6.30102e-06, gnorm=5.13, clip=100, loss_scale=0.25, train_wall=15, gb_free=21, wall=35300
2023-05-20 02:25:51 | INFO | train_inner | epoch 136:     18 / 65 loss=6.631, nll_loss=4.887, ppl=29.59, wps=3658.8, ups=0.31, wpb=11959.2, bsz=506.6, num_updates=8770, lr=6.27551e-06, gnorm=7.544, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35317
2023-05-20 02:26:06 | INFO | train_inner | epoch 136:     23 / 65 loss=6.667, nll_loss=4.927, ppl=30.42, wps=3906.8, ups=0.32, wpb=12106.4, bsz=521.2, num_updates=8775, lr=6.25e-06, gnorm=4.964, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=35332
2023-05-20 02:26:22 | INFO | train_inner | epoch 136:     28 / 65 loss=6.653, nll_loss=4.909, ppl=30.03, wps=3883.3, ups=0.32, wpb=12192.2, bsz=512, num_updates=8780, lr=6.22449e-06, gnorm=5.751, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35348
2023-05-20 02:26:37 | INFO | train_inner | epoch 136:     33 / 65 loss=6.686, nll_loss=4.952, ppl=30.95, wps=3953.5, ups=0.32, wpb=12206, bsz=591.2, num_updates=8785, lr=6.19898e-06, gnorm=4.781, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=35363
2023-05-20 02:26:55 | INFO | train_inner | epoch 136:     38 / 65 loss=6.643, nll_loss=4.894, ppl=29.73, wps=3595, ups=0.29, wpb=12466, bsz=565.4, num_updates=8790, lr=6.17347e-06, gnorm=8.108, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=35381
2023-05-20 02:27:11 | INFO | train_inner | epoch 136:     43 / 65 loss=6.713, nll_loss=4.975, ppl=31.44, wps=3577.7, ups=0.3, wpb=11878.8, bsz=555.4, num_updates=8795, lr=6.14796e-06, gnorm=6.548, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=35397
2023-05-20 02:27:28 | INFO | train_inner | epoch 136:     48 / 65 loss=6.725, nll_loss=4.99, ppl=31.78, wps=3662.2, ups=0.3, wpb=12190.2, bsz=584, num_updates=8800, lr=6.12245e-06, gnorm=3.645, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.3, wall=35414
2023-05-20 02:27:43 | INFO | train_inner | epoch 136:     53 / 65 loss=6.875, nll_loss=5.149, ppl=35.49, wps=4064, ups=0.33, wpb=12423.2, bsz=578.6, num_updates=8805, lr=6.09694e-06, gnorm=6.418, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=35429
2023-05-20 02:27:59 | INFO | train_inner | epoch 136:     58 / 65 loss=6.723, nll_loss=4.985, ppl=31.66, wps=3863.8, ups=0.31, wpb=12412.2, bsz=522.4, num_updates=8810, lr=6.07143e-06, gnorm=5.256, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35445
2023-05-20 02:28:15 | INFO | train_inner | epoch 136:     63 / 65 loss=6.701, nll_loss=4.968, ppl=31.29, wps=3835, ups=0.32, wpb=12131.2, bsz=602, num_updates=8815, lr=6.04592e-06, gnorm=7.274, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35461
2023-05-20 02:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:28:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:29:03 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 6.058 | nll_loss 4.162 | ppl 17.9 | bleu 5.59 | wps 830 | wpb 2785 | bsz 105.2 | num_updates 8817 | best_loss 6.054
2023-05-20 02:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 8817 updates
2023-05-20 02:29:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint136.pt
2023-05-20 02:29:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint136.pt
2023-05-20 02:29:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint136.pt (epoch 136 @ 8817 updates, score 6.058) (writing took 7.056346129626036 seconds)
2023-05-20 02:29:10 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2023-05-20 02:29:10 | INFO | train | epoch 136 | loss 6.724 | nll_loss 4.988 | ppl 31.73 | wps 3044.5 | ups 0.25 | wpb 12055.4 | bsz 559.1 | num_updates 8817 | lr 6.03571e-06 | gnorm 6.799 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.6 | wall 35515
2023-05-20 02:29:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:29:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:29:10 | INFO | fairseq.trainer | begin training epoch 137
2023-05-20 02:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:29:19 | INFO | train_inner | epoch 137:      3 / 65 loss=6.791, nll_loss=5.065, ppl=33.48, wps=820, ups=0.08, wpb=10418.4, bsz=536.8, num_updates=8820, lr=6.02041e-06, gnorm=18.186, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=35524
2023-05-20 02:29:35 | INFO | train_inner | epoch 137:      8 / 65 loss=6.785, nll_loss=5.038, ppl=32.85, wps=3795.6, ups=0.31, wpb=12284.4, bsz=575, num_updates=8825, lr=5.9949e-06, gnorm=6.455, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=35541
2023-05-20 02:29:52 | INFO | train_inner | epoch 137:     13 / 65 loss=6.664, nll_loss=4.923, ppl=30.33, wps=3549.7, ups=0.3, wpb=11799.2, bsz=491, num_updates=8830, lr=5.96939e-06, gnorm=5.845, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.7, wall=35557
2023-05-20 02:30:07 | INFO | train_inner | epoch 137:     18 / 65 loss=6.66, nll_loss=4.914, ppl=30.16, wps=3853.8, ups=0.32, wpb=12125.2, bsz=528, num_updates=8835, lr=5.94388e-06, gnorm=7.942, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35573
2023-05-20 02:30:23 | INFO | train_inner | epoch 137:     23 / 65 loss=6.672, nll_loss=4.931, ppl=30.5, wps=3678.9, ups=0.31, wpb=11838.2, bsz=523.6, num_updates=8840, lr=5.91837e-06, gnorm=15.583, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35589
2023-05-20 02:30:39 | INFO | train_inner | epoch 137:     28 / 65 loss=6.835, nll_loss=5.11, ppl=34.53, wps=3973.3, ups=0.32, wpb=12338.8, bsz=673, num_updates=8845, lr=5.89286e-06, gnorm=4.37, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=35605
2023-05-20 02:30:54 | INFO | train_inner | epoch 137:     33 / 65 loss=6.648, nll_loss=4.902, ppl=29.91, wps=3938, ups=0.33, wpb=12033.8, bsz=531.6, num_updates=8850, lr=5.86735e-06, gnorm=6.916, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=35620
2023-05-20 02:31:10 | INFO | train_inner | epoch 137:     38 / 65 loss=6.702, nll_loss=4.964, ppl=31.21, wps=3955, ups=0.32, wpb=12507.6, bsz=576.2, num_updates=8855, lr=5.84184e-06, gnorm=4.319, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=35636
2023-05-20 02:31:26 | INFO | train_inner | epoch 137:     43 / 65 loss=6.735, nll_loss=5.002, ppl=32.05, wps=3894.7, ups=0.32, wpb=12125.8, bsz=597.4, num_updates=8860, lr=5.81633e-06, gnorm=8.677, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=35651
2023-05-20 02:31:40 | INFO | train_inner | epoch 137:     48 / 65 loss=6.698, nll_loss=4.962, ppl=31.18, wps=4298.3, ups=0.34, wpb=12487.6, bsz=615.8, num_updates=8865, lr=5.79082e-06, gnorm=7.68, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=35666
2023-05-20 02:31:58 | INFO | train_inner | epoch 137:     53 / 65 loss=6.798, nll_loss=5.066, ppl=33.5, wps=3429.5, ups=0.28, wpb=12103.8, bsz=538.2, num_updates=8870, lr=5.76531e-06, gnorm=5.138, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=35683
2023-05-20 02:32:14 | INFO | train_inner | epoch 137:     58 / 65 loss=6.707, nll_loss=4.966, ppl=31.24, wps=3766.6, ups=0.31, wpb=12292.4, bsz=510.6, num_updates=8875, lr=5.7398e-06, gnorm=3.278, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=35700
2023-05-20 02:32:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 02:32:32 | INFO | train_inner | epoch 137:     64 / 65 loss=6.66, nll_loss=4.914, ppl=30.15, wps=3410.3, ups=0.28, wpb=12347.6, bsz=569.8, num_updates=8880, lr=5.71429e-06, gnorm=3.168, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=35718
2023-05-20 02:32:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:32:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:33:14 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 6.06 | nll_loss 4.161 | ppl 17.89 | bleu 5.56 | wps 910.8 | wpb 2785 | bsz 105.2 | num_updates 8881 | best_loss 6.054
2023-05-20 02:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 8881 updates
2023-05-20 02:33:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint137.pt
2023-05-20 02:33:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint137.pt
2023-05-20 02:33:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint137.pt (epoch 137 @ 8881 updates, score 6.06) (writing took 7.0197896137833595 seconds)
2023-05-20 02:33:21 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2023-05-20 02:33:21 | INFO | train | epoch 137 | loss 6.72 | nll_loss 4.982 | ppl 31.6 | wps 3074.9 | ups 0.25 | wpb 12064.4 | bsz 559.9 | num_updates 8881 | lr 5.70918e-06 | gnorm 6.441 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.5 | wall 35767
2023-05-20 02:33:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:33:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:33:21 | INFO | fairseq.trainer | begin training epoch 138
2023-05-20 02:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:33:32 | INFO | train_inner | epoch 138:      4 / 65 loss=6.882, nll_loss=5.159, ppl=35.72, wps=875.2, ups=0.08, wpb=10546, bsz=525, num_updates=8885, lr=5.68878e-06, gnorm=5.877, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.5, wall=35778
2023-05-20 02:33:49 | INFO | train_inner | epoch 138:      9 / 65 loss=6.697, nll_loss=4.953, ppl=30.97, wps=3541.6, ups=0.3, wpb=11971.2, bsz=508.8, num_updates=8890, lr=5.66327e-06, gnorm=7.237, clip=100, loss_scale=0.125, train_wall=17, gb_free=21, wall=35795
2023-05-20 02:34:04 | INFO | train_inner | epoch 138:     14 / 65 loss=6.696, nll_loss=4.953, ppl=30.97, wps=4035.8, ups=0.33, wpb=12113.6, bsz=542.4, num_updates=8895, lr=5.63776e-06, gnorm=3.33, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=35810
2023-05-20 02:34:19 | INFO | train_inner | epoch 138:     19 / 65 loss=6.655, nll_loss=4.907, ppl=30, wps=4118.8, ups=0.34, wpb=12123.6, bsz=536.2, num_updates=8900, lr=5.61224e-06, gnorm=4.601, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=35825
2023-05-20 02:34:35 | INFO | train_inner | epoch 138:     24 / 65 loss=6.855, nll_loss=5.123, ppl=34.85, wps=3745.1, ups=0.31, wpb=12257.8, bsz=590.2, num_updates=8905, lr=5.58673e-06, gnorm=4.022, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=35841
2023-05-20 02:34:53 | INFO | train_inner | epoch 138:     29 / 65 loss=6.684, nll_loss=4.944, ppl=30.78, wps=3372.2, ups=0.28, wpb=11902.4, bsz=536.2, num_updates=8910, lr=5.56122e-06, gnorm=4.141, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=35859
2023-05-20 02:35:09 | INFO | train_inner | epoch 138:     34 / 65 loss=6.766, nll_loss=5.031, ppl=32.7, wps=3831.4, ups=0.31, wpb=12198, bsz=569, num_updates=8915, lr=5.53571e-06, gnorm=3.359, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=35875
2023-05-20 02:35:25 | INFO | train_inner | epoch 138:     39 / 65 loss=6.726, nll_loss=4.993, ppl=31.86, wps=3804.9, ups=0.31, wpb=12259, bsz=648.2, num_updates=8920, lr=5.5102e-06, gnorm=3.769, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=35891
2023-05-20 02:35:41 | INFO | train_inner | epoch 138:     44 / 65 loss=6.737, nll_loss=5.004, ppl=32.09, wps=3782.5, ups=0.31, wpb=12022.4, bsz=596.4, num_updates=8925, lr=5.48469e-06, gnorm=3.469, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=35907
2023-05-20 02:35:56 | INFO | train_inner | epoch 138:     49 / 65 loss=6.586, nll_loss=4.826, ppl=28.37, wps=4116.9, ups=0.33, wpb=12415.8, bsz=558.2, num_updates=8930, lr=5.45918e-06, gnorm=6.964, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=35922
2023-05-20 02:36:14 | INFO | train_inner | epoch 138:     54 / 65 loss=6.661, nll_loss=4.917, ppl=30.22, wps=3337.1, ups=0.27, wpb=12287.6, bsz=507, num_updates=8935, lr=5.43367e-06, gnorm=3.806, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=35940
2023-05-20 02:36:30 | INFO | train_inner | epoch 138:     59 / 65 loss=6.702, nll_loss=4.966, ppl=31.26, wps=3847.9, ups=0.32, wpb=12166.2, bsz=560.2, num_updates=8940, lr=5.40816e-06, gnorm=3.156, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=35956
2023-05-20 02:36:44 | INFO | train_inner | epoch 138:     64 / 65 loss=6.714, nll_loss=4.977, ppl=31.5, wps=4578.3, ups=0.37, wpb=12465.2, bsz=604.2, num_updates=8945, lr=5.38265e-06, gnorm=7.804, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=35970
2023-05-20 02:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:36:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:37:31 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 6.058 | nll_loss 4.163 | ppl 17.91 | bleu 5.4 | wps 840.2 | wpb 2785 | bsz 105.2 | num_updates 8946 | best_loss 6.054
2023-05-20 02:37:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 8946 updates
2023-05-20 02:37:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint138.pt
2023-05-20 02:37:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint138.pt
2023-05-20 02:37:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint138.pt (epoch 138 @ 8946 updates, score 6.058) (writing took 6.736456550657749 seconds)
2023-05-20 02:37:38 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2023-05-20 02:37:38 | INFO | train | epoch 138 | loss 6.718 | nll_loss 4.978 | ppl 31.52 | wps 3050.6 | ups 0.25 | wpb 12051.4 | bsz 559.6 | num_updates 8946 | lr 5.37755e-06 | gnorm 4.838 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.1 | wall 36023
2023-05-20 02:37:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:37:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:37:38 | INFO | fairseq.trainer | begin training epoch 139
2023-05-20 02:37:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:37:52 | INFO | train_inner | epoch 139:      4 / 65 loss=6.877, nll_loss=5.144, ppl=35.36, wps=785.9, ups=0.07, wpb=10649.8, bsz=503, num_updates=8950, lr=5.35714e-06, gnorm=6.898, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.2, wall=36037
2023-05-20 02:38:08 | INFO | train_inner | epoch 139:      9 / 65 loss=6.787, nll_loss=5.059, ppl=33.34, wps=3823, ups=0.3, wpb=12572.2, bsz=683.4, num_updates=8955, lr=5.33163e-06, gnorm=3.575, clip=100, loss_scale=0.125, train_wall=16, gb_free=21, wall=36054
2023-05-20 02:38:23 | INFO | train_inner | epoch 139:     14 / 65 loss=6.712, nll_loss=4.973, ppl=31.41, wps=4303.8, ups=0.35, wpb=12317.4, bsz=527.2, num_updates=8960, lr=5.30612e-06, gnorm=14.696, clip=100, loss_scale=0.125, train_wall=14, gb_free=20.4, wall=36069
2023-05-20 02:38:37 | INFO | train_inner | epoch 139:     19 / 65 loss=6.686, nll_loss=4.947, ppl=30.84, wps=4250.3, ups=0.35, wpb=12184.6, bsz=562.2, num_updates=8965, lr=5.28061e-06, gnorm=4.511, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=36083
2023-05-20 02:38:54 | INFO | train_inner | epoch 139:     24 / 65 loss=6.7, nll_loss=4.96, ppl=31.12, wps=3519.3, ups=0.3, wpb=11627.6, bsz=524, num_updates=8970, lr=5.2551e-06, gnorm=9.321, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=36100
2023-05-20 02:39:09 | INFO | train_inner | epoch 139:     29 / 65 loss=6.696, nll_loss=4.957, ppl=31.06, wps=4022.7, ups=0.33, wpb=12113.6, bsz=562.4, num_updates=8975, lr=5.22959e-06, gnorm=8.505, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=36115
2023-05-20 02:39:25 | INFO | train_inner | epoch 139:     34 / 65 loss=6.728, nll_loss=4.999, ppl=31.98, wps=3659.6, ups=0.3, wpb=12105, bsz=560, num_updates=8980, lr=5.20408e-06, gnorm=3.926, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=36131
2023-05-20 02:39:42 | INFO | train_inner | epoch 139:     39 / 65 loss=6.654, nll_loss=4.911, ppl=30.08, wps=3748.6, ups=0.31, wpb=12177, bsz=570, num_updates=8985, lr=5.17857e-06, gnorm=5.178, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=36147
2023-05-20 02:39:57 | INFO | train_inner | epoch 139:     44 / 65 loss=6.751, nll_loss=5.014, ppl=32.31, wps=3793.6, ups=0.32, wpb=11933.6, bsz=550, num_updates=8990, lr=5.15306e-06, gnorm=5.068, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=36163
2023-05-20 02:40:14 | INFO | train_inner | epoch 139:     49 / 65 loss=6.686, nll_loss=4.95, ppl=30.9, wps=3604.8, ups=0.29, wpb=12346.4, bsz=566.6, num_updates=8995, lr=5.12755e-06, gnorm=5.697, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=36180
2023-05-20 02:40:30 | INFO | train_inner | epoch 139:     54 / 65 loss=6.638, nll_loss=4.89, ppl=29.65, wps=3850.9, ups=0.32, wpb=12068, bsz=552.8, num_updates=9000, lr=5.10204e-06, gnorm=3.018, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=36196
2023-05-20 02:40:46 | INFO | train_inner | epoch 139:     59 / 65 loss=6.659, nll_loss=4.912, ppl=30.11, wps=4006.5, ups=0.32, wpb=12414.6, bsz=543.4, num_updates=9005, lr=5.07653e-06, gnorm=5.984, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=36211
2023-05-20 02:41:02 | INFO | train_inner | epoch 139:     64 / 65 loss=6.745, nll_loss=5.011, ppl=32.25, wps=3701.2, ups=0.3, wpb=12166.6, bsz=544.2, num_updates=9010, lr=5.05102e-06, gnorm=5.119, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=36228
2023-05-20 02:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:41:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:41:46 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 6.053 | nll_loss 4.154 | ppl 17.8 | bleu 5.53 | wps 853.2 | wpb 2785 | bsz 105.2 | num_updates 9011 | best_loss 6.053
2023-05-20 02:41:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 9011 updates
2023-05-20 02:41:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint139.pt
2023-05-20 02:41:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint139.pt
2023-05-20 02:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint139.pt (epoch 139 @ 9011 updates, score 6.053) (writing took 11.878668520599604 seconds)
2023-05-20 02:41:57 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2023-05-20 02:41:57 | INFO | train | epoch 139 | loss 6.716 | nll_loss 4.978 | ppl 31.52 | wps 3016.6 | ups 0.25 | wpb 12056.9 | bsz 559.2 | num_updates 9011 | lr 5.04592e-06 | gnorm 6.173 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 20.4 | wall 36283
2023-05-20 02:41:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:41:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:41:57 | INFO | fairseq.trainer | begin training epoch 140
2023-05-20 02:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:42:11 | INFO | train_inner | epoch 140:      4 / 65 loss=6.669, nll_loss=4.923, ppl=30.35, wps=749, ups=0.07, wpb=10396, bsz=501.4, num_updates=9015, lr=5.02551e-06, gnorm=4.857, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.5, wall=36297
2023-05-20 02:42:27 | INFO | train_inner | epoch 140:      9 / 65 loss=6.7, nll_loss=4.962, ppl=31.18, wps=3996.7, ups=0.33, wpb=12234.2, bsz=591.6, num_updates=9020, lr=5e-06, gnorm=5.481, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=36313
2023-05-20 02:42:42 | INFO | train_inner | epoch 140:     14 / 65 loss=6.677, nll_loss=4.933, ppl=30.55, wps=4117.3, ups=0.33, wpb=12407.8, bsz=555.6, num_updates=9025, lr=4.97449e-06, gnorm=8.419, clip=100, loss_scale=0.125, train_wall=15, gb_free=21, wall=36328
2023-05-20 02:42:57 | INFO | train_inner | epoch 140:     19 / 65 loss=6.754, nll_loss=5.027, ppl=32.6, wps=4037.2, ups=0.32, wpb=12533.6, bsz=633.2, num_updates=9030, lr=4.94898e-06, gnorm=6.56, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=36343
2023-05-20 02:43:13 | INFO | train_inner | epoch 140:     24 / 65 loss=6.844, nll_loss=5.111, ppl=34.56, wps=3783.1, ups=0.32, wpb=12005.4, bsz=569.6, num_updates=9035, lr=4.92347e-06, gnorm=4.867, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=36359
2023-05-20 02:43:29 | INFO | train_inner | epoch 140:     29 / 65 loss=6.765, nll_loss=5.033, ppl=32.74, wps=3984.7, ups=0.33, wpb=12257, bsz=597.2, num_updates=9040, lr=4.89796e-06, gnorm=4.243, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=36374
2023-05-20 02:43:44 | INFO | train_inner | epoch 140:     34 / 65 loss=6.665, nll_loss=4.92, ppl=30.27, wps=3982.6, ups=0.32, wpb=12334.6, bsz=590.2, num_updates=9045, lr=4.87245e-06, gnorm=5.196, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=36390
2023-05-20 02:44:03 | INFO | train_inner | epoch 140:     39 / 65 loss=6.815, nll_loss=5.082, ppl=33.88, wps=3374.3, ups=0.27, wpb=12489, bsz=589.8, num_updates=9050, lr=4.84694e-06, gnorm=8.976, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.8, wall=36408
2023-05-20 02:44:18 | INFO | train_inner | epoch 140:     44 / 65 loss=6.629, nll_loss=4.88, ppl=29.45, wps=3846.4, ups=0.32, wpb=11909.2, bsz=521.8, num_updates=9055, lr=4.82143e-06, gnorm=3.857, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=36424
2023-05-20 02:44:34 | INFO | train_inner | epoch 140:     49 / 65 loss=6.66, nll_loss=4.918, ppl=30.23, wps=3812.3, ups=0.32, wpb=12100.8, bsz=511, num_updates=9060, lr=4.79592e-06, gnorm=5.303, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=36440
2023-05-20 02:44:50 | INFO | train_inner | epoch 140:     54 / 65 loss=6.655, nll_loss=4.908, ppl=30.02, wps=3607.8, ups=0.31, wpb=11640.2, bsz=474.8, num_updates=9065, lr=4.77041e-06, gnorm=5.514, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=36456
2023-05-20 02:45:05 | INFO | train_inner | epoch 140:     59 / 65 loss=6.683, nll_loss=4.942, ppl=30.75, wps=4093.7, ups=0.33, wpb=12262.2, bsz=590.6, num_updates=9070, lr=4.7449e-06, gnorm=8.731, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=36471
2023-05-20 02:45:21 | INFO | train_inner | epoch 140:     64 / 65 loss=6.698, nll_loss=4.961, ppl=31.15, wps=3921, ups=0.32, wpb=12104.8, bsz=553.8, num_updates=9075, lr=4.71939e-06, gnorm=9.543, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.4, wall=36486
2023-05-20 02:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:45:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:46:04 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 6.036 | nll_loss 4.137 | ppl 17.6 | bleu 5.56 | wps 882.7 | wpb 2785 | bsz 105.2 | num_updates 9076 | best_loss 6.036
2023-05-20 02:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 9076 updates
2023-05-20 02:46:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint140.pt
2023-05-20 02:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint140.pt
2023-05-20 02:46:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint140.pt (epoch 140 @ 9076 updates, score 6.036) (writing took 12.183832500129938 seconds)
2023-05-20 02:46:16 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2023-05-20 02:46:16 | INFO | train | epoch 140 | loss 6.709 | nll_loss 4.97 | ppl 31.34 | wps 3032.1 | ups 0.25 | wpb 12053.4 | bsz 558.9 | num_updates 9076 | lr 4.71429e-06 | gnorm 6.218 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.6 | wall 36542
2023-05-20 02:46:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:46:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:46:16 | INFO | fairseq.trainer | begin training epoch 141
2023-05-20 02:46:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:46:28 | INFO | train_inner | epoch 141:      4 / 65 loss=6.675, nll_loss=4.931, ppl=30.5, wps=736.8, ups=0.07, wpb=9990.8, bsz=432.8, num_updates=9080, lr=4.69388e-06, gnorm=13.52, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=36554
2023-05-20 02:46:45 | INFO | train_inner | epoch 141:      9 / 65 loss=6.992, nll_loss=5.28, ppl=38.85, wps=3799.1, ups=0.31, wpb=12293.4, bsz=672, num_updates=9085, lr=4.66837e-06, gnorm=7.389, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.7, wall=36570
2023-05-20 02:47:01 | INFO | train_inner | epoch 141:     14 / 65 loss=6.663, nll_loss=4.919, ppl=30.24, wps=3787.7, ups=0.31, wpb=12161.4, bsz=522.6, num_updates=9090, lr=4.64286e-06, gnorm=4.566, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=36586
2023-05-20 02:47:16 | INFO | train_inner | epoch 141:     19 / 65 loss=6.684, nll_loss=4.941, ppl=30.72, wps=3946.2, ups=0.32, wpb=12456.6, bsz=601.6, num_updates=9095, lr=4.61735e-06, gnorm=10.333, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=36602
2023-05-20 02:47:34 | INFO | train_inner | epoch 141:     24 / 65 loss=6.737, nll_loss=5.004, ppl=32.08, wps=3477.4, ups=0.29, wpb=12160, bsz=514.6, num_updates=9100, lr=4.59184e-06, gnorm=6.24, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.7, wall=36620
2023-05-20 02:47:49 | INFO | train_inner | epoch 141:     29 / 65 loss=6.611, nll_loss=4.858, ppl=28.99, wps=3966.5, ups=0.32, wpb=12214.2, bsz=530.6, num_updates=9105, lr=4.56633e-06, gnorm=4.431, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=36635
2023-05-20 02:48:05 | INFO | train_inner | epoch 141:     34 / 65 loss=6.741, nll_loss=5.006, ppl=32.12, wps=3721.4, ups=0.31, wpb=12059.8, bsz=611.4, num_updates=9110, lr=4.54082e-06, gnorm=5.7, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=36651
2023-05-20 02:48:21 | INFO | train_inner | epoch 141:     39 / 65 loss=6.694, nll_loss=4.953, ppl=30.97, wps=3832.1, ups=0.31, wpb=12275.8, bsz=549.8, num_updates=9115, lr=4.51531e-06, gnorm=5.119, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=36667
2023-05-20 02:48:38 | INFO | train_inner | epoch 141:     44 / 65 loss=6.721, nll_loss=4.975, ppl=31.46, wps=3563.1, ups=0.29, wpb=12081.6, bsz=539, num_updates=9120, lr=4.4898e-06, gnorm=4.238, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=36684
2023-05-20 02:48:56 | INFO | train_inner | epoch 141:     49 / 65 loss=6.71, nll_loss=4.978, ppl=31.51, wps=3673.2, ups=0.29, wpb=12670, bsz=607, num_updates=9125, lr=4.46429e-06, gnorm=6.309, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=36701
2023-05-20 02:49:10 | INFO | train_inner | epoch 141:     54 / 65 loss=6.694, nll_loss=4.954, ppl=30.99, wps=4143.8, ups=0.34, wpb=12249.8, bsz=569.6, num_updates=9130, lr=4.43878e-06, gnorm=9.097, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=36716
2023-05-20 02:49:25 | INFO | train_inner | epoch 141:     59 / 65 loss=6.73, nll_loss=5, ppl=32, wps=4188.3, ups=0.34, wpb=12327.6, bsz=598.8, num_updates=9135, lr=4.41327e-06, gnorm=5.38, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=36731
2023-05-20 02:49:40 | INFO | train_inner | epoch 141:     64 / 65 loss=6.657, nll_loss=4.912, ppl=30.11, wps=4098.1, ups=0.35, wpb=11850.8, bsz=526.8, num_updates=9140, lr=4.38776e-06, gnorm=4.216, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.5, wall=36745
2023-05-20 02:49:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:49:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:50:21 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 6.043 | nll_loss 4.143 | ppl 17.67 | bleu 5.7 | wps 907.3 | wpb 2785 | bsz 105.2 | num_updates 9141 | best_loss 6.036
2023-05-20 02:50:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 9141 updates
2023-05-20 02:50:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint141.pt
2023-05-20 02:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint141.pt
2023-05-20 02:50:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint141.pt (epoch 141 @ 9141 updates, score 6.043) (writing took 10.61322782933712 seconds)
2023-05-20 02:50:32 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2023-05-20 02:50:32 | INFO | train | epoch 141 | loss 6.716 | nll_loss 4.978 | ppl 31.51 | wps 3063.7 | ups 0.25 | wpb 12059.4 | bsz 560.1 | num_updates 9141 | lr 4.38265e-06 | gnorm 6.716 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.7 | wall 36797
2023-05-20 02:50:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:50:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:50:32 | INFO | fairseq.trainer | begin training epoch 142
2023-05-20 02:50:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:50:44 | INFO | train_inner | epoch 142:      4 / 65 loss=6.654, nll_loss=4.91, ppl=30.06, wps=812.2, ups=0.08, wpb=10392.6, bsz=456.4, num_updates=9145, lr=4.36224e-06, gnorm=5.974, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=36809
2023-05-20 02:51:00 | INFO | train_inner | epoch 142:      9 / 65 loss=6.749, nll_loss=5.011, ppl=32.25, wps=3749.5, ups=0.31, wpb=11910.4, bsz=555.6, num_updates=9150, lr=4.33673e-06, gnorm=3.589, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=36825
2023-05-20 02:51:16 | INFO | train_inner | epoch 142:     14 / 65 loss=6.765, nll_loss=5.032, ppl=32.72, wps=3745.2, ups=0.3, wpb=12396.6, bsz=595.4, num_updates=9155, lr=4.31122e-06, gnorm=7.826, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=36842
2023-05-20 02:51:34 | INFO | train_inner | epoch 142:     19 / 65 loss=6.689, nll_loss=4.949, ppl=30.89, wps=3417.6, ups=0.28, wpb=12147.8, bsz=563.4, num_updates=9160, lr=4.28571e-06, gnorm=6.367, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=36860
2023-05-20 02:51:50 | INFO | train_inner | epoch 142:     24 / 65 loss=6.652, nll_loss=4.907, ppl=29.99, wps=3805.8, ups=0.3, wpb=12502.4, bsz=527.4, num_updates=9165, lr=4.2602e-06, gnorm=4.395, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=36876
2023-05-20 02:52:07 | INFO | train_inner | epoch 142:     29 / 65 loss=6.608, nll_loss=4.852, ppl=28.88, wps=3718.5, ups=0.3, wpb=12429, bsz=567.6, num_updates=9170, lr=4.23469e-06, gnorm=4.117, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=36893
2023-05-20 02:52:22 | INFO | train_inner | epoch 142:     34 / 65 loss=6.715, nll_loss=4.982, ppl=31.61, wps=3990.8, ups=0.32, wpb=12368.4, bsz=669.4, num_updates=9175, lr=4.20918e-06, gnorm=8.525, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=36908
2023-05-20 02:52:40 | INFO | train_inner | epoch 142:     39 / 65 loss=6.655, nll_loss=4.912, ppl=30.1, wps=3496.6, ups=0.29, wpb=11916, bsz=509.4, num_updates=9180, lr=4.18367e-06, gnorm=3.851, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=36925
2023-05-20 02:52:56 | INFO | train_inner | epoch 142:     44 / 65 loss=6.825, nll_loss=5.096, ppl=34.2, wps=3688.6, ups=0.3, wpb=12221.6, bsz=576.4, num_updates=9185, lr=4.15816e-06, gnorm=6.625, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=36942
2023-05-20 02:53:11 | INFO | train_inner | epoch 142:     49 / 65 loss=6.671, nll_loss=4.927, ppl=30.42, wps=3977.3, ups=0.33, wpb=11891.4, bsz=539.6, num_updates=9190, lr=4.13265e-06, gnorm=2.169, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=36957
2023-05-20 02:53:27 | INFO | train_inner | epoch 142:     54 / 65 loss=6.71, nll_loss=4.975, ppl=31.44, wps=3801.1, ups=0.32, wpb=12019.4, bsz=544.4, num_updates=9195, lr=4.10714e-06, gnorm=5.04, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=36973
2023-05-20 02:53:45 | INFO | train_inner | epoch 142:     59 / 65 loss=6.84, nll_loss=5.106, ppl=34.45, wps=3398.7, ups=0.28, wpb=12206, bsz=605.6, num_updates=9200, lr=4.08163e-06, gnorm=7.671, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=36991
2023-05-20 02:54:00 | INFO | train_inner | epoch 142:     64 / 65 loss=6.715, nll_loss=4.975, ppl=31.46, wps=3868.2, ups=0.32, wpb=12092.8, bsz=550.6, num_updates=9205, lr=4.05612e-06, gnorm=4.462, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=37006
2023-05-20 02:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:54:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:54:44 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 6.041 | nll_loss 4.142 | ppl 17.66 | bleu 5.53 | wps 856.2 | wpb 2785 | bsz 105.2 | num_updates 9206 | best_loss 6.036
2023-05-20 02:54:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 9206 updates
2023-05-20 02:54:44 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint142.pt
2023-05-20 02:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint142.pt
2023-05-20 02:54:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint142.pt (epoch 142 @ 9206 updates, score 6.041) (writing took 6.8669935166835785 seconds)
2023-05-20 02:54:51 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2023-05-20 02:54:51 | INFO | train | epoch 142 | loss 6.714 | nll_loss 4.974 | ppl 31.43 | wps 3021 | ups 0.25 | wpb 12044.8 | bsz 558.9 | num_updates 9206 | lr 4.05102e-06 | gnorm 5.413 | clip 100 | loss_scale 0.125 | train_wall 210 | gb_free 20.4 | wall 37057
2023-05-20 02:54:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:54:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:54:51 | INFO | fairseq.trainer | begin training epoch 143
2023-05-20 02:54:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:55:03 | INFO | train_inner | epoch 143:      4 / 65 loss=6.681, nll_loss=4.933, ppl=30.56, wps=820, ups=0.08, wpb=10291.4, bsz=431.8, num_updates=9210, lr=4.03061e-06, gnorm=7.079, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.4, wall=37069
2023-05-20 02:55:18 | INFO | train_inner | epoch 143:      9 / 65 loss=6.704, nll_loss=4.969, ppl=31.32, wps=3992.7, ups=0.33, wpb=12107.4, bsz=568.4, num_updates=9215, lr=4.0051e-06, gnorm=4.558, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=37084
2023-05-20 02:55:33 | INFO | train_inner | epoch 143:     14 / 65 loss=6.75, nll_loss=5.02, ppl=32.45, wps=4082, ups=0.34, wpb=12041, bsz=567.6, num_updates=9220, lr=3.97959e-06, gnorm=5.527, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=37099
2023-05-20 02:55:49 | INFO | train_inner | epoch 143:     19 / 65 loss=6.659, nll_loss=4.914, ppl=30.16, wps=3874.7, ups=0.32, wpb=12208.8, bsz=544.6, num_updates=9225, lr=3.95408e-06, gnorm=5.646, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=37115
2023-05-20 02:56:04 | INFO | train_inner | epoch 143:     24 / 65 loss=6.796, nll_loss=5.066, ppl=33.51, wps=4173.9, ups=0.33, wpb=12504, bsz=607.4, num_updates=9230, lr=3.92857e-06, gnorm=5.076, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=37130
2023-05-20 02:56:22 | INFO | train_inner | epoch 143:     29 / 65 loss=6.663, nll_loss=4.916, ppl=30.19, wps=3223.9, ups=0.27, wpb=11834, bsz=475, num_updates=9235, lr=3.90306e-06, gnorm=3.593, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.4, wall=37148
2023-05-20 02:56:39 | INFO | train_inner | epoch 143:     34 / 65 loss=6.666, nll_loss=4.923, ppl=30.35, wps=3713.8, ups=0.31, wpb=12169.8, bsz=537.4, num_updates=9240, lr=3.87755e-06, gnorm=7.716, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=37164
2023-05-20 02:56:53 | INFO | train_inner | epoch 143:     39 / 65 loss=6.744, nll_loss=5.017, ppl=32.38, wps=4359.7, ups=0.34, wpb=12862, bsz=622.4, num_updates=9245, lr=3.85204e-06, gnorm=4.976, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=37179
2023-05-20 02:57:08 | INFO | train_inner | epoch 143:     44 / 65 loss=6.739, nll_loss=5.006, ppl=32.13, wps=4160.1, ups=0.34, wpb=12119.2, bsz=568.4, num_updates=9250, lr=3.82653e-06, gnorm=15.646, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=37194
2023-05-20 02:57:23 | INFO | train_inner | epoch 143:     49 / 65 loss=6.679, nll_loss=4.937, ppl=30.63, wps=3829.2, ups=0.32, wpb=11933.2, bsz=585.6, num_updates=9255, lr=3.80102e-06, gnorm=3.964, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.1, wall=37209
2023-05-20 02:57:38 | INFO | train_inner | epoch 143:     54 / 65 loss=6.764, nll_loss=5.032, ppl=32.73, wps=4019.5, ups=0.33, wpb=12046.4, bsz=574, num_updates=9260, lr=3.77551e-06, gnorm=6.513, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=37224
2023-05-20 02:57:55 | INFO | train_inner | epoch 143:     59 / 65 loss=6.628, nll_loss=4.88, ppl=29.46, wps=3751.9, ups=0.31, wpb=12220.6, bsz=532.4, num_updates=9265, lr=3.75e-06, gnorm=3.778, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=37241
2023-05-20 02:58:09 | INFO | train_inner | epoch 143:     64 / 65 loss=6.811, nll_loss=5.07, ppl=33.6, wps=4469.8, ups=0.36, wpb=12354.6, bsz=648.6, num_updates=9270, lr=3.72449e-06, gnorm=3.416, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.2, wall=37254
2023-05-20 02:58:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 02:58:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:58:50 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 6.045 | nll_loss 4.146 | ppl 17.71 | bleu 5.56 | wps 913 | wpb 2785 | bsz 105.2 | num_updates 9271 | best_loss 6.036
2023-05-20 02:58:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 9271 updates
2023-05-20 02:58:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint143.pt
2023-05-20 02:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint143.pt
2023-05-20 02:58:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint143.pt (epoch 143 @ 9271 updates, score 6.045) (writing took 7.012412257492542 seconds)
2023-05-20 02:58:57 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2023-05-20 02:58:57 | INFO | train | epoch 143 | loss 6.716 | nll_loss 4.978 | ppl 31.51 | wps 3180.2 | ups 0.26 | wpb 12062.3 | bsz 560 | num_updates 9271 | lr 3.71939e-06 | gnorm 5.965 | clip 100 | loss_scale 0.125 | train_wall 200 | gb_free 21.2 | wall 37303
2023-05-20 02:58:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 02:58:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 02:58:57 | INFO | fairseq.trainer | begin training epoch 144
2023-05-20 02:58:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 02:59:09 | INFO | train_inner | epoch 144:      4 / 65 loss=6.716, nll_loss=4.98, ppl=31.56, wps=891.8, ups=0.08, wpb=10759, bsz=491.6, num_updates=9275, lr=3.69898e-06, gnorm=9, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=37315
2023-05-20 02:59:25 | INFO | train_inner | epoch 144:      9 / 65 loss=6.624, nll_loss=4.875, ppl=29.35, wps=3663.5, ups=0.31, wpb=11947.4, bsz=530.6, num_updates=9280, lr=3.67347e-06, gnorm=11.84, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=37331
2023-05-20 02:59:41 | INFO | train_inner | epoch 144:     14 / 65 loss=6.678, nll_loss=4.939, ppl=30.67, wps=3944.6, ups=0.32, wpb=12258.4, bsz=535.6, num_updates=9285, lr=3.64796e-06, gnorm=7.845, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=37347
2023-05-20 02:59:58 | INFO | train_inner | epoch 144:     19 / 65 loss=6.688, nll_loss=4.949, ppl=30.9, wps=3676.5, ups=0.29, wpb=12668, bsz=574.6, num_updates=9290, lr=3.62245e-06, gnorm=3.453, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.2, wall=37364
2023-05-20 03:00:13 | INFO | train_inner | epoch 144:     24 / 65 loss=6.623, nll_loss=4.872, ppl=29.29, wps=3897.7, ups=0.32, wpb=12035, bsz=557.8, num_updates=9295, lr=3.59694e-06, gnorm=4.962, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=37379
2023-05-20 03:00:28 | INFO | train_inner | epoch 144:     29 / 65 loss=6.68, nll_loss=4.938, ppl=30.65, wps=4070.8, ups=0.33, wpb=12258.6, bsz=554.6, num_updates=9300, lr=3.57143e-06, gnorm=3.759, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=37394
2023-05-20 03:00:45 | INFO | train_inner | epoch 144:     34 / 65 loss=6.741, nll_loss=5.01, ppl=32.22, wps=3681.1, ups=0.3, wpb=12231, bsz=578.4, num_updates=9305, lr=3.54592e-06, gnorm=5.034, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=37411
2023-05-20 03:01:01 | INFO | train_inner | epoch 144:     39 / 65 loss=7.006, nll_loss=5.294, ppl=39.23, wps=3871.8, ups=0.31, wpb=12301.6, bsz=689.6, num_updates=9310, lr=3.52041e-06, gnorm=5.207, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=37427
2023-05-20 03:01:16 | INFO | train_inner | epoch 144:     44 / 65 loss=6.786, nll_loss=5.052, ppl=33.18, wps=3970.9, ups=0.33, wpb=12066, bsz=612.6, num_updates=9315, lr=3.4949e-06, gnorm=3.554, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=37442
2023-05-20 03:01:33 | INFO | train_inner | epoch 144:     49 / 65 loss=6.637, nll_loss=4.89, ppl=29.64, wps=3742.7, ups=0.3, wpb=12323.2, bsz=541.4, num_updates=9320, lr=3.46939e-06, gnorm=5.885, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=37458
2023-05-20 03:01:50 | INFO | train_inner | epoch 144:     54 / 65 loss=6.656, nll_loss=4.912, ppl=30.1, wps=3457.1, ups=0.29, wpb=11794.2, bsz=499, num_updates=9325, lr=3.44388e-06, gnorm=3.695, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=37475
2023-05-20 03:02:05 | INFO | train_inner | epoch 144:     59 / 65 loss=6.687, nll_loss=4.942, ppl=30.74, wps=3891.9, ups=0.33, wpb=11911.8, bsz=532.8, num_updates=9330, lr=3.41837e-06, gnorm=4.234, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=37491
2023-05-20 03:02:20 | INFO | train_inner | epoch 144:     64 / 65 loss=6.733, nll_loss=4.995, ppl=31.89, wps=4218.4, ups=0.34, wpb=12304.2, bsz=592, num_updates=9335, lr=3.39286e-06, gnorm=12.513, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.8, wall=37505
2023-05-20 03:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:02:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:03:04 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 6.042 | nll_loss 4.142 | ppl 17.65 | bleu 5.61 | wps 838.1 | wpb 2785 | bsz 105.2 | num_updates 9336 | best_loss 6.036
2023-05-20 03:03:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 9336 updates
2023-05-20 03:03:04 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint144.pt
2023-05-20 03:03:09 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint144.pt
2023-05-20 03:03:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint144.pt (epoch 144 @ 9336 updates, score 6.042) (writing took 6.7115236558020115 seconds)
2023-05-20 03:03:11 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2023-05-20 03:03:11 | INFO | train | epoch 144 | loss 6.711 | nll_loss 4.972 | ppl 31.39 | wps 3094.6 | ups 0.26 | wpb 12057.7 | bsz 559.9 | num_updates 9336 | lr 3.38776e-06 | gnorm 6.381 | clip 100 | loss_scale 0.125 | train_wall 204 | gb_free 21.7 | wall 37556
2023-05-20 03:03:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:03:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:03:11 | INFO | fairseq.trainer | begin training epoch 145
2023-05-20 03:03:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:03:24 | INFO | train_inner | epoch 145:      4 / 65 loss=6.628, nll_loss=4.876, ppl=29.37, wps=818.4, ups=0.08, wpb=10542.2, bsz=479.8, num_updates=9340, lr=3.36735e-06, gnorm=9.834, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.4, wall=37570
2023-05-20 03:03:40 | INFO | train_inner | epoch 145:      9 / 65 loss=6.658, nll_loss=4.91, ppl=30.06, wps=3701.8, ups=0.31, wpb=12082.4, bsz=577.4, num_updates=9345, lr=3.34184e-06, gnorm=6.691, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=37586
2023-05-20 03:03:56 | INFO | train_inner | epoch 145:     14 / 65 loss=6.696, nll_loss=4.958, ppl=31.07, wps=3880.8, ups=0.33, wpb=11791.6, bsz=601.8, num_updates=9350, lr=3.31633e-06, gnorm=7.865, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=37601
2023-05-20 03:04:11 | INFO | train_inner | epoch 145:     19 / 65 loss=6.709, nll_loss=4.978, ppl=31.51, wps=3897.3, ups=0.32, wpb=12332.8, bsz=582, num_updates=9355, lr=3.29082e-06, gnorm=4.138, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=37617
2023-05-20 03:04:28 | INFO | train_inner | epoch 145:     24 / 65 loss=6.663, nll_loss=4.916, ppl=30.18, wps=3748.7, ups=0.3, wpb=12297, bsz=524.6, num_updates=9360, lr=3.26531e-06, gnorm=4.306, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.9, wall=37633
2023-05-20 03:04:44 | INFO | train_inner | epoch 145:     29 / 65 loss=6.764, nll_loss=5.031, ppl=32.69, wps=3790.8, ups=0.3, wpb=12593.4, bsz=608.4, num_updates=9365, lr=3.2398e-06, gnorm=4.865, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=37650
2023-05-20 03:04:59 | INFO | train_inner | epoch 145:     34 / 65 loss=6.659, nll_loss=4.911, ppl=30.08, wps=3994.7, ups=0.33, wpb=12096.8, bsz=568.4, num_updates=9370, lr=3.21429e-06, gnorm=5.359, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=37665
2023-05-20 03:05:16 | INFO | train_inner | epoch 145:     39 / 65 loss=6.636, nll_loss=4.891, ppl=29.67, wps=3534.6, ups=0.29, wpb=12003.2, bsz=510.4, num_updates=9375, lr=3.18878e-06, gnorm=4.028, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=37682
2023-05-20 03:05:33 | INFO | train_inner | epoch 145:     44 / 65 loss=6.781, nll_loss=5.047, ppl=33.06, wps=3600.5, ups=0.3, wpb=12071.6, bsz=542.6, num_updates=9380, lr=3.16327e-06, gnorm=4.439, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.1, wall=37699
2023-05-20 03:05:49 | INFO | train_inner | epoch 145:     49 / 65 loss=6.678, nll_loss=4.934, ppl=30.57, wps=3808.5, ups=0.31, wpb=12125.2, bsz=532, num_updates=9385, lr=3.13776e-06, gnorm=8.157, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=37715
2023-05-20 03:06:04 | INFO | train_inner | epoch 145:     54 / 65 loss=6.689, nll_loss=4.946, ppl=30.82, wps=3964.3, ups=0.33, wpb=12044.8, bsz=535, num_updates=9390, lr=3.11224e-06, gnorm=5.629, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=37730
2023-05-20 03:06:22 | INFO | train_inner | epoch 145:     59 / 65 loss=6.895, nll_loss=5.161, ppl=35.78, wps=3402.9, ups=0.28, wpb=12350.6, bsz=590.4, num_updates=9395, lr=3.08673e-06, gnorm=2.226, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.4, wall=37748
2023-05-20 03:06:37 | INFO | train_inner | epoch 145:     64 / 65 loss=6.743, nll_loss=5.013, ppl=32.29, wps=4379.3, ups=0.35, wpb=12429.6, bsz=590, num_updates=9400, lr=3.06122e-06, gnorm=4.106, clip=100, loss_scale=0.25, train_wall=14, gb_free=20.7, wall=37762
2023-05-20 03:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:06:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:07:21 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 6.044 | nll_loss 4.146 | ppl 17.7 | bleu 5.57 | wps 839.9 | wpb 2785 | bsz 105.2 | num_updates 9401 | best_loss 6.036
2023-05-20 03:07:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 9401 updates
2023-05-20 03:07:21 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint145.pt
2023-05-20 03:07:26 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint145.pt
2023-05-20 03:07:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint145.pt (epoch 145 @ 9401 updates, score 6.044) (writing took 6.956893939524889 seconds)
2023-05-20 03:07:28 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2023-05-20 03:07:28 | INFO | train | epoch 145 | loss 6.709 | nll_loss 4.969 | ppl 31.31 | wps 3044.8 | ups 0.25 | wpb 12054.8 | bsz 557 | num_updates 9401 | lr 3.05612e-06 | gnorm 5.371 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21.4 | wall 37814
2023-05-20 03:07:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:07:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:07:28 | INFO | fairseq.trainer | begin training epoch 146
2023-05-20 03:07:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:07:41 | INFO | train_inner | epoch 146:      4 / 65 loss=6.7, nll_loss=4.965, ppl=31.24, wps=792.1, ups=0.08, wpb=10260.2, bsz=461, num_updates=9405, lr=3.03571e-06, gnorm=4.861, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=37827
2023-05-20 03:07:58 | INFO | train_inner | epoch 146:      9 / 65 loss=6.936, nll_loss=5.216, ppl=37.18, wps=3858.7, ups=0.3, wpb=12762.8, bsz=662.4, num_updates=9410, lr=3.0102e-06, gnorm=8.993, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.4, wall=37844
2023-05-20 03:08:13 | INFO | train_inner | epoch 146:     14 / 65 loss=6.747, nll_loss=5.009, ppl=32.19, wps=3880.4, ups=0.32, wpb=12021.4, bsz=555.8, num_updates=9415, lr=2.98469e-06, gnorm=6.341, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.4, wall=37859
2023-05-20 03:08:29 | INFO | train_inner | epoch 146:     19 / 65 loss=6.612, nll_loss=4.857, ppl=28.99, wps=3828.4, ups=0.32, wpb=11967, bsz=530.8, num_updates=9420, lr=2.95918e-06, gnorm=3.555, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=37875
2023-05-20 03:08:44 | INFO | train_inner | epoch 146:     24 / 65 loss=6.611, nll_loss=4.86, ppl=29.04, wps=4004.7, ups=0.33, wpb=11977.6, bsz=527.4, num_updates=9425, lr=2.93367e-06, gnorm=3.881, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=37890
2023-05-20 03:08:59 | INFO | train_inner | epoch 146:     29 / 65 loss=6.805, nll_loss=5.075, ppl=33.71, wps=4170.8, ups=0.33, wpb=12517, bsz=618.6, num_updates=9430, lr=2.90816e-06, gnorm=3.972, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=37905
2023-05-20 03:09:15 | INFO | train_inner | epoch 146:     34 / 65 loss=6.677, nll_loss=4.932, ppl=30.52, wps=3838.5, ups=0.32, wpb=12174.8, bsz=594.2, num_updates=9435, lr=2.88265e-06, gnorm=9.573, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=37921
2023-05-20 03:09:32 | INFO | train_inner | epoch 146:     39 / 65 loss=6.701, nll_loss=4.959, ppl=31.1, wps=3523.6, ups=0.29, wpb=11978, bsz=547, num_updates=9440, lr=2.85714e-06, gnorm=9.142, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.7, wall=37938
2023-05-20 03:09:47 | INFO | train_inner | epoch 146:     44 / 65 loss=6.665, nll_loss=4.921, ppl=30.3, wps=3923.7, ups=0.33, wpb=12010.8, bsz=538, num_updates=9445, lr=2.83163e-06, gnorm=3.049, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=37953
2023-05-20 03:10:04 | INFO | train_inner | epoch 146:     49 / 65 loss=6.651, nll_loss=4.907, ppl=30, wps=3595.7, ups=0.29, wpb=12338.6, bsz=503.4, num_updates=9450, lr=2.80612e-06, gnorm=4.761, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.2, wall=37970
2023-05-20 03:10:21 | INFO | train_inner | epoch 146:     54 / 65 loss=6.736, nll_loss=5.006, ppl=32.14, wps=3690.1, ups=0.3, wpb=12157.4, bsz=597, num_updates=9455, lr=2.78061e-06, gnorm=7.611, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=37987
2023-05-20 03:10:36 | INFO | train_inner | epoch 146:     59 / 65 loss=6.69, nll_loss=4.95, ppl=30.92, wps=4124.9, ups=0.34, wpb=12197.4, bsz=571.8, num_updates=9460, lr=2.7551e-06, gnorm=4.417, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=38001
2023-05-20 03:10:49 | INFO | train_inner | epoch 146:     64 / 65 loss=6.712, nll_loss=4.972, ppl=31.38, wps=4450.9, ups=0.36, wpb=12307.8, bsz=581.4, num_updates=9465, lr=2.72959e-06, gnorm=6.542, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=38015
2023-05-20 03:10:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:10:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:11:32 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 6.047 | nll_loss 4.149 | ppl 17.74 | bleu 5.62 | wps 870.7 | wpb 2785 | bsz 105.2 | num_updates 9466 | best_loss 6.036
2023-05-20 03:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 9466 updates
2023-05-20 03:11:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint146.pt
2023-05-20 03:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint146.pt
2023-05-20 03:11:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint146.pt (epoch 146 @ 9466 updates, score 6.047) (writing took 6.623830575495958 seconds)
2023-05-20 03:11:39 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2023-05-20 03:11:39 | INFO | train | epoch 146 | loss 6.712 | nll_loss 4.973 | ppl 31.4 | wps 3123.3 | ups 0.26 | wpb 12057.4 | bsz 560.5 | num_updates 9466 | lr 2.72449e-06 | gnorm 5.864 | clip 100 | loss_scale 0.25 | train_wall 203 | gb_free 21.8 | wall 38065
2023-05-20 03:11:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:11:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:11:39 | INFO | fairseq.trainer | begin training epoch 147
2023-05-20 03:11:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:11:52 | INFO | train_inner | epoch 147:      4 / 65 loss=6.99, nll_loss=5.271, ppl=38.63, wps=845.6, ups=0.08, wpb=10523.8, bsz=588.8, num_updates=9470, lr=2.70408e-06, gnorm=4.351, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=38077
2023-05-20 03:12:09 | INFO | train_inner | epoch 147:      9 / 65 loss=6.694, nll_loss=4.95, ppl=30.92, wps=3394.9, ups=0.28, wpb=12073.8, bsz=528, num_updates=9475, lr=2.67857e-06, gnorm=11.138, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.3, wall=38095
2023-05-20 03:12:24 | INFO | train_inner | epoch 147:     14 / 65 loss=6.811, nll_loss=5.079, ppl=33.79, wps=4336.5, ups=0.34, wpb=12615, bsz=642.2, num_updates=9480, lr=2.65306e-06, gnorm=2.54, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=38110
2023-05-20 03:12:41 | INFO | train_inner | epoch 147:     19 / 65 loss=6.687, nll_loss=4.95, ppl=30.91, wps=3568.1, ups=0.29, wpb=12188.2, bsz=568.2, num_updates=9485, lr=2.62755e-06, gnorm=6.483, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=38127
2023-05-20 03:12:56 | INFO | train_inner | epoch 147:     24 / 65 loss=6.668, nll_loss=4.924, ppl=30.36, wps=4104.9, ups=0.33, wpb=12596.4, bsz=559.6, num_updates=9490, lr=2.60204e-06, gnorm=10.145, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=38142
2023-05-20 03:13:13 | INFO | train_inner | epoch 147:     29 / 65 loss=6.627, nll_loss=4.878, ppl=29.4, wps=3701.5, ups=0.3, wpb=12143.8, bsz=514.8, num_updates=9495, lr=2.57653e-06, gnorm=9.887, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=38159
2023-05-20 03:13:29 | INFO | train_inner | epoch 147:     34 / 65 loss=6.692, nll_loss=4.951, ppl=30.94, wps=3550.5, ups=0.3, wpb=11796.6, bsz=538.4, num_updates=9500, lr=2.55102e-06, gnorm=4.54, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=38175
2023-05-20 03:13:46 | INFO | train_inner | epoch 147:     39 / 65 loss=6.713, nll_loss=4.974, ppl=31.44, wps=3541.7, ups=0.3, wpb=11929, bsz=541, num_updates=9505, lr=2.52551e-06, gnorm=7.108, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=38192
2023-05-20 03:14:02 | INFO | train_inner | epoch 147:     44 / 65 loss=6.648, nll_loss=4.9, ppl=29.86, wps=3809.9, ups=0.31, wpb=12114.2, bsz=523.4, num_updates=9510, lr=2.5e-06, gnorm=4.211, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.1, wall=38208
2023-05-20 03:14:20 | INFO | train_inner | epoch 147:     49 / 65 loss=6.632, nll_loss=4.883, ppl=29.5, wps=3295.8, ups=0.28, wpb=11847.4, bsz=528, num_updates=9515, lr=2.47449e-06, gnorm=10.786, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=38226
2023-05-20 03:14:36 | INFO | train_inner | epoch 147:     54 / 65 loss=6.683, nll_loss=4.944, ppl=30.78, wps=3888.3, ups=0.32, wpb=12304.8, bsz=529.4, num_updates=9520, lr=2.44898e-06, gnorm=5.634, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=38242
2023-05-20 03:14:50 | INFO | train_inner | epoch 147:     59 / 65 loss=6.627, nll_loss=4.876, ppl=29.36, wps=4235.5, ups=0.35, wpb=12121.2, bsz=557.2, num_updates=9525, lr=2.42347e-06, gnorm=8.611, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=38256
2023-05-20 03:15:05 | INFO | train_inner | epoch 147:     64 / 65 loss=6.714, nll_loss=4.978, ppl=31.51, wps=4154.4, ups=0.34, wpb=12245.6, bsz=591, num_updates=9530, lr=2.39796e-06, gnorm=4.278, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=38271
2023-05-20 03:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:15:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:15:48 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 6.05 | nll_loss 4.151 | ppl 17.77 | bleu 5.54 | wps 863.7 | wpb 2785 | bsz 105.2 | num_updates 9531 | best_loss 6.036
2023-05-20 03:15:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 9531 updates
2023-05-20 03:15:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint147.pt
2023-05-20 03:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint147.pt
2023-05-20 03:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint147.pt (epoch 147 @ 9531 updates, score 6.05) (writing took 6.87180844694376 seconds)
2023-05-20 03:15:55 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2023-05-20 03:15:55 | INFO | train | epoch 147 | loss 6.705 | nll_loss 4.965 | ppl 31.22 | wps 3060.8 | ups 0.25 | wpb 12049 | bsz 556.8 | num_updates 9531 | lr 2.39286e-06 | gnorm 8.595 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.7 | wall 38321
2023-05-20 03:15:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:15:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:15:55 | INFO | fairseq.trainer | begin training epoch 148
2023-05-20 03:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:16:07 | INFO | train_inner | epoch 148:      4 / 65 loss=6.7, nll_loss=4.961, ppl=31.15, wps=873.4, ups=0.08, wpb=10804.4, bsz=490, num_updates=9535, lr=2.37245e-06, gnorm=26.898, clip=100, loss_scale=0.25, train_wall=13, gb_free=21.6, wall=38333
2023-05-20 03:16:23 | INFO | train_inner | epoch 148:      9 / 65 loss=6.671, nll_loss=4.927, ppl=30.42, wps=3575, ups=0.3, wpb=11787.8, bsz=532.6, num_updates=9540, lr=2.34694e-06, gnorm=13.741, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=38349
2023-05-20 03:16:40 | INFO | train_inner | epoch 148:     14 / 65 loss=6.883, nll_loss=5.154, ppl=35.61, wps=3812.5, ups=0.3, wpb=12653.2, bsz=676.8, num_updates=9545, lr=2.32143e-06, gnorm=3.296, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=38366
2023-05-20 03:16:57 | INFO | train_inner | epoch 148:     19 / 65 loss=6.648, nll_loss=4.901, ppl=29.88, wps=3563, ups=0.3, wpb=12068.4, bsz=482.2, num_updates=9550, lr=2.29592e-06, gnorm=5.526, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.6, wall=38383
2023-05-20 03:17:12 | INFO | train_inner | epoch 148:     24 / 65 loss=6.915, nll_loss=5.19, ppl=36.51, wps=4194.4, ups=0.34, wpb=12245.6, bsz=665.4, num_updates=9555, lr=2.27041e-06, gnorm=3.429, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.3, wall=38397
2023-05-20 03:17:28 | INFO | train_inner | epoch 148:     29 / 65 loss=6.718, nll_loss=4.984, ppl=31.65, wps=3758.9, ups=0.3, wpb=12406.8, bsz=587.4, num_updates=9560, lr=2.2449e-06, gnorm=10.351, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=38414
2023-05-20 03:17:43 | INFO | train_inner | epoch 148:     34 / 65 loss=6.664, nll_loss=4.915, ppl=30.18, wps=4078.7, ups=0.33, wpb=12220, bsz=543.4, num_updates=9565, lr=2.21939e-06, gnorm=4.493, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=38429
2023-05-20 03:17:59 | INFO | train_inner | epoch 148:     39 / 65 loss=6.665, nll_loss=4.923, ppl=30.34, wps=3754.3, ups=0.31, wpb=12306.6, bsz=572.4, num_updates=9570, lr=2.19388e-06, gnorm=4.447, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=38445
2023-05-20 03:18:14 | INFO | train_inner | epoch 148:     44 / 65 loss=6.683, nll_loss=4.943, ppl=30.76, wps=4173, ups=0.34, wpb=12434.6, bsz=550.8, num_updates=9575, lr=2.16837e-06, gnorm=4.256, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=38460
2023-05-20 03:18:31 | INFO | train_inner | epoch 148:     49 / 65 loss=6.766, nll_loss=5.037, ppl=32.82, wps=3779.2, ups=0.31, wpb=12301.8, bsz=575, num_updates=9580, lr=2.14286e-06, gnorm=5.806, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=38476
2023-05-20 03:18:47 | INFO | train_inner | epoch 148:     54 / 65 loss=6.643, nll_loss=4.896, ppl=29.78, wps=3803.5, ups=0.31, wpb=12253.2, bsz=576, num_updates=9585, lr=2.11735e-06, gnorm=4.224, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=38492
2023-05-20 03:19:03 | INFO | train_inner | epoch 148:     59 / 65 loss=6.662, nll_loss=4.918, ppl=30.22, wps=3468.9, ups=0.3, wpb=11547.8, bsz=513.2, num_updates=9590, lr=2.09184e-06, gnorm=3.322, clip=100, loss_scale=0.25, train_wall=17, gb_free=20.5, wall=38509
2023-05-20 03:19:18 | INFO | train_inner | epoch 148:     64 / 65 loss=6.697, nll_loss=4.96, ppl=31.12, wps=4006, ups=0.34, wpb=11927.8, bsz=538, num_updates=9595, lr=2.06633e-06, gnorm=4.187, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=38524
2023-05-20 03:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:19:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:20:03 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 6.051 | nll_loss 4.154 | ppl 17.8 | bleu 5.44 | wps 852.8 | wpb 2785 | bsz 105.2 | num_updates 9596 | best_loss 6.036
2023-05-20 03:20:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 9596 updates
2023-05-20 03:20:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint148.pt
2023-05-20 03:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint148.pt
2023-05-20 03:20:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint148.pt (epoch 148 @ 9596 updates, score 6.051) (writing took 6.9352052845060825 seconds)
2023-05-20 03:20:09 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2023-05-20 03:20:09 | INFO | train | epoch 148 | loss 6.716 | nll_loss 4.977 | ppl 31.5 | wps 3075.6 | ups 0.26 | wpb 12051.2 | bsz 559.3 | num_updates 9596 | lr 2.06122e-06 | gnorm 5.869 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.8 | wall 38575
2023-05-20 03:20:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:20:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:20:10 | INFO | fairseq.trainer | begin training epoch 149
2023-05-20 03:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:20:22 | INFO | train_inner | epoch 149:      4 / 65 loss=6.686, nll_loss=4.951, ppl=30.92, wps=792.8, ups=0.08, wpb=10167.6, bsz=481.2, num_updates=9600, lr=2.04082e-06, gnorm=16.945, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=38588
2023-05-20 03:20:39 | INFO | train_inner | epoch 149:      9 / 65 loss=6.716, nll_loss=4.978, ppl=31.51, wps=3555.1, ups=0.3, wpb=11751.2, bsz=519.8, num_updates=9605, lr=2.01531e-06, gnorm=5.389, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.6, wall=38605
2023-05-20 03:20:57 | INFO | train_inner | epoch 149:     14 / 65 loss=6.655, nll_loss=4.912, ppl=30.1, wps=3426.2, ups=0.28, wpb=12182.6, bsz=534.4, num_updates=9610, lr=1.9898e-06, gnorm=7.674, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=38622
2023-05-20 03:21:14 | INFO | train_inner | epoch 149:     19 / 65 loss=6.625, nll_loss=4.875, ppl=29.35, wps=3638.2, ups=0.29, wpb=12426.6, bsz=564.6, num_updates=9615, lr=1.96429e-06, gnorm=6.644, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=38639
2023-05-20 03:21:31 | INFO | train_inner | epoch 149:     24 / 65 loss=6.847, nll_loss=5.109, ppl=34.52, wps=3444.8, ups=0.28, wpb=12167.2, bsz=558.6, num_updates=9620, lr=1.93878e-06, gnorm=5.301, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.7, wall=38657
2023-05-20 03:21:47 | INFO | train_inner | epoch 149:     29 / 65 loss=6.864, nll_loss=5.137, ppl=35.19, wps=4008.6, ups=0.32, wpb=12490.2, bsz=625.8, num_updates=9625, lr=1.91327e-06, gnorm=10.085, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=38673
2023-05-20 03:22:03 | INFO | train_inner | epoch 149:     34 / 65 loss=6.698, nll_loss=4.959, ppl=31.11, wps=3733.9, ups=0.31, wpb=12183.2, bsz=589, num_updates=9630, lr=1.88776e-06, gnorm=4.453, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=38689
2023-05-20 03:22:19 | INFO | train_inner | epoch 149:     39 / 65 loss=6.642, nll_loss=4.892, ppl=29.69, wps=4035.1, ups=0.33, wpb=12358, bsz=554.2, num_updates=9635, lr=1.86224e-06, gnorm=13.041, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=38704
2023-05-20 03:22:34 | INFO | train_inner | epoch 149:     44 / 65 loss=6.624, nll_loss=4.872, ppl=29.29, wps=3789.9, ups=0.32, wpb=12010.2, bsz=525.4, num_updates=9640, lr=1.83673e-06, gnorm=4.021, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=38720
2023-05-20 03:22:50 | INFO | train_inner | epoch 149:     49 / 65 loss=6.746, nll_loss=5.013, ppl=32.3, wps=4060.4, ups=0.33, wpb=12304.8, bsz=601.2, num_updates=9645, lr=1.81122e-06, gnorm=5.817, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=38735
2023-05-20 03:23:06 | INFO | train_inner | epoch 149:     54 / 65 loss=6.67, nll_loss=4.93, ppl=30.48, wps=3844.2, ups=0.31, wpb=12424, bsz=556.8, num_updates=9650, lr=1.78571e-06, gnorm=2.407, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.9, wall=38751
2023-05-20 03:23:22 | INFO | train_inner | epoch 149:     59 / 65 loss=6.788, nll_loss=5.059, ppl=33.33, wps=3582, ups=0.3, wpb=11936.4, bsz=581.4, num_updates=9655, lr=1.7602e-06, gnorm=4.373, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.1, wall=38768
2023-05-20 03:23:37 | INFO | train_inner | epoch 149:     64 / 65 loss=6.701, nll_loss=4.964, ppl=31.22, wps=4117.6, ups=0.34, wpb=12168.8, bsz=564.8, num_updates=9660, lr=1.73469e-06, gnorm=7.748, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.2, wall=38783
2023-05-20 03:23:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:23:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:24:20 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 6.05 | nll_loss 4.153 | ppl 17.79 | bleu 5.41 | wps 871.1 | wpb 2785 | bsz 105.2 | num_updates 9661 | best_loss 6.036
2023-05-20 03:24:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 9661 updates
2023-05-20 03:24:20 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint149.pt
2023-05-20 03:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint149.pt
2023-05-20 03:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint149.pt (epoch 149 @ 9661 updates, score 6.05) (writing took 6.632765471935272 seconds)
2023-05-20 03:24:27 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2023-05-20 03:24:27 | INFO | train | epoch 149 | loss 6.711 | nll_loss 4.971 | ppl 31.37 | wps 3045 | ups 0.25 | wpb 12056.4 | bsz 559.7 | num_updates 9661 | lr 1.72959e-06 | gnorm 7.07 | clip 100 | loss_scale 0.25 | train_wall 209 | gb_free 21.1 | wall 38833
2023-05-20 03:24:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:24:27 | INFO | fairseq.trainer | begin training epoch 150
2023-05-20 03:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:24:39 | INFO | train_inner | epoch 150:      4 / 65 loss=6.716, nll_loss=4.975, ppl=31.45, wps=820.1, ups=0.08, wpb=10171.6, bsz=446.6, num_updates=9665, lr=1.70918e-06, gnorm=18.579, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.7, wall=38845
2023-05-20 03:24:56 | INFO | train_inner | epoch 150:      9 / 65 loss=6.884, nll_loss=5.155, ppl=35.63, wps=3581.9, ups=0.3, wpb=11947.2, bsz=637, num_updates=9670, lr=1.68367e-06, gnorm=5.732, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=38862
2023-05-20 03:25:12 | INFO | train_inner | epoch 150:     14 / 65 loss=6.81, nll_loss=5.076, ppl=33.74, wps=3893.9, ups=0.32, wpb=12279.4, bsz=572.2, num_updates=9675, lr=1.65816e-06, gnorm=5.163, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=38877
2023-05-20 03:25:27 | INFO | train_inner | epoch 150:     19 / 65 loss=6.666, nll_loss=4.92, ppl=30.27, wps=4059.9, ups=0.32, wpb=12625.4, bsz=595.8, num_updates=9680, lr=1.63265e-06, gnorm=5.846, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=38893
2023-05-20 03:25:43 | INFO | train_inner | epoch 150:     24 / 65 loss=6.643, nll_loss=4.895, ppl=29.76, wps=3640, ups=0.31, wpb=11830.6, bsz=509.6, num_updates=9685, lr=1.60714e-06, gnorm=5.687, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=38909
2023-05-20 03:25:58 | INFO | train_inner | epoch 150:     29 / 65 loss=6.711, nll_loss=4.976, ppl=31.47, wps=4277.6, ups=0.35, wpb=12334.8, bsz=585.8, num_updates=9690, lr=1.58163e-06, gnorm=10.383, clip=100, loss_scale=0.25, train_wall=14, gb_free=20.9, wall=38924
2023-05-20 03:26:14 | INFO | train_inner | epoch 150:     34 / 65 loss=6.665, nll_loss=4.924, ppl=30.36, wps=3800.7, ups=0.31, wpb=12239.4, bsz=548.4, num_updates=9695, lr=1.55612e-06, gnorm=6.705, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=38940
2023-05-20 03:26:30 | INFO | train_inner | epoch 150:     39 / 65 loss=6.733, nll_loss=4.999, ppl=31.98, wps=3913.7, ups=0.31, wpb=12433.6, bsz=584.2, num_updates=9700, lr=1.53061e-06, gnorm=5.044, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=38956
2023-05-20 03:26:46 | INFO | train_inner | epoch 150:     44 / 65 loss=6.604, nll_loss=4.857, ppl=28.98, wps=3660.2, ups=0.3, wpb=12121.4, bsz=572, num_updates=9705, lr=1.5051e-06, gnorm=4.189, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=38972
2023-05-20 03:27:02 | INFO | train_inner | epoch 150:     49 / 65 loss=6.744, nll_loss=5.015, ppl=32.33, wps=3855.3, ups=0.32, wpb=12115.8, bsz=610, num_updates=9710, lr=1.47959e-06, gnorm=4.85, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=38988
2023-05-20 03:27:18 | INFO | train_inner | epoch 150:     54 / 65 loss=6.68, nll_loss=4.939, ppl=30.68, wps=3943.7, ups=0.32, wpb=12289.4, bsz=584, num_updates=9715, lr=1.45408e-06, gnorm=9.67, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=39003
2023-05-20 03:27:33 | INFO | train_inner | epoch 150:     59 / 65 loss=6.703, nll_loss=4.963, ppl=31.19, wps=3890.5, ups=0.32, wpb=12277.6, bsz=527.8, num_updates=9720, lr=1.42857e-06, gnorm=8.225, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.1, wall=39019
2023-05-20 03:27:50 | INFO | train_inner | epoch 150:     64 / 65 loss=6.671, nll_loss=4.931, ppl=30.5, wps=3685.4, ups=0.3, wpb=12123, bsz=516.6, num_updates=9725, lr=1.40306e-06, gnorm=4.043, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=39036
2023-05-20 03:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:27:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:28:33 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.42 | wps 862.4 | wpb 2785 | bsz 105.2 | num_updates 9726 | best_loss 6.036
2023-05-20 03:28:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 9726 updates
2023-05-20 03:28:33 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint150.pt
2023-05-20 03:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint150.pt
2023-05-20 03:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint150.pt (epoch 150 @ 9726 updates, score 6.05) (writing took 6.688557229936123 seconds)
2023-05-20 03:28:40 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2023-05-20 03:28:40 | INFO | train | epoch 150 | loss 6.712 | nll_loss 4.973 | ppl 31.4 | wps 3091.6 | ups 0.26 | wpb 12048.5 | bsz 559 | num_updates 9726 | lr 1.39796e-06 | gnorm 7.064 | clip 100 | loss_scale 0.25 | train_wall 205 | gb_free 21.6 | wall 39086
2023-05-20 03:28:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:28:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:28:40 | INFO | fairseq.trainer | begin training epoch 151
2023-05-20 03:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:28:52 | INFO | train_inner | epoch 151:      4 / 65 loss=6.664, nll_loss=4.921, ppl=30.29, wps=837.3, ups=0.08, wpb=10403.8, bsz=459, num_updates=9730, lr=1.37755e-06, gnorm=4.896, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=39098
2023-05-20 03:29:08 | INFO | train_inner | epoch 151:      9 / 65 loss=6.683, nll_loss=4.941, ppl=30.72, wps=3575, ups=0.31, wpb=11659.6, bsz=529.2, num_updates=9735, lr=1.35204e-06, gnorm=6.443, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=39114
2023-05-20 03:29:24 | INFO | train_inner | epoch 151:     14 / 65 loss=6.657, nll_loss=4.914, ppl=30.16, wps=3976.9, ups=0.33, wpb=12194.6, bsz=541.4, num_updates=9740, lr=1.32653e-06, gnorm=3.589, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=39129
2023-05-20 03:29:40 | INFO | train_inner | epoch 151:     19 / 65 loss=6.704, nll_loss=4.967, ppl=31.28, wps=3739.2, ups=0.3, wpb=12274, bsz=603.6, num_updates=9745, lr=1.30102e-06, gnorm=5.952, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=39146
2023-05-20 03:29:57 | INFO | train_inner | epoch 151:     24 / 65 loss=6.792, nll_loss=5.053, ppl=33.19, wps=3748.2, ups=0.3, wpb=12326, bsz=596.6, num_updates=9750, lr=1.27551e-06, gnorm=2.625, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=39162
2023-05-20 03:30:12 | INFO | train_inner | epoch 151:     29 / 65 loss=6.701, nll_loss=4.959, ppl=31.11, wps=3891.6, ups=0.32, wpb=12170.6, bsz=544.2, num_updates=9755, lr=1.25e-06, gnorm=4.947, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=39178
2023-05-20 03:30:28 | INFO | train_inner | epoch 151:     34 / 65 loss=6.715, nll_loss=4.979, ppl=31.53, wps=3893.8, ups=0.31, wpb=12458, bsz=564, num_updates=9760, lr=1.22449e-06, gnorm=5.579, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.4, wall=39194
2023-05-20 03:30:44 | INFO | train_inner | epoch 151:     39 / 65 loss=6.864, nll_loss=5.135, ppl=35.15, wps=3952, ups=0.32, wpb=12220.4, bsz=599.4, num_updates=9765, lr=1.19898e-06, gnorm=8.385, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.4, wall=39209
2023-05-20 03:30:59 | INFO | train_inner | epoch 151:     44 / 65 loss=6.752, nll_loss=5.016, ppl=32.35, wps=3987.5, ups=0.33, wpb=12131.2, bsz=622.2, num_updates=9770, lr=1.17347e-06, gnorm=4.45, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=39225
2023-05-20 03:31:18 | INFO | train_inner | epoch 151:     49 / 65 loss=6.666, nll_loss=4.919, ppl=30.26, wps=3160.8, ups=0.27, wpb=11831, bsz=518, num_updates=9775, lr=1.14796e-06, gnorm=4.471, clip=100, loss_scale=0.25, train_wall=19, gb_free=21.6, wall=39243
2023-05-20 03:31:34 | INFO | train_inner | epoch 151:     54 / 65 loss=6.694, nll_loss=4.953, ppl=30.97, wps=3699.2, ups=0.31, wpb=12058.8, bsz=537.4, num_updates=9780, lr=1.12245e-06, gnorm=4.197, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=39260
2023-05-20 03:31:51 | INFO | train_inner | epoch 151:     59 / 65 loss=6.684, nll_loss=4.943, ppl=30.76, wps=3673.6, ups=0.3, wpb=12233.2, bsz=518, num_updates=9785, lr=1.09694e-06, gnorm=7.77, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=39276
2023-05-20 03:32:06 | INFO | train_inner | epoch 151:     64 / 65 loss=6.699, nll_loss=4.96, ppl=31.13, wps=4088.5, ups=0.33, wpb=12437.6, bsz=614.2, num_updates=9790, lr=1.07143e-06, gnorm=4.836, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=39291
2023-05-20 03:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:32:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:32:48 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.44 | wps 876.8 | wpb 2785 | bsz 105.2 | num_updates 9791 | best_loss 6.036
2023-05-20 03:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 9791 updates
2023-05-20 03:32:48 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint151.pt
2023-05-20 03:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint151.pt
2023-05-20 03:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint151.pt (epoch 151 @ 9791 updates, score 6.05) (writing took 6.695434670895338 seconds)
2023-05-20 03:32:55 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2023-05-20 03:32:55 | INFO | train | epoch 151 | loss 6.715 | nll_loss 4.976 | ppl 31.47 | wps 3071.5 | ups 0.26 | wpb 12041.8 | bsz 559.4 | num_updates 9791 | lr 1.06633e-06 | gnorm 5.316 | clip 100 | loss_scale 0.25 | train_wall 207 | gb_free 21.6 | wall 39341
2023-05-20 03:32:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:32:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:32:55 | INFO | fairseq.trainer | begin training epoch 152
2023-05-20 03:32:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:33:07 | INFO | train_inner | epoch 152:      4 / 65 loss=6.705, nll_loss=4.968, ppl=31.31, wps=844.4, ups=0.08, wpb=10402, bsz=473, num_updates=9795, lr=1.04592e-06, gnorm=7.204, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.9, wall=39353
2023-05-20 03:33:25 | INFO | train_inner | epoch 152:      9 / 65 loss=6.649, nll_loss=4.903, ppl=29.92, wps=3288.6, ups=0.28, wpb=11936, bsz=477.2, num_updates=9800, lr=1.02041e-06, gnorm=17.344, clip=100, loss_scale=0.25, train_wall=18, gb_free=21, wall=39371
2023-05-20 03:33:41 | INFO | train_inner | epoch 152:     14 / 65 loss=6.712, nll_loss=4.976, ppl=31.47, wps=3817.7, ups=0.31, wpb=12194, bsz=576, num_updates=9805, lr=9.94898e-07, gnorm=3.5, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=39387
2023-05-20 03:33:58 | INFO | train_inner | epoch 152:     19 / 65 loss=6.975, nll_loss=5.26, ppl=38.33, wps=3842.4, ups=0.31, wpb=12576.8, bsz=674.6, num_updates=9810, lr=9.69388e-07, gnorm=5.02, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=39404
2023-05-20 03:34:14 | INFO | train_inner | epoch 152:     24 / 65 loss=6.806, nll_loss=5.076, ppl=33.72, wps=3726.3, ups=0.3, wpb=12384.6, bsz=600.6, num_updates=9815, lr=9.43878e-07, gnorm=5.74, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.3, wall=39420
2023-05-20 03:34:31 | INFO | train_inner | epoch 152:     29 / 65 loss=6.555, nll_loss=4.793, ppl=27.72, wps=3587.5, ups=0.3, wpb=12052, bsz=510.8, num_updates=9820, lr=9.18367e-07, gnorm=7.492, clip=100, loss_scale=0.25, train_wall=17, gb_free=21, wall=39437
2023-05-20 03:34:47 | INFO | train_inner | epoch 152:     34 / 65 loss=6.709, nll_loss=4.973, ppl=31.4, wps=3739.3, ups=0.31, wpb=12007.8, bsz=537.2, num_updates=9825, lr=8.92857e-07, gnorm=4.401, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=39453
2023-05-20 03:35:03 | INFO | train_inner | epoch 152:     39 / 65 loss=6.683, nll_loss=4.939, ppl=30.67, wps=3871.8, ups=0.32, wpb=12172.8, bsz=570.4, num_updates=9830, lr=8.67347e-07, gnorm=3.447, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.1, wall=39469
2023-05-20 03:35:18 | INFO | train_inner | epoch 152:     44 / 65 loss=6.685, nll_loss=4.944, ppl=30.78, wps=4205.7, ups=0.34, wpb=12253.4, bsz=580.6, num_updates=9835, lr=8.41837e-07, gnorm=4.33, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.5, wall=39483
2023-05-20 03:35:33 | INFO | train_inner | epoch 152:     49 / 65 loss=6.621, nll_loss=4.873, ppl=29.31, wps=3923.4, ups=0.32, wpb=12240.8, bsz=544.6, num_updates=9840, lr=8.16327e-07, gnorm=3.163, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=39499
2023-05-20 03:35:49 | INFO | train_inner | epoch 152:     54 / 65 loss=6.729, nll_loss=4.995, ppl=31.89, wps=3908.5, ups=0.31, wpb=12429.8, bsz=614, num_updates=9845, lr=7.90816e-07, gnorm=14.96, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=39515
2023-05-20 03:36:05 | INFO | train_inner | epoch 152:     59 / 65 loss=6.735, nll_loss=5.002, ppl=32.05, wps=3868.4, ups=0.32, wpb=12155.6, bsz=573.6, num_updates=9850, lr=7.65306e-07, gnorm=4.864, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=39531
2023-05-20 03:36:20 | INFO | train_inner | epoch 152:     64 / 65 loss=6.669, nll_loss=4.925, ppl=30.37, wps=4003.2, ups=0.33, wpb=12012.8, bsz=552.4, num_updates=9855, lr=7.39796e-07, gnorm=4.704, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=39546
2023-05-20 03:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:36:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:37:01 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.77 | bleu 5.47 | wps 896.8 | wpb 2785 | bsz 105.2 | num_updates 9856 | best_loss 6.036
2023-05-20 03:37:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 9856 updates
2023-05-20 03:37:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint152.pt
2023-05-20 03:37:06 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint152.pt
2023-05-20 03:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint152.pt (epoch 152 @ 9856 updates, score 6.05) (writing took 6.635294858366251 seconds)
2023-05-20 03:37:08 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2023-05-20 03:37:08 | INFO | train | epoch 152 | loss 6.71 | nll_loss 4.972 | ppl 31.37 | wps 3099.3 | ups 0.26 | wpb 12054.2 | bsz 559.6 | num_updates 9856 | lr 7.34694e-07 | gnorm 7.109 | clip 100 | loss_scale 0.25 | train_wall 206 | gb_free 21.5 | wall 39594
2023-05-20 03:37:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:37:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:37:08 | INFO | fairseq.trainer | begin training epoch 153
2023-05-20 03:37:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:37:21 | INFO | train_inner | epoch 153:      4 / 65 loss=6.709, nll_loss=4.977, ppl=31.49, wps=901.6, ups=0.08, wpb=11009.8, bsz=531.8, num_updates=9860, lr=7.14286e-07, gnorm=15.083, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.6, wall=39607
2023-05-20 03:37:38 | INFO | train_inner | epoch 153:      9 / 65 loss=6.699, nll_loss=4.958, ppl=31.08, wps=3481.3, ups=0.29, wpb=12143.4, bsz=602.2, num_updates=9865, lr=6.88776e-07, gnorm=3.83, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=39624
2023-05-20 03:37:54 | INFO | train_inner | epoch 153:     14 / 65 loss=6.896, nll_loss=5.167, ppl=35.93, wps=3844.5, ups=0.31, wpb=12268.4, bsz=584.2, num_updates=9870, lr=6.63265e-07, gnorm=6.233, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=39640
2023-05-20 03:38:09 | INFO | train_inner | epoch 153:     19 / 65 loss=6.834, nll_loss=5.097, ppl=34.23, wps=4180.1, ups=0.34, wpb=12219.8, bsz=588.8, num_updates=9875, lr=6.37755e-07, gnorm=8.237, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.9, wall=39655
2023-05-20 03:38:25 | INFO | train_inner | epoch 153:     24 / 65 loss=6.697, nll_loss=4.96, ppl=31.12, wps=3646.4, ups=0.3, wpb=12027.6, bsz=551.4, num_updates=9880, lr=6.12245e-07, gnorm=3.945, clip=100, loss_scale=0.25, train_wall=16, gb_free=20.5, wall=39671
2023-05-20 03:38:41 | INFO | train_inner | epoch 153:     29 / 65 loss=6.72, nll_loss=4.984, ppl=31.65, wps=4037.3, ups=0.33, wpb=12248.8, bsz=579.8, num_updates=9885, lr=5.86735e-07, gnorm=4.979, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=39686
2023-05-20 03:38:57 | INFO | train_inner | epoch 153:     34 / 65 loss=6.673, nll_loss=4.928, ppl=30.45, wps=3760.5, ups=0.31, wpb=12259.8, bsz=531.2, num_updates=9890, lr=5.61224e-07, gnorm=7.876, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=39703
2023-05-20 03:39:12 | INFO | train_inner | epoch 153:     39 / 65 loss=6.688, nll_loss=4.949, ppl=30.89, wps=4258.5, ups=0.33, wpb=12748.2, bsz=609, num_updates=9895, lr=5.35714e-07, gnorm=3.097, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.2, wall=39718
2023-05-20 03:39:28 | INFO | train_inner | epoch 153:     44 / 65 loss=6.678, nll_loss=4.932, ppl=30.52, wps=3638.6, ups=0.31, wpb=11924.4, bsz=523.8, num_updates=9900, lr=5.10204e-07, gnorm=9.609, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=39734
2023-05-20 03:39:45 | INFO | train_inner | epoch 153:     49 / 65 loss=6.657, nll_loss=4.914, ppl=30.15, wps=3492.2, ups=0.29, wpb=11957.6, bsz=519.8, num_updates=9905, lr=4.84694e-07, gnorm=6.387, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.3, wall=39751
2023-05-20 03:40:02 | INFO | train_inner | epoch 153:     54 / 65 loss=6.715, nll_loss=4.977, ppl=31.49, wps=3558.6, ups=0.3, wpb=11824.8, bsz=569.8, num_updates=9910, lr=4.59184e-07, gnorm=4.745, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.6, wall=39768
2023-05-20 03:40:18 | INFO | train_inner | epoch 153:     59 / 65 loss=6.629, nll_loss=4.882, ppl=29.49, wps=3860.9, ups=0.32, wpb=12105.6, bsz=550.4, num_updates=9915, lr=4.33673e-07, gnorm=3.736, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.7, wall=39783
2023-05-20 03:40:33 | INFO | train_inner | epoch 153:     64 / 65 loss=6.637, nll_loss=4.888, ppl=29.61, wps=3991.6, ups=0.33, wpb=12036, bsz=545.8, num_updates=9920, lr=4.08163e-07, gnorm=3.041, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.5, wall=39798
2023-05-20 03:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:40:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:41:14 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 6.05 | nll_loss 4.153 | ppl 17.79 | bleu 5.4 | wps 902.5 | wpb 2785 | bsz 105.2 | num_updates 9921 | best_loss 6.036
2023-05-20 03:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 9921 updates
2023-05-20 03:41:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint153.pt
2023-05-20 03:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint153.pt
2023-05-20 03:41:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint153.pt (epoch 153 @ 9921 updates, score 6.05) (writing took 6.515961814671755 seconds)
2023-05-20 03:41:21 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2023-05-20 03:41:21 | INFO | train | epoch 153 | loss 6.711 | nll_loss 4.971 | ppl 31.36 | wps 3096.6 | ups 0.26 | wpb 12059.2 | bsz 559.7 | num_updates 9921 | lr 4.03061e-07 | gnorm 5.599 | clip 100 | loss_scale 0.5 | train_wall 206 | gb_free 21.3 | wall 39847
2023-05-20 03:41:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:41:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:41:21 | INFO | fairseq.trainer | begin training epoch 154
2023-05-20 03:41:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:41:36 | INFO | train_inner | epoch 154:      4 / 65 loss=6.644, nll_loss=4.898, ppl=29.82, wps=770, ups=0.08, wpb=9734, bsz=396, num_updates=9925, lr=3.82653e-07, gnorm=4.025, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.7, wall=39862
2023-05-20 03:41:51 | INFO | train_inner | epoch 154:      9 / 65 loss=6.705, nll_loss=4.967, ppl=31.29, wps=4064.8, ups=0.33, wpb=12481.2, bsz=597.4, num_updates=9930, lr=3.57143e-07, gnorm=4.864, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=39877
2023-05-20 03:42:07 | INFO | train_inner | epoch 154:     14 / 65 loss=6.7, nll_loss=4.964, ppl=31.2, wps=3881.9, ups=0.32, wpb=12201.8, bsz=587.8, num_updates=9935, lr=3.31633e-07, gnorm=7.055, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.4, wall=39893
2023-05-20 03:42:25 | INFO | train_inner | epoch 154:     19 / 65 loss=6.877, nll_loss=5.146, ppl=35.41, wps=3529.8, ups=0.28, wpb=12401.6, bsz=596.8, num_updates=9940, lr=3.06122e-07, gnorm=6.633, clip=100, loss_scale=0.5, train_wall=18, gb_free=21.8, wall=39910
2023-05-20 03:42:40 | INFO | train_inner | epoch 154:     24 / 65 loss=6.706, nll_loss=4.963, ppl=31.2, wps=4105.5, ups=0.33, wpb=12335.4, bsz=557, num_updates=9945, lr=2.80612e-07, gnorm=2.846, clip=100, loss_scale=0.5, train_wall=15, gb_free=21.7, wall=39925
2023-05-20 03:42:56 | INFO | train_inner | epoch 154:     29 / 65 loss=6.597, nll_loss=4.842, ppl=28.69, wps=3731.7, ups=0.31, wpb=11984.4, bsz=505, num_updates=9950, lr=2.55102e-07, gnorm=6.147, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=39941
2023-05-20 03:43:12 | INFO | train_inner | epoch 154:     34 / 65 loss=6.804, nll_loss=5.073, ppl=33.66, wps=3668, ups=0.3, wpb=12298, bsz=570.4, num_updates=9955, lr=2.29592e-07, gnorm=4.749, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.7, wall=39958
2023-05-20 03:43:29 | INFO | train_inner | epoch 154:     39 / 65 loss=6.694, nll_loss=4.958, ppl=31.07, wps=3614.3, ups=0.3, wpb=11991, bsz=603.6, num_updates=9960, lr=2.04082e-07, gnorm=2.472, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.8, wall=39975
2023-05-20 03:43:45 | INFO | train_inner | epoch 154:     44 / 65 loss=6.66, nll_loss=4.913, ppl=30.13, wps=3757.1, ups=0.31, wpb=12095.8, bsz=496.6, num_updates=9965, lr=1.78571e-07, gnorm=12.56, clip=100, loss_scale=0.5, train_wall=16, gb_free=21.6, wall=39991
2023-05-20 03:44:01 | INFO | train_inner | epoch 154:     49 / 65 loss=6.735, nll_loss=5.001, ppl=32.03, wps=3896.2, ups=0.32, wpb=12220.8, bsz=594.6, num_updates=9970, lr=1.53061e-07, gnorm=9.699, clip=100, loss_scale=0.5, train_wall=16, gb_free=20.9, wall=40006
2023-05-20 03:44:18 | INFO | train_inner | epoch 154:     54 / 65 loss=6.699, nll_loss=4.966, ppl=31.25, wps=3712.1, ups=0.3, wpb=12511.2, bsz=599.6, num_updates=9975, lr=1.27551e-07, gnorm=10.013, clip=100, loss_scale=0.5, train_wall=17, gb_free=21.5, wall=40023
2023-05-20 03:44:34 | INFO | train_inner | epoch 154:     59 / 65 loss=6.646, nll_loss=4.898, ppl=29.81, wps=3501.9, ups=0.3, wpb=11732, bsz=514.4, num_updates=9980, lr=1.02041e-07, gnorm=6.558, clip=100, loss_scale=0.5, train_wall=17, gb_free=20.5, wall=40040
2023-05-20 03:44:48 | INFO | train_inner | epoch 154:     64 / 65 loss=6.642, nll_loss=4.894, ppl=29.74, wps=4508.5, ups=0.36, wpb=12618.6, bsz=617.2, num_updates=9985, lr=7.65306e-08, gnorm=4.659, clip=100, loss_scale=0.5, train_wall=14, gb_free=21.6, wall=40054
2023-05-20 03:44:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:45:31 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.42 | wps 876.6 | wpb 2785 | bsz 105.2 | num_updates 9986 | best_loss 6.036
2023-05-20 03:45:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 9986 updates
2023-05-20 03:45:31 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint154.pt
2023-05-20 03:45:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint154.pt
2023-05-20 03:45:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint154.pt (epoch 154 @ 9986 updates, score 6.05) (writing took 6.547121498733759 seconds)
2023-05-20 03:45:38 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2023-05-20 03:45:38 | INFO | train | epoch 154 | loss 6.702 | nll_loss 4.962 | ppl 31.17 | wps 3047.1 | ups 0.25 | wpb 12048.9 | bsz 556.4 | num_updates 9986 | lr 7.14286e-08 | gnorm 6.442 | clip 100 | loss_scale 0.5 | train_wall 209 | gb_free 21.4 | wall 40104
2023-05-20 03:45:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:45:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:45:38 | INFO | fairseq.trainer | begin training epoch 155
2023-05-20 03:45:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-05-20 03:45:54 | INFO | train_inner | epoch 155:      5 / 65 loss=6.653, nll_loss=4.906, ppl=29.99, wps=773.8, ups=0.08, wpb=10195.6, bsz=427.6, num_updates=9990, lr=5.10204e-08, gnorm=7.677, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.5, wall=40120
2023-05-20 03:46:11 | INFO | train_inner | epoch 155:     10 / 65 loss=6.687, nll_loss=4.944, ppl=30.79, wps=3647.3, ups=0.3, wpb=12102.8, bsz=500.8, num_updates=9995, lr=2.55102e-08, gnorm=4.738, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=40137
2023-05-20 03:46:26 | INFO | train_inner | epoch 155:     15 / 65 loss=6.738, nll_loss=5.007, ppl=32.15, wps=4166.4, ups=0.34, wpb=12408.4, bsz=628, num_updates=10000, lr=0, gnorm=5.468, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.9, wall=40151
2023-05-20 03:46:43 | INFO | train_inner | epoch 155:     20 / 65 loss=6.75, nll_loss=5.016, ppl=32.35, wps=3602.3, ups=0.29, wpb=12354.2, bsz=594, num_updates=10005, lr=0, gnorm=3.218, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=40169
2023-05-20 03:46:59 | INFO | train_inner | epoch 155:     25 / 65 loss=6.684, nll_loss=4.945, ppl=30.81, wps=3924.1, ups=0.32, wpb=12420.4, bsz=636.4, num_updates=10010, lr=0, gnorm=4.067, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=40184
2023-05-20 03:47:14 | INFO | train_inner | epoch 155:     30 / 65 loss=6.684, nll_loss=4.946, ppl=30.82, wps=3805.6, ups=0.32, wpb=11926.6, bsz=555.6, num_updates=10015, lr=0, gnorm=14.645, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=40200
2023-05-20 03:47:30 | INFO | train_inner | epoch 155:     35 / 65 loss=6.757, nll_loss=5.023, ppl=32.51, wps=3786.5, ups=0.32, wpb=11962.8, bsz=576, num_updates=10020, lr=0, gnorm=10.399, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=40216
2023-05-20 03:47:46 | INFO | train_inner | epoch 155:     40 / 65 loss=6.842, nll_loss=5.108, ppl=34.48, wps=3923, ups=0.32, wpb=12335.4, bsz=569.4, num_updates=10025, lr=0, gnorm=3.009, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=40232
2023-05-20 03:48:02 | INFO | train_inner | epoch 155:     45 / 65 loss=6.733, nll_loss=4.996, ppl=31.91, wps=3767.8, ups=0.31, wpb=12157.6, bsz=550, num_updates=10030, lr=0, gnorm=3.14, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=40248
2023-05-20 03:48:18 | INFO | train_inner | epoch 155:     50 / 65 loss=6.733, nll_loss=4.999, ppl=31.99, wps=3825.3, ups=0.3, wpb=12605.6, bsz=612, num_updates=10035, lr=0, gnorm=3.951, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=40264
2023-05-20 03:48:33 | INFO | train_inner | epoch 155:     55 / 65 loss=6.578, nll_loss=4.819, ppl=28.22, wps=4010.2, ups=0.34, wpb=11900.2, bsz=502.8, num_updates=10040, lr=0, gnorm=7.326, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=40279
2023-05-20 03:48:48 | INFO | train_inner | epoch 155:     60 / 65 loss=6.739, nll_loss=4.997, ppl=31.94, wps=4034.7, ups=0.33, wpb=12122.6, bsz=568.8, num_updates=10045, lr=0, gnorm=9.531, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=40294
2023-05-20 03:49:03 | INFO | train_inner | epoch 155:     65 / 65 loss=6.647, nll_loss=4.899, ppl=29.84, wps=3612.1, ups=0.35, wpb=10448.2, bsz=484.6, num_updates=10050, lr=0, gnorm=5.868, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.5, wall=40309
2023-05-20 03:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:49:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:49:46 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 832.8 | wpb 2785 | bsz 105.2 | num_updates 10050 | best_loss 6.036
2023-05-20 03:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 10050 updates
2023-05-20 03:49:46 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint155.pt
2023-05-20 03:49:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint155.pt
2023-05-20 03:49:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint155.pt (epoch 155 @ 10050 updates, score 6.05) (writing took 6.627833798527718 seconds)
2023-05-20 03:49:52 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2023-05-20 03:49:52 | INFO | train | epoch 155 | loss 6.711 | nll_loss 4.972 | ppl 31.38 | wps 3034.3 | ups 0.25 | wpb 12051.1 | bsz 561 | num_updates 10050 | lr 0 | gnorm 6.321 | clip 100 | loss_scale 0.25 | train_wall 204 | gb_free 21.5 | wall 40358
2023-05-20 03:49:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:49:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:49:52 | INFO | fairseq.trainer | begin training epoch 156
2023-05-20 03:49:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:50:10 | INFO | train_inner | epoch 156:      5 / 65 loss=6.801, nll_loss=5.069, ppl=33.57, wps=932.2, ups=0.07, wpb=12562.8, bsz=590, num_updates=10055, lr=0, gnorm=7.362, clip=100, loss_scale=0.25, train_wall=18, gb_free=21.6, wall=40376
2023-05-20 03:50:27 | INFO | train_inner | epoch 156:     10 / 65 loss=6.789, nll_loss=5.058, ppl=33.31, wps=3471.6, ups=0.29, wpb=11842.2, bsz=549.2, num_updates=10060, lr=0, gnorm=2.645, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.5, wall=40393
2023-05-20 03:50:44 | INFO | train_inner | epoch 156:     15 / 65 loss=6.653, nll_loss=4.909, ppl=30.03, wps=3784.3, ups=0.3, wpb=12448, bsz=626.8, num_updates=10065, lr=0, gnorm=10.079, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.3, wall=40409
2023-05-20 03:50:59 | INFO | train_inner | epoch 156:     20 / 65 loss=6.617, nll_loss=4.865, ppl=29.13, wps=3969.8, ups=0.34, wpb=11780.4, bsz=497.2, num_updates=10070, lr=0, gnorm=4.035, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.6, wall=40424
2023-05-20 03:51:14 | INFO | train_inner | epoch 156:     25 / 65 loss=6.731, nll_loss=4.991, ppl=31.8, wps=3933.1, ups=0.32, wpb=12186, bsz=619.6, num_updates=10075, lr=0, gnorm=5.002, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=40440
2023-05-20 03:51:30 | INFO | train_inner | epoch 156:     30 / 65 loss=6.682, nll_loss=4.939, ppl=30.67, wps=3997.1, ups=0.32, wpb=12509.8, bsz=557.6, num_updates=10080, lr=0, gnorm=2.442, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=40455
2023-05-20 03:51:47 | INFO | train_inner | epoch 156:     35 / 65 loss=6.669, nll_loss=4.929, ppl=30.46, wps=3513.8, ups=0.29, wpb=12092, bsz=522.8, num_updates=10085, lr=0, gnorm=8.415, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.2, wall=40473
2023-05-20 03:52:03 | INFO | train_inner | epoch 156:     40 / 65 loss=6.649, nll_loss=4.899, ppl=29.84, wps=3811.8, ups=0.32, wpb=11957.8, bsz=555, num_updates=10090, lr=0, gnorm=11.973, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=40488
2023-05-20 03:52:17 | INFO | train_inner | epoch 156:     45 / 65 loss=6.733, nll_loss=5, ppl=32.01, wps=4206.3, ups=0.34, wpb=12532.2, bsz=600.6, num_updates=10095, lr=0, gnorm=5.468, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.8, wall=40503
2023-05-20 03:52:34 | INFO | train_inner | epoch 156:     50 / 65 loss=6.637, nll_loss=4.889, ppl=29.63, wps=3634.8, ups=0.3, wpb=12028.6, bsz=496.2, num_updates=10100, lr=0, gnorm=4.92, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.9, wall=40520
2023-05-20 03:52:50 | INFO | train_inner | epoch 156:     55 / 65 loss=6.592, nll_loss=4.838, ppl=28.61, wps=3626, ups=0.31, wpb=11831.6, bsz=492.6, num_updates=10105, lr=0, gnorm=5.768, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=40536
2023-05-20 03:53:07 | INFO | train_inner | epoch 156:     60 / 65 loss=6.917, nll_loss=5.195, ppl=36.63, wps=3705.1, ups=0.3, wpb=12283, bsz=635.6, num_updates=10110, lr=0, gnorm=4.451, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=40553
2023-05-20 03:53:21 | INFO | train_inner | epoch 156:     65 / 65 loss=6.759, nll_loss=5.034, ppl=32.77, wps=3867.8, ups=0.37, wpb=10588.6, bsz=527.2, num_updates=10115, lr=0, gnorm=12.544, clip=100, loss_scale=0.25, train_wall=14, gb_free=21.1, wall=40566
2023-05-20 03:53:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:53:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:54:03 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 842.6 | wpb 2785 | bsz 105.2 | num_updates 10115 | best_loss 6.036
2023-05-20 03:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 10115 updates
2023-05-20 03:54:03 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint156.pt
2023-05-20 03:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint156.pt
2023-05-20 03:54:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint156.pt (epoch 156 @ 10115 updates, score 6.05) (writing took 6.721030633896589 seconds)
2023-05-20 03:54:09 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2023-05-20 03:54:09 | INFO | train | epoch 156 | loss 6.71 | nll_loss 4.971 | ppl 31.35 | wps 3043.5 | ups 0.25 | wpb 12049.5 | bsz 559.3 | num_updates 10115 | lr 0 | gnorm 6.546 | clip 100 | loss_scale 0.25 | train_wall 208 | gb_free 21.1 | wall 40615
2023-05-20 03:54:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:54:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:54:10 | INFO | fairseq.trainer | begin training epoch 157
2023-05-20 03:54:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:54:26 | INFO | train_inner | epoch 157:      5 / 65 loss=6.688, nll_loss=4.95, ppl=30.9, wps=921.5, ups=0.08, wpb=12075.6, bsz=544.2, num_updates=10120, lr=0, gnorm=8.156, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.7, wall=40632
2023-05-20 03:54:43 | INFO | train_inner | epoch 157:     10 / 65 loss=6.712, nll_loss=4.975, ppl=31.45, wps=3571.5, ups=0.29, wpb=12257.8, bsz=561.8, num_updates=10125, lr=0, gnorm=12.568, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.7, wall=40649
2023-05-20 03:54:59 | INFO | train_inner | epoch 157:     15 / 65 loss=6.667, nll_loss=4.924, ppl=30.37, wps=3811.6, ups=0.32, wpb=11881.6, bsz=522.8, num_updates=10130, lr=0, gnorm=3.687, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.2, wall=40665
2023-05-20 03:55:15 | INFO | train_inner | epoch 157:     20 / 65 loss=6.652, nll_loss=4.904, ppl=29.95, wps=3783.5, ups=0.31, wpb=12387.4, bsz=563.6, num_updates=10135, lr=0, gnorm=9.185, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.6, wall=40681
2023-05-20 03:55:30 | INFO | train_inner | epoch 157:     25 / 65 loss=6.719, nll_loss=4.982, ppl=31.61, wps=4025, ups=0.34, wpb=11964, bsz=591.4, num_updates=10140, lr=0, gnorm=8.276, clip=100, loss_scale=0.25, train_wall=15, gb_free=21.7, wall=40696
2023-05-20 03:55:46 | INFO | train_inner | epoch 157:     30 / 65 loss=6.712, nll_loss=4.975, ppl=31.45, wps=3974, ups=0.32, wpb=12430.6, bsz=583.4, num_updates=10145, lr=0, gnorm=7.304, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.8, wall=40711
2023-05-20 03:56:02 | INFO | train_inner | epoch 157:     35 / 65 loss=6.693, nll_loss=4.954, ppl=30.99, wps=3838.5, ups=0.31, wpb=12484.8, bsz=576.2, num_updates=10150, lr=0, gnorm=4.547, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.4, wall=40728
2023-05-20 03:56:19 | INFO | train_inner | epoch 157:     40 / 65 loss=6.711, nll_loss=4.977, ppl=31.49, wps=3526.4, ups=0.29, wpb=12261.4, bsz=593.8, num_updates=10155, lr=0, gnorm=9.036, clip=100, loss_scale=0.25, train_wall=17, gb_free=21.8, wall=40745
2023-05-20 03:56:35 | INFO | train_inner | epoch 157:     45 / 65 loss=6.664, nll_loss=4.921, ppl=30.3, wps=3944.6, ups=0.33, wpb=12112.6, bsz=573.6, num_updates=10160, lr=0, gnorm=3.175, clip=100, loss_scale=0.25, train_wall=15, gb_free=20.6, wall=40760
2023-05-20 03:56:51 | INFO | train_inner | epoch 157:     50 / 65 loss=6.692, nll_loss=4.952, ppl=30.96, wps=3744.3, ups=0.31, wpb=12094.4, bsz=503.2, num_updates=10165, lr=0, gnorm=4.128, clip=100, loss_scale=0.25, train_wall=16, gb_free=21.5, wall=40777
2023-05-20 03:57:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-05-20 03:57:10 | INFO | train_inner | epoch 157:     56 / 65 loss=6.744, nll_loss=5.006, ppl=32.12, wps=3183.3, ups=0.27, wpb=11969.6, bsz=535.6, num_updates=10170, lr=0, gnorm=6.615, clip=100, loss_scale=0.125, train_wall=19, gb_free=21.6, wall=40795
2023-05-20 03:57:25 | INFO | train_inner | epoch 157:     61 / 65 loss=6.673, nll_loss=4.928, ppl=30.44, wps=3843.9, ups=0.32, wpb=12034.8, bsz=546, num_updates=10175, lr=0, gnorm=13.649, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=40811
2023-05-20 03:57:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 03:57:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:58:14 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 911.2 | wpb 2785 | bsz 105.2 | num_updates 10179 | best_loss 6.036
2023-05-20 03:58:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 10179 updates
2023-05-20 03:58:14 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint157.pt
2023-05-20 03:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint157.pt
2023-05-20 03:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint157.pt (epoch 157 @ 10179 updates, score 6.05) (writing took 6.571161173284054 seconds)
2023-05-20 03:58:21 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2023-05-20 03:58:21 | INFO | train | epoch 157 | loss 6.697 | nll_loss 4.956 | ppl 31.04 | wps 3065.3 | ups 0.25 | wpb 12048 | bsz 554.5 | num_updates 10179 | lr 0 | gnorm 7.424 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.8 | wall 40867
2023-05-20 03:58:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 03:58:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 03:58:21 | INFO | fairseq.trainer | begin training epoch 158
2023-05-20 03:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 03:58:24 | INFO | train_inner | epoch 158:      1 / 65 loss=6.718, nll_loss=4.968, ppl=31.31, wps=908.6, ups=0.09, wpb=10619.6, bsz=512.2, num_updates=10180, lr=0, gnorm=5.27, clip=100, loss_scale=0.125, train_wall=12, gb_free=21.7, wall=40870
2023-05-20 03:58:40 | INFO | train_inner | epoch 158:      6 / 65 loss=6.659, nll_loss=4.918, ppl=30.23, wps=3717.9, ups=0.31, wpb=11956.8, bsz=507.2, num_updates=10185, lr=0, gnorm=6.388, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=40886
2023-05-20 03:58:56 | INFO | train_inner | epoch 158:     11 / 65 loss=6.773, nll_loss=5.049, ppl=33.11, wps=3965.4, ups=0.32, wpb=12527.2, bsz=640, num_updates=10190, lr=0, gnorm=4.373, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.1, wall=40901
2023-05-20 03:59:12 | INFO | train_inner | epoch 158:     16 / 65 loss=6.678, nll_loss=4.935, ppl=30.6, wps=3685.4, ups=0.31, wpb=11887.8, bsz=558.6, num_updates=10195, lr=0, gnorm=5.071, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=40918
2023-05-20 03:59:27 | INFO | train_inner | epoch 158:     21 / 65 loss=6.736, nll_loss=5, ppl=32, wps=3902.6, ups=0.32, wpb=12107, bsz=564, num_updates=10200, lr=0, gnorm=5.817, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=40933
2023-05-20 03:59:45 | INFO | train_inner | epoch 158:     26 / 65 loss=6.749, nll_loss=5.02, ppl=32.44, wps=3454.6, ups=0.28, wpb=12196.8, bsz=547.6, num_updates=10205, lr=0, gnorm=8.761, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=40951
2023-05-20 04:00:00 | INFO | train_inner | epoch 158:     31 / 65 loss=6.943, nll_loss=5.212, ppl=37.07, wps=3921.2, ups=0.33, wpb=11927.8, bsz=587, num_updates=10210, lr=0, gnorm=7.165, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.6, wall=40966
2023-05-20 04:00:16 | INFO | train_inner | epoch 158:     36 / 65 loss=6.655, nll_loss=4.909, ppl=30.05, wps=3953.9, ups=0.32, wpb=12208.6, bsz=551.2, num_updates=10215, lr=0, gnorm=15.024, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.9, wall=40981
2023-05-20 04:00:31 | INFO | train_inner | epoch 158:     41 / 65 loss=6.653, nll_loss=4.904, ppl=29.94, wps=3943.7, ups=0.33, wpb=11938.4, bsz=517, num_updates=10220, lr=0, gnorm=7.204, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=40996
2023-05-20 04:00:46 | INFO | train_inner | epoch 158:     46 / 65 loss=6.704, nll_loss=4.959, ppl=31.1, wps=4063.2, ups=0.34, wpb=12043.6, bsz=563.2, num_updates=10225, lr=0, gnorm=4.741, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=41011
2023-05-20 04:01:03 | INFO | train_inner | epoch 158:     51 / 65 loss=6.652, nll_loss=4.911, ppl=30.08, wps=3728.1, ups=0.29, wpb=13016.4, bsz=662.4, num_updates=10230, lr=0, gnorm=3.473, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=41029
2023-05-20 04:01:20 | INFO | train_inner | epoch 158:     56 / 65 loss=6.649, nll_loss=4.903, ppl=29.91, wps=3603.4, ups=0.3, wpb=12033, bsz=514.8, num_updates=10235, lr=0, gnorm=4.443, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=41045
2023-05-20 04:01:36 | INFO | train_inner | epoch 158:     61 / 65 loss=6.689, nll_loss=4.947, ppl=30.84, wps=3963.2, ups=0.31, wpb=12734, bsz=604.8, num_updates=10240, lr=0, gnorm=6.029, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=41062
2023-05-20 04:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:01:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:02:27 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 902.3 | wpb 2785 | bsz 105.2 | num_updates 10244 | best_loss 6.036
2023-05-20 04:02:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 10244 updates
2023-05-20 04:02:27 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint158.pt
2023-05-20 04:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint158.pt
2023-05-20 04:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint158.pt (epoch 158 @ 10244 updates, score 6.05) (writing took 6.636959511786699 seconds)
2023-05-20 04:02:34 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2023-05-20 04:02:34 | INFO | train | epoch 158 | loss 6.711 | nll_loss 4.971 | ppl 31.37 | wps 3100.7 | ups 0.26 | wpb 12057.5 | bsz 558.9 | num_updates 10244 | lr 0 | gnorm 6.505 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.8 | wall 41120
2023-05-20 04:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:02:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:02:34 | INFO | fairseq.trainer | begin training epoch 159
2023-05-20 04:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:02:38 | INFO | train_inner | epoch 159:      1 / 65 loss=6.84, nll_loss=5.114, ppl=34.63, wps=831.7, ups=0.08, wpb=10283.6, bsz=477.8, num_updates=10245, lr=0, gnorm=6.092, clip=100, loss_scale=0.125, train_wall=15, gb_free=21, wall=41123
2023-05-20 04:02:54 | INFO | train_inner | epoch 159:      6 / 65 loss=6.673, nll_loss=4.931, ppl=30.5, wps=3657.9, ups=0.3, wpb=12040, bsz=504.4, num_updates=10250, lr=0, gnorm=6.541, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=41140
2023-05-20 04:03:10 | INFO | train_inner | epoch 159:     11 / 65 loss=6.633, nll_loss=4.882, ppl=29.49, wps=3557.9, ups=0.3, wpb=11697.4, bsz=497, num_updates=10255, lr=0, gnorm=5.961, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=41156
2023-05-20 04:03:26 | INFO | train_inner | epoch 159:     16 / 65 loss=6.683, nll_loss=4.939, ppl=30.69, wps=4042.4, ups=0.32, wpb=12445.2, bsz=555.6, num_updates=10260, lr=0, gnorm=5.955, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=41172
2023-05-20 04:03:43 | INFO | train_inner | epoch 159:     21 / 65 loss=6.669, nll_loss=4.928, ppl=30.45, wps=3555, ups=0.29, wpb=12184, bsz=547, num_updates=10265, lr=0, gnorm=3.816, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=41189
2023-05-20 04:04:00 | INFO | train_inner | epoch 159:     26 / 65 loss=6.722, nll_loss=4.991, ppl=31.8, wps=3607.8, ups=0.3, wpb=11910.8, bsz=601, num_updates=10270, lr=0, gnorm=50.47, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=41205
2023-05-20 04:04:15 | INFO | train_inner | epoch 159:     31 / 65 loss=6.656, nll_loss=4.911, ppl=30.08, wps=3923, ups=0.32, wpb=12327.6, bsz=561.2, num_updates=10275, lr=0, gnorm=7.276, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=41221
2023-05-20 04:04:33 | INFO | train_inner | epoch 159:     36 / 65 loss=6.717, nll_loss=4.979, ppl=31.53, wps=3324.5, ups=0.28, wpb=12054.4, bsz=533.4, num_updates=10280, lr=0, gnorm=4.648, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=41239
2023-05-20 04:04:49 | INFO | train_inner | epoch 159:     41 / 65 loss=6.63, nll_loss=4.883, ppl=29.5, wps=3915.4, ups=0.32, wpb=12382.8, bsz=589.2, num_updates=10285, lr=0, gnorm=7.594, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=41255
2023-05-20 04:05:05 | INFO | train_inner | epoch 159:     46 / 65 loss=6.884, nll_loss=5.158, ppl=35.7, wps=3875.4, ups=0.31, wpb=12532, bsz=610.6, num_updates=10290, lr=0, gnorm=10.828, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=41271
2023-05-20 04:05:21 | INFO | train_inner | epoch 159:     51 / 65 loss=6.639, nll_loss=4.889, ppl=29.64, wps=3793.7, ups=0.32, wpb=12031.4, bsz=549.8, num_updates=10295, lr=0, gnorm=4.624, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=41287
2023-05-20 04:05:36 | INFO | train_inner | epoch 159:     56 / 65 loss=6.728, nll_loss=4.988, ppl=31.74, wps=4168, ups=0.34, wpb=12356.8, bsz=592, num_updates=10300, lr=0, gnorm=3.007, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=41302
2023-05-20 04:05:54 | INFO | train_inner | epoch 159:     61 / 65 loss=6.699, nll_loss=4.958, ppl=31.08, wps=3357, ups=0.28, wpb=12081.8, bsz=552.4, num_updates=10305, lr=0, gnorm=3.311, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.3, wall=41320
2023-05-20 04:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:06:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:06:47 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 842.7 | wpb 2785 | bsz 105.2 | num_updates 10309 | best_loss 6.036
2023-05-20 04:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 10309 updates
2023-05-20 04:06:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint159.pt
2023-05-20 04:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint159.pt
2023-05-20 04:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint159.pt (epoch 159 @ 10309 updates, score 6.05) (writing took 6.740438230335712 seconds)
2023-05-20 04:06:54 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2023-05-20 04:06:54 | INFO | train | epoch 159 | loss 6.707 | nll_loss 4.967 | ppl 31.28 | wps 3016.9 | ups 0.25 | wpb 12058 | bsz 560.1 | num_updates 10309 | lr 0 | gnorm 9.61 | clip 100 | loss_scale 0.125 | train_wall 210 | gb_free 21.8 | wall 41379
2023-05-20 04:06:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:06:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:06:54 | INFO | fairseq.trainer | begin training epoch 160
2023-05-20 04:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:06:57 | INFO | train_inner | epoch 160:      1 / 65 loss=6.735, nll_loss=5.002, ppl=32.04, wps=855.9, ups=0.08, wpb=10720.8, bsz=540.6, num_updates=10310, lr=0, gnorm=11.593, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.6, wall=41382
2023-05-20 04:07:13 | INFO | train_inner | epoch 160:      6 / 65 loss=6.788, nll_loss=5.056, ppl=33.27, wps=3773.9, ups=0.31, wpb=12011, bsz=582.4, num_updates=10315, lr=0, gnorm=9.458, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=41398
2023-05-20 04:07:29 | INFO | train_inner | epoch 160:     11 / 65 loss=6.907, nll_loss=5.18, ppl=36.25, wps=3853.8, ups=0.31, wpb=12502.6, bsz=660.6, num_updates=10320, lr=0, gnorm=9.233, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=41415
2023-05-20 04:07:46 | INFO | train_inner | epoch 160:     16 / 65 loss=6.764, nll_loss=5.037, ppl=32.83, wps=3611.5, ups=0.29, wpb=12481.2, bsz=586, num_updates=10325, lr=0, gnorm=11.387, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=41432
2023-05-20 04:08:03 | INFO | train_inner | epoch 160:     21 / 65 loss=6.619, nll_loss=4.869, ppl=29.23, wps=3452.3, ups=0.29, wpb=11880.6, bsz=487.2, num_updates=10330, lr=0, gnorm=4.016, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=41449
2023-05-20 04:08:19 | INFO | train_inner | epoch 160:     26 / 65 loss=6.663, nll_loss=4.919, ppl=30.25, wps=3959, ups=0.31, wpb=12798.6, bsz=652.6, num_updates=10335, lr=0, gnorm=6.415, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=41465
2023-05-20 04:08:36 | INFO | train_inner | epoch 160:     31 / 65 loss=6.685, nll_loss=4.937, ppl=30.62, wps=3658.8, ups=0.31, wpb=11941, bsz=549.2, num_updates=10340, lr=0, gnorm=5.038, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=41482
2023-05-20 04:08:51 | INFO | train_inner | epoch 160:     36 / 65 loss=6.701, nll_loss=4.96, ppl=31.12, wps=4014.7, ups=0.33, wpb=12270.2, bsz=569, num_updates=10345, lr=0, gnorm=10.343, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.3, wall=41497
2023-05-20 04:09:08 | INFO | train_inner | epoch 160:     41 / 65 loss=6.639, nll_loss=4.892, ppl=29.7, wps=3515.9, ups=0.3, wpb=11860.8, bsz=489.2, num_updates=10350, lr=0, gnorm=8.863, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=41514
2023-05-20 04:09:24 | INFO | train_inner | epoch 160:     46 / 65 loss=6.764, nll_loss=5.031, ppl=32.7, wps=3771.4, ups=0.31, wpb=11987, bsz=548, num_updates=10355, lr=0, gnorm=2.963, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.5, wall=41530
2023-05-20 04:09:40 | INFO | train_inner | epoch 160:     51 / 65 loss=6.677, nll_loss=4.936, ppl=30.61, wps=3693.5, ups=0.3, wpb=12127.6, bsz=555, num_updates=10360, lr=0, gnorm=3.287, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=41546
2023-05-20 04:09:57 | INFO | train_inner | epoch 160:     56 / 65 loss=6.656, nll_loss=4.915, ppl=30.17, wps=3773, ups=0.3, wpb=12428.8, bsz=539, num_updates=10365, lr=0, gnorm=15.203, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.4, wall=41562
2023-05-20 04:10:14 | INFO | train_inner | epoch 160:     61 / 65 loss=6.655, nll_loss=4.91, ppl=30.07, wps=3438.2, ups=0.29, wpb=12049.4, bsz=579.4, num_updates=10370, lr=0, gnorm=3.95, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=41580
2023-05-20 04:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:10:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:11:05 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 868.5 | wpb 2785 | bsz 105.2 | num_updates 10374 | best_loss 6.036
2023-05-20 04:11:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10374 updates
2023-05-20 04:11:05 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint160.pt
2023-05-20 04:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint160.pt
2023-05-20 04:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint160.pt (epoch 160 @ 10374 updates, score 6.05) (writing took 6.914900463074446 seconds)
2023-05-20 04:11:12 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2023-05-20 04:11:12 | INFO | train | epoch 160 | loss 6.713 | nll_loss 4.973 | ppl 31.41 | wps 3031.6 | ups 0.25 | wpb 12058.2 | bsz 559.1 | num_updates 10374 | lr 0 | gnorm 7.472 | clip 100 | loss_scale 0.125 | train_wall 210 | gb_free 21.8 | wall 41638
2023-05-20 04:11:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:11:12 | INFO | fairseq.trainer | begin training epoch 161
2023-05-20 04:11:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:11:15 | INFO | train_inner | epoch 161:      1 / 65 loss=6.779, nll_loss=5.042, ppl=32.96, wps=861.8, ups=0.08, wpb=10514.4, bsz=510.4, num_updates=10375, lr=0, gnorm=5.913, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.4, wall=41641
2023-05-20 04:11:32 | INFO | train_inner | epoch 161:      6 / 65 loss=6.686, nll_loss=4.942, ppl=30.74, wps=3591.7, ups=0.29, wpb=12189.8, bsz=532.2, num_updates=10380, lr=0, gnorm=7.077, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=41658
2023-05-20 04:11:49 | INFO | train_inner | epoch 161:     11 / 65 loss=6.764, nll_loss=5.031, ppl=32.7, wps=3629.1, ups=0.29, wpb=12381, bsz=608.4, num_updates=10385, lr=0, gnorm=6.275, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=41675
2023-05-20 04:12:07 | INFO | train_inner | epoch 161:     16 / 65 loss=6.711, nll_loss=4.97, ppl=31.35, wps=3372.3, ups=0.28, wpb=11957.4, bsz=540.4, num_updates=10390, lr=0, gnorm=6.018, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.1, wall=41693
2023-05-20 04:12:24 | INFO | train_inner | epoch 161:     21 / 65 loss=6.621, nll_loss=4.867, ppl=29.17, wps=3469.2, ups=0.29, wpb=12127.2, bsz=514.4, num_updates=10395, lr=0, gnorm=3.899, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.7, wall=41710
2023-05-20 04:12:41 | INFO | train_inner | epoch 161:     26 / 65 loss=6.737, nll_loss=5.006, ppl=32.12, wps=3561, ups=0.3, wpb=12070.2, bsz=571.2, num_updates=10400, lr=0, gnorm=11.387, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=41727
2023-05-20 04:12:56 | INFO | train_inner | epoch 161:     31 / 65 loss=6.924, nll_loss=5.203, ppl=36.84, wps=4436, ups=0.35, wpb=12551.4, bsz=671.2, num_updates=10405, lr=0, gnorm=8.545, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=41741
2023-05-20 04:13:12 | INFO | train_inner | epoch 161:     36 / 65 loss=6.76, nll_loss=5.029, ppl=32.65, wps=3779, ups=0.31, wpb=12303.6, bsz=586.2, num_updates=10410, lr=0, gnorm=3.834, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=41758
2023-05-20 04:13:29 | INFO | train_inner | epoch 161:     41 / 65 loss=6.668, nll_loss=4.924, ppl=30.36, wps=3360, ups=0.28, wpb=11832.6, bsz=491.8, num_updates=10415, lr=0, gnorm=5.483, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.8, wall=41775
2023-05-20 04:13:44 | INFO | train_inner | epoch 161:     46 / 65 loss=6.763, nll_loss=5.026, ppl=32.57, wps=4229.6, ups=0.34, wpb=12406.4, bsz=589.2, num_updates=10420, lr=0, gnorm=5.355, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=41790
2023-05-20 04:13:59 | INFO | train_inner | epoch 161:     51 / 65 loss=6.612, nll_loss=4.864, ppl=29.12, wps=3987.8, ups=0.33, wpb=12159.4, bsz=536, num_updates=10425, lr=0, gnorm=7.385, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=41805
2023-05-20 04:14:16 | INFO | train_inner | epoch 161:     56 / 65 loss=6.686, nll_loss=4.946, ppl=30.82, wps=3693.8, ups=0.31, wpb=12043, bsz=593.4, num_updates=10430, lr=0, gnorm=4.124, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=41821
2023-05-20 04:14:32 | INFO | train_inner | epoch 161:     61 / 65 loss=6.653, nll_loss=4.909, ppl=30.05, wps=3629.9, ups=0.3, wpb=12138.2, bsz=554, num_updates=10435, lr=0, gnorm=10.082, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=41838
2023-05-20 04:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:14:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:15:24 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 879.5 | wpb 2785 | bsz 105.2 | num_updates 10439 | best_loss 6.036
2023-05-20 04:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 10439 updates
2023-05-20 04:15:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint161.pt
2023-05-20 04:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint161.pt
2023-05-20 04:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint161.pt (epoch 161 @ 10439 updates, score 6.05) (writing took 9.085906445980072 seconds)
2023-05-20 04:15:33 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2023-05-20 04:15:33 | INFO | train | epoch 161 | loss 6.713 | nll_loss 4.973 | ppl 31.41 | wps 3006.6 | ups 0.25 | wpb 12061.7 | bsz 559.6 | num_updates 10439 | lr 0 | gnorm 6.752 | clip 100 | loss_scale 0.125 | train_wall 210 | gb_free 21 | wall 41899
2023-05-20 04:15:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:15:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:15:33 | INFO | fairseq.trainer | begin training epoch 162
2023-05-20 04:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:15:36 | INFO | train_inner | epoch 162:      1 / 65 loss=6.703, nll_loss=4.96, ppl=31.12, wps=821.4, ups=0.08, wpb=10411.6, bsz=456.4, num_updates=10440, lr=0, gnorm=10.938, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.5, wall=41902
2023-05-20 04:15:51 | INFO | train_inner | epoch 162:      6 / 65 loss=6.6, nll_loss=4.845, ppl=28.74, wps=3908.2, ups=0.33, wpb=11872.6, bsz=537, num_updates=10445, lr=0, gnorm=8.274, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=41917
2023-05-20 04:16:05 | INFO | train_inner | epoch 162:     11 / 65 loss=6.677, nll_loss=4.935, ppl=30.58, wps=4293.9, ups=0.35, wpb=12246.8, bsz=591.6, num_updates=10450, lr=0, gnorm=6.508, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.7, wall=41931
2023-05-20 04:16:20 | INFO | train_inner | epoch 162:     16 / 65 loss=6.881, nll_loss=5.153, ppl=35.57, wps=4077.9, ups=0.33, wpb=12229.8, bsz=615.4, num_updates=10455, lr=0, gnorm=4.361, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=41946
2023-05-20 04:16:36 | INFO | train_inner | epoch 162:     21 / 65 loss=6.835, nll_loss=5.109, ppl=34.51, wps=4031.6, ups=0.32, wpb=12446, bsz=619.8, num_updates=10460, lr=0, gnorm=5.621, clip=100, loss_scale=0.125, train_wall=15, gb_free=20.6, wall=41961
2023-05-20 04:16:51 | INFO | train_inner | epoch 162:     26 / 65 loss=6.699, nll_loss=4.963, ppl=31.19, wps=4110.9, ups=0.34, wpb=12259.6, bsz=603, num_updates=10465, lr=0, gnorm=3.115, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=41976
2023-05-20 04:17:07 | INFO | train_inner | epoch 162:     31 / 65 loss=6.719, nll_loss=4.984, ppl=31.65, wps=3834.2, ups=0.31, wpb=12284.8, bsz=560.2, num_updates=10470, lr=0, gnorm=7.262, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=41992
2023-05-20 04:17:22 | INFO | train_inner | epoch 162:     36 / 65 loss=6.676, nll_loss=4.935, ppl=30.59, wps=3788.7, ups=0.32, wpb=12016, bsz=553.6, num_updates=10475, lr=0, gnorm=2.952, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=42008
2023-05-20 04:17:41 | INFO | train_inner | epoch 162:     41 / 65 loss=6.693, nll_loss=4.955, ppl=31.01, wps=3435.1, ups=0.28, wpb=12417, bsz=591.4, num_updates=10480, lr=0, gnorm=6.452, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=42026
2023-05-20 04:17:57 | INFO | train_inner | epoch 162:     46 / 65 loss=6.709, nll_loss=4.97, ppl=31.35, wps=3627, ups=0.3, wpb=12235.6, bsz=561.8, num_updates=10485, lr=0, gnorm=13.668, clip=100, loss_scale=0.125, train_wall=17, gb_free=20.4, wall=42043
2023-05-20 04:18:14 | INFO | train_inner | epoch 162:     51 / 65 loss=6.694, nll_loss=4.954, ppl=30.99, wps=3510.5, ups=0.29, wpb=12022.4, bsz=517.8, num_updates=10490, lr=0, gnorm=14.334, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.3, wall=42060
2023-05-20 04:18:32 | INFO | train_inner | epoch 162:     56 / 65 loss=6.678, nll_loss=4.935, ppl=30.58, wps=3389.5, ups=0.28, wpb=11924.6, bsz=510, num_updates=10495, lr=0, gnorm=5.232, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.7, wall=42078
2023-05-20 04:18:47 | INFO | train_inner | epoch 162:     61 / 65 loss=6.643, nll_loss=4.89, ppl=29.65, wps=4286.4, ups=0.34, wpb=12443.2, bsz=569.8, num_updates=10500, lr=0, gnorm=7.255, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=42092
2023-05-20 04:18:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:18:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:19:41 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 834.5 | wpb 2785 | bsz 105.2 | num_updates 10504 | best_loss 6.036
2023-05-20 04:19:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 10504 updates
2023-05-20 04:19:41 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint162.pt
2023-05-20 04:19:45 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint162.pt
2023-05-20 04:19:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint162.pt (epoch 162 @ 10504 updates, score 6.05) (writing took 6.892233457416296 seconds)
2023-05-20 04:19:48 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2023-05-20 04:19:48 | INFO | train | epoch 162 | loss 6.71 | nll_loss 4.97 | ppl 31.34 | wps 3076.9 | ups 0.26 | wpb 12052.5 | bsz 558.7 | num_updates 10504 | lr 0 | gnorm 7.303 | clip 100 | loss_scale 0.125 | train_wall 205 | gb_free 21.6 | wall 42153
2023-05-20 04:19:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:19:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:19:48 | INFO | fairseq.trainer | begin training epoch 163
2023-05-20 04:19:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:19:51 | INFO | train_inner | epoch 163:      1 / 65 loss=6.644, nll_loss=4.898, ppl=29.81, wps=796.5, ups=0.08, wpb=10218.8, bsz=403.6, num_updates=10505, lr=0, gnorm=7.335, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.8, wall=42157
2023-05-20 04:20:07 | INFO | train_inner | epoch 163:      6 / 65 loss=6.703, nll_loss=4.959, ppl=31.1, wps=3741.5, ups=0.31, wpb=12192.4, bsz=564.2, num_updates=10510, lr=0, gnorm=7.318, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.6, wall=42173
2023-05-20 04:20:23 | INFO | train_inner | epoch 163:     11 / 65 loss=6.74, nll_loss=5.004, ppl=32.1, wps=3637.2, ups=0.31, wpb=11815.6, bsz=551.2, num_updates=10515, lr=0, gnorm=20.837, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=42189
2023-05-20 04:20:40 | INFO | train_inner | epoch 163:     16 / 65 loss=6.749, nll_loss=5.012, ppl=32.28, wps=3676.1, ups=0.3, wpb=12210.8, bsz=583, num_updates=10520, lr=0, gnorm=3.656, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.5, wall=42206
2023-05-20 04:20:55 | INFO | train_inner | epoch 163:     21 / 65 loss=6.834, nll_loss=5.099, ppl=34.28, wps=4137.3, ups=0.33, wpb=12567, bsz=596.8, num_updates=10525, lr=0, gnorm=5.402, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=42221
2023-05-20 04:21:13 | INFO | train_inner | epoch 163:     26 / 65 loss=6.684, nll_loss=4.945, ppl=30.8, wps=3456.3, ups=0.28, wpb=12357, bsz=553.2, num_updates=10530, lr=0, gnorm=3.519, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.5, wall=42239
2023-05-20 04:21:27 | INFO | train_inner | epoch 163:     31 / 65 loss=6.639, nll_loss=4.892, ppl=29.7, wps=4451.8, ups=0.36, wpb=12476, bsz=568.2, num_updates=10535, lr=0, gnorm=11.547, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.6, wall=42253
2023-05-20 04:21:43 | INFO | train_inner | epoch 163:     36 / 65 loss=6.688, nll_loss=4.947, ppl=30.84, wps=3785.9, ups=0.31, wpb=12256.8, bsz=550, num_updates=10540, lr=0, gnorm=3.551, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=42269
2023-05-20 04:22:01 | INFO | train_inner | epoch 163:     41 / 65 loss=6.69, nll_loss=4.953, ppl=30.97, wps=3393.7, ups=0.28, wpb=12274.2, bsz=604.4, num_updates=10545, lr=0, gnorm=7.577, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=42287
2023-05-20 04:22:16 | INFO | train_inner | epoch 163:     46 / 65 loss=6.652, nll_loss=4.904, ppl=29.94, wps=4083.8, ups=0.34, wpb=11931.4, bsz=514.8, num_updates=10550, lr=0, gnorm=4.964, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.5, wall=42302
2023-05-20 04:22:31 | INFO | train_inner | epoch 163:     51 / 65 loss=6.745, nll_loss=5.009, ppl=32.19, wps=4131.7, ups=0.34, wpb=12186, bsz=612, num_updates=10555, lr=0, gnorm=5.997, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=42316
2023-05-20 04:22:46 | INFO | train_inner | epoch 163:     56 / 65 loss=6.649, nll_loss=4.901, ppl=29.88, wps=3832.7, ups=0.32, wpb=12153, bsz=557.8, num_updates=10560, lr=0, gnorm=5.155, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.9, wall=42332
2023-05-20 04:23:04 | INFO | train_inner | epoch 163:     61 / 65 loss=6.737, nll_loss=5, ppl=32, wps=3480.4, ups=0.29, wpb=11901.2, bsz=507.4, num_updates=10565, lr=0, gnorm=4.628, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=42349
2023-05-20 04:23:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-05-20 04:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:23:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:23:58 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 843.1 | wpb 2785 | bsz 105.2 | num_updates 10568 | best_loss 6.036
2023-05-20 04:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 10568 updates
2023-05-20 04:23:58 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint163.pt
2023-05-20 04:24:03 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint163.pt
2023-05-20 04:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint163.pt (epoch 163 @ 10568 updates, score 6.05) (writing took 6.50389014557004 seconds)
2023-05-20 04:24:05 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2023-05-20 04:24:05 | INFO | train | epoch 163 | loss 6.709 | nll_loss 4.969 | ppl 31.32 | wps 2999.7 | ups 0.25 | wpb 12053.7 | bsz 557.3 | num_updates 10568 | lr 0 | gnorm 6.815 | clip 100 | loss_scale 0.0625 | train_wall 208 | gb_free 21.5 | wall 42410
2023-05-20 04:24:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:24:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:24:05 | INFO | fairseq.trainer | begin training epoch 164
2023-05-20 04:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:24:11 | INFO | train_inner | epoch 164:      2 / 65 loss=6.682, nll_loss=4.94, ppl=30.69, wps=764.7, ups=0.07, wpb=10285.6, bsz=524.4, num_updates=10570, lr=0, gnorm=3.935, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.1, wall=42417
2023-05-20 04:24:26 | INFO | train_inner | epoch 164:      7 / 65 loss=6.707, nll_loss=4.967, ppl=31.28, wps=4094.7, ups=0.33, wpb=12455.2, bsz=627.8, num_updates=10575, lr=0, gnorm=4.271, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=42432
2023-05-20 04:24:42 | INFO | train_inner | epoch 164:     12 / 65 loss=6.636, nll_loss=4.888, ppl=29.62, wps=3769.2, ups=0.32, wpb=11818.4, bsz=548.4, num_updates=10580, lr=0, gnorm=4.594, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21, wall=42447
2023-05-20 04:24:58 | INFO | train_inner | epoch 164:     17 / 65 loss=6.73, nll_loss=4.994, ppl=31.87, wps=3719.3, ups=0.3, wpb=12380.2, bsz=546.2, num_updates=10585, lr=0, gnorm=5.905, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=42464
2023-05-20 04:25:15 | INFO | train_inner | epoch 164:     22 / 65 loss=6.707, nll_loss=4.971, ppl=31.35, wps=3576.2, ups=0.3, wpb=12068.8, bsz=553, num_updates=10590, lr=0, gnorm=19.27, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21, wall=42481
2023-05-20 04:25:31 | INFO | train_inner | epoch 164:     27 / 65 loss=6.717, nll_loss=4.976, ppl=31.48, wps=3864.4, ups=0.31, wpb=12278.8, bsz=562.2, num_updates=10595, lr=0, gnorm=5.783, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=42497
2023-05-20 04:25:47 | INFO | train_inner | epoch 164:     32 / 65 loss=6.683, nll_loss=4.94, ppl=30.69, wps=3681, ups=0.32, wpb=11668.2, bsz=511.4, num_updates=10600, lr=0, gnorm=6.461, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=42513
2023-05-20 04:26:03 | INFO | train_inner | epoch 164:     37 / 65 loss=6.816, nll_loss=5.088, ppl=34.01, wps=3977.4, ups=0.32, wpb=12461.8, bsz=597.4, num_updates=10605, lr=0, gnorm=3.484, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=42528
2023-05-20 04:26:18 | INFO | train_inner | epoch 164:     42 / 65 loss=6.703, nll_loss=4.964, ppl=31.22, wps=3990.7, ups=0.32, wpb=12279, bsz=548.6, num_updates=10610, lr=0, gnorm=6.72, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.6, wall=42544
2023-05-20 04:26:33 | INFO | train_inner | epoch 164:     47 / 65 loss=6.933, nll_loss=5.216, ppl=37.16, wps=4173.5, ups=0.34, wpb=12439.6, bsz=669.8, num_updates=10615, lr=0, gnorm=3.456, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=42559
2023-05-20 04:26:50 | INFO | train_inner | epoch 164:     52 / 65 loss=6.639, nll_loss=4.889, ppl=29.63, wps=3611.1, ups=0.29, wpb=12241.4, bsz=489.6, num_updates=10620, lr=0, gnorm=4.596, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=42576
2023-05-20 04:27:05 | INFO | train_inner | epoch 164:     57 / 65 loss=6.676, nll_loss=4.931, ppl=30.5, wps=4143.5, ups=0.34, wpb=12171.2, bsz=568.8, num_updates=10625, lr=0, gnorm=5.898, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=42590
2023-05-20 04:27:21 | INFO | train_inner | epoch 164:     62 / 65 loss=6.667, nll_loss=4.929, ppl=30.46, wps=3684.3, ups=0.3, wpb=12409.2, bsz=575.4, num_updates=10630, lr=0, gnorm=2.356, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.1, wall=42607
2023-05-20 04:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:27:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:28:08 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 907.9 | wpb 2785 | bsz 105.2 | num_updates 10633 | best_loss 6.036
2023-05-20 04:28:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 10633 updates
2023-05-20 04:28:08 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint164.pt
2023-05-20 04:28:13 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint164.pt
2023-05-20 04:28:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint164.pt (epoch 164 @ 10633 updates, score 6.05) (writing took 6.654124062508345 seconds)
2023-05-20 04:28:15 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2023-05-20 04:28:15 | INFO | train | epoch 164 | loss 6.712 | nll_loss 4.972 | ppl 31.39 | wps 3133.8 | ups 0.26 | wpb 12057.2 | bsz 559.5 | num_updates 10633 | lr 0 | gnorm 5.979 | clip 100 | loss_scale 0.0625 | train_wall 204 | gb_free 21.3 | wall 42661
2023-05-20 04:28:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:28:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:28:15 | INFO | fairseq.trainer | begin training epoch 165
2023-05-20 04:28:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:28:21 | INFO | train_inner | epoch 165:      2 / 65 loss=6.64, nll_loss=4.889, ppl=29.63, wps=839.2, ups=0.08, wpb=10080.4, bsz=435.8, num_updates=10635, lr=0, gnorm=4.572, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.4, wall=42667
2023-05-20 04:28:36 | INFO | train_inner | epoch 165:      7 / 65 loss=6.643, nll_loss=4.894, ppl=29.73, wps=3916, ups=0.34, wpb=11639.2, bsz=496.4, num_updates=10640, lr=0, gnorm=18.279, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.1, wall=42682
2023-05-20 04:28:52 | INFO | train_inner | epoch 165:     12 / 65 loss=6.693, nll_loss=4.947, ppl=30.85, wps=3934.2, ups=0.33, wpb=12087.8, bsz=517.6, num_updates=10645, lr=0, gnorm=6.131, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.9, wall=42697
2023-05-20 04:29:08 | INFO | train_inner | epoch 165:     17 / 65 loss=6.689, nll_loss=4.952, ppl=30.96, wps=3711.7, ups=0.31, wpb=12103.6, bsz=579.8, num_updates=10650, lr=0, gnorm=4.398, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=42714
2023-05-20 04:29:24 | INFO | train_inner | epoch 165:     22 / 65 loss=6.778, nll_loss=5.038, ppl=32.86, wps=3835.3, ups=0.32, wpb=12122, bsz=547.4, num_updates=10655, lr=0, gnorm=8.816, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=42730
2023-05-20 04:29:41 | INFO | train_inner | epoch 165:     27 / 65 loss=6.715, nll_loss=4.972, ppl=31.38, wps=3511.6, ups=0.29, wpb=12237.6, bsz=605.8, num_updates=10660, lr=0, gnorm=10.575, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=42747
2023-05-20 04:29:56 | INFO | train_inner | epoch 165:     32 / 65 loss=6.67, nll_loss=4.929, ppl=30.46, wps=4224.7, ups=0.33, wpb=12646, bsz=611.2, num_updates=10665, lr=0, gnorm=3.614, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=42762
2023-05-20 04:30:12 | INFO | train_inner | epoch 165:     37 / 65 loss=6.758, nll_loss=5.022, ppl=32.5, wps=3904.3, ups=0.31, wpb=12486, bsz=577, num_updates=10670, lr=0, gnorm=8.991, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=42778
2023-05-20 04:30:27 | INFO | train_inner | epoch 165:     42 / 65 loss=6.683, nll_loss=4.941, ppl=30.73, wps=4121.3, ups=0.33, wpb=12340.8, bsz=589.6, num_updates=10675, lr=0, gnorm=5.277, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=42793
2023-05-20 04:30:44 | INFO | train_inner | epoch 165:     47 / 65 loss=6.68, nll_loss=4.943, ppl=30.76, wps=3636.9, ups=0.3, wpb=12201.4, bsz=527, num_updates=10680, lr=0, gnorm=6.034, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.7, wall=42810
2023-05-20 04:30:59 | INFO | train_inner | epoch 165:     52 / 65 loss=6.656, nll_loss=4.914, ppl=30.14, wps=4213.7, ups=0.33, wpb=12728.8, bsz=654.8, num_updates=10685, lr=0, gnorm=6.296, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=42825
2023-05-20 04:31:17 | INFO | train_inner | epoch 165:     57 / 65 loss=6.691, nll_loss=4.951, ppl=30.94, wps=3260.3, ups=0.28, wpb=11754, bsz=530.2, num_updates=10690, lr=0, gnorm=4.449, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.5, wall=42843
2023-05-20 04:31:34 | INFO | train_inner | epoch 165:     62 / 65 loss=6.732, nll_loss=4.999, ppl=31.98, wps=3554.7, ups=0.3, wpb=11972.6, bsz=521.6, num_updates=10695, lr=0, gnorm=4.351, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=42860
2023-05-20 04:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:31:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:32:24 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 836.2 | wpb 2785 | bsz 105.2 | num_updates 10698 | best_loss 6.036
2023-05-20 04:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 10698 updates
2023-05-20 04:32:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint165.pt
2023-05-20 04:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint165.pt
2023-05-20 04:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint165.pt (epoch 165 @ 10698 updates, score 6.05) (writing took 6.754272438585758 seconds)
2023-05-20 04:32:31 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2023-05-20 04:32:31 | INFO | train | epoch 165 | loss 6.697 | nll_loss 4.957 | ppl 31.05 | wps 3056.5 | ups 0.25 | wpb 12045.6 | bsz 555 | num_updates 10698 | lr 0 | gnorm 7.17 | clip 100 | loss_scale 0.0625 | train_wall 206 | gb_free 21.7 | wall 42917
2023-05-20 04:32:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:32:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:32:31 | INFO | fairseq.trainer | begin training epoch 166
2023-05-20 04:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:32:38 | INFO | train_inner | epoch 166:      2 / 65 loss=6.781, nll_loss=5.046, ppl=33.03, wps=821.5, ups=0.08, wpb=10517.6, bsz=534.4, num_updates=10700, lr=0, gnorm=8.086, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.2, wall=42924
2023-05-20 04:32:53 | INFO | train_inner | epoch 166:      7 / 65 loss=6.718, nll_loss=4.981, ppl=31.58, wps=3950.5, ups=0.33, wpb=12068.6, bsz=587, num_updates=10705, lr=0, gnorm=13.279, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=42939
2023-05-20 04:33:09 | INFO | train_inner | epoch 166:     12 / 65 loss=6.859, nll_loss=5.13, ppl=35.01, wps=3933.9, ups=0.32, wpb=12404.8, bsz=620.4, num_updates=10710, lr=0, gnorm=7.452, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.4, wall=42955
2023-05-20 04:33:25 | INFO | train_inner | epoch 166:     17 / 65 loss=6.645, nll_loss=4.9, ppl=29.86, wps=3733.6, ups=0.31, wpb=12107.8, bsz=536, num_updates=10715, lr=0, gnorm=3.561, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=42971
2023-05-20 04:33:43 | INFO | train_inner | epoch 166:     22 / 65 loss=6.631, nll_loss=4.882, ppl=29.49, wps=3425.3, ups=0.29, wpb=11997, bsz=464.8, num_updates=10720, lr=0, gnorm=5.012, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=42988
2023-05-20 04:33:58 | INFO | train_inner | epoch 166:     27 / 65 loss=6.69, nll_loss=4.946, ppl=30.83, wps=3939.5, ups=0.32, wpb=12210.4, bsz=546, num_updates=10725, lr=0, gnorm=6.17, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=43004
2023-05-20 04:34:14 | INFO | train_inner | epoch 166:     32 / 65 loss=6.676, nll_loss=4.939, ppl=30.67, wps=3646.4, ups=0.31, wpb=11831.2, bsz=514.6, num_updates=10730, lr=0, gnorm=3.499, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=43020
2023-05-20 04:34:29 | INFO | train_inner | epoch 166:     37 / 65 loss=6.747, nll_loss=5.013, ppl=32.3, wps=4356.6, ups=0.34, wpb=12769.4, bsz=669, num_updates=10735, lr=0, gnorm=11.615, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.1, wall=43035
2023-05-20 04:34:45 | INFO | train_inner | epoch 166:     42 / 65 loss=6.696, nll_loss=4.956, ppl=31.05, wps=3875.3, ups=0.32, wpb=12231.8, bsz=584.8, num_updates=10740, lr=0, gnorm=7.942, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=43051
2023-05-20 04:35:01 | INFO | train_inner | epoch 166:     47 / 65 loss=6.661, nll_loss=4.919, ppl=30.25, wps=3639.2, ups=0.3, wpb=12113.8, bsz=539, num_updates=10745, lr=0, gnorm=6.592, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=43067
2023-05-20 04:35:17 | INFO | train_inner | epoch 166:     52 / 65 loss=6.747, nll_loss=5.011, ppl=32.25, wps=3968.8, ups=0.32, wpb=12266.2, bsz=605, num_updates=10750, lr=0, gnorm=17.371, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=43083
2023-05-20 04:35:32 | INFO | train_inner | epoch 166:     57 / 65 loss=6.661, nll_loss=4.917, ppl=30.21, wps=3921.1, ups=0.32, wpb=12102, bsz=527.4, num_updates=10755, lr=0, gnorm=6.36, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.4, wall=43098
2023-05-20 04:35:48 | INFO | train_inner | epoch 166:     62 / 65 loss=6.726, nll_loss=4.987, ppl=31.71, wps=3952.2, ups=0.32, wpb=12226.4, bsz=557.2, num_updates=10760, lr=0, gnorm=5.694, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.3, wall=43114
2023-05-20 04:35:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:35:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:36:36 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 908 | wpb 2785 | bsz 105.2 | num_updates 10763 | best_loss 6.036
2023-05-20 04:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 10763 updates
2023-05-20 04:36:36 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint166.pt
2023-05-20 04:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint166.pt
2023-05-20 04:36:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint166.pt (epoch 166 @ 10763 updates, score 6.05) (writing took 6.549005255103111 seconds)
2023-05-20 04:36:42 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2023-05-20 04:36:42 | INFO | train | epoch 166 | loss 6.712 | nll_loss 4.973 | ppl 31.4 | wps 3118.1 | ups 0.26 | wpb 12053.9 | bsz 559.2 | num_updates 10763 | lr 0 | gnorm 7.852 | clip 100 | loss_scale 0.0625 | train_wall 205 | gb_free 21.3 | wall 43168
2023-05-20 04:36:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:36:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:36:42 | INFO | fairseq.trainer | begin training epoch 167
2023-05-20 04:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:36:49 | INFO | train_inner | epoch 167:      2 / 65 loss=6.693, nll_loss=4.954, ppl=30.99, wps=848, ups=0.08, wpb=10427.4, bsz=454.4, num_updates=10765, lr=0, gnorm=6.681, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=43175
2023-05-20 04:37:05 | INFO | train_inner | epoch 167:      7 / 65 loss=6.639, nll_loss=4.893, ppl=29.7, wps=3770.6, ups=0.32, wpb=11862.4, bsz=488.4, num_updates=10770, lr=0, gnorm=8.429, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=43191
2023-05-20 04:37:24 | INFO | train_inner | epoch 167:     12 / 65 loss=6.675, nll_loss=4.933, ppl=30.54, wps=3114.8, ups=0.27, wpb=11652.6, bsz=481, num_updates=10775, lr=0, gnorm=6.669, clip=100, loss_scale=0.0625, train_wall=19, gb_free=21.1, wall=43209
2023-05-20 04:37:43 | INFO | train_inner | epoch 167:     17 / 65 loss=6.881, nll_loss=5.158, ppl=35.7, wps=3283.2, ups=0.27, wpb=12351.6, bsz=625.4, num_updates=10780, lr=0, gnorm=3.518, clip=100, loss_scale=0.0625, train_wall=19, gb_free=21.6, wall=43228
2023-05-20 04:37:58 | INFO | train_inner | epoch 167:     22 / 65 loss=6.711, nll_loss=4.978, ppl=31.52, wps=4024.8, ups=0.32, wpb=12463, bsz=574, num_updates=10785, lr=0, gnorm=13.004, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.5, wall=43244
2023-05-20 04:38:13 | INFO | train_inner | epoch 167:     27 / 65 loss=6.644, nll_loss=4.895, ppl=29.75, wps=4089.4, ups=0.34, wpb=12051.6, bsz=566.8, num_updates=10790, lr=0, gnorm=13.363, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=43259
2023-05-20 04:38:29 | INFO | train_inner | epoch 167:     32 / 65 loss=6.63, nll_loss=4.879, ppl=29.42, wps=3717.7, ups=0.31, wpb=12168.2, bsz=538.4, num_updates=10795, lr=0, gnorm=4.603, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21, wall=43275
2023-05-20 04:38:44 | INFO | train_inner | epoch 167:     37 / 65 loss=6.728, nll_loss=4.991, ppl=31.81, wps=4072.8, ups=0.33, wpb=12228.2, bsz=591.6, num_updates=10800, lr=0, gnorm=2.795, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=43290
2023-05-20 04:39:00 | INFO | train_inner | epoch 167:     42 / 65 loss=6.647, nll_loss=4.9, ppl=29.86, wps=3804.4, ups=0.32, wpb=11790.4, bsz=527.6, num_updates=10805, lr=0, gnorm=5.848, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.4, wall=43305
2023-05-20 04:39:16 | INFO | train_inner | epoch 167:     47 / 65 loss=6.878, nll_loss=5.149, ppl=35.49, wps=3800, ups=0.3, wpb=12646.6, bsz=665.2, num_updates=10810, lr=0, gnorm=3.294, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.5, wall=43322
2023-05-20 04:39:33 | INFO | train_inner | epoch 167:     52 / 65 loss=6.716, nll_loss=4.978, ppl=31.52, wps=3684.3, ups=0.3, wpb=12214.6, bsz=587.4, num_updates=10815, lr=0, gnorm=9.927, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=43339
2023-05-20 04:39:48 | INFO | train_inner | epoch 167:     57 / 65 loss=6.651, nll_loss=4.908, ppl=30.01, wps=4240.6, ups=0.34, wpb=12631.4, bsz=616, num_updates=10820, lr=0, gnorm=4.762, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=43354
2023-05-20 04:40:03 | INFO | train_inner | epoch 167:     62 / 65 loss=6.675, nll_loss=4.929, ppl=30.46, wps=3968.3, ups=0.33, wpb=12151.6, bsz=547.2, num_updates=10825, lr=0, gnorm=5.331, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=43369
2023-05-20 04:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:40:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:40:50 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 901.2 | wpb 2785 | bsz 105.2 | num_updates 10828 | best_loss 6.036
2023-05-20 04:40:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 10828 updates
2023-05-20 04:40:50 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint167.pt
2023-05-20 04:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint167.pt
2023-05-20 04:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint167.pt (epoch 167 @ 10828 updates, score 6.05) (writing took 6.628174405544996 seconds)
2023-05-20 04:40:57 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2023-05-20 04:40:57 | INFO | train | epoch 167 | loss 6.707 | nll_loss 4.967 | ppl 31.27 | wps 3078.6 | ups 0.26 | wpb 12053.9 | bsz 559.6 | num_updates 10828 | lr 0 | gnorm 7.162 | clip 100 | loss_scale 0.0625 | train_wall 207 | gb_free 21.2 | wall 43422
2023-05-20 04:40:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:40:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:40:57 | INFO | fairseq.trainer | begin training epoch 168
2023-05-20 04:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:41:03 | INFO | train_inner | epoch 168:      2 / 65 loss=6.698, nll_loss=4.957, ppl=31.06, wps=855.7, ups=0.08, wpb=10213.8, bsz=454.2, num_updates=10830, lr=0, gnorm=12.094, clip=100, loss_scale=0.0625, train_wall=13, gb_free=21.7, wall=43429
2023-05-20 04:41:18 | INFO | train_inner | epoch 168:      7 / 65 loss=6.579, nll_loss=4.825, ppl=28.34, wps=3943.6, ups=0.32, wpb=12217.2, bsz=508.8, num_updates=10835, lr=0, gnorm=6.934, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=43444
2023-05-20 04:41:36 | INFO | train_inner | epoch 168:     12 / 65 loss=6.757, nll_loss=5.027, ppl=32.62, wps=3607.6, ups=0.29, wpb=12487.2, bsz=634, num_updates=10840, lr=0, gnorm=7.004, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.1, wall=43461
2023-05-20 04:41:51 | INFO | train_inner | epoch 168:     17 / 65 loss=6.698, nll_loss=4.959, ppl=31.1, wps=3957, ups=0.33, wpb=12120.2, bsz=550, num_updates=10845, lr=0, gnorm=7.105, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=43477
2023-05-20 04:42:07 | INFO | train_inner | epoch 168:     22 / 65 loss=6.691, nll_loss=4.954, ppl=31, wps=3759.5, ups=0.31, wpb=12237.4, bsz=616.4, num_updates=10850, lr=0, gnorm=5.851, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=43493
2023-05-20 04:42:23 | INFO | train_inner | epoch 168:     27 / 65 loss=6.695, nll_loss=4.955, ppl=31.01, wps=3785.8, ups=0.31, wpb=12120.6, bsz=520.2, num_updates=10855, lr=0, gnorm=9.715, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=43509
2023-05-20 04:42:40 | INFO | train_inner | epoch 168:     32 / 65 loss=6.961, nll_loss=5.243, ppl=37.88, wps=3690.2, ups=0.3, wpb=12320.2, bsz=618.4, num_updates=10860, lr=0, gnorm=3.696, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.9, wall=43526
2023-05-20 04:42:55 | INFO | train_inner | epoch 168:     37 / 65 loss=6.592, nll_loss=4.83, ppl=28.45, wps=3856.6, ups=0.32, wpb=11997, bsz=498.8, num_updates=10865, lr=0, gnorm=7.6, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=43541
2023-05-20 04:43:10 | INFO | train_inner | epoch 168:     42 / 65 loss=6.614, nll_loss=4.862, ppl=29.09, wps=4069.5, ups=0.33, wpb=12166, bsz=542, num_updates=10870, lr=0, gnorm=15.797, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.4, wall=43556
2023-05-20 04:43:25 | INFO | train_inner | epoch 168:     47 / 65 loss=6.692, nll_loss=4.953, ppl=30.97, wps=4036.4, ups=0.33, wpb=12182.4, bsz=584, num_updates=10875, lr=0, gnorm=4.898, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=43571
2023-05-20 04:43:42 | INFO | train_inner | epoch 168:     52 / 65 loss=6.721, nll_loss=4.99, ppl=31.77, wps=3585.7, ups=0.3, wpb=12029, bsz=522.8, num_updates=10880, lr=0, gnorm=5.84, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=43588
2023-05-20 04:43:57 | INFO | train_inner | epoch 168:     57 / 65 loss=6.651, nll_loss=4.909, ppl=30.04, wps=4178, ups=0.34, wpb=12362.8, bsz=569.8, num_updates=10885, lr=0, gnorm=4.332, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=43603
2023-05-20 04:44:14 | INFO | train_inner | epoch 168:     62 / 65 loss=6.879, nll_loss=5.157, ppl=35.67, wps=3739.5, ups=0.3, wpb=12368, bsz=649, num_updates=10890, lr=0, gnorm=4.225, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=43619
2023-05-20 04:44:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:44:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:45:02 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 841.2 | wpb 2785 | bsz 105.2 | num_updates 10893 | best_loss 6.036
2023-05-20 04:45:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 10893 updates
2023-05-20 04:45:02 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint168.pt
2023-05-20 04:45:07 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint168.pt
2023-05-20 04:45:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint168.pt (epoch 168 @ 10893 updates, score 6.05) (writing took 6.642626199871302 seconds)
2023-05-20 04:45:09 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2023-05-20 04:45:09 | INFO | train | epoch 168 | loss 6.71 | nll_loss 4.971 | ppl 31.36 | wps 3108.8 | ups 0.26 | wpb 12058.5 | bsz 560 | num_updates 10893 | lr 0 | gnorm 6.753 | clip 100 | loss_scale 0.0625 | train_wall 203 | gb_free 21.8 | wall 43675
2023-05-20 04:45:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:45:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:45:09 | INFO | fairseq.trainer | begin training epoch 169
2023-05-20 04:45:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:45:15 | INFO | train_inner | epoch 169:      2 / 65 loss=6.715, nll_loss=4.973, ppl=31.4, wps=849.1, ups=0.08, wpb=10420.4, bsz=529, num_updates=10895, lr=0, gnorm=3.867, clip=100, loss_scale=0.0625, train_wall=12, gb_free=21.4, wall=43681
2023-05-20 04:45:33 | INFO | train_inner | epoch 169:      7 / 65 loss=6.66, nll_loss=4.916, ppl=30.19, wps=3412.5, ups=0.28, wpb=12050.4, bsz=504.6, num_updates=10900, lr=0, gnorm=4.645, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.7, wall=43698
2023-05-20 04:45:48 | INFO | train_inner | epoch 169:     12 / 65 loss=6.68, nll_loss=4.94, ppl=30.7, wps=3935.2, ups=0.33, wpb=12102.4, bsz=525.8, num_updates=10905, lr=0, gnorm=13.369, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21, wall=43714
2023-05-20 04:46:04 | INFO | train_inner | epoch 169:     17 / 65 loss=6.726, nll_loss=4.986, ppl=31.68, wps=3863.7, ups=0.31, wpb=12327.2, bsz=557.6, num_updates=10910, lr=0, gnorm=12.835, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.1, wall=43730
2023-05-20 04:46:20 | INFO | train_inner | epoch 169:     22 / 65 loss=6.694, nll_loss=4.955, ppl=31.03, wps=3848, ups=0.32, wpb=12074.4, bsz=548.8, num_updates=10915, lr=0, gnorm=12.703, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=43745
2023-05-20 04:46:35 | INFO | train_inner | epoch 169:     27 / 65 loss=6.649, nll_loss=4.905, ppl=29.95, wps=3958.9, ups=0.31, wpb=12593, bsz=593.2, num_updates=10920, lr=0, gnorm=8.393, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=43761
2023-05-20 04:46:51 | INFO | train_inner | epoch 169:     32 / 65 loss=6.715, nll_loss=4.977, ppl=31.5, wps=3812.7, ups=0.31, wpb=12191.6, bsz=564.4, num_updates=10925, lr=0, gnorm=5.787, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.7, wall=43777
2023-05-20 04:47:08 | INFO | train_inner | epoch 169:     37 / 65 loss=6.653, nll_loss=4.903, ppl=29.91, wps=3592.7, ups=0.31, wpb=11674.8, bsz=507.2, num_updates=10930, lr=0, gnorm=13.122, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21, wall=43793
2023-05-20 04:47:24 | INFO | train_inner | epoch 169:     42 / 65 loss=6.921, nll_loss=5.195, ppl=36.62, wps=3800.1, ups=0.31, wpb=12262.4, bsz=631.2, num_updates=10935, lr=0, gnorm=2.918, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=43810
2023-05-20 04:47:39 | INFO | train_inner | epoch 169:     47 / 65 loss=6.693, nll_loss=4.952, ppl=30.95, wps=4128.4, ups=0.33, wpb=12509, bsz=571.8, num_updates=10940, lr=0, gnorm=4.664, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=43825
2023-05-20 04:47:55 | INFO | train_inner | epoch 169:     52 / 65 loss=6.757, nll_loss=5.018, ppl=32.41, wps=3792.2, ups=0.31, wpb=12351, bsz=599.4, num_updates=10945, lr=0, gnorm=4.076, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=43841
2023-05-20 04:48:13 | INFO | train_inner | epoch 169:     57 / 65 loss=6.731, nll_loss=5.003, ppl=32.06, wps=3389.5, ups=0.28, wpb=12042.6, bsz=619, num_updates=10950, lr=0, gnorm=3.099, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.8, wall=43859
2023-05-20 04:48:29 | INFO | train_inner | epoch 169:     62 / 65 loss=6.689, nll_loss=4.95, ppl=30.91, wps=3891.6, ups=0.32, wpb=12166.6, bsz=573.6, num_updates=10955, lr=0, gnorm=4.448, clip=100, loss_scale=0.0625, train_wall=16, gb_free=20.8, wall=43874
2023-05-20 04:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:48:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:49:15 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 908.7 | wpb 2785 | bsz 105.2 | num_updates 10958 | best_loss 6.036
2023-05-20 04:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 10958 updates
2023-05-20 04:49:15 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint169.pt
2023-05-20 04:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint169.pt
2023-05-20 04:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint169.pt (epoch 169 @ 10958 updates, score 6.05) (writing took 6.691824279725552 seconds)
2023-05-20 04:49:21 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2023-05-20 04:49:21 | INFO | train | epoch 169 | loss 6.71 | nll_loss 4.971 | ppl 31.36 | wps 3105.6 | ups 0.26 | wpb 12059.8 | bsz 559.9 | num_updates 10958 | lr 0 | gnorm 7.332 | clip 100 | loss_scale 0.0625 | train_wall 206 | gb_free 20.5 | wall 43927
2023-05-20 04:49:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:49:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:49:21 | INFO | fairseq.trainer | begin training epoch 170
2023-05-20 04:49:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:49:28 | INFO | train_inner | epoch 170:      2 / 65 loss=6.669, nll_loss=4.925, ppl=30.38, wps=883, ups=0.08, wpb=10391.8, bsz=478, num_updates=10960, lr=0, gnorm=5.925, clip=100, loss_scale=0.0625, train_wall=12, gb_free=20.6, wall=43933
2023-05-20 04:49:43 | INFO | train_inner | epoch 170:      7 / 65 loss=6.674, nll_loss=4.935, ppl=30.59, wps=3903.7, ups=0.32, wpb=12219.6, bsz=533.8, num_updates=10965, lr=0, gnorm=5.821, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.9, wall=43949
2023-05-20 04:50:00 | INFO | train_inner | epoch 170:     12 / 65 loss=6.661, nll_loss=4.917, ppl=30.22, wps=3638.3, ups=0.3, wpb=12118.4, bsz=560.6, num_updates=10970, lr=0, gnorm=3.269, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=43966
2023-05-20 04:50:15 | INFO | train_inner | epoch 170:     17 / 65 loss=6.702, nll_loss=4.964, ppl=31.22, wps=4186, ups=0.34, wpb=12327.2, bsz=597, num_updates=10975, lr=0, gnorm=5.606, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=43980
2023-05-20 04:50:31 | INFO | train_inner | epoch 170:     22 / 65 loss=6.757, nll_loss=5.026, ppl=32.57, wps=3703.7, ups=0.3, wpb=12416, bsz=582.8, num_updates=10980, lr=0, gnorm=6.238, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.4, wall=43997
2023-05-20 04:50:48 | INFO | train_inner | epoch 170:     27 / 65 loss=6.639, nll_loss=4.886, ppl=29.57, wps=3448.9, ups=0.29, wpb=11826.4, bsz=527.4, num_updates=10985, lr=0, gnorm=10.75, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=44014
2023-05-20 04:51:03 | INFO | train_inner | epoch 170:     32 / 65 loss=6.873, nll_loss=5.141, ppl=35.28, wps=4034.2, ups=0.34, wpb=12030, bsz=563.6, num_updates=10990, lr=0, gnorm=4.062, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=44029
2023-05-20 04:51:18 | INFO | train_inner | epoch 170:     37 / 65 loss=6.682, nll_loss=4.938, ppl=30.66, wps=4241.4, ups=0.35, wpb=12215.4, bsz=550.2, num_updates=10995, lr=0, gnorm=5.537, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.5, wall=44044
2023-05-20 04:51:34 | INFO | train_inner | epoch 170:     42 / 65 loss=6.664, nll_loss=4.915, ppl=30.17, wps=3913.4, ups=0.32, wpb=12301, bsz=562.6, num_updates=11000, lr=0, gnorm=7.917, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=44059
2023-05-20 04:51:50 | INFO | train_inner | epoch 170:     47 / 65 loss=6.893, nll_loss=5.17, ppl=36.01, wps=3751.7, ups=0.31, wpb=12097, bsz=607.8, num_updates=11005, lr=0, gnorm=4.637, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=44075
2023-05-20 04:52:07 | INFO | train_inner | epoch 170:     52 / 65 loss=6.673, nll_loss=4.934, ppl=30.57, wps=3359.4, ups=0.29, wpb=11762.8, bsz=556.4, num_updates=11010, lr=0, gnorm=2.913, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.2, wall=44093
2023-05-20 04:52:22 | INFO | train_inner | epoch 170:     57 / 65 loss=6.663, nll_loss=4.922, ppl=30.31, wps=4239.1, ups=0.34, wpb=12364.2, bsz=580, num_updates=11015, lr=0, gnorm=2.609, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=44107
2023-05-20 04:52:40 | INFO | train_inner | epoch 170:     62 / 65 loss=6.66, nll_loss=4.914, ppl=30.15, wps=3321.6, ups=0.27, wpb=12320, bsz=520.8, num_updates=11020, lr=0, gnorm=6.901, clip=100, loss_scale=0.0625, train_wall=19, gb_free=21.8, wall=44126
2023-05-20 04:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:52:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:53:32 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 836.9 | wpb 2785 | bsz 105.2 | num_updates 11023 | best_loss 6.036
2023-05-20 04:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 11023 updates
2023-05-20 04:53:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint170.pt
2023-05-20 04:53:36 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint170.pt
2023-05-20 04:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint170.pt (epoch 170 @ 11023 updates, score 6.05) (writing took 6.7029563039541245 seconds)
2023-05-20 04:53:38 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2023-05-20 04:53:38 | INFO | train | epoch 170 | loss 6.713 | nll_loss 4.973 | ppl 31.41 | wps 3048.5 | ups 0.25 | wpb 12054.7 | bsz 559.6 | num_updates 11023 | lr 0 | gnorm 5.556 | clip 100 | loss_scale 0.0625 | train_wall 207 | gb_free 21.7 | wall 44184
2023-05-20 04:53:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:53:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:53:38 | INFO | fairseq.trainer | begin training epoch 171
2023-05-20 04:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:53:45 | INFO | train_inner | epoch 171:      2 / 65 loss=6.746, nll_loss=5.015, ppl=32.33, wps=831, ups=0.08, wpb=10714.6, bsz=514.8, num_updates=11025, lr=0, gnorm=4.428, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.6, wall=44190
2023-05-20 04:54:01 | INFO | train_inner | epoch 171:      7 / 65 loss=6.717, nll_loss=4.981, ppl=31.59, wps=3751.5, ups=0.31, wpb=12240.4, bsz=562.6, num_updates=11030, lr=0, gnorm=3.738, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=44207
2023-05-20 04:54:17 | INFO | train_inner | epoch 171:     12 / 65 loss=6.67, nll_loss=4.927, ppl=30.43, wps=3635.5, ups=0.31, wpb=11854.2, bsz=526.6, num_updates=11035, lr=0, gnorm=6.253, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=44223
2023-05-20 04:54:32 | INFO | train_inner | epoch 171:     17 / 65 loss=6.573, nll_loss=4.813, ppl=28.11, wps=4077.4, ups=0.34, wpb=12023.8, bsz=531.4, num_updates=11040, lr=0, gnorm=9.337, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=44238
2023-05-20 04:54:48 | INFO | train_inner | epoch 171:     22 / 65 loss=6.692, nll_loss=4.953, ppl=30.98, wps=3864.9, ups=0.31, wpb=12445.6, bsz=587.8, num_updates=11045, lr=0, gnorm=8.03, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=44254
2023-05-20 04:55:04 | INFO | train_inner | epoch 171:     27 / 65 loss=6.708, nll_loss=4.971, ppl=31.36, wps=3749.2, ups=0.31, wpb=12136.8, bsz=557.4, num_updates=11050, lr=0, gnorm=2.625, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=44270
2023-05-20 04:55:22 | INFO | train_inner | epoch 171:     32 / 65 loss=6.65, nll_loss=4.902, ppl=29.91, wps=3449.4, ups=0.29, wpb=11856, bsz=533.8, num_updates=11055, lr=0, gnorm=9.881, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.3, wall=44287
2023-05-20 04:55:37 | INFO | train_inner | epoch 171:     37 / 65 loss=6.723, nll_loss=4.985, ppl=31.67, wps=4125.1, ups=0.33, wpb=12446.2, bsz=605.4, num_updates=11060, lr=0, gnorm=10.314, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.2, wall=44302
2023-05-20 04:55:53 | INFO | train_inner | epoch 171:     42 / 65 loss=6.688, nll_loss=4.945, ppl=30.8, wps=3638.7, ups=0.3, wpb=12172.6, bsz=517.4, num_updates=11065, lr=0, gnorm=6.488, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.6, wall=44319
2023-05-20 04:56:09 | INFO | train_inner | epoch 171:     47 / 65 loss=6.887, nll_loss=5.169, ppl=35.98, wps=4013.2, ups=0.33, wpb=12275.4, bsz=631.6, num_updates=11070, lr=0, gnorm=3.269, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=44334
2023-05-20 04:56:26 | INFO | train_inner | epoch 171:     52 / 65 loss=6.683, nll_loss=4.94, ppl=30.7, wps=3599.8, ups=0.29, wpb=12331.2, bsz=597, num_updates=11075, lr=0, gnorm=4.502, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.1, wall=44352
2023-05-20 04:56:41 | INFO | train_inner | epoch 171:     57 / 65 loss=6.819, nll_loss=5.081, ppl=33.85, wps=3886.9, ups=0.32, wpb=12144.6, bsz=538, num_updates=11080, lr=0, gnorm=11.942, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=44367
2023-05-20 04:56:58 | INFO | train_inner | epoch 171:     62 / 65 loss=6.715, nll_loss=4.975, ppl=31.46, wps=3700.1, ups=0.3, wpb=12234.8, bsz=570.2, num_updates=11085, lr=0, gnorm=6.305, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=44384
2023-05-20 04:57:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 04:57:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:57:47 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 849.8 | wpb 2785 | bsz 105.2 | num_updates 11088 | best_loss 6.036
2023-05-20 04:57:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 11088 updates
2023-05-20 04:57:47 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint171.pt
2023-05-20 04:57:52 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint171.pt
2023-05-20 04:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint171.pt (epoch 171 @ 11088 updates, score 6.05) (writing took 6.6523867808282375 seconds)
2023-05-20 04:57:54 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2023-05-20 04:57:54 | INFO | train | epoch 171 | loss 6.713 | nll_loss 4.974 | ppl 31.43 | wps 3068.2 | ups 0.25 | wpb 12054.9 | bsz 559.6 | num_updates 11088 | lr 0 | gnorm 6.857 | clip 100 | loss_scale 0.125 | train_wall 206 | gb_free 21.7 | wall 44439
2023-05-20 04:57:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 04:57:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 04:57:54 | INFO | fairseq.trainer | begin training epoch 172
2023-05-20 04:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 04:58:01 | INFO | train_inner | epoch 172:      2 / 65 loss=6.693, nll_loss=4.952, ppl=30.96, wps=812.2, ups=0.08, wpb=10184.6, bsz=465.4, num_updates=11090, lr=0, gnorm=9.809, clip=100, loss_scale=0.125, train_wall=14, gb_free=21.3, wall=44446
2023-05-20 04:58:18 | INFO | train_inner | epoch 172:      7 / 65 loss=6.705, nll_loss=4.968, ppl=31.29, wps=3428, ups=0.29, wpb=11990.4, bsz=543, num_updates=11095, lr=0, gnorm=6.701, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=44464
2023-05-20 04:58:34 | INFO | train_inner | epoch 172:     12 / 65 loss=6.717, nll_loss=4.983, ppl=31.63, wps=3819.7, ups=0.32, wpb=12104.4, bsz=597.2, num_updates=11100, lr=0, gnorm=8.064, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.8, wall=44480
2023-05-20 04:58:49 | INFO | train_inner | epoch 172:     17 / 65 loss=6.663, nll_loss=4.918, ppl=30.23, wps=3995.6, ups=0.33, wpb=12160.2, bsz=521, num_updates=11105, lr=0, gnorm=2.931, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.6, wall=44495
2023-05-20 04:59:04 | INFO | train_inner | epoch 172:     22 / 65 loss=6.658, nll_loss=4.914, ppl=30.15, wps=4176.5, ups=0.34, wpb=12341.4, bsz=548.8, num_updates=11110, lr=0, gnorm=8.321, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.8, wall=44510
2023-05-20 04:59:20 | INFO | train_inner | epoch 172:     27 / 65 loss=6.7, nll_loss=4.962, ppl=31.18, wps=3805.1, ups=0.31, wpb=12099.4, bsz=556.8, num_updates=11115, lr=0, gnorm=6.065, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.6, wall=44526
2023-05-20 04:59:37 | INFO | train_inner | epoch 172:     32 / 65 loss=6.644, nll_loss=4.898, ppl=29.82, wps=3673.1, ups=0.3, wpb=12296.8, bsz=528.6, num_updates=11120, lr=0, gnorm=3.899, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.4, wall=44542
2023-05-20 04:59:52 | INFO | train_inner | epoch 172:     37 / 65 loss=6.629, nll_loss=4.876, ppl=29.36, wps=4155.1, ups=0.33, wpb=12446.4, bsz=591.4, num_updates=11125, lr=0, gnorm=6.087, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=44557
2023-05-20 05:00:08 | INFO | train_inner | epoch 172:     42 / 65 loss=6.761, nll_loss=5.032, ppl=32.72, wps=3720.8, ups=0.31, wpb=12012.2, bsz=581, num_updates=11130, lr=0, gnorm=4.816, clip=100, loss_scale=0.125, train_wall=16, gb_free=20.8, wall=44574
2023-05-20 05:00:25 | INFO | train_inner | epoch 172:     47 / 65 loss=6.656, nll_loss=4.91, ppl=30.05, wps=3477.6, ups=0.28, wpb=12213, bsz=531.8, num_updates=11135, lr=0, gnorm=7.274, clip=100, loss_scale=0.125, train_wall=18, gb_free=21.6, wall=44591
2023-05-20 05:00:43 | INFO | train_inner | epoch 172:     52 / 65 loss=6.905, nll_loss=5.177, ppl=36.17, wps=3586.5, ups=0.29, wpb=12404.4, bsz=664.8, num_updates=11140, lr=0, gnorm=7.352, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.9, wall=44608
2023-05-20 05:00:58 | INFO | train_inner | epoch 172:     57 / 65 loss=6.78, nll_loss=5.042, ppl=32.94, wps=4063.1, ups=0.32, wpb=12669.4, bsz=618.6, num_updates=11145, lr=0, gnorm=3.505, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=44624
2023-05-20 05:01:14 | INFO | train_inner | epoch 172:     62 / 65 loss=6.705, nll_loss=4.964, ppl=31.21, wps=3821.8, ups=0.32, wpb=12001.4, bsz=551.4, num_updates=11150, lr=0, gnorm=5.312, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.5, wall=44640
2023-05-20 05:01:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:01:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:02:01 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 899.2 | wpb 2785 | bsz 105.2 | num_updates 11153 | best_loss 6.036
2023-05-20 05:02:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 11153 updates
2023-05-20 05:02:01 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint172.pt
2023-05-20 05:02:05 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint172.pt
2023-05-20 05:02:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint172.pt (epoch 172 @ 11153 updates, score 6.05) (writing took 6.677988361567259 seconds)
2023-05-20 05:02:08 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2023-05-20 05:02:08 | INFO | train | epoch 172 | loss 6.712 | nll_loss 4.972 | ppl 31.39 | wps 3088.5 | ups 0.26 | wpb 12065.2 | bsz 560.6 | num_updates 11153 | lr 0 | gnorm 5.95 | clip 100 | loss_scale 0.125 | train_wall 207 | gb_free 21.5 | wall 44693
2023-05-20 05:02:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:02:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:02:08 | INFO | fairseq.trainer | begin training epoch 173
2023-05-20 05:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:02:13 | INFO | train_inner | epoch 173:      2 / 65 loss=6.731, nll_loss=5.001, ppl=32.02, wps=869.7, ups=0.08, wpb=10347.6, bsz=460.6, num_updates=11155, lr=0, gnorm=4.493, clip=100, loss_scale=0.125, train_wall=13, gb_free=21.4, wall=44699
2023-05-20 05:02:28 | INFO | train_inner | epoch 173:      7 / 65 loss=7.128, nll_loss=5.42, ppl=42.81, wps=4245.6, ups=0.34, wpb=12432.6, bsz=740, num_updates=11160, lr=0, gnorm=6.202, clip=100, loss_scale=0.125, train_wall=15, gb_free=21.7, wall=44714
2023-05-20 05:02:44 | INFO | train_inner | epoch 173:     12 / 65 loss=6.742, nll_loss=5.008, ppl=32.18, wps=3722.8, ups=0.3, wpb=12220, bsz=532.6, num_updates=11165, lr=0, gnorm=3.745, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.7, wall=44730
2023-05-20 05:03:01 | INFO | train_inner | epoch 173:     17 / 65 loss=6.693, nll_loss=4.955, ppl=31.01, wps=3583.6, ups=0.3, wpb=11986, bsz=520.2, num_updates=11170, lr=0, gnorm=4.593, clip=100, loss_scale=0.125, train_wall=17, gb_free=21.6, wall=44747
2023-05-20 05:03:17 | INFO | train_inner | epoch 173:     22 / 65 loss=6.647, nll_loss=4.904, ppl=29.94, wps=3930.1, ups=0.32, wpb=12453.8, bsz=564.2, num_updates=11175, lr=0, gnorm=3.638, clip=100, loss_scale=0.125, train_wall=16, gb_free=21.3, wall=44763
2023-05-20 05:03:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-05-20 05:03:35 | INFO | train_inner | epoch 173:     28 / 65 loss=6.664, nll_loss=4.923, ppl=30.33, wps=3341.8, ups=0.27, wpb=12297, bsz=573, num_updates=11180, lr=0, gnorm=5.573, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.6, wall=44781
2023-05-20 05:03:51 | INFO | train_inner | epoch 173:     33 / 65 loss=6.581, nll_loss=4.823, ppl=28.31, wps=3858.4, ups=0.32, wpb=12118.4, bsz=542.4, num_updates=11185, lr=0, gnorm=4.784, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=44797
2023-05-20 05:04:08 | INFO | train_inner | epoch 173:     38 / 65 loss=6.683, nll_loss=4.941, ppl=30.72, wps=3557.8, ups=0.3, wpb=11842.8, bsz=547.8, num_updates=11190, lr=0, gnorm=9.211, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=44814
2023-05-20 05:04:25 | INFO | train_inner | epoch 173:     43 / 65 loss=6.748, nll_loss=5.02, ppl=32.44, wps=3488.7, ups=0.29, wpb=11877.6, bsz=608.8, num_updates=11195, lr=0, gnorm=5.084, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=44831
2023-05-20 05:04:41 | INFO | train_inner | epoch 173:     48 / 65 loss=6.609, nll_loss=4.854, ppl=28.91, wps=3648.9, ups=0.31, wpb=11732, bsz=474.4, num_updates=11200, lr=0, gnorm=5.32, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.2, wall=44847
2023-05-20 05:04:59 | INFO | train_inner | epoch 173:     53 / 65 loss=6.775, nll_loss=5.048, ppl=33.08, wps=3453.4, ups=0.27, wpb=12723.2, bsz=635.6, num_updates=11205, lr=0, gnorm=3.25, clip=100, loss_scale=0.0625, train_wall=18, gb_free=20.6, wall=44865
2023-05-20 05:05:15 | INFO | train_inner | epoch 173:     58 / 65 loss=6.625, nll_loss=4.876, ppl=29.37, wps=4038.4, ups=0.32, wpb=12613.4, bsz=558.2, num_updates=11210, lr=0, gnorm=5.564, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=44881
2023-05-20 05:05:30 | INFO | train_inner | epoch 173:     63 / 65 loss=6.64, nll_loss=4.889, ppl=29.64, wps=3945.1, ups=0.33, wpb=12093.2, bsz=518.8, num_updates=11215, lr=0, gnorm=4.481, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=44896
2023-05-20 05:05:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:05:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:06:16 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 879 | wpb 2785 | bsz 105.2 | num_updates 11217 | best_loss 6.036
2023-05-20 05:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 11217 updates
2023-05-20 05:06:16 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint173.pt
2023-05-20 05:06:20 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint173.pt
2023-05-20 05:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint173.pt (epoch 173 @ 11217 updates, score 6.05) (writing took 6.7477962747216225 seconds)
2023-05-20 05:06:22 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2023-05-20 05:06:22 | INFO | train | epoch 173 | loss 6.711 | nll_loss 4.971 | ppl 31.37 | wps 3029.4 | ups 0.25 | wpb 12055.5 | bsz 559.8 | num_updates 11217 | lr 0 | gnorm 5.67 | clip 100 | loss_scale 0.0625 | train_wall 207 | gb_free 21.6 | wall 44948
2023-05-20 05:06:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:06:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:06:22 | INFO | fairseq.trainer | begin training epoch 174
2023-05-20 05:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:06:32 | INFO | train_inner | epoch 174:      3 / 65 loss=6.799, nll_loss=5.072, ppl=33.64, wps=891.9, ups=0.08, wpb=10959.8, bsz=593.4, num_updates=11220, lr=0, gnorm=11.849, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.5, wall=44957
2023-05-20 05:06:48 | INFO | train_inner | epoch 174:      8 / 65 loss=6.706, nll_loss=4.967, ppl=31.28, wps=3716.9, ups=0.31, wpb=12079.6, bsz=562.8, num_updates=11225, lr=0, gnorm=5.052, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=44974
2023-05-20 05:07:03 | INFO | train_inner | epoch 174:     13 / 65 loss=6.653, nll_loss=4.901, ppl=29.89, wps=4079.6, ups=0.34, wpb=12163.4, bsz=520.2, num_updates=11230, lr=0, gnorm=6.791, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.6, wall=44989
2023-05-20 05:07:19 | INFO | train_inner | epoch 174:     18 / 65 loss=6.635, nll_loss=4.883, ppl=29.5, wps=3697.8, ups=0.31, wpb=11966.4, bsz=523.2, num_updates=11235, lr=0, gnorm=3.486, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=45005
2023-05-20 05:07:36 | INFO | train_inner | epoch 174:     23 / 65 loss=6.721, nll_loss=4.987, ppl=31.71, wps=3585.1, ups=0.3, wpb=12098, bsz=538.6, num_updates=11240, lr=0, gnorm=6.655, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.6, wall=45022
2023-05-20 05:07:51 | INFO | train_inner | epoch 174:     28 / 65 loss=6.643, nll_loss=4.895, ppl=29.76, wps=3953, ups=0.32, wpb=12241, bsz=632.4, num_updates=11245, lr=0, gnorm=3.587, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=45037
2023-05-20 05:08:07 | INFO | train_inner | epoch 174:     33 / 65 loss=6.894, nll_loss=5.171, ppl=36.02, wps=4123.3, ups=0.33, wpb=12602.4, bsz=646.4, num_updates=11250, lr=0, gnorm=9.992, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=45052
2023-05-20 05:08:23 | INFO | train_inner | epoch 174:     38 / 65 loss=6.667, nll_loss=4.922, ppl=30.31, wps=3897.1, ups=0.31, wpb=12410, bsz=538.4, num_updates=11255, lr=0, gnorm=19.348, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=45068
2023-05-20 05:08:40 | INFO | train_inner | epoch 174:     43 / 65 loss=6.685, nll_loss=4.949, ppl=30.88, wps=3530.5, ups=0.29, wpb=12227.8, bsz=559.8, num_updates=11260, lr=0, gnorm=5.448, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=45086
2023-05-20 05:08:56 | INFO | train_inner | epoch 174:     48 / 65 loss=6.869, nll_loss=5.146, ppl=35.42, wps=3907.4, ups=0.31, wpb=12447, bsz=589.4, num_updates=11265, lr=0, gnorm=6.918, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=45102
2023-05-20 05:09:12 | INFO | train_inner | epoch 174:     53 / 65 loss=6.7, nll_loss=4.962, ppl=31.16, wps=3652.3, ups=0.3, wpb=12159.4, bsz=558, num_updates=11270, lr=0, gnorm=6.188, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.5, wall=45118
2023-05-20 05:09:28 | INFO | train_inner | epoch 174:     58 / 65 loss=6.644, nll_loss=4.899, ppl=29.83, wps=3791.5, ups=0.33, wpb=11616.6, bsz=539.8, num_updates=11275, lr=0, gnorm=2.463, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=45134
2023-05-20 05:09:43 | INFO | train_inner | epoch 174:     63 / 65 loss=6.671, nll_loss=4.926, ppl=30.4, wps=3783, ups=0.32, wpb=11801, bsz=510.2, num_updates=11280, lr=0, gnorm=2.763, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=45149
2023-05-20 05:09:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:09:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:10:32 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 827.7 | wpb 2785 | bsz 105.2 | num_updates 11282 | best_loss 6.036
2023-05-20 05:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 11282 updates
2023-05-20 05:10:32 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint174.pt
2023-05-20 05:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint174.pt
2023-05-20 05:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint174.pt (epoch 174 @ 11282 updates, score 6.05) (writing took 6.954332869499922 seconds)
2023-05-20 05:10:39 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2023-05-20 05:10:39 | INFO | train | epoch 174 | loss 6.713 | nll_loss 4.973 | ppl 31.41 | wps 3054.8 | ups 0.25 | wpb 12058.1 | bsz 560.1 | num_updates 11282 | lr 0 | gnorm 6.356 | clip 100 | loss_scale 0.0625 | train_wall 206 | gb_free 21.6 | wall 45205
2023-05-20 05:10:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:10:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:10:39 | INFO | fairseq.trainer | begin training epoch 175
2023-05-20 05:10:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:10:49 | INFO | train_inner | epoch 175:      3 / 65 loss=6.657, nll_loss=4.914, ppl=30.15, wps=785.5, ups=0.08, wpb=10239.2, bsz=426.6, num_updates=11285, lr=0, gnorm=3.142, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.2, wall=45214
2023-05-20 05:11:03 | INFO | train_inner | epoch 175:      8 / 65 loss=6.819, nll_loss=5.095, ppl=34.18, wps=4080.6, ups=0.34, wpb=12130.8, bsz=653.4, num_updates=11290, lr=0, gnorm=9.12, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=45229
2023-05-20 05:11:20 | INFO | train_inner | epoch 175:     13 / 65 loss=6.788, nll_loss=5.056, ppl=33.26, wps=3732, ups=0.31, wpb=12216.2, bsz=571, num_updates=11295, lr=0, gnorm=15.38, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=45246
2023-05-20 05:11:36 | INFO | train_inner | epoch 175:     18 / 65 loss=6.671, nll_loss=4.928, ppl=30.45, wps=3653.3, ups=0.31, wpb=11974.4, bsz=548, num_updates=11300, lr=0, gnorm=6.081, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=45262
2023-05-20 05:11:52 | INFO | train_inner | epoch 175:     23 / 65 loss=6.665, nll_loss=4.919, ppl=30.26, wps=3786.4, ups=0.31, wpb=12350.2, bsz=543.4, num_updates=11305, lr=0, gnorm=3.842, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=45278
2023-05-20 05:12:09 | INFO | train_inner | epoch 175:     28 / 65 loss=6.658, nll_loss=4.913, ppl=30.12, wps=3780, ups=0.3, wpb=12537, bsz=539.6, num_updates=11310, lr=0, gnorm=3.557, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.8, wall=45295
2023-05-20 05:12:25 | INFO | train_inner | epoch 175:     33 / 65 loss=6.693, nll_loss=4.956, ppl=31.03, wps=3952.3, ups=0.32, wpb=12268.8, bsz=584.8, num_updates=11315, lr=0, gnorm=7.586, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21, wall=45310
2023-05-20 05:12:40 | INFO | train_inner | epoch 175:     38 / 65 loss=6.906, nll_loss=5.18, ppl=36.26, wps=4009, ups=0.33, wpb=12272.8, bsz=635.6, num_updates=11320, lr=0, gnorm=5.03, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.7, wall=45326
2023-05-20 05:12:56 | INFO | train_inner | epoch 175:     43 / 65 loss=6.702, nll_loss=4.966, ppl=31.25, wps=4008.2, ups=0.32, wpb=12625.4, bsz=550.4, num_updates=11325, lr=0, gnorm=9.023, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=45341
2023-05-20 05:13:13 | INFO | train_inner | epoch 175:     48 / 65 loss=6.607, nll_loss=4.856, ppl=28.96, wps=3400.6, ups=0.29, wpb=11770.8, bsz=513.2, num_updates=11330, lr=0, gnorm=3.324, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.4, wall=45359
2023-05-20 05:13:31 | INFO | train_inner | epoch 175:     53 / 65 loss=6.675, nll_loss=4.933, ppl=30.55, wps=3420.3, ups=0.28, wpb=12051.4, bsz=539.8, num_updates=11335, lr=0, gnorm=5.89, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.8, wall=45376
2023-05-20 05:13:46 | INFO | train_inner | epoch 175:     58 / 65 loss=6.669, nll_loss=4.925, ppl=30.37, wps=3922.1, ups=0.32, wpb=12154, bsz=577.2, num_updates=11340, lr=0, gnorm=5.217, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=45392
2023-05-20 05:14:00 | INFO | train_inner | epoch 175:     63 / 65 loss=6.672, nll_loss=4.928, ppl=30.45, wps=4342.7, ups=0.35, wpb=12250.6, bsz=573.4, num_updates=11345, lr=0, gnorm=7.131, clip=100, loss_scale=0.0625, train_wall=14, gb_free=20.4, wall=45406
2023-05-20 05:14:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:14:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:14:45 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 889.6 | wpb 2785 | bsz 105.2 | num_updates 11347 | best_loss 6.036
2023-05-20 05:14:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 11347 updates
2023-05-20 05:14:45 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint175.pt
2023-05-20 05:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint175.pt
2023-05-20 05:14:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint175.pt (epoch 175 @ 11347 updates, score 6.05) (writing took 6.967900715768337 seconds)
2023-05-20 05:14:52 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2023-05-20 05:14:52 | INFO | train | epoch 175 | loss 6.709 | nll_loss 4.97 | ppl 31.35 | wps 3093.1 | ups 0.26 | wpb 12061.3 | bsz 559.9 | num_updates 11347 | lr 0 | gnorm 6.485 | clip 100 | loss_scale 0.0625 | train_wall 206 | gb_free 21.8 | wall 45458
2023-05-20 05:14:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:14:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:14:52 | INFO | fairseq.trainer | begin training epoch 176
2023-05-20 05:14:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:15:02 | INFO | train_inner | epoch 176:      3 / 65 loss=6.837, nll_loss=5.109, ppl=34.5, wps=834.1, ups=0.08, wpb=10243.2, bsz=502.6, num_updates=11350, lr=0, gnorm=9.245, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.8, wall=45467
2023-05-20 05:15:16 | INFO | train_inner | epoch 176:      8 / 65 loss=6.683, nll_loss=4.942, ppl=30.73, wps=4254.3, ups=0.34, wpb=12556, bsz=596.2, num_updates=11355, lr=0, gnorm=7.158, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=45482
2023-05-20 05:15:34 | INFO | train_inner | epoch 176:     13 / 65 loss=6.844, nll_loss=5.124, ppl=34.88, wps=3437.8, ups=0.28, wpb=12250.4, bsz=649.6, num_updates=11360, lr=0, gnorm=5.343, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.6, wall=45500
2023-05-20 05:15:50 | INFO | train_inner | epoch 176:     18 / 65 loss=6.637, nll_loss=4.888, ppl=29.61, wps=3905.6, ups=0.32, wpb=12053.2, bsz=536.8, num_updates=11365, lr=0, gnorm=10.689, clip=100, loss_scale=0.0625, train_wall=15, gb_free=20.7, wall=45515
2023-05-20 05:16:06 | INFO | train_inner | epoch 176:     23 / 65 loss=6.65, nll_loss=4.902, ppl=29.9, wps=3817.7, ups=0.31, wpb=12363.6, bsz=538, num_updates=11370, lr=0, gnorm=4.226, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.8, wall=45532
2023-05-20 05:16:21 | INFO | train_inner | epoch 176:     28 / 65 loss=6.615, nll_loss=4.868, ppl=29.2, wps=4205.7, ups=0.33, wpb=12627.8, bsz=585.4, num_updates=11375, lr=0, gnorm=4.058, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=45547
2023-05-20 05:16:35 | INFO | train_inner | epoch 176:     33 / 65 loss=6.9, nll_loss=5.17, ppl=36.01, wps=4204.5, ups=0.35, wpb=12073.4, bsz=625.4, num_updates=11380, lr=0, gnorm=4.756, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.7, wall=45561
2023-05-20 05:16:52 | INFO | train_inner | epoch 176:     38 / 65 loss=6.711, nll_loss=4.973, ppl=31.41, wps=3556.7, ups=0.3, wpb=11823.8, bsz=521.2, num_updates=11385, lr=0, gnorm=4.065, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.3, wall=45578
2023-05-20 05:17:08 | INFO | train_inner | epoch 176:     43 / 65 loss=6.701, nll_loss=4.962, ppl=31.17, wps=3881, ups=0.31, wpb=12498.2, bsz=586, num_updates=11390, lr=0, gnorm=2.328, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=45594
2023-05-20 05:17:26 | INFO | train_inner | epoch 176:     48 / 65 loss=6.711, nll_loss=4.974, ppl=31.44, wps=3372.8, ups=0.28, wpb=11931.8, bsz=550.2, num_updates=11395, lr=0, gnorm=9.269, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21, wall=45611
2023-05-20 05:17:41 | INFO | train_inner | epoch 176:     53 / 65 loss=6.644, nll_loss=4.896, ppl=29.77, wps=3985.4, ups=0.33, wpb=12171.4, bsz=526.8, num_updates=11400, lr=0, gnorm=5.052, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=45627
2023-05-20 05:17:57 | INFO | train_inner | epoch 176:     58 / 65 loss=6.673, nll_loss=4.929, ppl=30.47, wps=3741.3, ups=0.31, wpb=11977.8, bsz=504.6, num_updates=11405, lr=0, gnorm=5.369, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=45643
2023-05-20 05:18:12 | INFO | train_inner | epoch 176:     63 / 65 loss=6.705, nll_loss=4.965, ppl=31.24, wps=4044.1, ups=0.33, wpb=12349.6, bsz=558.8, num_updates=11410, lr=0, gnorm=7.748, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=45658
2023-05-20 05:18:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:18:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:18:57 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 903.7 | wpb 2785 | bsz 105.2 | num_updates 11412 | best_loss 6.036
2023-05-20 05:18:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 11412 updates
2023-05-20 05:18:57 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint176.pt
2023-05-20 05:19:02 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint176.pt
2023-05-20 05:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint176.pt (epoch 176 @ 11412 updates, score 6.05) (writing took 6.761484459042549 seconds)
2023-05-20 05:19:04 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2023-05-20 05:19:04 | INFO | train | epoch 176 | loss 6.713 | nll_loss 4.973 | ppl 31.41 | wps 3110.4 | ups 0.26 | wpb 12053 | bsz 558.5 | num_updates 11412 | lr 0 | gnorm 6.169 | clip 100 | loss_scale 0.0625 | train_wall 205 | gb_free 21.7 | wall 45710
2023-05-20 05:19:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:19:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:19:04 | INFO | fairseq.trainer | begin training epoch 177
2023-05-20 05:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:19:14 | INFO | train_inner | epoch 177:      3 / 65 loss=6.608, nll_loss=4.858, ppl=28.99, wps=825.8, ups=0.08, wpb=10148.2, bsz=447.2, num_updates=11415, lr=0, gnorm=8.33, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.3, wall=45719
2023-05-20 05:19:32 | INFO | train_inner | epoch 177:      8 / 65 loss=6.66, nll_loss=4.913, ppl=30.14, wps=3358.6, ups=0.28, wpb=12120.2, bsz=532, num_updates=11420, lr=0, gnorm=28.632, clip=100, loss_scale=0.0625, train_wall=18, gb_free=21.6, wall=45737
2023-05-20 05:19:47 | INFO | train_inner | epoch 177:     13 / 65 loss=6.72, nll_loss=4.979, ppl=31.53, wps=4051.5, ups=0.33, wpb=12430.4, bsz=610, num_updates=11425, lr=0, gnorm=10.844, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.3, wall=45753
2023-05-20 05:20:02 | INFO | train_inner | epoch 177:     18 / 65 loss=6.941, nll_loss=5.211, ppl=37.03, wps=4123.1, ups=0.33, wpb=12490, bsz=655.4, num_updates=11430, lr=0, gnorm=3.308, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=45768
2023-05-20 05:20:18 | INFO | train_inner | epoch 177:     23 / 65 loss=6.724, nll_loss=4.991, ppl=31.8, wps=3808.7, ups=0.31, wpb=12318.4, bsz=586, num_updates=11435, lr=0, gnorm=4.785, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=45784
2023-05-20 05:20:34 | INFO | train_inner | epoch 177:     28 / 65 loss=6.738, nll_loss=4.998, ppl=31.95, wps=3830.9, ups=0.32, wpb=11915, bsz=575.4, num_updates=11440, lr=0, gnorm=3.839, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=45800
2023-05-20 05:20:49 | INFO | train_inner | epoch 177:     33 / 65 loss=6.715, nll_loss=4.978, ppl=31.51, wps=4077, ups=0.33, wpb=12226.4, bsz=611, num_updates=11445, lr=0, gnorm=3.341, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.7, wall=45815
2023-05-20 05:21:04 | INFO | train_inner | epoch 177:     38 / 65 loss=6.658, nll_loss=4.914, ppl=30.14, wps=3915.1, ups=0.33, wpb=11930.8, bsz=537.4, num_updates=11450, lr=0, gnorm=3.442, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.5, wall=45830
2023-05-20 05:21:20 | INFO | train_inner | epoch 177:     43 / 65 loss=6.648, nll_loss=4.9, ppl=29.86, wps=3711.7, ups=0.31, wpb=11990.2, bsz=507, num_updates=11455, lr=0, gnorm=4.293, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.3, wall=45846
2023-05-20 05:21:37 | INFO | train_inner | epoch 177:     48 / 65 loss=6.695, nll_loss=4.958, ppl=31.08, wps=3625, ups=0.29, wpb=12495.8, bsz=561.6, num_updates=11460, lr=0, gnorm=5.971, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=45863
2023-05-20 05:21:55 | INFO | train_inner | epoch 177:     53 / 65 loss=6.671, nll_loss=4.933, ppl=30.54, wps=3539.5, ups=0.29, wpb=12339, bsz=519.4, num_updates=11465, lr=0, gnorm=26.674, clip=100, loss_scale=0.0625, train_wall=17, gb_free=21.7, wall=45881
2023-05-20 05:22:11 | INFO | train_inner | epoch 177:     58 / 65 loss=6.6, nll_loss=4.844, ppl=28.72, wps=3726.7, ups=0.31, wpb=11995.8, bsz=532.4, num_updates=11470, lr=0, gnorm=8.618, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=45897
2023-05-20 05:22:26 | INFO | train_inner | epoch 177:     63 / 65 loss=6.746, nll_loss=5.016, ppl=32.36, wps=4084.4, ups=0.34, wpb=12014.6, bsz=550.6, num_updates=11475, lr=0, gnorm=10.951, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=45911
2023-05-20 05:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:22:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:23:12 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 847 | wpb 2785 | bsz 105.2 | num_updates 11477 | best_loss 6.036
2023-05-20 05:23:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 11477 updates
2023-05-20 05:23:12 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint177.pt
2023-05-20 05:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint177.pt
2023-05-20 05:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint177.pt (epoch 177 @ 11477 updates, score 6.05) (writing took 6.874789413064718 seconds)
2023-05-20 05:23:19 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2023-05-20 05:23:19 | INFO | train | epoch 177 | loss 6.706 | nll_loss 4.966 | ppl 31.26 | wps 3069.4 | ups 0.25 | wpb 12049.7 | bsz 558.8 | num_updates 11477 | lr 0 | gnorm 9.492 | clip 100 | loss_scale 0.0625 | train_wall 206 | gb_free 21.5 | wall 45965
2023-05-20 05:23:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:23:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:23:19 | INFO | fairseq.trainer | begin training epoch 178
2023-05-20 05:23:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:23:28 | INFO | train_inner | epoch 178:      3 / 65 loss=6.927, nll_loss=5.204, ppl=36.85, wps=853.5, ups=0.08, wpb=10687.8, bsz=580.4, num_updates=11480, lr=0, gnorm=6.513, clip=100, loss_scale=0.0625, train_wall=13, gb_free=21.7, wall=45974
2023-05-20 05:23:44 | INFO | train_inner | epoch 178:      8 / 65 loss=6.678, nll_loss=4.929, ppl=30.47, wps=3956.8, ups=0.31, wpb=12594.4, bsz=616.6, num_updates=11485, lr=0, gnorm=4.276, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=45990
2023-05-20 05:24:01 | INFO | train_inner | epoch 178:     13 / 65 loss=6.64, nll_loss=4.894, ppl=29.73, wps=3609.7, ups=0.3, wpb=12228.6, bsz=533.4, num_updates=11490, lr=0, gnorm=5.165, clip=100, loss_scale=0.0625, train_wall=17, gb_free=20.9, wall=46007
2023-05-20 05:24:17 | INFO | train_inner | epoch 178:     18 / 65 loss=6.746, nll_loss=5.011, ppl=32.25, wps=3800.6, ups=0.32, wpb=11777.8, bsz=542.6, num_updates=11495, lr=0, gnorm=4.738, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.6, wall=46022
2023-05-20 05:24:31 | INFO | train_inner | epoch 178:     23 / 65 loss=6.768, nll_loss=5.036, ppl=32.8, wps=4521, ups=0.36, wpb=12553.6, bsz=617.2, num_updates=11500, lr=0, gnorm=4.285, clip=100, loss_scale=0.0625, train_wall=14, gb_free=21.7, wall=46036
2023-05-20 05:24:47 | INFO | train_inner | epoch 178:     28 / 65 loss=6.66, nll_loss=4.918, ppl=30.23, wps=3715, ups=0.31, wpb=12020.2, bsz=489.4, num_updates=11505, lr=0, gnorm=30.083, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=46052
2023-05-20 05:25:02 | INFO | train_inner | epoch 178:     33 / 65 loss=6.612, nll_loss=4.861, ppl=29.05, wps=4099.1, ups=0.33, wpb=12420.6, bsz=540, num_updates=11510, lr=0, gnorm=5.473, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.8, wall=46068
2023-05-20 05:25:18 | INFO | train_inner | epoch 178:     38 / 65 loss=6.75, nll_loss=5.015, ppl=32.33, wps=3810.5, ups=0.32, wpb=12069.4, bsz=599.8, num_updates=11515, lr=0, gnorm=8.442, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=46083
2023-05-20 05:25:34 | INFO | train_inner | epoch 178:     43 / 65 loss=6.703, nll_loss=4.965, ppl=31.22, wps=3747.9, ups=0.31, wpb=12117.6, bsz=548.6, num_updates=11520, lr=0, gnorm=7.084, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=46100
2023-05-20 05:25:49 | INFO | train_inner | epoch 178:     48 / 65 loss=6.674, nll_loss=4.93, ppl=30.49, wps=4171.1, ups=0.34, wpb=12380, bsz=582.8, num_updates=11525, lr=0, gnorm=7.059, clip=100, loss_scale=0.0625, train_wall=15, gb_free=21.1, wall=46114
2023-05-20 05:26:05 | INFO | train_inner | epoch 178:     53 / 65 loss=6.724, nll_loss=4.987, ppl=31.7, wps=3692.3, ups=0.31, wpb=11890.4, bsz=561, num_updates=11530, lr=0, gnorm=5.67, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=46131
2023-05-20 05:26:21 | INFO | train_inner | epoch 178:     58 / 65 loss=6.658, nll_loss=4.912, ppl=30.1, wps=3786.6, ups=0.31, wpb=12343.2, bsz=547.4, num_updates=11535, lr=0, gnorm=10.003, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=46147
2023-05-20 05:26:37 | INFO | train_inner | epoch 178:     63 / 65 loss=6.748, nll_loss=5.01, ppl=32.23, wps=3701.6, ups=0.32, wpb=11731.8, bsz=555, num_updates=11540, lr=0, gnorm=3.593, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=46163
2023-05-20 05:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-05-20 05:26:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:27:24 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 6.05 | nll_loss 4.152 | ppl 17.78 | bleu 5.45 | wps 872.9 | wpb 2785 | bsz 105.2 | num_updates 11542 | best_loss 6.036
2023-05-20 05:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 11542 updates
2023-05-20 05:27:24 | INFO | fairseq.trainer | Saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint178.pt
2023-05-20 05:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /data_new/private/hongyining/dualEnc_virtual/fairseq/training/stage1/checkpoint178.pt
2023-05-20 05:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint training/stage1/checkpoint178.pt (epoch 178 @ 11542 updates, score 6.05) (writing took 6.867112159729004 seconds)
2023-05-20 05:27:31 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2023-05-20 05:27:31 | INFO | train | epoch 178 | loss 6.71 | nll_loss 4.97 | ppl 31.34 | wps 3114.4 | ups 0.26 | wpb 12052.2 | bsz 559.2 | num_updates 11542 | lr 0 | gnorm 7.837 | clip 100 | loss_scale 0.0625 | train_wall 203 | gb_free 21.8 | wall 46217
2023-05-20 05:27:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True
2023-05-20 05:27:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 65
2023-05-20 05:27:31 | INFO | fairseq.trainer | begin training epoch 179
2023-05-20 05:27:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-05-20 05:27:41 | INFO | train_inner | epoch 179:      3 / 65 loss=6.685, nll_loss=4.947, ppl=30.84, wps=783.9, ups=0.08, wpb=9988.8, bsz=456.8, num_updates=11545, lr=0, gnorm=7.019, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.5, wall=46226
2023-05-20 05:27:56 | INFO | train_inner | epoch 179:      8 / 65 loss=6.79, nll_loss=5.055, ppl=33.25, wps=3904.5, ups=0.32, wpb=12165.6, bsz=563.2, num_updates=11550, lr=0, gnorm=4.303, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=46242
2023-05-20 05:28:13 | INFO | train_inner | epoch 179:     13 / 65 loss=6.677, nll_loss=4.935, ppl=30.59, wps=3739.7, ups=0.31, wpb=12193.8, bsz=564.4, num_updates=11555, lr=0, gnorm=3.498, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.6, wall=46258
2023-05-20 05:28:28 | INFO | train_inner | epoch 179:     18 / 65 loss=6.637, nll_loss=4.894, ppl=29.73, wps=3808.2, ups=0.32, wpb=11928.6, bsz=534.6, num_updates=11560, lr=0, gnorm=2.882, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.7, wall=46274
2023-05-20 05:28:44 | INFO | train_inner | epoch 179:     23 / 65 loss=6.656, nll_loss=4.913, ppl=30.13, wps=3760.7, ups=0.31, wpb=12031, bsz=547.4, num_updates=11565, lr=0, gnorm=8.002, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.2, wall=46290
slurmstepd: error: *** JOB 116888 ON 99server CANCELLED AT 2023-05-20T05:28:52 ***
2023-05-20 05:29:00 | INFO | train_inner | epoch 179:     28 / 65 loss=6.682, nll_loss=4.938, ppl=30.66, wps=3758.7, ups=0.31, wpb=12092.6, bsz=508.4, num_updates=11570, lr=0, gnorm=8.077, clip=100, loss_scale=0.0625, train_wall=16, gb_free=21.4, wall=46306
2023-05-20 05:29:20 | INFO | train_inner | epoch 179:     33 / 65 loss=6.706, nll_loss=4.97, ppl=31.35, wps=3095.2, ups=0.25, wpb=12489.8, bsz=607.2, num_updates=11575, lr=0, gnorm=4.023, clip=100, loss_scale=0.0625, train_wall=20, gb_free=21.5, wall=46326
